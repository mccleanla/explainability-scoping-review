*explanation as communiation / meaningful transparency*

**Venditti, R. _et al._ (2025) ‘Construal Level Theory (CLT) for designing explanation interfaces in operational contexts’,** in _AHFE International_. _13th International Conference on Human Interaction & Emerging Technologies: Artificial Intelligence & Future Applications_, AHFE International. Available at: https://doi.org/10.54941/ahfe1005918.

"==Operational Explainability (OpXAI) refers to the need for end users to receive clear, relevant, and reliable information on how an AI system reaches its conclusions.==  Operational explainability, distinct from technical explainability, is an ==emergent requirement== for Human-AI Teams where humans collaborate with Intelligent Assistants.  It ==emphasises the delivery information at the right level of detail and timing, ensuring that explanations are clear, actionable, and tailored to real-world operational needs==.  Key questions: 
- What explanations do the operators need? 
 - How (at what level of abstraction) should they be presented? 
 - When? For how long?

European Union Aviation Safety Agency (EASA) introduced new dimensions to XAI, such as “level of abstraction” and “time required to obtain an explanation”.

---

**Macrae, C. (2022) ‘Learning from the Failure of Autonomous and Intelligent Systems: Accidents, Safety, and Sociotechnical Sources of Risk’,** _Risk analysis: an official publication of the Society for Risk Analysis_, 42(9), pp. 1999–2025. Available at: https://doi.org/10.1111/risa.13850.

**System Transparency:** 
- mechanisms to render the sociotechnical system surrounding AIS transparent and legible, encompassing technical, human and organisational processes"

"Transparency is a core principle in AIS safety (Jobin et al., 2019; Winfield & Jirotka, 2018): it must be possible to ==understand what an AIS is doing and why==, both 
- ==to safely interact with and supervise these technologies (Sarter et al., 1997; Wortham, Theodorou, & Bryson, 2017) 
- and to retrospectively investigate failures (Bryson & Winfield, 2017; Winfield et al., 2021).

Prior ==discussions of transparency== have focused on the importance of making the working of intelligent technologies transparent, interpretable, ==and explainable== but the analysis developed here extends this principle, emphasizing the importance of building mechanisms that can help to render entire sociotechnical systems transparent— encompassing core technologies as well as the human activities and organizational processes that surround them. 
- For instance, organizational processes and technological methods need to be incorporated into AIS that can
	- ==identify pockets of invisible automation 
	- ==highlight the potential for failure cascades==, 
	- ==flag the possibility of hazard masking,== 
	- ==reveal areas of supervisory degradation== and 
	- acknowledge sources of existential pressure. 
- As such, ==transparency is not purely a technical requirement but a sociotechnical one==: to manage risk and learn from failure, the net- work of organizational decisions, cultural values, and human interactions that AIS are embedded within must also be made legible and open to scrutiny (Kroll, 2018).

invisible automation  - definition
- weaknesses or gaps in processes that ==maintain awareness==, provide insight and issue alerts regarding the status, activities and decisions of automated systems 
- *when combined with sensitivity smoothing features ("don't react too quickly, it might be a false alert") and (possibly inadvertent) hazard masking - leads to a failure cascade*
	- example from uber: no alerting process to inform a vehicle operator that a hazard was detected yet the system was initiating action suppression and delaying automated response, by design*

---

**Sujan, M., Pool, R. and Salmon, P. (2022) ‘Eight human factors and ergonomics principles for healthcare artificial intelligence’,** _BMJ health & care informatics_, 29(1), p. e100516. Available at: https://doi.org/10.1136/bmjhci-2021-100516.

*"Many approaches to explainable AI simply focus on providing detailed accounts of how an algorithm operates, but for explanations to be useful they ==need to be able to accommodate and be responsive to the needs of different users across a range of situations==,== for example, a patient might benefit from a different type of explanation compared with a healthcare professional. In this sense, rather than providing a description of a specific decision, ==explanation might be better regarded as a social process and a dialogue that allows the user to explore AI decision- making by interacting with the AI and by interrogating AI decisions==."*

---
**Bach, T. _et al._ (2023) ‘Unpacking human-AI interaction in safety-critical industries: A systematic literature review’,** _arXiv [cs.HC]_. Available at: http://arxiv.org/abs/2310.03392.

explainability and interpretability

In the literature, these terms have been used both interchangeably [121] and given explicitly different definitions [122]

explainability and interpretability are ==both part of ensuring that miscommunication is minimized== between the AIenabled system’s explanation of its rationale and what the users understand about its rationale [122].
- Explainability means that the ==rationale behind an AI-enabled system’s decision== is being explained to users in the ==most efficient and effective manner at the right time==. 
- Interpretability here means that ==users can understand what is being explained correctly, accurately, and in a timely manner. 

The difference between AI output and explainability and interpretability is that:
- the former focuses on how AI output is delivered and explained to users, 
- ==latter focuses on explaining the thinking process (the rationale) prior to producing an output. 

They are both focused on ==closing the gap between what is being explained to users and what they understand==. 
	- In other words, ==minimizing miscommunication and misinterpretation.


