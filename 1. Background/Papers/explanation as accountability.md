Macrae, C. (2022) ‘Learning from the Failure of Autonomous and Intelligent Systems: Accidents, Safety, and Sociotechnical Sources of Risk’, _Risk analysis: an official publication of the Society for Risk Analysis_, 42(9), pp. 1999–2025. Available at: https://doi.org/10.1111/risa.13850.

"Transparency is a core principle in AIS safety (Jobin et al., 2019; Winfield & Jirotka, 2018): ==it must be possible to understand what an AIS is doing and why, both to safely interact with and supervise these technologies (Sarter et al., 1997; Wortham, Theodorou, & Bryson, 2017) and to retrospectively investigate failures (Bryson & Winfield, 2017; Win- field et al., 2021).==

Prior discussions of transparency have focused on the importance of making the working of intelligent technologies transparent, interpretable, and explainable but the analysis developed here extends this principle, emphasizing the importance of building mechanisms that can help to render entire sociotechnical systems transparent— encompassing core technologies as well as the human activities and organizational processes that surround them. 
- For instance, organizational processes and technological methods need to be incorporated into AIS that can identify pockets of invisible automation, highlight the potential for failure cascades, flag the possibility of hazard masking, reveal areas of supervisory degradation and acknowledge sources of existential pressure. 
- As such, transparency is not purely a technical requirement but a sociotechnical one: to manage risk and learn from failure, the net- work of organizational decisions, cultural values, and human interactions that AIS are embedded within must also be made legible and open to scrutiny (Kroll, 2018).


Krügel, S., Ostermaier, A. and Uhl, M. (2022) ‘Zombies in the loop? Humans trust untrustworthy AI-advisors for ethical decisions’, _Philosophy & technology_, 35(1). Available at: https://doi.org/10.1007/s13347-022-00511-9.

"==explainability - lawmakers propose but are reluctant to specify"


---

Lawton, T. _et al._ (2024) ‘Clinicians risk becoming “liability sinks” for artificial intelligence’, _Future healthcare journal_, 11. Available at: https://doi.org/10.1016/j.fhj.2024.100007.

Ryan Conmy, P.M. _et al._ (2023) ‘What’s my role? Modelling responsibility for AI-based safety-critical systems’, p. 22. Available at: https://eprints.whiterose.ac.uk/206868/ (Accessed: 12 June 2024).

Porter, Z. _et al._ (2022) ‘Distinguishing two features of accountability for AI technologies’, _Nature machine intelligence_, 4(9), pp. 734–736. Available at: https://doi.org/10.1038/s42256-022-00533-0.

Habli, I. _et al._ (2018) ‘What is the safety case for health IT? A study of assurance practices in England’, _Safety science_, 110, pp. 324–335. Available at: https://doi.org/10.1016/j.ssci.2018.09.001.

Kempt, H., Heilinger, J.-C. and Nagel, S.K. (2022) ‘Relative explainability and double standards in medical decision-making: Should medical AI be subjected to higher standards in medical decision-making than doctors?’, _Ethics and information technology_, 24(2). Available at: https://doi.org/10.1007/s10676-022-09646-x.

Dekker, S. (2016) _Drift into Failure_. 1st Edition. CRC Press. Available at: https://doi.org/10.1201/9781315257396.

Stoop, J. (2018) ‘Drift into failure, an obsolete construct’, _AUP advances_, 1(1), pp. 99–117. Available at: https://doi.org/10.5117/adv2018.1.007.stoo.

Woods, D.D. (2018) ‘The theory of graceful extensibility: basic rules that govern adaptive systems’, _Environment Systems and Decisions_, 38(4), pp. 433–457. Available at: https://doi.org/10.1007/s10669-018-9708-3.

---
Clemmensen, T. (2021) _Human Work Interaction Design: A Platform for Theory and Action_. Cham: Springer International Publishing (Human–Computer Interaction Series). Available at: https://play.google.com/store/books/details?id=QlQ0zgEACAAJ (Accessed: 26 September 2023).


---
