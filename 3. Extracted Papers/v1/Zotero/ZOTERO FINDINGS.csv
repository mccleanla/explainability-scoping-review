doi,title,publisher,year,abstract,volume,issue,page,issn_array,authors,total_cites,supporting_cites,mentioning_cites,contrasting_cites,num_citing_publications,num_source_practice_guidelines
10.1007/978-3-030-60117-1_33,Human-Centered Explainable AI: Towards a Reflective Sociotechnical Approach,Springer International Publishing,2020,,,,449-466,"['0302-9743', '1611-3349']","[{'given': 'Upol', 'orcid': None, 'family': 'Ehsan', 'affiliation': None}, {'given': 'Mark O.', 'orcid': None, 'family': 'Riedl', 'affiliation': None}]",75,2,73,0,159,0
10.1007/978-3-031-76827-9_9,Human Centered Approaches and Taxonomies for Explainable Artificial Intelligence,Springer Nature Switzerland,2024,,,,144-163,"['0302-9743', '1611-3349']","[{'given': 'Helen', 'orcid': '0000-0001-6894-0996', 'family': 'Sheridan', 'affiliation': None}, {'given': 'Emma', 'orcid': '0000-0001-6738-3067', 'family': 'Murphy', 'affiliation': None}, {'given': 'Dympna', 'orcid': '0000-0003-2841-9738', 'family': 'O’Sullivan', 'affiliation': None}]",0,0,0,0,1,0
10.1007/s00146-020-01004-z,A taxonomy of human–machine collaboration: capturing automation and technical autonomy,Springer Science and Business Media LLC,2020,"Due to the ongoing advancements in technology, socio-technical collaboration has become increasingly prevalent. This poses challenges in terms of governance and accountability, as well as issues in various other fields. Therefore, it is crucial to familiarize decision-makers and researchers with the core of human-machine collaboration. This study introduces a taxonomy that enables identification of the very nature of human-machine interaction. A literature review has revealed that automation and technical autonomy are main parameters for describing and understanding such interaction. Both aspects must be carefully evaluated, as their increase has potentially far-reaching consequences. Hence, these two concepts comprise the taxonomy's axes. Five levels of automation and five levels of technical autonomy are introduced below, based on the assumption that both automation and autonomy are gradual. The levels of automation were developed from existing approaches; those of autonomy were carefully derived from a review of the literature. The taxonomy's use is also explained, as are its limitations and avenues for further research.",36,1,239-250,"['0951-5666', '1435-5655']","[{'given': 'Monika', 'orcid': '0000-0002-2644-8116', 'family': 'Simmler', 'affiliation': None}, {'given': 'Ruth', 'orcid': '0000-0002-8739-2611', 'family': 'Frischknecht', 'affiliation': None}]",23,0,23,0,50,0
10.1007/s00766-022-00393-5,Explainable software systems: from requirements analysis to system evaluation,Springer Science and Business Media LLC,2022,"AbstractThe growing complexity of software systems and the influence of software-supported decisions in our society sparked the need for software that is transparent, accountable, and trustworthy. Explainability has been identified as a means to achieve these qualities. It is recognized as an emerging non-functional requirement (NFR) that has a significant impact on system quality. Accordingly, software engineers need means to assist them in incorporating this NFR into systems. This requires an early analysis of the benefits and possible design issues that arise from interrelationships between different quality aspects. However, explainability is currently under-researched in the domain of requirements engineering, and there is a lack of artifacts that support the requirements engineering process and system design. In this work, we remedy this deficit by proposing four artifacts: a definition of explainability, a conceptual model, a knowledge catalogue, and a reference model for explainable systems. These artifacts should support software and requirements engineers in understanding the definition of explainability and how it interacts with other quality aspects. Besides that, they may be considered a starting point to provide practical value in the refinement of explainability from high-level requirements to concrete design choices, as well as on the identification of methods and metrics for the evaluation of the implemented requirements.",27,4,457-487,"['0947-3602', '1432-010X']","[{'given': 'Larissa', 'orcid': '0000-0001-6093-8875', 'family': 'Chazette', 'affiliation': None}, {'given': 'Wasja', 'orcid': None, 'family': 'Brunotte', 'affiliation': None}, {'given': 'Timo', 'orcid': None, 'family': 'Speith', 'affiliation': None}]",2,0,0,0,19,0
10.1007/s00766-024-00432-3,"How mature is requirements engineering for AI-based systems? A systematic mapping study on practices, challenges, and future research directions",Springer Science and Business Media LLC,2024,"AbstractArtificial intelligence (AI) permeates all fields of life, which resulted in new challenges in requirements engineering for artificial intelligence (RE4AI), e.g., the difficulty in specifying and validating requirements for AI or considering new quality requirements due to emerging ethical implications. It is currently unclear if existing RE methods are sufficient or if new ones are needed to address these challenges. Therefore, our goal is to provide a comprehensive overview of RE4AI to researchers and practitioners. What has been achieved so far, i.e., what practices are available, and what research gaps and challenges still need to be addressed? To achieve this, we conducted a systematic mapping study combining query string search and extensive snowballing. The extracted data was aggregated, and results were synthesized using thematic analysis. Our selection process led to the inclusion of 126 primary studies. Existing RE4AI research focuses mainly on requirements analysis and elicitation, with most practices applied in these areas. Furthermore, we identified requirements specification, explainability, and the gap between machine learning engineers and end-users as the most prevalent challenges, along with a few others. Additionally, we proposed seven potential research directions to address these challenges. Practitioners can use our results to identify and select suitable RE methods for working on their AI-based systems, while researchers can build on the identified gaps and research directions to push the field forward.",29,4,567-600,"['0947-3602', '1432-010X']","[{'given': 'Umm-e-', 'orcid': '0000-0001-8953-9624', 'family': 'Habiba', 'affiliation': None}, {'given': 'Markus', 'orcid': None, 'family': 'Haug', 'affiliation': None}, {'given': 'Justus', 'orcid': None, 'family': 'Bogner', 'affiliation': None}, {'given': 'Stefan', 'orcid': None, 'family': 'Wagner', 'affiliation': None}]",0,0,0,0,0,0
10.1007/s10111-024-00765-7,Understanding the influence of AI autonomy on AI explainability levels in human-AI teams using a mixed methods approach,Springer Science and Business Media LLC,2024,"AbstractAn obstacle to effective teaming between humans and AI is the agent’s ""black box"" design. AI explanations have proven benefits, but few studies have explored the effects that explanations can have in a teaming environment with AI agents operating at heightened levels of autonomy. We conducted two complementary studies, an experiment and participatory design sessions, investigating the effect that varying levels of AI explainability and AI autonomy have on the participants’ perceived trust and competence of an AI teammate to address this research gap. The results of the experiment were counter-intuitive, where the participants actually perceived the lower explainability agent as both more trustworthy and more competent. The participatory design sessions further revealed how a team’s need to know influences when and what teammates need explained from AI teammates. Based on these findings, several design recommendations were developed for the HCI community to guide how AI teammates should share decision information with their human counterparts considering the careful balance between trust and competence in human-AI teams.",26,3,435-455,"['1435-5558', '1435-5566']","[{'given': 'Allyson I.', 'orcid': None, 'family': 'Hauptman', 'affiliation': None}, {'given': 'Beau G.', 'orcid': None, 'family': 'Schelble', 'affiliation': None}, {'given': 'Wen', 'orcid': None, 'family': 'Duan', 'affiliation': None}, {'given': 'Christopher', 'orcid': None, 'family': 'Flathmann', 'affiliation': None}, {'given': 'Nathan J.', 'orcid': None, 'family': 'McNeese', 'affiliation': None}]",0,0,0,0,6,0
10.1007/s10458-019-09408-y,Explainability in human–agent systems,Springer Science and Business Media LLC,2019,,33,6,673-705,"['1387-2532', '1573-7454']","[{'given': 'Avi', 'orcid': None, 'family': 'Rosenfeld', 'affiliation': None}, {'given': 'Ariella', 'orcid': None, 'family': 'Richardson', 'affiliation': None}]",129,2,125,0,192,0
10.1007/s10664-024-10565-2,How do ML practitioners perceive explainability? an interview study of practices and challenges,Springer Science and Business Media LLC,2024,"AbstractExplainable artificial intelligence (XAI) is a field of study that focuses on the development process of AI-based systems while making their decision-making processes understandable and transparent for users. Research already identified explainability as an emerging requirement for AI-based systems that use machine learning (ML) techniques. However, there is a notable absence of studies investigating how ML practitioners perceive the concept of explainability, the challenges they encounter, and the potential trade-offs with other quality attributes. In this study, we want to discover how practitioners define explainability for AI-based systems and what challenges they encounter in making them explainable. Furthermore, we explore how explainability interacts with other quality attributes. To this end, we conducted semi-structured interviews with 14 ML practitioners from 11 companies. Our study reveals diverse viewpoints on explainability and applied practices. Results suggest that the importance of explainability lies in enhancing transparency, refining models, and mitigating bias. Methods like SHapley Additive exPlanations (SHAP) and Local Interpretable Model-Agnostic Explanation (LIME) are frequently used by ML practitioners to understand how models work, while tailored approaches are typically adopted to meet the specific requirements of stakeholders. Moreover, we have discerned emerging challenges in eight categories. Issues such as effective communication with non-technical stakeholders and the absence of standardized approaches are frequently stated as recurring hurdles. We contextualize these findings in terms of requirements engineering and conclude that industry currently lacks a standardized framework to address arising explainability needs.",30,1,,"['1382-3256', '1573-7616']","[{'given': 'Umm-e-', 'orcid': '0000-0001-8953-9624', 'family': 'Habiba', 'affiliation': None}, {'given': 'Mohammad Kasra', 'orcid': '0000-0002-1272-9873', 'family': 'Habib', 'affiliation': None}, {'given': 'Justus', 'orcid': '0000-0001-5788-0991', 'family': 'Bogner', 'affiliation': None}, {'given': 'Jonas', 'orcid': '0000-0002-6121-2731', 'family': 'Fritzsch', 'affiliation': None}, {'given': 'Stefan', 'orcid': '0000-0002-5256-8429', 'family': 'Wagner', 'affiliation': None}]",0,0,0,0,1,0
10.1007/s10676-010-9253-3,Explanation and trust: what to tell the user in security and AI?,Springer Science and Business Media LLC,2010,"
There is a common problem in artificial intelligence (AI) and information security. In AI, an expert system needs to be able to justify and explain a decision to the user. In information security, experts need to be able to explain to the public why a system is secure. In both cases, an important goal of explanation is to acquire or maintain the users' trust. In this paper, I investigate the relation between explanation and trust in the context of computing science. This analysis draws on literature study and concept analysis, using elements from system theory as well as actor-network theory. I apply the conceptual framework to both AI and information security, and show the benefit of the framework for both fields by means of examples. The main focus is on expert systems (AI) and electronic voting systems (security). Finally, I discuss consequences of the analysis for ethics in terms of (un)informed consent and dissent, and the associated division of responsibilities.
",13,1,53-64,"['1388-1957', '1572-8439']","[{'given': 'Wolter', 'orcid': None, 'family': 'Pieters', 'affiliation': None}]",93,4,89,0,141,0
10.1007/s10676-022-09632-3,Explanatory pragmatism: a context-sensitive framework for explainable medical AI,Springer Science and Business Media LLC,2022,"AbstractExplainable artificial intelligence (XAI) is an emerging, multidisciplinary field of research that seeks to develop methods and tools for making AI systems more explainable or interpretable. XAI researchers increasingly recognise explainability as a context-, audience- and purpose-sensitive phenomenon, rather than a single well-defined property that can be directly measured and optimised. However, since there is currently no overarching definition of explainability, this poses a risk of miscommunication between the many different researchers within this multidisciplinary space. This is the problem we seek to address in this paper. We outline a framework, called Explanatory Pragmatism, which we argue has two attractive features. First, it allows us to conceptualise explainability in explicitly context-, audience- and purpose-relative terms, while retaining a unified underlying definition of explainability. Second, it makes visible any normative disagreements that may underpin conflicting claims about explainability regarding the purposes for which explanations are sought. Third, it allows us to distinguish several dimensions of AI explainability. We illustrate this framework by applying it to a case study involving a machine learning model for predicting whether patients suffering disorders of consciousness were likely to recover consciousness.",24,1,,"['1388-1957', '1572-8439']","[{'given': 'Rune', 'orcid': '0000-0002-9880-6912', 'family': 'Nyrup', 'affiliation': None}, {'given': 'Diana', 'orcid': '0000-0001-7468-0123', 'family': 'Robinson', 'affiliation': None}]",18,0,18,0,36,0
10.1007/s11280-021-00916-0,Explainable recommendation: when design meets trust calibration,Springer Science and Business Media LLC,2021,"AbstractHuman-AI collaborative decision-making tools are being increasingly applied in critical domains such as healthcare. However, these tools are often seen as closed and intransparent for human decision-makers. An essential requirement for their success is the ability to provide explanations about themselves that are understandable and meaningful to the users. While explanations generally have positive connotations, studies showed that the assumption behind users interacting and engaging with these explanations could introduce trust calibration errors such as facilitating irrational or less thoughtful agreement or disagreement with the AI recommendation. In this paper, we explore how to help trust calibration through explanation interaction design. Our research method included two main phases. We first conducted a think-aloud study with 16 participants aiming to reveal main trust calibration errors concerning explainability in AI-Human collaborative decision-making tools. Then, we conducted two co-design sessions with eight participants to identify design principles and techniques for explanations that help trust calibration. As a conclusion of our research, we provide five design principles: Design for engagement, challenging habitual actions, attention guidance, friction and support training and learning. Our findings are meant to pave the way towards a more integrated framework for designing explanations with trust calibration as a primary goal.",24,5,1857-1884,"['1386-145X', '1573-1413']","[{'given': 'Mohammad', 'orcid': None, 'family': 'Naiseh', 'affiliation': None}, {'given': 'Dena', 'orcid': None, 'family': 'Al-Thani', 'affiliation': None}, {'given': 'Nan', 'orcid': None, 'family': 'Jiang', 'affiliation': None}, {'given': 'Raian', 'orcid': None, 'family': 'Ali', 'affiliation': None}]",16,0,16,0,33,0
10.1007/s12369-024-01148-8,A Taxonomy of Explanation Types and Need Indicators in Human–Agent Collaborations,Springer Science and Business Media LLC,2024,"AbstractIn recent years, explanations have become a pressing matter in AI research. This development was caused by the increased use of black-box models and a realization of the importance of trustworthy AI. In particular, explanations are necessary for human–agent interactions to ensure that the user can trust the agent and that collaborations are effective. Human–agent interactions are complex social scenarios involving a user, an autonomous agent, and an environment or task with its own distinct properties. Thus, such interactions require a wide variety of explanations, which are not covered by the methods of a single AI discipline, such as computer vision or natural language processing. In this paper, we map out what types of explanations are important for human–agent interactions, surveying the field via a scoping review. In addition to the typical introspective explanation tackled by explainability researchers, we look at assistive explanations, aiming to support the user with their task. Secondly, we survey what causes the need for an explanation in the first place. We identify a variety of human–agent interaction-specific causes and categorize them by whether they are centered on the agent’s behavior, the user’s mental state, or an external entity. Our overview aims to guide robotics practitioners in designing agents with more comprehensive explanation-related capacities, considering different explanation types and the concrete times when explanations should be given.",16,7,1681-1692,"['1875-4791', '1875-4805']","[{'given': 'Lennart', 'orcid': '0000-0001-6105-609X', 'family': 'Wachowiak', 'affiliation': None}, {'given': 'Andrew', 'orcid': None, 'family': 'Coles', 'affiliation': None}, {'given': 'Gerard', 'orcid': None, 'family': 'Canal', 'affiliation': None}, {'given': 'Oya', 'orcid': None, 'family': 'Celiktutan', 'affiliation': None}]",0,0,0,0,0,0
10.1007/s12525-022-00606-3,A nascent design theory for explainable intelligent systems,Springer Science and Business Media LLC,2022,"AbstractDue to computational advances in the past decades, so-called intelligent systems can learn from increasingly complex data, analyze situations, and support users in their decision-making to address them. However, in practice, the complexity of these intelligent systems renders the user hardly able to comprehend the inherent decision logic of the underlying machine learning model. As a result, the adoption of this technology, especially for high-stake scenarios, is hampered. In this context, explainable artificial intelligence offers numerous starting points for making the inherent logic explainable to people. While research manifests the necessity for incorporating explainable artificial intelligence into intelligent systems, there is still a lack of knowledge about how to socio-technically design these systems to address acceptance barriers among different user groups. In response, we have derived and evaluated a nascent design theory for explainable intelligent systems based on a structured literature review, two qualitative expert studies, a real-world use case application, and quantitative research. Our design theory includes design requirements, design principles, and design features covering the topics of global explainability, local explainability, personalized interface design, as well as psychological/emotional factors.",32,4,2185-2205,"['1019-6781', '1422-8890']","[{'given': 'Lukas-Valentin', 'orcid': '0000-0002-0101-5429', 'family': 'Herm', 'affiliation': None}, {'given': 'Theresa', 'orcid': None, 'family': 'Steinbach', 'affiliation': None}, {'given': 'Jonas', 'orcid': '0000-0002-0118-7757', 'family': 'Wanner', 'affiliation': None}, {'given': 'Christian', 'orcid': '0000-0002-8050-123X', 'family': 'Janiesch', 'affiliation': None}]",4,0,4,0,15,0
10.1007/s12652-023-04594-w,The role of explainable Artificial Intelligence in high-stakes decision-making systems: a systematic review,Springer Science and Business Media LLC,2023,"A high-stakes event is an extreme risk with a low probability of occurring, but severe consequences (e.g., life-threatening conditions or economic collapse). The accompanying lack of information is a source of high-stress pressure and anxiety for emergency medical services authorities. Deciding on the best proactive plan and action in this environment is a complicated process, which calls for intelligent agents to automatically produce knowledge in the manner of human-like intelligence. Research in high-stakes decision-making systems has increasingly focused on eXplainable Artificial Intelligence (XAI), but recent developments in prediction systems give little prominence to explanations based on human-like intelligence. This work investigates XAI based on cause-and-effect interpretations for supporting high-stakes decisions. We review recent applications in the first aid and medical emergency fields based on three perspectives: available data, desirable knowledge, and the use of intelligence. We identify the limitations of recent AI, and discuss the potential of XAI for dealing with such limitations. We propose an architecture for high-stakes decision-making driven by XAI, and highlight likely future trends and directions.",14,6,7827-7843,"['1868-5137', '1868-5145']","[{'given': 'Bukhoree', 'orcid': '0000-0002-5953-9874', 'family': 'Sahoh', 'affiliation': None}, {'given': 'Anant', 'orcid': None, 'family': 'Choksuriwong', 'affiliation': None}]",4,0,4,0,33,0
10.1007/s13218-018-0559-3,Defining Explainable AI for Requirements Analysis,Springer Science and Business Media LLC,2018,,32,4,261-266,"['0933-1875', '1610-1987']","[{'given': 'Raymond', 'orcid': None, 'family': 'Sheh', 'affiliation': None}, {'given': 'Isaac', 'orcid': None, 'family': 'Monteath', 'affiliation': None}]",21,0,21,0,36,0
10.1007/s13218-023-00806-9,Increasing the Value of XAI for Users: A Psychological Perspective,Springer Science and Business Media LLC,2023,"AbstractThis paper summarizes the psychological insights and related design challenges that have emerged in the field of Explainable AI (XAI). This summary is organized as a set of principles, some of which have recently been instantiated in XAI research. The primary aspects of implementation to which the principles refer are the design and evaluation stages of XAI system development, that is, principles concerning the design of explanations and the design of experiments for evaluating the performance of XAI systems. The principles can serve as guidance, to ensure that AI systems are human-centered and effectively assist people in solving difficult problems.",37,2-4,237-247,"['0933-1875', '1610-1987']","[{'given': 'Robert R.', 'orcid': '0000-0002-1387-7659', 'family': 'Hoffman', 'affiliation': None}, {'given': 'Timothy', 'orcid': None, 'family': 'Miller', 'affiliation': None}, {'given': 'Gary', 'orcid': None, 'family': 'Klein', 'affiliation': None}, {'given': 'Shane T.', 'orcid': None, 'family': 'Mueller', 'affiliation': None}, {'given': 'William J.', 'orcid': None, 'family': 'Clancey', 'affiliation': None}]",14,0,14,0,9,0
10.1007/s43681-022-00217-w,Explainable AI lacks regulative reasons: why AI and human decision-making are not equally opaque,Springer Science and Business Media LLC,2022,"AbstractMany artificial intelligence (AI) systems currently used for decision-making are opaque, i.e., the internal factors that determine their decisions are not fully known to people due to the systems’ computational complexity. In response to this problem, several researchers have argued that human decision-making is equally opaque and since simplifying, reason-giving explanations (rather than exhaustive causal accounts) of a decision are typically viewed as sufficient in the human case, the same should hold for algorithmic decision-making. Here, I contend that this argument overlooks that human decision-making is sometimes significantly more transparent and trustworthy than algorithmic decision-making. This is because when people explain their decisions by giving reasons for them, this frequently prompts those giving the reasons to govern or regulate themselves so as to think and act in ways that confirm their reason reports. AI explanation systems lack this self-regulative feature. Overlooking it when comparing algorithmic and human decision-making can result in underestimations of the transparency of human decision-making and in the development of explainable AI that may mislead people by activating generally warranted beliefs about the regulative dimension of reason-giving.",3,3,963-974,"['2730-5953', '2730-5961']","[{'given': 'Uwe', 'orcid': None, 'family': 'Peters', 'affiliation': None}]",11,0,11,0,28,0
10.1007/s43681-024-00622-3,Explanation needs and ethical demands: unpacking the instrumental value of XAI,Springer Science and Business Media LLC,2024,"AbstractThe call for XAI rests on a normative claim: ‘Good AI is explainable AI’ or even the stronger claim: ‘Only explainable AI is good AI.’ However, this valorization runs the risk of being overgeneralized because explanations are not per se useful, appropriate, or demanded. Explainability should not be seen as a value in itself but as a means to certain ends. In this paper, we put the valorization of explainability into question, which is discursively connected to the idea of ‘users’ needs’ and the will to design and develop ethically aligned AI systems. By making the instrumental character of the value of explainability explicit, we address two key issues that necessitate more theoretical attention: (i) to analyze the link between explainability and its presumed purpose; and (ii) to clarify the conceptions of these presumed purposes, namely users’ needs and ethical principles XAI is meant to promote. From a philosophical and from a psychological perspective, we constructively criticize the undertheorized and undercomplex way of talking about ‘users’ needs’ and ethical demands. We plea to carefully differentiate the value of explainable AI in social contexts and signal further need for research.",5,3,3015-3033,"['2730-5953', '2730-5961']","[{'given': 'Suzana', 'orcid': '0000-0003-0682-576X', 'family': 'Alpsancar', 'affiliation': None}, {'given': 'Heike M.', 'orcid': None, 'family': 'Buhl', 'affiliation': None}, {'given': 'Tobias', 'orcid': None, 'family': 'Matzner', 'affiliation': None}, {'given': 'Ingrid', 'orcid': None, 'family': 'Scharlau', 'affiliation': None}]",0,0,0,0,0,0
10.1007/s43681-024-00648-7,Insights into suggested Responsible AI (RAI) practices in real-world settings: a systematic literature review,Springer Science and Business Media LLC,2025,,5,3,3185-3232,"['2730-5953', '2730-5961']","[{'given': 'Tita Alissa', 'orcid': '0000-0001-8536-6826', 'family': 'Bach', 'affiliation': None}, {'given': 'Magnhild', 'orcid': None, 'family': 'Kaarstad', 'affiliation': None}, {'given': 'Elizabeth', 'orcid': None, 'family': 'Solberg', 'affiliation': None}, {'given': 'Aleksandar', 'orcid': None, 'family': 'Babic', 'affiliation': None}]",0,0,0,0,0,0
10.1016/j.artint.2018.07.007,Explanation in artificial intelligence: Insights from the social sciences,Elsevier BV,2019,"
There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a 'good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.
",267,,1-38,['0004-3702'],"[{'given': 'Tim', 'orcid': None, 'family': 'Miller', 'affiliation': None}]",2900,38,2822,3,4025,0
10.1016/j.artint.2021.103473,What do we want from Explainable Artificial Intelligence (XAI)? – A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research,Elsevier BV,2021,"Previous research in Explainable Artificial Intelligence (XAI) suggests that a main aim of explainability approaches is to satisfy specific interests, goals, expectations, needs, and demands regarding artificial systems (we call these 'stakeholders' desiderata') in a variety of contexts. However, the literature on XAI is vast, spreads out across multiple largely disconnected disciplines, and it often remains unclear how explainability approaches are supposed to achieve the goal of satisfying stakeholders' desiderata. This paper discusses the main classes of stakeholders calling for explainability of artificial systems and reviews their desiderata.",296,,103473,['0004-3702'],"[{'given': 'Markus', 'orcid': '0000-0002-8165-1803', 'family': 'Langer', 'affiliation': None}, {'given': 'Daniel', 'orcid': '0000-0003-2761-6262', 'family': 'Oster', 'affiliation': None}, {'given': 'Timo', 'orcid': '0000-0002-6675-154X', 'family': 'Speith', 'affiliation': None}, {'given': 'Holger', 'orcid': None, 'family': 'Hermanns', 'affiliation': None}, {'given': 'Lena', 'orcid': '0000-0002-8747-6911', 'family': 'Kästner', 'affiliation': None}, {'given': 'Eva', 'orcid': '0000-0002-7305-7126', 'family': 'Schmidt', 'affiliation': None}, {'given': 'Andreas', 'orcid': '0000-0003-0894-5407', 'family': 'Sesing', 'affiliation': None}, {'given': 'Kevin', 'orcid': None, 'family': 'Baum', 'affiliation': None}]",270,4,266,0,439,0
10.1016/j.caeai.2021.100041,Conceptualizing AI literacy: An exploratory review,Elsevier BV,2021,,2,,100041,['2666-920X'],"[{'given': 'Davy Tsz Kit', 'orcid': '0000-0002-2380-7814', 'family': 'Ng', 'affiliation': None}, {'given': 'Jac Ka Lok', 'orcid': '0000-0001-6490-7005', 'family': 'Leung', 'affiliation': None}, {'given': 'Samuel Kai Wah', 'orcid': '0000-0003-1557-2776', 'family': 'Chu', 'affiliation': None}, {'given': 'Maggie Shen', 'orcid': None, 'family': 'Qiao', 'affiliation': None}]",314,5,302,1,653,0
10.1016/j.chb.2022.107574,"Supporting Human-AI Teams:Transparency, explainability, and situation awareness",Elsevier BV,2023,,140,,107574,['0747-5632'],"[{'given': 'Mica R.', 'orcid': '0000-0002-2359-947X', 'family': 'Endsley', 'affiliation': None}]",50,2,48,0,126,0
10.1016/j.ijinfomgt.2022.102538,Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability,Elsevier BV,2023,,69,,102538,['0268-4012'],"[{'given': 'Lukas-Valentin', 'orcid': None, 'family': 'Herm', 'affiliation': None}, {'given': 'Kai', 'orcid': None, 'family': 'Heinrich', 'affiliation': None}, {'given': 'Jonas', 'orcid': None, 'family': 'Wanner', 'affiliation': None}, {'given': 'Christian', 'orcid': None, 'family': 'Janiesch', 'affiliation': None}]",41,0,38,0,91,0
10.1016/j.inffus.2019.12.012,"Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",Elsevier BV,2020,"
In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.
",58,,82-115,['1566-2535'],"[{'given': 'Alejandro', 'orcid': None, 'family': 'Barredo Arrieta', 'affiliation': None}, {'given': 'Natalia', 'orcid': None, 'family': 'Díaz-Rodríguez', 'affiliation': None}, {'given': 'Javier', 'orcid': None, 'family': 'Del Ser', 'affiliation': None}, {'given': 'Adrien', 'orcid': None, 'family': 'Bennetot', 'affiliation': None}, {'given': 'Siham', 'orcid': None, 'family': 'Tabik', 'affiliation': None}, {'given': 'Alberto', 'orcid': None, 'family': 'Barbado', 'affiliation': None}, {'given': 'Salvador', 'orcid': None, 'family': 'Garcia', 'affiliation': None}, {'given': 'Sergio', 'orcid': None, 'family': 'Gil-Lopez', 'affiliation': None}, {'given': 'Daniel', 'orcid': None, 'family': 'Molina', 'affiliation': None}, {'given': 'Richard', 'orcid': None, 'family': 'Benjamins', 'affiliation': None}, {'given': 'Raja', 'orcid': None, 'family': 'Chatila', 'affiliation': None}, {'given': 'Francisco', 'orcid': None, 'family': 'Herrera', 'affiliation': None}]",3065,14,3004,0,6529,0
10.1016/j.knosys.2021.107587,Explainability in supply chain operational risk management: A systematic literature review,Elsevier BV,2022,,235,,107587,['0950-7051'],"[{'given': 'Sonia Farhana', 'orcid': '0000-0003-1788-1329', 'family': 'Nimmy', 'affiliation': None}, {'given': 'Omar K.', 'orcid': '0000-0002-5738-6560', 'family': 'Hussain', 'affiliation': None}, {'given': 'Ripon K.', 'orcid': '0000-0002-7373-0149', 'family': 'Chakrabortty', 'affiliation': None}, {'given': 'Farookh Khadeer', 'orcid': None, 'family': 'Hussain', 'affiliation': None}, {'given': 'Morteza', 'orcid': None, 'family': 'Saberi', 'affiliation': None}]",20,0,20,0,64,0
10.1016/j.patter.2024.100971,Explainability pitfalls: Beyond dark patterns in explainable AI,Elsevier BV,2024,,5,6,100971,['2666-3899'],"[{'given': 'Upol', 'orcid': '0000-0002-4911-0409', 'family': 'Ehsan', 'affiliation': None}, {'given': 'Mark O.', 'orcid': None, 'family': 'Riedl', 'affiliation': None}]",0,0,0,0,11,0
10.1080/08839514.2023.2282834,Engineering Responsible And Explainable Models In Human-Agent Collectives,Informa UK Limited,2023,,38,1,,"['0883-9514', '1087-6545']","[{'given': 'Dhaminda B.', 'orcid': '0000-0002-4423-0284', 'family': 'Abeywickrama', 'affiliation': 'Department of Computer Science, University of Bristol, Bristol, UK'}, {'given': 'Sarvapali D.', 'orcid': '0000-0001-9686-4302', 'family': 'Ramchurn', 'affiliation': 'Electronics &amp; Computer Science, University of Southampton, Southampton, UK'}]",0,0,0,0,2,0
10.1080/08839514.2024.2318670,The SAGE Framework for Explaining Context in Explainable Artificial Intelligence,Informa UK Limited,2024,,38,1,,"['0883-9514', '1087-6545']","[{'given': 'Eleanor', 'orcid': '0000-0001-9039-8654', 'family': 'Mill', 'affiliation': 'Surrey Business School, University of Surrey, Guildford, UK'}, {'given': 'Wolfgang', 'orcid': None, 'family': 'Garn', 'affiliation': 'Surrey Business School, University of Surrey, Guildford, UK'}, {'given': 'Nick', 'orcid': None, 'family': 'Ryman-Tubb', 'affiliation': 'Surrey Business School, University of Surrey, Guildford, UK'}, {'given': 'Chris', 'orcid': None, 'family': 'Turner', 'affiliation': 'Surrey Business School, University of Surrey, Guildford, UK'}]",0,0,0,0,3,0
10.1080/10447318.2022.2041900,Transitioning to Human Interaction with AI Systems: New Challenges and Opportunities for HCI Professionals to Enable Human-Centered AI,Informa UK Limited,2022,,39,3,494-518,"['1044-7318', '1532-7590']","[{'given': 'Wei', 'orcid': None, 'family': 'Xu', 'affiliation': 'Center for Psychological Sciences, Zhejiang University, Hangzhou, China'}, {'given': 'Marvin J.', 'orcid': None, 'family': 'Dainoff', 'affiliation': 'Department of Psychology, Miami University, Oxford, OH, USA'}, {'given': 'Liezhong', 'orcid': None, 'family': 'Ge', 'affiliation': 'Center for Psychological Sciences, Zhejiang University, Hangzhou, China'}, {'given': 'Zaifeng', 'orcid': None, 'family': 'Gao', 'affiliation': 'Department of Psychology and Behavioral Sciences, Zhejiang University, Hangzhou, China'}]",85,2,77,0,184,0
10.1080/10447318.2022.2093863,A Situation Awareness Perspective on Human-AI Interaction: Tensions and Opportunities,Informa UK Limited,2022,,39,9,1789-1806,"['1044-7318', '1532-7590']","[{'given': 'Jinglu', 'orcid': '0000-0001-6464-9683', 'family': 'Jiang', 'affiliation': 'School of Management, Binghamton University, Binghamton, NY, USA'}, {'given': 'Alexander J.', 'orcid': None, 'family': 'Karran', 'affiliation': 'HEC Montréal, Montreal, Canada'}, {'given': 'Constantinos K.', 'orcid': '0000-0001-9301-3289', 'family': 'Coursaris', 'affiliation': 'HEC Montréal, Montreal, Canada'}, {'given': 'Pierre-Majorique', 'orcid': None, 'family': 'Léger', 'affiliation': 'HEC Montréal, Montreal, Canada'}, {'given': 'Joerg', 'orcid': None, 'family': 'Beringer', 'affiliation': 'Blue Yonder, Scottsdale, AZ, USA'}]",27,0,27,0,42,0
10.1080/10447318.2022.2101698,Explainable Artificial Intelligence: Evaluating the Objective and Subjective Impacts of xAI on Human-Agent Interaction,Informa UK Limited,2022,"Echoing the results of our primary investigation with the full survey, here we present results according to our reduced xAI survey. An ANCOVA showed that certain conditions in our experiment were rated as significantly more explainable than others (F (7, 277) = 3.14, p = 0.003). Our independent variable is the explainability method and our dependent variable is the explainability score. We include as a covariate the participant's baseline explainability score. A Shapiro-Wilk test revealed that our data were not normally distributed, but we proceed with an ANCOVA due to a lack of non-parametric alternative and the robustness of the F-test (Cochran, 1947;Glass, Peckham, & Sanders, 1972;Hack, 1958;Pearson, 1931). A Tukey's HSD post-hoc analysis reveals that Counterfactual was rated as more explainable than Probability Scores (p = 0.002), as shown in Figure A1 The reduced questionnaire, after a factor analysis and verification is given in Table A1.",39,7,1390-1404,"['1044-7318', '1532-7590']","[{'given': 'Andrew', 'orcid': '0000-0002-0317-5135', 'family': 'Silva', 'affiliation': 'College of Computing, Georgia Institute of Technology, Atlanta, GA, USA'}, {'given': 'Mariah', 'orcid': None, 'family': 'Schrum', 'affiliation': 'College of Computing, Georgia Institute of Technology, Atlanta, GA, USA'}, {'given': 'Erin', 'orcid': None, 'family': 'Hedlund-Botti', 'affiliation': 'College of Computing, Georgia Institute of Technology, Atlanta, GA, USA'}, {'given': 'Nakul', 'orcid': None, 'family': 'Gopalan', 'affiliation': 'College of Computing, Georgia Institute of Technology, Atlanta, GA, USA'}, {'given': 'Matthew', 'orcid': '0000-0002-5321-6038', 'family': 'Gombolay', 'affiliation': 'College of Computing, Georgia Institute of Technology, Atlanta, GA, USA'}]",34,0,33,1,60,0
10.1080/10447318.2022.2126812,Explainable AI: The Effect of Contradictory Decisions and Explanations on Users’ Acceptance of AI Systems,Informa UK Limited,2022,,39,9,1807-1826,"['1044-7318', '1532-7590']","[{'given': 'Carolin', 'orcid': None, 'family': 'Ebermann', 'affiliation': 'Business Psychology, PFH Private University of Applied Sciences, Göttingen, Germany'}, {'given': 'Matthias', 'orcid': None, 'family': 'Selisky', 'affiliation': 'Business Psychology, PFH Private University of Applied Sciences, Göttingen, Germany'}, {'given': 'Stephan', 'orcid': None, 'family': 'Weibelzahl', 'affiliation': 'Business Psychology, PFH Private University of Applied Sciences, Göttingen, Germany'}]",3,0,3,0,21,0
10.1080/10580530.2020.1849465,"Explainable Artificial Intelligence: Objectives, Stakeholders, and Future Research Opportunities",Informa UK Limited,2020,"Artificial Intelligence (AI) has diffused into many areas of our private and professional life. In this research note, we describe exemplary risks of black-box AI, the consequent need for explainability, and previous research on Explainable AI (XAI) in information systems research. Moreover, we discuss the origin of the term XAI, generalized XAI objectives, and stakeholder groups, as well as quality criteria of personalized explanations. We conclude with an outlook to future research on XAI.",39,1,53-63,"['1058-0530', '1934-8703']","[{'given': 'Christian', 'orcid': '0000-0001-5637-9433', 'family': 'Meske', 'affiliation': 'Department of Information Systems, Freie Universität Berlin and Einstein Center Digital Future, Berlin, Germany'}, {'given': 'Enrico', 'orcid': None, 'family': 'Bunde', 'affiliation': 'Department of Information Systems, Freie Universität Berlin and Einstein Center Digital Future, Berlin, Germany'}, {'given': 'Johannes', 'orcid': None, 'family': 'Schneider', 'affiliation': 'Institute of Information Systems, University of Liechtenstein, Vaduz, Liechtenstein'}, {'given': 'Martin', 'orcid': None, 'family': 'Gersch', 'affiliation': 'Department of Information Systems, Freie Universität Berlin, Einstein Center Digital Future as Well as Digital Entrepreneurship Hub, Berlin, Germany'}]",179,2,177,0,291,0
10.1098/rsta.2020.0363,Artificial intelligence explainability: the technical and ethical dimensions,The Royal Society,2021,"In recent years, several new technical methods have been developed to make AI-models more transparent and interpretable. These techniques are often referred to collectively as ‘AI explainability’ or ‘XAI’ methods. This paper presents an overview of XAI methods, and links them to stakeholder purposes for seeking an explanation. Because the underlying stakeholder purposes are broadly ethical in nature, we see this analysis as a contribution towards bringing together the technical and ethical dimensions of XAI. We emphasize that use of XAI methods must be linked to explanations of human decisions made during the development life cycle. Situated within that wider accountability framework, our analysis may offer a helpful starting point for designers, safety engineers, service providers and regulators who need to make practical judgements about which XAI methods to employ or to require.
          This article is part of the theme issue ‘Towards symbiotic autonomous systems’.",379,2207,20200363,"['1364-503X', '1471-2962']","[{'given': 'John A.', 'orcid': '0000-0003-4745-4272', 'family': 'McDermid', 'affiliation': 'Department of Computer Science, University of York, Deramore Lane, York YO10 5GH, UK'}, {'given': 'Yan', 'orcid': None, 'family': 'Jia', 'affiliation': 'Department of Computer Science, University of York, Deramore Lane, York YO10 5GH, UK'}, {'given': 'Zoe', 'orcid': None, 'family': 'Porter', 'affiliation': 'Department of Computer Science, University of York, Deramore Lane, York YO10 5GH, UK'}, {'given': 'Ibrahim', 'orcid': None, 'family': 'Habli', 'affiliation': 'Department of Computer Science, University of York, Deramore Lane, York YO10 5GH, UK'}]",44,0,44,0,87,0
10.1109/access.2018.2870052,Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI),Institute of Electrical and Electronics Engineers (IEEE),2018,,6,,52138-52160,['2169-3536'],"[{'given': 'Amina', 'orcid': '0000-0002-9697-666X', 'family': 'Adadi', 'affiliation': None}, {'given': 'Mohammed', 'orcid': None, 'family': 'Berrada', 'affiliation': None}]",2944,15,2883,1,4618,0
10.1109/mc.2021.3076131,Explainable Recommendations and Calibrated Trust: Two Systematic User Errors,Institute of Electrical and Electronics Engineers (IEEE),2021,"The increased adoption of collaborative Human-AI decision-making tools triggered a need to explain the recommendations for safe and effective collaboration. However, evidence from the recent literature showed that current implementation of AI explanations is failing to achieve adequate trust calibration. Such failure has lead decision-makers to either end-up with over-trust, e.g., people follow incorrect recommendations or under-trust, they reject a correct recommendation. In this paper, we explore how users interact with explanations and why trust calibration errors occur. We take clinical decision-support systems as a case study. Our empirical investigation is based on think-aloud protocol and observations, supported by scenarios and decision-making exercise utilizing a set of explainable recommendations interfaces. Our study involved 16 participants from medical domain who use clinical decision support systems frequently. Our findings showed that participants had two systematic errors while interacting with the explanations either by skipping them or misapplying them in their task.",54,10,28-37,"['0018-9162', '1558-0814']","[{'given': 'Mohammad', 'orcid': '0000-0002-4927-5086', 'family': 'Naiseh', 'affiliation': 'Bournemouth University, Poole, BH12 5BB Dorset, United Kingdom of Great Britain and Northern Ireland'}, {'given': 'Deniz', 'orcid': None, 'family': 'Cemiloglu', 'affiliation': None}, {'given': 'Dena', 'orcid': '0000-0002-1474-2692', 'family': 'Al Thani', 'affiliation': 'College of Science and Engineering, Hamad Bin Khalifa University, Doha, Doha, Qatar'}, {'given': 'Nan', 'orcid': None, 'family': 'Jiang', 'affiliation': 'Bournemouth University, Poole, Dorset, United Kingdom of Great Britain and Northern Ireland'}, {'given': 'Raian', 'orcid': None, 'family': 'Ali', 'affiliation': 'Hamad Bin Khalifa University, Doha, Ad Dawhah, Qatar'}]",12,0,12,0,24,0
10.1109/mc.2023.3317329,The Safety of Autonomy: A Systematic Approach,Institute of Electrical and Electronics Engineers (IEEE),2024,"Autonomy does not subvert existing safety processes, but they must be supplemented with methods that address autonomy's challenges, especially where perception and decision-making tasks are implemented with machine learning. We present an approach to address the safety of autonomous systems, building on and complementing established safety engineering methods.
Traditionally, safety-related systems, like aircraft, cars, and factory robots, have operated under human control or supervision. With autonomous systems (ASs), the role of the human is lessened-perhaps just to the extent of initiating autonomous operation:An AS can operate independently of human control.ASs have existed for some time, for example, in rail, including the Docklands Light Railway in London. Although safety-critical, such systems operate within well-defined and, to an extent, controlled environments; for example, there are physical controls on human access to the tracks, and traffic movement is controlled through a signaling system. The introduction of such ASs has been successful, and they have good safety records.By contrast, emerging ASs, such as autonomous vehicles (AVs) on the roads or collaborative robots (called cobots) in factories, operate in significantly more challenging",57,4,16-25,"['0018-9162', '1558-0814']","[{'given': 'John A.', 'orcid': '0000-0003-4745-4272', 'family': 'McDermid', 'affiliation': 'University of York, York, U.K.'}, {'given': 'Radu', 'orcid': '0000-0002-2678-9260', 'family': 'Calinescu', 'affiliation': 'University of York, York, U.K.'}, {'given': 'Ibrahim', 'orcid': '0000-0003-2736-8238', 'family': 'Habli', 'affiliation': 'University of York, York, U.K.'}, {'given': 'Richard', 'orcid': '0000-0001-7347-3413', 'family': 'Hawkins', 'affiliation': 'University of York, York, U.K.'}, {'given': 'Yan', 'orcid': '0000-0002-5446-6565', 'family': 'Jia', 'affiliation': 'University of York, York, U.K.'}, {'given': 'John', 'orcid': '0000-0003-0921-5908', 'family': 'Molloy', 'affiliation': 'University of York, York, U.K.'}, {'given': 'Matt', 'orcid': '0000-0002-9941-4531', 'family': 'Osborne', 'affiliation': 'University of York, York, U.K.'}, {'given': 'Colin', 'orcid': '0000-0002-6678-3752', 'family': 'Paterson', 'affiliation': 'University of York, York, U.K.'}, {'given': 'Zoe', 'orcid': '0000-0002-4467-3288', 'family': 'Porter', 'affiliation': 'University of York, York, U.K.'}, {'given': 'Philippa Ryan', 'orcid': '0000-0003-1307-5207', 'family': 'Conmy', 'affiliation': 'University of York, York, U.K.'}]",0,0,0,0,1,0
10.1109/mis.2017.54,"Explaining Explanation, Part 1: Theoretical Foundations",Institute of Electrical and Electronics Engineers (IEEE),2017,,32,3,68-73,['1541-1672'],"[{'given': 'Robert R.', 'orcid': None, 'family': 'Hoffman', 'affiliation': None}, {'given': 'Gary', 'orcid': None, 'family': 'Klein', 'affiliation': None}]",30,0,28,0,74,0
10.1136/bmjhci-2019-100081,Human factors challenges for the safe use of artificial intelligence in patient care,BMJ,2019,"The use of artificial intelligence (AI) in patient care can offer significant benefits. However, there is a lack of independent evaluation considering AI in use. The paper argues that consideration should be given to how AI will be incorporated into clinical processes and services. Human factors challenges that are likely to arise at this level include cognitive aspects (automation bias and human performance), handover and communication between clinicians and AI systems, situation awareness and the impact on the interaction with patients. Human factors research should accompany the development of AI from the outset.",26,1,e100081,['2632-1009'],"[{'given': 'Mark', 'orcid': '0000-0001-6895-946X', 'family': 'Sujan', 'affiliation': None}, {'given': 'Dominic', 'orcid': None, 'family': 'Furniss', 'affiliation': None}, {'given': 'Kath', 'orcid': None, 'family': 'Grundy', 'affiliation': None}, {'given': 'Howard', 'orcid': None, 'family': 'Grundy', 'affiliation': None}, {'given': 'David', 'orcid': None, 'family': 'Nelson', 'affiliation': None}, {'given': 'Matthew', 'orcid': None, 'family': 'Elliott', 'affiliation': None}, {'given': 'Sean', 'orcid': None, 'family': 'White', 'affiliation': None}, {'given': 'Ibrahim', 'orcid': None, 'family': 'Habli', 'affiliation': None}, {'given': 'Nick', 'orcid': None, 'family': 'Reynolds', 'affiliation': None}]",94,1,93,0,104,0
10.1145/2470654.2466246,Benevolent deception in human computer interaction,ACM,2013,"
Though it has been asserted that ""good design is honest,""[42] deception exists throughout human-computer interaction research and practice. Because of the stigma associated with deception-in many cases rightfully so-the research community has focused its energy on eradicating malicious deception, and ignored instances in which deception is positively employed. In this paper we present the notion of benevolent deception, deception aimed at benefitting the user as well as the developer. We frame our discussion using a criminology-inspired model and ground components in various examples. We assert that this provides us with a set of tools and principles that not only helps us with system and interface design, but that opens new research areas. After all, as Cockton claims in his 2004 paper ""ValueCentered HCI"" [13], ""Traditional disciplines have delivered truth. The goal of HCI is to deliver value.""
Author
",,,1863-1872,[],"[{'given': 'Eytan', 'orcid': None, 'family': 'Adar', 'affiliation': 'University of Michigan, Ann Arbor, Michigan, USA'}, {'given': 'Desney S.', 'orcid': None, 'family': 'Tan', 'affiliation': 'Microsoft Research, Redmond, Washington, USA'}, {'given': 'Jaime', 'orcid': None, 'family': 'Teevan', 'affiliation': 'Microsoft Research, Redmond, Washington, USA'}]",37,0,37,0,77,0
10.1145/2702123.2702521,"""Automation Surprise"" in Aviation",ACM,2015,"
Conflicts between the pilot and the automation, when pilots detect but do not understand them, cause ""automation surprise"" situations and jeopardize flight safety. We conducted an experiment in a 3-axis motion flight simulator with 16 pilots equipped with an eye-tracker to analyze their behavior and eye movements during the occurrence of such a situation. The results revealed that this conflict engages participant's attentional abilities resulting in excessive and inefficient visual search patterns. This experiment confirmed the crucial need to design solutions for detecting the occurrence of conflictual situations and to assist the pilots. We therefore proposed an approach to formally identify the occurrence of ""automation surprise"" conflicts based on the analysis of ""silent mode changes"" of the autopilot. A demonstrator was implemented and allowed for the automatic trigger of messages in the cockpit that explains the autopilot behavior. We implemented a real-time demonstrator that was tested as a proof-of-concept with 7 subjects facing 3 different conflicts with automation. The results shown the efficacy of this approach which could be implemented in existing cockpits.
",,,2525-2534,[],"[{'given': 'Frederic', 'orcid': None, 'family': 'Dehais', 'affiliation': 'ISAE, Toulouse, Midi Pyrénée, France'}, {'given': 'Vsevolod', 'orcid': None, 'family': 'Peysakhovich', 'affiliation': 'ISAE, Toulouse, France'}, {'given': 'Sébastien', 'orcid': None, 'family': 'Scannella', 'affiliation': 'ISAE, Toulouse, France'}, {'given': 'Jennifer', 'orcid': None, 'family': 'Fongue', 'affiliation': 'ISAE, Toulouse, France'}, {'given': 'Thibault', 'orcid': None, 'family': 'Gateau', 'affiliation': 'ISAE, Toulouse, France'}]",60,0,58,0,72,0
10.1145/3290605.3300831,Designing Theory-Driven User-Centric Explainable AI,ACM,2019,,,,1-15,[],"[{'given': 'Danding', 'orcid': None, 'family': 'Wang', 'affiliation': 'National University of Singapore, Singapore, Singapore'}, {'given': 'Qian', 'orcid': None, 'family': 'Yang', 'affiliation': 'Carnegie Mellon University, Pittsburgh, PA, PA, USA'}, {'given': 'Ashraf', 'orcid': None, 'family': 'Abdul', 'affiliation': 'National University of Singapore, Singapore, Singapore'}, {'given': 'Brian Y.', 'orcid': None, 'family': 'Lim', 'affiliation': 'National University of Singapore, Singapore, Singapore'}]",507,7,496,2,699,0
10.1145/3313831.3376301,"Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design",ACM,2020,"Artificial Intelligence (AI) plays an increasingly important role in improving HCI and user experience. Yet many challenges persist in designing and innovating valuable human-AI interactions. For example, AI systems can make unpredictable errors, and these errors damage UX and even lead to undesired societal impact. However, HCI routinely grapples with complex technologies and mitigates their unintended consequences. What makes AI different? What makes human-AI interaction appear particularly difficult to design? This paper investigates these questions. We synthesize prior research, our own design and research experience, and our observations when teaching human-AI interaction. We identify two sources of AI's distinctive design challenges: 1) uncertainty surrounding AI's capabilities, 2) AI's output complexity, spanning from simple to adaptive complex. We identify four levels of AI systems. On each level, designers encounter a different subset of the design challenges. We demonstrate how these findings reveal new insights for designers, researchers, and design tool makers in productively addressing the challenges of human-AI interaction going forward.",,,1-13,[],"[{'given': 'Qian', 'orcid': None, 'family': 'Yang', 'affiliation': 'Carnegie Mellon University, Pittsburgh, PA, USA'}, {'given': 'Aaron', 'orcid': None, 'family': 'Steinfeld', 'affiliation': 'Carnegie Mellon University, Pittsburgh, PA, USA'}, {'given': 'Carolyn', 'orcid': None, 'family': 'Rosé', 'affiliation': 'Carnegie Mellon University, Pittsburgh, PA, USA'}, {'given': 'John', 'orcid': None, 'family': 'Zimmerman', 'affiliation': 'Carnegie Mellon University, Pittsburgh, PA, USA'}]",209,2,206,1,417,0
10.1145/3313831.3376590,Questioning the AI: Informing Design Practices for Explainable AI User Experiences,ACM,2020,"A pervasive design issue of AI systems is their explainability-how to provide appropriate information to help users understand the AI.The technical field of explainable AI (XAI) has produced a rich toolbox of techniques. Designers are now tasked with the challenges of how to select the most suitable XAI techniques and translate them into UX solutions. Informed by our previous work studying design challenges around XAI UX, this work proposes a design process to tackle these challenges. We review our and related prior work to identify requirements that the process should fulfill, and accordingly, propose a Question-Driven Design Process that grounds the user needs, choices of XAI techniques, design, and evaluation of XAI UX all in the user questions. We provide a mapping guide between prototypical user questions and exemplars of XAI techniques, serving as boundary objects to support collaboration between designers and AI engineers. We demonstrate it with a use case of designing XAI for healthcare adverse events prediction, and discuss lessons learned for tackling design challenges of AI systems.CCS Concepts: • Human-centered computing → Interaction design process and methods; • Computing methodologies → Artificial intelligence.",,,1-15,[],"[{'given': 'Q. Vera', 'orcid': None, 'family': 'Liao', 'affiliation': 'IBM Research AI, Yorktown Heights, NY, USA'}, {'given': 'Daniel', 'orcid': None, 'family': 'Gruen', 'affiliation': 'IBM Research, Cambridge, MA, USA'}, {'given': 'Sarah', 'orcid': None, 'family': 'Miller', 'affiliation': 'IBM Research, Cambridge, MA, USA'}]",306,4,301,0,634,0
10.1145/3328485,Toward human-centered AI,Association for Computing Machinery (ACM),2019,,26,4,42-46,"['1072-5520', '1558-3449']","[{'given': 'Wei', 'orcid': None, 'family': 'Xu', 'affiliation': 'Intel Corporation'}]",119,0,108,0,304,0
10.1145/3351095.3372852,Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making,ACM,2020,"
Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.
",,,295-305,[],"[{'given': 'Yunfeng', 'orcid': None, 'family': 'Zhang', 'affiliation': 'IBM Research AI'}, {'given': 'Q. Vera', 'orcid': None, 'family': 'Liao', 'affiliation': 'IBM Research AI'}, {'given': 'Rachel K. E.', 'orcid': None, 'family': 'Bellamy', 'affiliation': 'IBM Research AI'}]",451,12,436,3,600,0
10.1145/3397481.3450644,"I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI",ACM,2021,"Unintended consequences of deployed AI systems fueled the call for more interpretability in AI systems. Often explainable AI (XAI) systems provide users with simplifying local explanations for individual predictions but leave it up to them to construct a global understanding of the model behavior. In this work, we examine if non-technical users of XAI fall for an illusion of explanatory depth when interpreting additive local explanations. We applied a mixed methods approach consisting of a moderated study with 40 participants and an unmoderated study with 107 crowd workers using a spreadsheet-like explanation interface based on the SHAP framework. We observed what non-technical users do to form their mental models of global AI model behavior from local explanations and how their perception of understanding decreases when it is examined.
CCS CONCEPTS• Human-centered computing → HCI design and evaluation methods; User studies.",,,307-317,[],"[{'given': 'Michael', 'orcid': None, 'family': 'Chromik', 'affiliation': 'LMU Munich, Germany'}, {'given': 'Malin', 'orcid': None, 'family': 'Eiband', 'affiliation': 'LMU Munich, Germany'}, {'given': 'Felicitas', 'orcid': None, 'family': 'Buchner', 'affiliation': 'LMU Munich, Germany'}, {'given': 'Adrian', 'orcid': None, 'family': 'Krüger', 'affiliation': 'LMU Munich, Germany'}, {'given': 'Andreas', 'orcid': None, 'family': 'Butz', 'affiliation': 'LMU Munich, Germany'}]",57,1,56,0,93,0
10.1145/3397481.3450650,Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making,ACM,2021,"This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy-improve people's understanding of the AI model, help people recognize the model uncertainty, and support people's calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making.
CCS CONCEPTS• Human-centered computing → Empirical studies in HCI; • Computing methodologies → Machine learning.",,,318-328,[],"[{'given': 'Xinru', 'orcid': None, 'family': 'Wang', 'affiliation': 'Purdue University, United States'}, {'given': 'Ming', 'orcid': None, 'family': 'Yin', 'affiliation': 'Purdue University, United States'}]",108,1,106,1,224,0
10.1145/3411763.3441342,Operationalizing Human-Centered Perspectives in Explainable AI,ACM,2021,"The realm of Artifcial Intelligence (AI)'s impact on our lives is far reaching -with AI systems proliferating high-stakes domains such as healthcare, fnance, mobility, law, etc., these systems must be able to explain their decision to diverse end-users comprehensibly. Yet the discourse of Explainable AI (XAI) has been predominantly focused on algorithm-centered approaches, sufering from gaps in meeting user needs and exacerbating issues of algorithmic opacity. To address these issues, researchers have called for human-centered approaches to XAI. There is a need to chart the domain and shape the discourse of XAI with refective discussions from diverse stakeholders. The goal of this workshop is to examine how human-centered perspectives in XAI can be operationalized at the conceptual, methodological, and technical levels. Encouraging holistic (historical, sociological, and technical) approaches, we put an emphasis on ""operationalizing"", aiming to produce actionable frameworks, transferable evaluation methods, concrete design guidelines, and articulate a coordinated research agenda for XAI.
CCS CONCEPTS• Human-centered computing → HCI theory, concepts and models; Visualization theory, concepts and paradigms; Visualization design and evaluation methods; • Computing methodologies → Philosophical/theoretical foundations of artifcial intelligence.",,,1-6,[],"[{'given': 'Upol', 'orcid': None, 'family': 'Ehsan', 'affiliation': 'Georgia Institute ofGeorgia Institute of Technology Atlanta GA, USA'}, {'given': 'Philipp', 'orcid': None, 'family': 'Wintersberger', 'affiliation': 'CARISSMA Technische Hochschule Ingolstadt (THI) Ingolstadt Bavaria, Germany'}, {'given': 'Q. Vera', 'orcid': None, 'family': 'Liao', 'affiliation': 'IBM Research AI Yorktown Heights NY, USA'}, {'given': 'Martina', 'orcid': None, 'family': 'Mara', 'affiliation': 'Johannes Kepler University Linz Linz Upper Austria, Austria'}, {'given': 'Marc', 'orcid': None, 'family': 'Streit', 'affiliation': 'Johannes Kepler University LinzJohannes Kepler University Linz Linz Upper Austria, Austria'}, {'given': 'Sandra', 'orcid': None, 'family': 'Wachter', 'affiliation': 'Oxford Internet Institute University of Oxford Oxford England, United Kingdom'}, {'given': 'Andreas', 'orcid': None, 'family': 'Riener', 'affiliation': 'Technische Hochschule Ingolstadt (THI) Ingolstadt Bavaria, Germany'}, {'given': 'Mark O.', 'orcid': None, 'family': 'Riedl', 'affiliation': 'Georgia Institute of Technology Atlanta GA, USA'}]",54,0,52,0,95,0
10.1145/3411764.3445088,Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs,ACM,2021,"To ensure accountability and mitigate harm, it is critical that diverse stakeholders can interrogate black-box automated systems and find information that is understandable, relevant, and useful to them. In this paper, we eschew prior expertise-and role-based categorizations of interpretability stakeholders in favor of a more granular framework that decouples stakeholders' knowledge from their interpretability needs. We characterize stakeholders by their formal, instrumental, and personal knowledge and how it manifests in the contexts of machine learning, the data domain, and the general milieu. We additionally distill a hierarchical typology of stakeholder needs that distinguishes higher-level domain goals from lower-level interpretability tasks. In assessing the descriptive, evaluative, and generative powers of our framework, we find our more nuanced treatment of stakeholders reveals gaps and opportunities in the interpretability literature, adds precision to the design and comparison of user studies, and facilitates a more reflexive approach to conducting this research.",,,1-16,[],"[{'given': 'Harini', 'orcid': None, 'family': 'Suresh', 'affiliation': 'MIT, United States'}, {'given': 'Steven R.', 'orcid': None, 'family': 'Gomez', 'affiliation': 'MIT, United States'}, {'given': 'Kevin K.', 'orcid': None, 'family': 'Nam', 'affiliation': 'MIT, United States'}, {'given': 'Arvind', 'orcid': None, 'family': 'Satyanarayan', 'affiliation': 'MIT, United States'}]",68,0,68,0,108,0
10.1145/3411764.3445717,Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance,ACM,2021,,,,1-16,[],"[{'given': 'Gagan', 'orcid': None, 'family': 'Bansal', 'affiliation': 'University of Washington, United States'}, {'given': 'Tongshuang', 'orcid': None, 'family': 'Wu', 'affiliation': 'University of Washington, United States'}, {'given': 'Joyce', 'orcid': None, 'family': 'Zhou', 'affiliation': 'Paul G. Allen School of Computer Science &amp; Engineering University of Washington, United States'}, {'given': 'Raymond', 'orcid': None, 'family': 'Fok', 'affiliation': 'Paul G. Allen School of Computer Science &amp; Engineering University of Washington, United States'}, {'given': 'Besmira', 'orcid': None, 'family': 'Nushi', 'affiliation': 'Microsoft Research, United States'}, {'given': 'Ece', 'orcid': None, 'family': 'Kamar', 'affiliation': 'Microsoft Research, United States'}, {'given': 'Marco Tulio', 'orcid': None, 'family': 'Ribeiro', 'affiliation': 'Microsoft Research, United States'}, {'given': 'Daniel', 'orcid': None, 'family': 'Weld', 'affiliation': 'Paul G. Allen School of Computer Science &amp; Engineering University of Washington, United States'}]",359,10,342,7,452,0
10.1145/3430524.3442704,From ”Explainable AI” to ”Graspable AI”,ACM,2021,"Since the advent of Artificial Intelligence (AI) and Machine Learning (ML), researchers have asked how intelligent computing systems could interact with and relate to their users and their surroundings, leading to debates around issues of biased AI systems, ML black-box, user trust, user's perception of control over the system, and system's transparency, to name a few. All of these issues are related to how humans interact with AI or ML systems, through an interface which uses different interaction modalities. Prior studies address these issues from a variety of perspectives, spanning from understanding and framing the problems through ethics and Science and Technology Studies (STS) perspectives to finding effective technical solutions to the problems. But what is shared among almost all those efforts is an assumption that if systems can explain the how and why of their predictions, people will have a better perception of control and therefore will trust such systems more, and even can correct their shortcomings. This research field has been called Explainable AI (XAI). In this studio, we take stock on prior efforts in this area; however, we focus on using Tangible and Embodied Interaction (TEI) as an interaction modality for understanding ML. We note that the affordances of physical forms and their behaviors potentially can not only contribute to the explainability of ML systems, but also can contribute to an open environment for criticism. This studio seeks to both critique explainable ML terminology and to map the opportunities that TEI can offer to the HCI for designing more sustainable, graspable and just intelligent systems.",,,,[],"[{'given': 'Maliheh', 'orcid': None, 'family': 'Ghajargar', 'affiliation': 'Arts and Communication Malmö University, Sweden'}, {'given': 'Jeffrey', 'orcid': None, 'family': 'Bardzell', 'affiliation': 'College of Information Sciences and Technology The Pennsylvania State University, United States'}, {'given': 'Alison Smith', 'orcid': None, 'family': 'Renner', 'affiliation': 'Machine Learning Visualization Lab Decisive Analytics Corporation, United States'}, {'given': 'Peter Gall', 'orcid': None, 'family': 'Krogh', 'affiliation': 'Department of Enginering Aarhus University, Denmark'}, {'given': 'Kristina', 'orcid': None, 'family': 'Höök', 'affiliation': 'Media Technology and Interaction Design KTH Royal Institute of Technology, Sweden'}, {'given': 'David', 'orcid': None, 'family': 'Cuartielles', 'affiliation': 'Arts and Communication Malmö University, Sweden'}, {'given': 'Laurens', 'orcid': None, 'family': 'Boer', 'affiliation': 'IT University Copenhagen, Denmark'}, {'given': 'Mikael', 'orcid': None, 'family': 'Wiberg', 'affiliation': 'Dept of Informatics Umeå university, Sweden'}]",1,0,1,0,21,0
10.1145/3461778.3462131,"Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle",ACM,2021,,,,1591-1602,[],"[{'given': 'Shipi', 'orcid': None, 'family': 'Dhanorkar', 'affiliation': 'Pennsylvania State University, United States'}, {'given': 'Christine T.', 'orcid': None, 'family': 'Wolf', 'affiliation': 'Independent Researcher, United States'}, {'given': 'Kun', 'orcid': None, 'family': 'Qian', 'affiliation': 'Amazon.com, Inc., United States'}, {'given': 'Anbang', 'orcid': None, 'family': 'Xu', 'affiliation': 'IBM Research - Almaden, United States'}, {'given': 'Lucian', 'orcid': None, 'family': 'Popa', 'affiliation': 'IBM Research - Almaden, United States'}, {'given': 'Yunyao', 'orcid': None, 'family': 'Li', 'affiliation': 'Almaden Research Lab IBM Research, United States'}]",47,3,44,0,85,0
10.1145/3491101.3503727,Human-Centered Explainable AI (HCXAI): Beyond Opening the Black-Box of AI,ACM,2022,,,,1-7,[],"[{'given': 'Upol', 'orcid': None, 'family': 'Ehsan', 'affiliation': 'Interactive Computing, Georgia Institute of Technology, United States'}, {'given': 'Philipp', 'orcid': None, 'family': 'Wintersberger', 'affiliation': 'Institute of Visual Computing and Human-Centered Technology, TU Wien, Austria'}, {'given': 'Q. Vera', 'orcid': None, 'family': 'Liao', 'affiliation': 'Microsoft Research, Canada'}, {'given': 'Elizabeth Anne', 'orcid': None, 'family': 'Watkins', 'affiliation': 'Center for Information Technology Policy, Princeton University, United States'}, {'given': 'Carina', 'orcid': None, 'family': 'Manger', 'affiliation': 'Human-Computer Interaction Group, Technische Hochschule Ingolstadt, Germany'}, {'given': 'Hal', 'orcid': None, 'family': 'Daumé III', 'affiliation': 'University of Maryland, College Park, United States and Microsoft Research, New York City, USA'}, {'given': 'Andreas', 'orcid': None, 'family': 'Riener', 'affiliation': 'Human-Computer Interaction Group, Technische Hochschule Ingolstadt, Germany'}, {'given': 'Mark O', 'orcid': None, 'family': 'Riedl', 'affiliation': 'Georgia Tech, United States'}]",35,0,35,0,80,0
10.1145/3491102.3502104,Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems,ACM,2022,,,,1-9,[],"[{'given': 'Cecilia', 'orcid': None, 'family': 'Panigutti', 'affiliation': 'Scuola Normale Superiore, Italy and Computer Science, University of Pisa, Italy'}, {'given': 'Andrea', 'orcid': None, 'family': 'Beretta', 'affiliation': 'ISTI, CNR - Italian National Research Council, Italy'}, {'given': 'Fosca', 'orcid': None, 'family': 'Giannotti', 'affiliation': 'CNR - Italian National Research Council, Italy and Scuola Normale Superiore, Italy'}, {'given': 'Dino', 'orcid': None, 'family': 'Pedreschi', 'affiliation': 'Computer Science, University of Pisa, Italy'}]",34,3,31,0,95,0
10.1145/3514257,"An Agile New Research Framework for Hybrid Human-AI Teaming: Trust, Transparency, and Transferability",Association for Computing Machinery (ACM),2022,"We propose a new research framework by which the nascent discipline of human-AI teaming can be explored within experimental environments in preparation for transferal to real-world contexts. We examine the existing literature and unanswered research questions through the lens of an Agile approach to construct our proposed framework. Our framework aims to provide a structure for understanding the macro features of this research landscape, supporting holistic research into the acceptability of human-AI teaming to human team members and the affordances of AI team members. The framework has the potential to enhance decision-making and performance of hybrid human-AI teams. Further, our framework proposes the application of Agile methodology for research management and knowledge discovery. We propose a transferability pathway for hybrid teaming to be initially tested in a safe environment, such as a real-time strategy video game, with elements of lessons learned that can be transferred to real-world situations.",12,3,1-36,"['2160-6455', '2160-6463']","[{'given': 'Sabrina', 'orcid': None, 'family': 'Caldwell', 'affiliation': 'Australian National University, Canberra, ACT, Australia'}, {'given': 'Penny', 'orcid': '0000-0002-6543-557X', 'family': 'Sweetser', 'affiliation': 'Australian National University, Canberra, ACT, Australia'}, {'given': 'Nicholas', 'orcid': None, 'family': 'O’Donnell', 'affiliation': 'Queensland University of Technology, Brisbane City, QLD, Australia'}, {'given': 'Matthew J.', 'orcid': None, 'family': 'Knight', 'affiliation': 'Defence Science and Technology, Edinburgh, SA, Australia'}, {'given': 'Matthew', 'orcid': None, 'family': 'Aitchison', 'affiliation': 'Australian National University, Canberra, ACT, Australia'}, {'given': 'Tom', 'orcid': None, 'family': 'Gedeon', 'affiliation': 'Australian National University, Canberra, ACT, Australia'}, {'given': 'Daniel', 'orcid': None, 'family': 'Johnson', 'affiliation': 'Queensland University of Technology, Brisbane City, QLD, Australia'}, {'given': 'Margot', 'orcid': None, 'family': 'Brereton', 'affiliation': 'Queensland University of Technology, Brisbane City, QLD, Australia'}, {'given': 'Marcus', 'orcid': None, 'family': 'Gallagher', 'affiliation': 'University of Queensland, St Lucia, QLD, Australia'}, {'given': 'David', 'orcid': None, 'family': 'Conroy', 'affiliation': 'Queensland University of Technology, Brisbane City, QLD, Australia'}]",4,0,4,0,38,0
10.1145/3544548.3580959,What is Human-Centered about Human-Centered AI? A Map of the Research Landscape,ACM,2023,,,,1-23,[],"[{'given': 'Tara', 'orcid': '0000-0003-3260-0644', 'family': 'Capel', 'affiliation': 'School of Computer Science, Queensland University of Technology, Australia'}, {'given': 'Margot', 'orcid': '0000-0002-0982-3404', 'family': 'Brereton', 'affiliation': 'School of Computer Science, Queensland University of Technology, Australia'}]",14,0,14,0,88,0
10.1145/3544548.3581001,"""Help Me Help the AI"": Understanding How Explainability Can Support Human-AI Interaction",ACM,2023,,,,1-17,[],"[{'given': 'Sunnie S. Y.', 'orcid': None, 'family': 'Kim', 'affiliation': None}, {'given': 'Elizabeth Anne', 'orcid': None, 'family': 'Watkins', 'affiliation': None}, {'given': 'Olga', 'orcid': None, 'family': 'Russakovsky', 'affiliation': None}, {'given': 'Ruth', 'orcid': None, 'family': 'Fong', 'affiliation': None}, {'given': 'Andrés', 'orcid': None, 'family': 'Monroy-Hernández', 'affiliation': None}]",33,1,32,0,98,0
10.1145/3544549.3573832,Human-Centered Explainable AI (HCXAI): Coming of Age,ACM,2023,,,,1-7,[],"[{'given': 'Upol', 'orcid': '0000-0002-4911-0409', 'family': 'Ehsan', 'affiliation': 'Interactive Computing, Georgia Institute of Technology, United States'}, {'given': 'Philipp', 'orcid': '0000-0001-9287-3770', 'family': 'Wintersberger', 'affiliation': 'Department of Informatics, Communications and Media, University of Applied Sciences Upper Austria, Austria and Institute of Visual Computing and Human-Centered Technology, TU Wien, Austria'}, {'given': 'Elizabeth A', 'orcid': '0000-0002-1434-589X', 'family': 'Watkins', 'affiliation': 'Intelligent Systems Research (ISR), Intel Labs, United States'}, {'given': 'Carina', 'orcid': '0000-0002-9813-0051', 'family': 'Manger', 'affiliation': 'Human-Computer Interaction Group, Technische Hochschule Ingolstadt, Germany'}, {'given': 'Gonzalo', 'orcid': '0000-0003-4198-5021', 'family': 'Ramos', 'affiliation': 'Microsoft Research, United States'}, {'given': 'Justin D.', 'orcid': '0000-0003-2228-2398', 'family': 'Weisz', 'affiliation': 'IBM Research AI, United States'}, {'given': 'Hal', 'orcid': '0000-0002-3760-345X', 'family': 'Daumé Iii', 'affiliation': 'University of Maryland, United States'}, {'given': 'Andreas', 'orcid': '0000-0002-9174-8895', 'family': 'Riener', 'affiliation': 'Human-Computer Interaction Group, Technische Hochschule Ingolstadt, Germany'}, {'given': 'Mark O', 'orcid': '0000-0001-5283-6588', 'family': 'Riedl', 'affiliation': 'Georgia Tech, United States'}]",6,0,6,0,16,0
10.1145/3579467,Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI,Association for Computing Machinery (ACM),2023,"Explainable AI (XAI) systems are sociotechnical in nature; thus, they are subject to the sociotechnical gap-divide between the technical affordances and the social needs. However, charting this gap is challenging. In the context of XAI, we argue that charting the gap improves our problem understanding, which can reflexively provide actionable insights to improve explainability. Utilizing two case studies in distinct domains, we empirically derive a framework that facilitates systematic charting of the sociotechnical gap by connecting AI guidelines in the context of XAI and elucidating how to use them to address the gap. We apply the framework to a third case in a new domain, showcasing its affordances. Finally, we discuss conceptual implications of the framework, share practical considerations in its operationalization, and offer guidance on transferring it to new contexts. By making conceptual and practical contributions to understanding the sociotechnical gap in XAI, the framework expands the XAI design space.",7,CSCW1,1-32,['2573-0142'],"[{'given': 'Upol', 'orcid': '0000-0002-4911-0409', 'family': 'Ehsan', 'affiliation': 'Georgia Institute of Technology, Atlanta, GA, USA'}, {'given': 'Koustuv', 'orcid': '0000-0002-8872-2934', 'family': 'Saha', 'affiliation': 'Microsoft Research, Montreal, Canada'}, {'given': 'Munmun', 'orcid': '0000-0002-8939-264X', 'family': 'De Choudhury', 'affiliation': 'Georgia Institute of Technology, Atlanta, GA, USA'}, {'given': 'Mark O.', 'orcid': '0000-0001-5283-6588', 'family': 'Riedl', 'affiliation': 'Georgia Tech, Altanta, GA, USA'}]",15,0,15,0,43,0
10.1145/3579605,Explanations Can Reduce Overreliance on AI Systems During Decision-Making,Association for Computing Machinery (ACM),2023,"Prior work has identified a resilient phenomenon that threatens the performance of human-AI decision-making teams: overreliance, when people agree with an AI, even when it is incorrect. Surprisingly, overreliance does not reduce when the AI produces explanations for its predictions, compared to only providing predictions. Some have argued that overreliance results from cognitive biases or uncalibrated trust, attributing overreliance to an inevitability of human cognition. By contrast, our paper argues that people strategically choose whether or not to engage with an AI explanation, demonstrating empirically that there are scenarios where AI explanations reduce overreliance. To achieve this, we formalize this strategic choice in a cost-benefit framework, where the costs and benefits of engaging with the task are weighed against the costs and benefits of relying on the AI. We manipulate the costs and benefits in a maze task, where participants collaborate with a simulated AI to find the exit of a maze. Through 5 studies (N = 731), we find that costs such as task difficulty (Study 1), explanation difficulty (Study 2, 3), and benefits such as monetary compensation (Study 4) affect overreliance. Finally, Study 5 adapts the Cognitive Effort Discounting paradigm to quantify the utility of different explanations, providing further support for our framework. Our results suggest that some of the null effects found in literature could be due in part to the explanation not sufficiently reducing the costs of verifying the AI's prediction.",7,CSCW1,1-38,['2573-0142'],"[{'given': 'Helena', 'orcid': '0000-0002-6649-6905', 'family': 'Vasconcelos', 'affiliation': 'Stanford University, Stanford, CA, USA'}, {'given': 'Matthew', 'orcid': '0000-0003-2972-462X', 'family': 'Jörke', 'affiliation': 'Stanford University, Stanford, CA, USA'}, {'given': 'Madeleine', 'orcid': '0000-0001-7290-068X', 'family': 'Grunde-McLaughlin', 'affiliation': 'University of Washington, Seattle, WA, USA'}, {'given': 'Tobias', 'orcid': '0000-0002-9162-0779', 'family': 'Gerstenberg', 'affiliation': 'Stanford University, Stanford, CA, USA'}, {'given': 'Michael S.', 'orcid': '0000-0001-8020-9434', 'family': 'Bernstein', 'affiliation': 'Stanford University, Stanford, CA, USA'}, {'given': 'Ranjay', 'orcid': '0000-0001-8784-2531', 'family': 'Krishna', 'affiliation': 'University of Washington, Seattle, WA, USA'}]",30,5,24,1,136,0
10.1145/3593013.3594087,Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies,ACM,2023,,,,1369-1385,[],"[{'given': 'Vivian', 'orcid': '0000-0003-3165-6136', 'family': 'Lai', 'affiliation': 'University of Colorado Boulder, USA'}, {'given': 'Chacha', 'orcid': '0009-0000-6101-2150', 'family': 'Chen', 'affiliation': 'University of Chicago, USA'}, {'given': 'Alison', 'orcid': '0000-0002-6600-267X', 'family': 'Smith-Renner', 'affiliation': 'Dataminr Inc., USA'}, {'given': 'Q. Vera', 'orcid': '0000-0003-4543-7196', 'family': 'Liao', 'affiliation': 'Microsoft Research, Canada'}, {'given': 'Chenhao', 'orcid': '0000-0002-3981-2116', 'family': 'Tan', 'affiliation': 'University of Chicago, USA'}]",23,1,22,0,81,0
10.1145/3610219,Understanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations,Association for Computing Machinery (ACM),2023,"AI explanations are often mentioned as a way to improve human-AI decision-making, but empirical studies have not found consistent evidence of explanations' effectiveness and, on the contrary, suggest that they can increase overreliance when the AI system is wrong. While many factors may affect reliance on AI support, one important factor is how decision-makers reconcile their own intuition---beliefs or heuristics, based on prior knowledge, experience, or pattern recognition, used to make judgments---with the information provided by the AI system to determine when to override AI predictions. We conduct a think-aloud, mixed-methods study with two explanation types (feature- and example-based) for two prediction tasks to explore how decision-makers' intuition affects their use of AI predictions and explanations, and ultimately their choice of when to rely on AI. Our results identify three types of intuition involved in reasoning about AI predictions and explanations: intuition about the task outcome, features, and AI limitations. Building on these, we summarize three observed pathways for decision-makers to apply their own intuition and override AI predictions. We use these pathways to explain why (1) the feature-based explanations we used did not improve participants' decision outcomes and increased their overreliance on AI, and (2) the example-based explanations we used improved decision-makers' performance over feature-based explanations and helped achieve complementary human-AI performance. Overall, our work identifies directions for further development of AI decision-support systems and explanation methods that help decision-makers effectively apply their intuition to achieve appropriate reliance on AI.",7,CSCW2,1-32,['2573-0142'],"[{'given': 'Valerie', 'orcid': '0009-0007-2783-0265', 'family': 'Chen', 'affiliation': 'Carnegie Mellon University, Pittsburgh, PA, USA'}, {'given': 'Q. Vera', 'orcid': '0000-0003-4543-7196', 'family': 'Liao', 'affiliation': 'Microsoft Research, Montreal, Canada'}, {'given': 'Jennifer', 'orcid': '0000-0002-7807-2018', 'family': 'Wortman Vaughan', 'affiliation': 'Microsoft Research, New York, NY, USA'}, {'given': 'Gagan', 'orcid': '0000-0002-7741-3861', 'family': 'Bansal', 'affiliation': 'Microsoft Research, Redmond, WA, USA'}]",19,0,19,0,68,0
10.1145/3613904.3642352,User Characteristics in Explainable AI: The Rabbit Hole of Personalization?,ACM,2024,,,,1-13,[],"[{'given': 'Robert', 'orcid': '0009-0006-5857-6469', 'family': 'Nimmo', 'affiliation': 'School of Computing Science, University of Glasgow, United Kingdom'}, {'given': 'Marios', 'orcid': '0000-0003-1454-0641', 'family': 'Constantinides', 'affiliation': 'Nokia Bell Labs, United Kingdom'}, {'given': 'Ke', 'orcid': '0000-0001-7177-9152', 'family': 'Zhou', 'affiliation': 'Nokia Bell Labs, United Kingdom'}, {'given': 'Daniele', 'orcid': '0000-0001-9461-5804', 'family': 'Quercia', 'affiliation': 'Nokia Bell Labs, United Kingdom'}, {'given': 'Simone', 'orcid': '0000-0001-6482-1973', 'family': 'Stumpf', 'affiliation': 'School of Computing Science, University of Glasgow, United Kingdom'}]",0,0,0,0,6,0
10.1145/3613904.3642398,A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations,ACM,2024,"Responsible design of AI systems is a shared goal across HCI and AI communities. Responsible AI (RAI) tools have been developed to support practitioners to identify, assess, and mitigate ethical issues during AI development. These tools take many forms (e.g., design playbooks, software toolkits, documentation protocols). However, research suggests that use of RAI tools is shaped by organizational contexts, raising questions about how effective such tools are in practice. To better understand how RAI tools are-and might beevaluated, we conducted a qualitative analysis of 37 publications that discuss evaluations of RAI tools. We find that most evaluations focus on usability, while questions of tools' effectiveness in changing AI development are sidelined. While usability evaluations are an important approach to evaluate RAI tools, we draw on evaluation approaches from other fields to highlight developer-and communitylevel steps to support evaluations of RAI tools' effectiveness in shaping AI development practices and outcomes.CCS Concepts: • General and reference → Evaluation; • Human-centered computing → HCI design and evaluation methods; • Software and its engineering → Designing software.",,,1-24,[],"[{'given': 'Glen', 'orcid': '0000-0003-3249-0190', 'family': 'Berman', 'affiliation': 'School of Engineering, Australian National University, Australia'}, {'given': 'Nitesh', 'orcid': '0000-0002-4666-1926', 'family': 'Goyal', 'affiliation': 'Google Research, United States'}, {'given': 'Michael', 'orcid': '0000-0001-5772-0488', 'family': 'Madaio', 'affiliation': 'Google Research, United States'}]",0,0,0,0,15,0
10.1145/3613904.3642474,The Who in XAI: How AI Background Shapes Perceptions of AI Explanations,ACM,2024,,,,1-32,[],"[{'given': 'Upol', 'orcid': '0000-0002-4911-0409', 'family': 'Ehsan', 'affiliation': 'Interactive Computing, Georgia Institute of Technology, United States'}, {'given': 'Samir', 'orcid': '0000-0002-7921-3820', 'family': 'Passi', 'affiliation': 'Microsoft, United States'}, {'given': 'Q. Vera', 'orcid': '0000-0003-4543-7196', 'family': 'Liao', 'affiliation': 'Microsoft Research, Canada'}, {'given': 'Larry', 'orcid': '0000-0002-3882-713X', 'family': 'Chan', 'affiliation': 'Illumio, United States'}, {'given': 'I-Hsiang', 'orcid': '0000-0003-1261-7174', 'family': 'Lee', 'affiliation': 'School of Industrial &amp; Systems Engineering, Georgia Institute of Technology, United States'}, {'given': 'Michael', 'orcid': '0000-0001-7860-163X', 'family': 'Muller', 'affiliation': 'Human AI Collaboration, IBM Research, United States'}, {'given': 'Mark O', 'orcid': '0000-0001-5283-6588', 'family': 'Riedl', 'affiliation': 'Georgia Institute of Technology, United States'}]",2,0,2,0,22,0
10.1145/3613904.3642551,"``It Is a Moving Process"": Understanding the Evolution of Explainability Needs of Clinicians in Pulmonary Medicine",ACM,2024,,,,1-21,[],"[{'given': 'Lorenzo', 'orcid': '0000-0002-4200-1664', 'family': 'Corti', 'affiliation': 'Delft University of Technology, Netherlands'}, {'given': 'Rembrandt', 'orcid': '0009-0002-2928-9402', 'family': 'Oltmans', 'affiliation': 'Delft University of Technology, Netherlands'}, {'given': 'Jiwon', 'orcid': '0000-0002-0157-196X', 'family': 'Jung', 'affiliation': 'Delft University of Technology, Netherlands and Erasmus MC, University Medical Center Rotterdam, Netherlands'}, {'given': 'Agathe', 'orcid': '0000-0003-2725-5305', 'family': 'Balayn', 'affiliation': 'Delft University of Technology, Netherlands'}, {'given': 'Marlies', 'orcid': '0000-0002-4527-6962', 'family': 'Wijsenbeek', 'affiliation': 'Erasmus MC, University Medical Center Rotterdam, Netherlands'}, {'given': 'Jie', 'orcid': '0000-0002-0350-0313', 'family': 'Yang', 'affiliation': 'Delft University of Technology, Netherlands'}]",0,0,0,0,5,0
10.1145/3613904.3642733,Explorable Explainable AI: Improving AI Understanding for Community Health Workers in India,ACM,2024,,,,1-21,[],"[{'given': 'Ian René', 'orcid': '0000-0003-3641-2923', 'family': 'Solano-Kamaiko', 'affiliation': 'Cornell Tech, United States'}, {'given': 'Dibyendu', 'orcid': '0000-0003-3114-7278', 'family': 'Mishra', 'affiliation': 'Cornell University, United States'}, {'given': 'Nicola', 'orcid': '0000-0002-6119-705X', 'family': 'Dell', 'affiliation': 'Jacobs Institute, Cornell Tech, United States'}, {'given': 'Aditya', 'orcid': '0000-0001-5693-3326', 'family': 'Vashistha', 'affiliation': 'Cornell University, United States'}]",1,0,1,0,8,0
10.1145/3613905.3636311,Human-Centered Explainable AI (HCXAI): Reloading Explainability in the Era of Large Language Models (LLMs),ACM,2024,,,,1-6,[],"[{'given': 'Upol', 'orcid': '0000-0002-4911-0409', 'family': 'Ehsan', 'affiliation': 'Interactive Computing, Georgia Institute of Technology, United States'}, {'given': 'Elizabeth A', 'orcid': '0000-0002-1434-589X', 'family': 'Watkins', 'affiliation': 'Intelligent Systems Research (ISR), Intel Labs, United States'}, {'given': 'Philipp', 'orcid': '0000-0001-9287-3770', 'family': 'Wintersberger', 'affiliation': 'Department of Digital Media, University of Applied Sciences Upper Austria, Austria'}, {'given': 'Carina', 'orcid': '0000-0002-9813-0051', 'family': 'Manger', 'affiliation': 'Human-Computer Interaction Group, Technische Hochschule Ingolstadt, Germany'}, {'given': 'Sunnie S. Y.', 'orcid': '0000-0002-8901-7233', 'family': 'Kim', 'affiliation': 'Computer Science, Princeton University, United States'}, {'given': 'Niels', 'orcid': '0000-0001-5106-7692', 'family': 'Van Berkel', 'affiliation': 'Department of Computer Science, Aalborg University, Denmark'}, {'given': 'Andreas', 'orcid': '0000-0002-9174-8895', 'family': 'Riener', 'affiliation': 'Human-Computer Interaction Group, Technische Hochschule Ingolstadt, Germany'}, {'given': 'Mark O', 'orcid': '0000-0001-5283-6588', 'family': 'Riedl', 'affiliation': 'Georgia Institute of Technology, United States'}]",0,0,0,0,5,0
10.1145/3635113,AI Explainability and Acceptance: A Case Study for Underwater Mine Hunting,Association for Computing Machinery (ACM),2024,"In critical operational context such as Mine Warfare, Automatic Target Recognition (ATR) algorithms are still hardly accepted. The complexity of their decision-making hampers understanding of predictions despite performances approaching human expert ones. Much research has been done in Explainability Artificial Intelligence (XAI) field to avoid this ”black box” effect. This field of research attempts to provide explanations for the decision-making of complex networks to promote their acceptability. Most of the explanation methods applied on image classifier networks provide heat maps. These maps highlight pixels according to their importance in decision-making.
          In this work, we first implement different XAI methods for the automatic classification of Synthetic Aperture Sonar (SAS) images by convolutional neural networks (CNN). These different methods are based on a Post-Hoc approach. We study and compare the different heat maps obtained.
          Secondly, we evaluate the benefits and the usefulness of explainability in an operational framework for collaboration. To do this, different user tests are carried out with different levels of assistance ranging from classification for an unaided operator, to classification with explained ATR. These tests allow us to study whether heat maps are useful in this context.
          The results obtained show that the heat maps explanation have a disputed utility according to the operators. Heat map presence does not increase the quality of the classifications. On the contrary, it even increases the response time. Nevertheless, half of operators see a certain usefulness in heat maps explanation.",16,1,1-20,"['1936-1955', '1936-1963']","[{'given': 'Guy-Junior', 'orcid': '0000-0002-1249-0321', 'family': 'Richard', 'affiliation': 'Thales DMS, IMT Atlantique, Brest, FRANCE'}, {'given': 'Jérôme', 'orcid': '0009-0003-1600-0881', 'family': 'Habonneau', 'affiliation': 'Thales DMS, Brest, FRANCE'}, {'given': 'Didier', 'orcid': '0000-0002-3320-1234', 'family': 'Guériot', 'affiliation': 'IMT Atlantique, Brest, FRANCE'}, {'given': 'Jean-Marc', 'orcid': '0000-0001-9939-9231', 'family': 'Le Caillec', 'affiliation': 'IMT Atlantique, Brest, FRANCE'}]",2,1,1,0,3,0
10.1145/3637396,Seamful XAI: Operationalizing Seamful Design in Explainable AI,Association for Computing Machinery (ACM),2024,"Mistakes in AI systems are inevitable, arising from both technical limitations and sociotechnical gaps. While black-boxing AI systems can make the user experience seamless, hiding the seams risks disempowering users to mitigate fallouts from AI mistakes. Instead of hiding these AI imperfections, can we leverage them to help the user? While Explainable AI (XAI) has predominantly tackled algorithmic opaqueness, we propose that seamful design can foster AI explainability by revealing and leveraging sociotechnical and infrastructural mismatches. We introduce the concept of Seamful XAI by (1) conceptually transferring ""seams"" to the AI context and (2) developing a design process that helps stakeholders anticipate and design with seams. We explore this process with 43 AI practitioners and real end-users, using a scenario-based co-design activity informed by real-world use cases. We found that the Seamful XAI design process helped users foresee AI harms, identify underlying reasons (seams), locate them in the AI's lifecycle, learn how to leverage seamful information to improve XAI and user agency. We share empirical insights, implications, and reflections on how this process can help practitioners anticipate and craft seams in AI, how seamfulness can improve explainability, empower end-users, and facilitate Responsible AI.",8,CSCW1,1-29,['2573-0142'],"[{'given': 'Upol', 'orcid': '0000-0002-4911-0409', 'family': 'Ehsan', 'affiliation': 'Interactive Computing, Georgia Institute of Technology, Atlanta, GA, USA'}, {'given': 'Q. Vera', 'orcid': '0000-0003-4543-7196', 'family': 'Liao', 'affiliation': 'Microsoft Research, Montreal, QC, Canada'}, {'given': 'Samir', 'orcid': '0000-0002-7921-3820', 'family': 'Passi', 'affiliation': 'Microsoft, Redmond, WA, USA'}, {'given': 'Mark O.', 'orcid': '0000-0001-5283-6588', 'family': 'Riedl', 'affiliation': 'Georgia Institute of Technology, Altanta, GA, USA'}, {'given': 'Hal', 'orcid': '0000-0002-3760-345X', 'family': 'Daumé', 'affiliation': 'University of Maryland &amp; Microsoft Research, College Park, MD, USA'}]",0,0,0,0,5,0
10.1145/3706599.3719941,Exploring the Impact of Explainability in Large Language Model (LLM) Applications on User Experience,ACM,2025,,,,1-8,[],"[{'given': 'Yanyun', 'orcid': '0009-0009-3145-9678', 'family': 'Wang', 'affiliation': 'Alibaba Cloud computing, Hangzhou, China'}, {'given': 'Xumei', 'orcid': '0009-0004-6200-723X', 'family': 'Fang', 'affiliation': 'Alibaba Cloud Computing, Hangzhou, China'}, {'given': 'Zan', 'orcid': '0009-0006-6810-1416', 'family': 'Xu', 'affiliation': 'Alibaba Cloud computing, Hangzhou, China'}, {'given': 'Jianye', 'orcid': '0009-0003-2409-1250', 'family': 'Li', 'affiliation': 'Alibaba Cloud Computing, Beijing, China'}, {'given': 'Luping', 'orcid': '0009-0007-9377-8928', 'family': 'Wang', 'affiliation': 'Alibaba Cloud Computing, Hangzhou, China'}]",0,0,0,0,0,0
10.1145/3706599.3720036,"""Good"" XAI Design: For What? In Which Ways?",ACM,2025,,,,1-13,[],"[{'given': 'Lingqing', 'orcid': '0000-0001-5888-3545', 'family': 'Wang', 'affiliation': 'Georgia Institute of Technology, Atlanta, Georgia, USA'}, {'given': 'Yifan', 'orcid': '0000-0001-5261-3958', 'family': 'Liu', 'affiliation': 'Georgia Institute of Technology, Atlanta, Georgia, USA'}, {'given': 'Ashok K.', 'orcid': '0000-0003-4043-0614', 'family': 'Goel', 'affiliation': 'Designing Intelligence Lab, Georgia Institute of Technology, Atlanta, Georgia, USA'}]",0,0,0,0,0,0
10.1145/3706599.3721096,Explainability and Contestability for the Responsible Use of Public Sector AI,ACM,2025,,,,1-6,[],"[{'given': 'Timothée', 'orcid': '0009-0006-8276-4670', 'family': 'Schmude', 'affiliation': 'Faculty of Computer Science, Research Network Data Science, Doctoral School Computer Science, University of Vienna, Vienna, Austria'}]",0,0,0,0,0,0
10.1145/3708359.3712133,Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational XAI Assistant,ACM,2025,,,,907-924,[],"[{'given': 'Gaole', 'orcid': '0000-0002-8152-4791', 'family': 'He', 'affiliation': 'Delft University of Technology, Delft, Netherlands,'}, {'given': 'Nilay', 'orcid': '0009-0006-8118-1879', 'family': 'Aishwarya', 'affiliation': 'Delft University of Technology, Delft, Netherlands,'}, {'given': 'Ujwal', 'orcid': '0000-0002-6189-6539', 'family': 'Gadiraju', 'affiliation': 'Delft University of Technology, Delft, Netherlands,'}]",0,0,0,0,1,0
10.1155/2023/4637678,AI Trust: Can Explainable AI Enhance Warranted Trust?,Wiley,2023,"Explainable artificial intelligence (XAI), known to produce explanations so that predictions from AI models can be understood, is commonly used to mitigate possible AI mistrust. The underlying premise is that the explanations of the XAI models enhance AI trust. However, such an increase may depend on many factors. This article examined how trust in an AI recommendation system is affected by the presence of explanations, the performance of the system, and the level of risk. Our experimental study, conducted with 215 participants, has shown that the presence of explanations increases AI trust, but only in certain conditions. AI trust was higher when explanations with feature importance were provided than with counterfactual explanations. Moreover, when the system performance is not guaranteed, the use of explanations seems to lead to an overreliance on the system. Lastly, system performance had a stronger impact on trust, compared to the effects of other factors (explanation and risk).",2023,,1-12,['2578-1863'],"[{'given': 'Regina', 'orcid': '0000-0003-0249-8319', 'family': 'de Brito Duarte', 'affiliation': 'INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Portugal'}, {'given': 'Filipa', 'orcid': '0000-0002-1592-9694', 'family': 'Correia', 'affiliation': 'Interactive Technologies Institute, LARSyS, Instituto Superior Técnico, Universidade de Lisboa, Portugal'}, {'given': 'Patrícia', 'orcid': '0000-0001-5766-0489', 'family': 'Arriaga', 'affiliation': 'ISCTE-Instituto Universitário de Lisboa (IUL), CIS-IUL, Lisboa, Portugal'}, {'given': 'Ana', 'orcid': '0000-0003-3998-5188', 'family': 'Paiva', 'affiliation': 'INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Portugal'}]",0,0,0,0,9,0
10.1177/0141076817712252,Drawing on human factors engineering to evaluate the effectiveness of health information technology,SAGE Publications,2017,,110,8,309-315,"['0141-0768', '1758-1095']","[{'given': 'Kathrin M', 'orcid': None, 'family': 'Cresswell', 'affiliation': 'Centre for Medical Informatics, Usher Institute of Population Health Sciences and Informatics, The University of Edinburgh, Edinburgh EH8 9YL, UK'}, {'given': 'Ann', 'orcid': None, 'family': 'Blandford', 'affiliation': 'Institute of Digital Health, University College London, London WC1E 6BT, UK'}, {'given': 'Aziz', 'orcid': None, 'family': 'Sheikh', 'affiliation': 'Centre for Medical Informatics, Usher Institute of Population Health Sciences and Informatics, The University of Edinburgh, Edinburgh EH8 9YL, UK'}]",11,0,11,0,28,0
10.1186/1748-5908-1-1,Welcome to Implementation Science,Springer Science and Business Media LLC,2006,"
AbstractImplementation research is the scientific study of methods to promote the systematic uptake of research findings and other evidence-based practices into routine practice, and, hence, to improve the quality and effectiveness of health services and care. This relatively new field includes the study of influences on healthcare professional and organisational behaviour.Implementation Science will encompass all aspects of research in this field, in clinical, community and policy contexts. This online journal will provide a unique platform for this type of research and will publish a broad range of articles -study protocols, debate, theoretical and conceptual articles, rigorous evaluations of the process of change, and articles on methodology and rigorously developed tools -that will enhance the development and refinement of implementation research. No one discipline, research design, or paradigm will be favoured.Implementation Science looks forward to receiving manuscripts that facilitate the continued development of the field, and contribute to healthcare policy and practice.
Implementation ScienceResearch continually produces new findings that can contribute to effective and efficient healthcare. However, such research cannot change outcomes unless health services and healthcare professionals adopt the findings into practice. Uneven uptake of research findings -and thus inappropriate care -occurs across settings, specialities and countries [1][2][3].Implementation research is the scientific study of methods to promote the systematic uptake of research findings and other evidence-based practices into routine practice, and, hence, to improve the quality and effectiveness of health services. It includes the study of influences on healthcare professional and organisational behaviour.
",1,1,,['1748-5908'],"[{'given': 'Martin P', 'orcid': None, 'family': 'Eccles', 'affiliation': None}, {'given': 'Brian S', 'orcid': None, 'family': 'Mittman', 'affiliation': None}]",1142,5,1103,0,1844,0
10.1371/journal.pdig.0000016,To explain or not to explain?—Artificial intelligence explainability in clinical decision support systems,Public Library of Science (PLoS),2022,"Explainability for artificial intelligence (AI) in medicine is a hotly debated topic. Our paper presents a review of the key arguments in favor and against explainability for AI-powered Clinical Decision Support System (CDSS) applied to a concrete use case, namely an AI-powered CDSS currently used in the emergency call setting to identify patients with life-threatening cardiac arrest. More specifically, we performed a normative analysis using socio-technical scenarios to provide a nuanced account of the role of explainability for CDSSs for the concrete use case, allowing for abstractions to a more general level. Our analysis focused on three layers: technical considerations, human factors, and the designated system role in decision-making. Our findings suggest that whether explainability can provide added value to CDSS depends on several key questions: technical feasibility, the level of validation in case of explainable algorithms, the characteristics of the context in which the system is implemented, the designated role in the decision-making process, and the key user group(s). Thus, each CDSS will require an individualized assessment of explainability needs and we provide an example of how such an assessment could look like in practice.",1,2,e0000016,['2767-3170'],"[{'given': 'Julia', 'orcid': '0000-0003-2155-5286', 'family': 'Amann', 'affiliation': None}, {'given': 'Dennis', 'orcid': '0000-0002-5977-5535', 'family': 'Vetter', 'affiliation': None}, {'given': 'Stig Nikolaj', 'orcid': None, 'family': 'Blomberg', 'affiliation': None}, {'given': 'Helle Collatz', 'orcid': None, 'family': 'Christensen', 'affiliation': None}, {'given': 'Megan', 'orcid': '0000-0002-4581-111X', 'family': 'Coffee', 'affiliation': None}, {'given': 'Sara', 'orcid': '0000-0002-5718-3982', 'family': 'Gerke', 'affiliation': None}, {'given': 'Thomas K.', 'orcid': '0000-0003-1029-4535', 'family': 'Gilbert', 'affiliation': None}, {'given': 'Thilo', 'orcid': None, 'family': 'Hagendorff', 'affiliation': None}, {'given': 'Sune', 'orcid': '0000-0002-3812-7942', 'family': 'Holm', 'affiliation': None}, {'given': 'Michelle', 'orcid': '0000-0002-8277-4733', 'family': 'Livne', 'affiliation': None}, {'given': 'Andy', 'orcid': '0000-0001-7685-0448', 'family': 'Spezzatti', 'affiliation': None}, {'given': 'Inga', 'orcid': '0000-0003-1820-6544', 'family': 'Strümke', 'affiliation': None}, {'given': 'Roberto V.', 'orcid': '0000-0003-2792-9162', 'family': 'Zicari', 'affiliation': None}, {'given': 'Vince Istvan', 'orcid': '0000-0002-8552-6954', 'family': 'Madai', 'affiliation': None}, {'given': None, 'orcid': None, 'family': None, 'affiliation': None}]",55,0,53,0,138,0
10.1609/aimag.v40i2.2850,DARPA's Explainable Artificial Intelligence Program,Wiley,2019,"Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.",40,2,44-58,"['0738-4602', '2371-9621']","[{'given': 'David', 'orcid': None, 'family': 'Gunning', 'affiliation': None}, {'given': 'David W.', 'orcid': None, 'family': 'Aha', 'affiliation': None}]",810,0,799,0,1252,0
10.2196/50475,Integrating Explainable Machine Learning in Clinical Decision Support Systems: Study Involving a Modified Design Thinking Approach,JMIR Publications Inc.,2024,"
            Background
            Though there has been considerable effort to implement machine learning (ML) methods for health care, clinical implementation has lagged. Incorporating explainable machine learning (XML) methods through the development of a decision support tool using a design thinking approach is expected to lead to greater uptake of such tools.
          
          
            Objective
            This work aimed to explore how constant engagement of clinician end users can address the lack of adoption of ML tools in clinical contexts due to their lack of transparency and address challenges related to presenting explainability in a decision support interface.
          
          
            Methods
            We used a design thinking approach augmented with additional theoretical frameworks to provide more robust approaches to different phases of design. In particular, in the problem definition phase, we incorporated the nonadoption, abandonment, scale-up, spread, and sustainability of technology in health care (NASSS) framework to assess these aspects in a health care network. This process helped focus on the development of a prognostic tool that predicted the likelihood of admission to an intensive care ward based on disease severity in chest x-ray images. In the ideate, prototype, and test phases, we incorporated a metric framework to assess physician trust in artificial intelligence (AI) tools. This allowed us to compare physicians’ assessments of the domain representation, action ability, and consistency of the tool.
          
          
            Results
            Physicians found the design of the prototype elegant, and domain appropriate representation of data was displayed in the tool. They appreciated the simplified explainability overlay, which only displayed the most predictive patches that cumulatively explained 90% of the final admission risk score. Finally, in terms of consistency, physicians unanimously appreciated the capacity to compare multiple x-ray images in the same view. They also appreciated the ability to toggle the explainability overlay so that both options made it easier for them to assess how consistently the tool was identifying elements of the x-ray image they felt would contribute to overall disease severity.
          
          
            Conclusions
            The adopted approach is situated in an evolving space concerned with incorporating XML or AI technologies into health care software. We addressed the alignment of AI as it relates to clinician trust, describing an approach to wire framing and prototyping, which incorporates the use of a theoretical framework for trust in the design process itself. Moreover, we proposed that alignment of AI is dependent upon integration of end users throughout the larger design process. Our work shows the importance and value of engaging end users prior to tool development. We believe that the described approach is a unique and valuable contribution that outlines a direction for ML experts, user experience designers, and clinician end users on how to collaborate in the creation of trustworthy and usable XML-based clinical decision support tools.
          ",8,,e50475,['2561-326X'],"[{'given': 'Michael', 'orcid': '0000-0002-1550-9956', 'family': 'Shulha', 'affiliation': None}, {'given': 'Jordan', 'orcid': '0000-0002-6363-441X', 'family': 'Hovdebo', 'affiliation': None}, {'given': 'Vinita', 'orcid': '0000-0002-4023-6822', 'family': 'D’Souza', 'affiliation': None}, {'given': 'Francis', 'orcid': '0009-0006-7442-5695', 'family': 'Thibault', 'affiliation': None}, {'given': 'Rola', 'orcid': '0000-0002-1848-9073', 'family': 'Harmouche', 'affiliation': None}]",2,0,2,0,6,0
10.3389/fcomp.2023.1096257,"Measures for explainable AI: Explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance",Frontiers Media SA,2023,"If a user is presented an AI system that portends to explain how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? This question entails some key concepts of measurement such as explanation goodness and trust. We present methods for enabling developers and researchers to: (1) Assess the a priori goodness of explanations, (2) Assess users' satisfaction with explanations, (3) Reveal user's mental model of an AI system, (4) Assess user's curiosity or need for explanations, (5) Assess whether the user's trust and reliance on the AI are appropriate, and finally, (6) Assess how the human-XAI work system performs. The methods we present derive from our integration of extensive research literatures and our own psychometric evaluations. We point to the previous research that led to the measurement scales which we aggregated and tailored specifically for the XAI context. Scales are presented in sufficient detail to enable their use by XAI researchers. For Mental Model assessment and Work System Performance, XAI researchers have choices. We point to a number of methods, expressed in terms of methods' strengths and weaknesses, and pertinent measurement issues.",5,,,['2624-9898'],"[{'given': 'Robert R.', 'orcid': None, 'family': 'Hoffman', 'affiliation': None}, {'given': 'Shane T.', 'orcid': None, 'family': 'Mueller', 'affiliation': None}, {'given': 'Gary', 'orcid': None, 'family': 'Klein', 'affiliation': None}, {'given': 'Jordan', 'orcid': None, 'family': 'Litman', 'affiliation': None}]",108,2,106,0,127,0
10.3389/fcomp.2023.1117848,"Explainable AI: roles and stakeholders, desirements and challenges",Frontiers Media SA,2023,"IntroductionThe purpose of the Stakeholder Playbook is to enable the developers of explainable AI systems to take into account the different ways in which different stakeholders or role-holders need to “look inside” the AI/XAI systems.MethodWe conducted structured cognitive interviews with senior and mid-career professionals who had direct experience either developing or using AI and/or autonomous systems.ResultsThe results show that role-holders need access to others (e.g., trusted engineers and trusted vendors) for them to be able to develop satisfying mental models of AI systems. They need to know how it fails and misleads as much as they need to know how it works. Some stakeholders need to develop an understanding that enables them to explain the AI to someone else and not just satisfy their own sense-making requirements. Only about half of our interviewees said they always wanted explanations or even needed better explanations than the ones that were provided. Based on our empirical evidence, we created a “Playbook” that lists explanation desires, explanation challenges, and explanation cautions for a variety of stakeholder groups and roles.DiscussionThis and other findings seem surprising, if not paradoxical, but they can be resolved by acknowledging that different role-holders have differing skill sets and have different sense-making desires. Individuals often serve in multiple roles and, therefore, can have different immediate goals. The goal of the Playbook is to help XAI developers by guiding the development process and creating explanations that support the different roles.",5,,,['2624-9898'],"[{'given': 'Robert R.', 'orcid': None, 'family': 'Hoffman', 'affiliation': None}, {'given': 'Shane T.', 'orcid': None, 'family': 'Mueller', 'affiliation': None}, {'given': 'Gary', 'orcid': None, 'family': 'Klein', 'affiliation': None}, {'given': 'Mohammadreza', 'orcid': None, 'family': 'Jalaeian', 'affiliation': None}, {'given': 'Connor', 'orcid': None, 'family': 'Tate', 'affiliation': None}]",2,0,2,0,10,0
10.3389/fcomp.2024.1521066,Human-AI collaboration is not very collaborative yet: a taxonomy of interaction patterns in AI-assisted decision making from a systematic review,Frontiers Media SA,2025,"Leveraging Artificial Intelligence (AI) in decision support systems has disproportionately focused on technological advancements, often overlooking the alignment between algorithmic outputs and human expectations. A human-centered perspective attempts to alleviate this concern by designing AI solutions for seamless integration with existing processes. Determining what information AI should provide to aid humans is vital, a concept underscored by explainable AI's efforts to justify AI predictions. However, how the information is presented, e.g., the sequence of recommendations and solicitation of interpretations, is equally crucial as complex interactions may emerge between humans and AI. While empirical studies have evaluated human-AI dynamics across domains, a common vocabulary for human-AI interaction protocols is lacking. To promote more deliberate consideration of interaction designs, we introduce a taxonomy of interaction patterns that delineate various modes of human-AI interactivity. We summarize the results of a systematic review of AI-assisted decision making literature and identify trends and opportunities in existing interactions across application domains from 105 articles. We find that current interactions are dominated by simplistic collaboration paradigms, leading to little support for truly interactive functionality. Our taxonomy offers a tool to understand interactivity with AI in decision-making and foster interaction designs for achieving clear communication, trustworthiness, and collaboration.",6,,,['2624-9898'],"[{'given': 'Catalina', 'orcid': None, 'family': 'Gomez', 'affiliation': None}, {'given': 'Sue Min', 'orcid': None, 'family': 'Cho', 'affiliation': None}, {'given': 'Shichang', 'orcid': None, 'family': 'Ke', 'affiliation': None}, {'given': 'Chien-Ming', 'orcid': None, 'family': 'Huang', 'affiliation': None}, {'given': 'Mathias', 'orcid': None, 'family': 'Unberath', 'affiliation': None}]",0,0,0,0,0,0
10.3389/fpsyg.2021.589585,"Human–Autonomy Teaming: Definitions, Debates, and Directions",Frontiers Media SA,2021,"Researchers are beginning to transition from studying human–automation interaction to human–autonomy teaming. This distinction has been highlighted in recent literature, and theoretical reasons why the psychological experience of humans interacting with autonomy may vary and affect subsequent collaboration outcomes are beginning to emerge (de Visser et al., 2018; Wynne and Lyons, 2018). In this review, we do a deep dive into human–autonomy teams (HATs) by explaining the differences between automation and autonomy and by reviewing the domain of human–human teaming to make inferences for HATs. We examine the domain of human–human teaming to extrapolate a few core factors that could have relevance for HATs. Notably, these factors involve critical social elements within teams that are central (as argued in this review) for HATs. We conclude by highlighting some research gaps that researchers should strive toward answering, which will ultimately facilitate a more nuanced and complete understanding of HATs in a variety of real-world contexts.",12,,,['1664-1078'],"[{'given': 'Joseph B.', 'orcid': None, 'family': 'Lyons', 'affiliation': None}, {'given': 'Katia', 'orcid': None, 'family': 'Sycara', 'affiliation': None}, {'given': 'Michael', 'orcid': None, 'family': 'Lewis', 'affiliation': None}, {'given': 'August', 'orcid': None, 'family': 'Capiola', 'affiliation': None}]",65,0,65,0,120,0
10.3389/frai.2023.1250725,Defining human-AI teaming the human-centered way: a scoping review and network analysis,Frontiers Media SA,2023,"IntroductionWith the advancement of technology and the increasing utilization of AI, the nature of human work is evolving, requiring individuals to collaborate not only with other humans but also with AI technologies to accomplish complex goals. This requires a shift in perspective from technology-driven questions to a human-centered research and design agenda putting people and evolving teams in the center of attention. A socio-technical approach is needed to view AI as more than just a technological tool, but as a team member, leading to the emergence of human-AI teaming (HAIT). In this new form of work, humans and AI synergistically combine their respective capabilities to accomplish shared goals.MethodsThe aim of our work is to uncover current research streams on HAIT and derive a unified understanding of the construct through a bibliometric network analysis, a scoping review and synthetization of a definition from a socio-technical point of view. In addition, antecedents and outcomes examined in the literature are extracted to guide future research in this field.ResultsThrough network analysis, five clusters with different research focuses on HAIT were identified. These clusters revolve around (1) human and (2) task-dependent variables, (3) AI explainability, (4) AI-driven robotic systems, and (5) the effects of AI performance on human perception. Despite these diverse research focuses, the current body of literature is predominantly driven by a technology-centric and engineering perspective, with no consistent definition or terminology of HAIT emerging to date.DiscussionWe propose a unifying definition combining a human-centered and team-oriented perspective as well as summarize what is still needed in future research regarding HAIT. Thus, this work contributes to support the idea of the Frontiers Research Topic of a theoretical and conceptual basis for human work with AI systems.",6,,,['2624-8212'],"[{'given': 'Sophie', 'orcid': None, 'family': 'Berretta', 'affiliation': None}, {'given': 'Alina', 'orcid': None, 'family': 'Tausch', 'affiliation': None}, {'given': 'Greta', 'orcid': None, 'family': 'Ontrup', 'affiliation': None}, {'given': 'Björn', 'orcid': None, 'family': 'Gilles', 'affiliation': None}, {'given': 'Corinna', 'orcid': None, 'family': 'Peifer', 'affiliation': None}, {'given': 'Annette', 'orcid': None, 'family': 'Kluge', 'affiliation': None}]",7,0,7,0,33,0
10.48550/arxiv.2210.11584,Towards Human-centered Explainable AI: User Studies for Model Explanations,arXiv,2022,,,,,,"[{'given': 'Yao', 'orcid': None, 'family': 'Rong', 'affiliation': None}, {'given': 'Tobias', 'orcid': None, 'family': 'Leemann', 'affiliation': None}, {'given': 'Thai-trang', 'orcid': None, 'family': 'Nguyen', 'affiliation': None}, {'given': 'Lisa', 'orcid': None, 'family': 'Fiedler', 'affiliation': None}, {'given': 'Peizhu', 'orcid': None, 'family': 'Qian', 'affiliation': None}, {'given': 'Vaibhav', 'orcid': None, 'family': 'Unhelkar', 'affiliation': None}, {'given': 'Tina', 'orcid': None, 'family': 'Seidel', 'affiliation': None}, {'given': 'Gjergji', 'orcid': None, 'family': 'Kasneci', 'affiliation': None}, {'given': 'Enkelejda', 'orcid': None, 'family': 'Kasneci', 'affiliation': None}]",6,0,6,0,7,0
10.48550/arxiv.2301.09656,Selective Explanations: Leveraging Human Input to Align Explainable AI,arXiv,2023,"While a vast collection of explainable AI (XAI) algorithms have been developed in recent years, they are often criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective-a fundamental property of human explanations-by selectively presenting a subset from a large set of model reasons based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small sample.This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three out of a broader possible set of paradigms based on our proposed framework: in Study 1, we ask the participants to provide their own input to generate selective explanations, with either open-ended or critique-based input. In Study 2, we show participants selective explanations based on input from a panel of similar users (annotators). Our experiments demonstrate the promise of selective explanations in reducing over-reliance on AI and improving decision outcomes and subjective perceptions of the AI, but also paint a nuanced picture that attributes some of these positive effects to the opportunity to provide one's own input to augment AI explanations. Overall, our work proposes a novel XAI framework inspired by human communication behaviors and demonstrates its potentials to encourage future work to better align AI explanations with human production and consumption of explanations. CCS Concepts: • Human-centered computing → Collaborative and social computing; • Computing methodologies → Artificial intelligence; • Applied computing → Law, social and behavioral sciences.",,,,,"[{'given': 'Vivian', 'orcid': None, 'family': 'Lai', 'affiliation': None}, {'given': 'Yiming', 'orcid': None, 'family': 'Zhang', 'affiliation': None}, {'given': 'Chacha', 'orcid': None, 'family': 'Chen', 'affiliation': None}, {'given': 'Q. Vera', 'orcid': None, 'family': 'Liao', 'affiliation': None}, {'given': 'Chenhao', 'orcid': None, 'family': 'Tan', 'affiliation': None}]",4,0,4,0,5,0
10.5220/0009382903780385,Human-agent Explainability: An Experimental Case Study on the Filtering of Explanations,SCITEPRESS - Science and Technology Publications,2020,,,,,[],"[{'given': 'Yazan', 'orcid': None, 'family': 'Mualla', 'affiliation': 'CIAD, Univ. Bourgogne Franche-Comté, UTBM, 90010 Belfort, France, --- Select a Country ---'}, {'given': 'Igor', 'orcid': None, 'family': 'Tchappi', 'affiliation': 'CIAD, Univ. Bourgogne Franche-Comté, UTBM, 90010 Belfort, France, Faculty of Sciences, University of Ngaoundere, B.P. 454 Ngaoundere, Cameroon, --- Select a Country ---'}, {'given': 'Amro', 'orcid': None, 'family': 'Najjar', 'affiliation': 'AI-Robolab/ICR, Computer Science and Communications, University of Luxembourg, 4365 Esch-sur-Alzette, Luxembourg, --- Select a Country ---'}, {'given': 'Timotheus', 'orcid': None, 'family': 'Kampik', 'affiliation': 'Department of Computing Science, Umeå University, 90187 Umeå, Sweden, --- Select a Country ---'}, {'given': 'Stéphane', 'orcid': None, 'family': 'Galland', 'affiliation': 'CIAD, Univ. Bourgogne Franche-Comté, UTBM, 90010 Belfort, France, --- Select a Country ---'}, {'given': 'Christophe', 'orcid': None, 'family': 'Nicolle', 'affiliation': 'CIAD, Univ. Bourgogne Franche-Comté, UB, 21000 Dijon, France, --- Select a Country ---'}]",5,0,5,0,8,0
