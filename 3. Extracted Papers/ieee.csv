ieee,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
XAI for Communication Networks,S. Mukherjee; J. Rupe; J. Zhu,"Next-Gen Systems, CableLabs, Santa Clara, CA; Wired Networking, CableLabs, Louisville, CO; Wired Networking, CableLabs, Louisville, CO",2022 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),26 Dec 2022,2022,,,359,364,"Explainable AI (XAI) is a topic of intense activity in the research community today. However, for AI models deployed in the critical infrastructure of communications networks, explainability alone is not enough to earn the trust of network operations teams comprising human experts with many decades of collective experience. In the present work we discuss some use cases in communications networks and state some of the additional properties, including accountability, that XAI models would have to satisfy before they can be widely deployed. In particular, we advocate for a human-in-the-Ioop approach to train and validate XAI models. Additionally, we discuss the use cases of XAI models around improving data preprocessing and data augmentation techniques, and refining data labeling rules for producing consistently labeled network datasets.",,978-1-6654-7679-9,10.1109/ISSREW55968.2022.00093,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985214,XAI;trust;accountability;communications networks,Wireless communication;Friction;Supervised learning;Refining;Human in the loop;Software reliability;Communication networks,,4,,13,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences
Fault Tolerance Tool for Human and Machine Interaction & Application to Civilian Aircraft,J. Viaña; K. Cohen,"Department of the Aerospace Engineering & Engineering Mechanics, University of Cincinnati, Cincinnati, USA; Department of the Aerospace Engineering & Engineering Mechanics, University of Cincinnati, Cincinnati, USA",2019 IEEE Latin American Conference on Computational Intelligence (LA-CCI),19 Mar 2020,2019,,,1,2,"Enhancing human-machine interaction is critical to aerospace applications. An essential requirement in safety critical systems is the clear need to guarantee trustworthiness of a system as well as V&V (Verification and Validation). However, the current state of the art concerning decision support systems lacks effective tools in this area. The Coherence Function Package, introduced in this research, is a tool towards providing assurance that the action needed has the approval of both the human and the machine in terms of SAFETY. These algorithms shed light on the future of an Explainable Artificial Intelligence (XAI, [1]), that fosters a synergy between these two factors. This vital requirement that has been further underscored after the tragic events of the Boeing 737 Max 8 crashes [2]. Preliminary results show that the proposed approach is not only able to detect any errors in the system, it also assists in circumventing conflicts leading to incoherence and suggests a preferred solution in real-time.",,978-1-7281-5666-8,10.1109/LA-CCI47412.2019.9037045,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9037045,Human-Machine Interaction;Flight Critical Systems;Aircraft Safety;Tautology;Implications;Equivalences;Recursive Functions,Aircraft;Artificial intelligence;Coherence;Tools;Man-machine systems;Software;Aircraft navigation,,,,7,IEEE,19 Mar 2020,,,IEEE,IEEE Conferences
A tool for assessing the text legibility of digital human machine interfaces,R. Lew; R. L. Boring; T. A. Ulrich,"Virtual Technology and Design Department, University of Idaho, Moscow, ID, USA; Virtual Technology and Design Department, University of Idaho, Moscow, ID, USA; Human Factors, Idaho National Laboratory Idaho Falls, Idaho, USA",2015 Resilience Week (RWS),8 Oct 2015,2015,,,1,5,A tool intended to aid qualified professionals in the assessment of the legibility of text presented on a digital display is described. The assessment of legibility is primarily for the purposes of designing and analyzing human machine interfaces in accordance with NUREG-0700 and MIL-STD 1472G. The tool addresses shortcomings of existing guidelines by providing more accurate metrics of text legibility with greater sensitivity to design alternatives.,,978-1-4799-8594-4,10.1109/RWEEK.2015.7287437,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7287437,Human Factors;Text Legibility;Human Machine Interface,Guidelines;Ergonomics;Sociology;Statistics;Workstations;Testing;Standards,,,,8,IEEE,8 Oct 2015,,,IEEE,IEEE Conferences
A Temporal Type-2 Fuzzy System for Time-Dependent Explainable Artificial Intelligence,M. Kiani; J. Andreu-Perez; H. Hagras,"School of Computer and Electronic Engineering, University of Essex, Colchester, U.K.; School of Computer and Electronic Engineering, University of Essex, Colchester, U.K.; School of Computer and Electronic Engineering, University of Essex, Colchester, U.K.",IEEE Transactions on Artificial Intelligence,24 May 2023,2023,4,3,573,586,"Explainable artificial intelligence (XAI) focuses on transparent AI models and decisions, which are easy to understand, analyze, and augment by a nontechnical audience. Fuzzy logic systems (FLS)-based XAI provides an explainable framework while also modeling uncertainties in real-world environments. However, most real-life processes are not characterized by high uncertainty alone; they are also inherently time dependent, i.e., the processes are time variant. In this work, we present a novel temporal type-2 FLS-based approach for time-dependent XAI (TXAI) systems, which can account for the likelihood of a sample occurrence in the time domain by its the frequency. In the proposed temporal type-2 fuzzy sets (TT2FSs), a 4-D time-dependent membership function integrates the universe of discourse, its membership, and its frequency of occurrence across time. The TXAI system manifested better classification prowess in cross-validation tests, with a mean recall of 95.40% than a standard XAI system (based on nontemporal general type-2 fuzzy sets) that had a mean recall of 87.04%. TXAI also performed significantly better than most nonexplainable AI systems, with between 3.95% and 19.04% improvement gain in mean recall. In addition, TXAI can also outline the most likely time-dependent trajectories using the frequency and time dimensions embedded in the TXAI model; viz. given a rule at a determined time interval, what will be the next most likely rule at a subsequent time interval. In this regard, the proposed TXAI system can have profound implications for delineating the evolution of real-life time-dependent processes, such as behavioral or biological processes.",2691-4581,,10.1109/TAI.2022.3210895,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9906417,Artificial intelligence;explainable artificial intelligence;fuzzy systems;human computer interaction;human in the loop;time-varying;trusted computing,Fuzzy sets;Artificial intelligence;Uncertainty;Fuzzy logic;Computational modeling;Temperature measurement;Numerical models,,8,,30,IEEE,30 Sep 2022,,,IEEE,IEEE Journals
Explainable AI in Large Language Models: A Review,S. S; R. B. Narenthranath; R. S. B. Krishna,"School of Computing, Sathyabama Institute of Science and Technology, Chennai, India; School of Computing, Sathyabama Institute of Science and Technology, Chennai, India; School of Computing, Sathyabama Institute of Science and Technology, Chennai, India",2024 International Conference on Emerging Research in Computational Science (ICERCS),27 Feb 2025,2024,,,1,6,"Explainable AI in Large Language Models (LLMs) represents an exciting frontier in Artificial Intelligence. In recent times, LLMs provides various AI applications ranging from chatbots to content generation. While these applications are exciting, their decision-making process behind the intelligent systems plays a major role. These processes are also a mystery and they operate as ""Black boxes"", where the processes are often challenging to interpret. This paper helps to give a sense of the processes that occurs behind these decisions. It helps to understand why the AI has chosen one sentence rather than the other. It explores key breakthroughs of XAI techniques that helps in solving these complexities. Methods such as Attention Visualization, Feature Importance Analysis and Counterfactual Explanations provide insights on why certain decisions are made. These techniques provide a voice to the intricate processes and address real-world challenges, which makes it more reliable. This enables better transparency and makes it trust-worthy. XAI has the potential to address ethical concerns, enhance user trust and improve Human-AI collaboration. This research highlights how better transparency in AI is not merely a technical challenge but a foundational step towards building a future where humans and AI work together seamlessly and responsibly.",,979-8-3315-3496-7,10.1109/ICERCS63125.2024.10895578,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10895578,Explainable AI (XAI);Large Language Models (LLMs);AI Transparency;Human-AI Collaboration;Responsible AI,Visualization;Technological innovation;Explainable AI;Scientific computing;Reviews;Large language models;Predictive models;Reliability;Artificial intelligence;Intelligent systems,,,,15,IEEE,27 Feb 2025,,,IEEE,IEEE Conferences
Resource Reservation in Sliced Networks: An Explainable Artificial Intelligence (XAI) Approach,P. Barnard; I. Macaluso; N. Marchetti; L. A. DaSilva,"Connect Research Centre, Trinity College, Dublin, Ireland; Connect Research Centre, Trinity College, Dublin, Ireland; Connect Research Centre, Trinity College, Dublin, Ireland; Commonwealth Cyber Initiative, Virginia Tech, USA",ICC 2022 - IEEE International Conference on Communications,11 Aug 2022,2022,,,1530,1535,"The growing complexity of wireless networks has sparked an upsurge in the use of artificial intelligence (AI) within the telecommunication industry in recent years. In network slicing, a key component of 5G that enables network operators to lease their resources to third-party tenants, AI models may be employed in complex tasks, such as short-term resource reservation (STRR). When AI is used to make complex resource management decisions with financial and service quality implications, it is important that these decisions be understood by a human-in-the-loop. In this paper, we apply state-of-the-art techniques from the field of Explainable AI (XAI) to the problem of STRR. Using real-world data to develop an AI model for STRR, we demonstrate how our XAI methodology can be used to explain the real-time decisions of the model, to reveal trends about the model’s general behaviour, as well as aid in the diagnosis of potential faults during the model’s development. In addition, we quantitatively validate the faithfulness of the explanations across an extensive range of XAI metrics to ensure they remain trustworthy and actionable.",1938-1883,978-1-5386-8347-7,10.1109/ICC45855.2022.9838766,Science Foundation Ireland; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9838766,Explainable Artificial Intelligence (XAI);Network Resource Management (RM),Analytical models;Wireless networks;Market research;Data models;Real-time systems;Communications technology;Resource management,,14,,17,IEEE,11 Aug 2022,,,IEEE,IEEE Conferences
Explainable Artificial Intelligence Model for Predictive Maintenance in Smart Agricultural Facilities,M. Kisten; A. E. -S. Ezugwu; M. O. Olusanya,"Unit for Data Science and Computing, North-West University, Potchefstroom, South Africa; Unit for Data Science and Computing, North-West University, Potchefstroom, South Africa; Department of Computer Science and Information Technology, Sol Plaatje University, Kimberley, South Africa",IEEE Access,16 Feb 2024,2024,12,,24348,24367,"Artificial Intelligence (AI) in Smart Agricultural Facilities (SAF) often lacks explainability, hindering farmers from taking full advantage of their capabilities. This study tackles this gap by introducing a model that combines eXplainable Artificial Intelligence (XAI), with Predictive Maintenance (PdM). The model aims to provide both predictive insights and explanations across four key dimensions, namely data, model, outcome, and end-user. This approach marks a shift in agricultural AI, reshaping how these technologies are understood and applied. The model outperforms related studies, showing quantifiable improvements. Specifically, the Long-Short-Term Memory (LSTM) classifier shows a 5.81% rise in accuracy. The eXtreme Gradient Boosting (XGBoost) classifier exhibits a 7.09% higher F1 score, 10.66% increased accuracy, and a 4.29% increase in Receiver Operating Characteristic-Area Under the Curve (ROC-AUC). These results could lead to more precise maintenance predictions in real-world settings. This study also provides insights into data purity, global and local explanations, and counterfactual scenarios for PdM in SAF. It advances AI by emphasising the importance of explainability beyond traditional accuracy metrics. The results confirm the superiority of the proposed model, marking a significant contribution to PdM in SAF. Moreover, this study promotes the understanding of AI in agriculture, emphasising explainability dimensions. Future research directions are advocated, including multi-modal data integration and implementing Human-in-the-Loop (HITL) systems aimed at improving the effectiveness of AI and addressing ethical concerns such as Fairness, Accountability, and Transparency (FAT) in agricultural AI applications.",2169-3536,,10.1109/ACCESS.2024.3365586,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433503,Agriculture;smart agricultural facilities;predictive maintenance;machine learning;deep learning;explainable artificial intelligence,Artificial intelligence;Explainable AI;Predictive models;Data models;Boosting;Analytical models;Predictive maintenance;Smart agriculture;Deep learning,,11,,70,CCBY,13 Feb 2024,,,IEEE,IEEE Journals
Explainable AI for Healthcare 5.0: Opportunities and Challenges,D. Saraswat; P. Bhattacharya; A. Verma; V. K. Prasad; S. Tanwar; G. Sharma; P. N. Bokoro; R. Sharma,"Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, Gujarat, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, Gujarat, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, Gujarat, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, Gujarat, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, Gujarat, India; Department of Electrical Engineering Technology, University of Johannesburg, Johannesburg, South Africa; Department of Electrical Engineering Technology, University of Johannesburg, Johannesburg, South Africa; Centre for Inter-Disciplinary Research and Innovation, University of Petroleum and Energy Studies, Dehradun, India",IEEE Access,17 Aug 2022,2022,10,,84486,84517,"In the healthcare domain, a transformative shift is envisioned towards Healthcare 5.0. It expands the operational boundaries of Healthcare 4.0 and leverages patient-centric digital wellness. Healthcare 5.0 focuses on real-time patient monitoring, ambient control and wellness, and privacy compliance through assisted technologies like artificial intelligence (AI), Internet-of-Things (IoT), big data, and assisted networking channels. However, healthcare operational procedures, verifiability of prediction models, resilience, and lack of ethical and regulatory frameworks are potential hindrances to the realization of Healthcare 5.0. Recently, explainable AI (EXAI) has been a disruptive trend in AI that focuses on the explainability of traditional AI models by leveraging the decision-making of the models and prediction outputs. The explainability factor opens new opportunities to the black-box models and brings confidence in healthcare stakeholders to interpret the machine learning (ML) and deep learning (DL) models. EXAI is focused on improving clinical health practices and brings transparency to the predictive analysis, which is crucial in the healthcare domain. Recent surveys on EXAI in healthcare have not significantly focused on the data analysis and interpretation of models, which lowers its practical deployment opportunities. Owing to the gap, the proposed survey explicitly details the requirements of EXAI in Healthcare 5.0, the operational and data collection process. Based on the review method and presented research questions, systematically, the article unfolds a proposed architecture that presents an EXAI ensemble on the computerized tomography (CT) image classification and segmentation process. A solution taxonomy of EXAI in Healthcare 5.0 is proposed, and operational challenges are presented. A supported case study on electrocardiogram (ECG) monitoring is presented that preserves the privacy of local models via federated learning (FL) and EXAI for metric validation. The case-study is supported through experimental validation. The analysis proves the efficacy of EXAI in health setups that envisions real-life model deployments in a wide range of clinical applications.",2169-3536,,10.1109/ACCESS.2022.3197671,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852458,Explainable AI;healthcare 50;metrics;deep learning,Medical services;Artificial intelligence;Predictive models;Analytical models;Prediction algorithms;Medical diagnostic imaging;Deep learning,,176,,150,CCBY,8 Aug 2022,,,IEEE,IEEE Journals
Evaluation of Icons to Support Safety Risk Monitoring of Autonomous Small Unmanned Aircraft Systems,M. Friedrich; D. Richards; J. -P. Huttner,"German Aerospace Center, Braunschweig, Germany; Thales UK, Reading, United Kingdom; German Aerospace Center, Braunschweig, Germany",2024 IEEE 4th International Conference on Human-Machine Systems (ICHMS),19 Jun 2024,2024,,,1,6,"The integration of small Unmanned Aircraft Systems (sUAS) into low-altitude urban airspace is gaining momentum. Safety challenges arise, necessitating automated (and in some cases autonomous) technical solutions. In order for the operator to monitor and engage with the sUAS an effective human-machine interface (HMI) is a vital component. Equally, the manner in how information is presented on this HMI requires careful consideration – more so when the operator may very well have more than one aerial platform under their control and/or supervision. This study explores the role of icon design for autonomous sUAS supervisory control to contribute to the growing body of research on explainable artificial intelligence. Altogether, 14 icons are proposed as representations of safety-critical functions related to autonomous sUAS operation in low-altitude urban airspace. In an online questionnaire study, 46 participants with experience in operating sUAS rated the icons on established icon-function fit metrics. The analysis of agreement scores indicates that the icons related to battery health, geofence conformance, and meteorological constraints were well-recognized, while those representing casualty risk, positional accuracy, airspace conformance, and sensor health performed poorly. These findings emphasize the importance of concreteness, familiarity, and semantic distance in icon design, where higher values positively influence icon recognition and thus icon-function fit. The integration of empirically derived icon design principles is proposed to enhance transparency in safety-critical autonomous systems. This study underscores the significance of targeted usage of unambiguous icons to facilitate deeper user understanding through making system-side decision-making processes transparent to the user, enabling more effective interaction between humans and autonomous systems.",,979-8-3503-1579-0,10.1109/ICHMS59971.2024.10555752,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555752,human machine interface;icon design;unmanned aircraft systems;autonomous systems,Measurement;Autonomous systems;Human-machine systems;Semantics;Decision making;Supervisory control;Safety,,,,24,IEEE,19 Jun 2024,,,IEEE,IEEE Conferences
A Principal Component Analysis-Based Approach for Single Morphing Attack Detection,L. Dargaud; M. Ibsen; J. Tapia; C. Busch,"Technical University of Denmark (DTU), Kongens Lyngby, Denmark; Hochschule Darmstadt (hda), Germany; Hochschule Darmstadt (hda), Germany; Hochschule Darmstadt (hda), Germany",2023 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),7 Feb 2023,2023,,,683,692,"This paper proposes an explicit method for single face image morphing attack detection, using an RGB decomposition based on Principal Component Analysis from texture patterns. Handcrafted detection algorithms can be advantageous over deep learning-based methods as they constitute increased explainability, showcased in this work by visualizing relevant face areas for morphing attack detection. Such information can be relevant for deployed systems in real-world scenarios with humans in the loop. The morphing detection capability of the proposed method is evaluated extensively across three datasets and six morphing algorithms in single, cross-dataset and cross-morphed scenarios and compared to a fine-tuned MobileNetV2 architecture. The results show how single image morphing attack detection remains challenging, especially in cross-domain scenarios involving realistic diversity of morphing algorithms, including StyleGAN-based approaches. In such conditions, the proposed method can be as good or even better than the evaluated MobileNetV2 approach.",2690-621X,979-8-3503-2056-5,10.1109/WACVW58289.2023.00075,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10031166,,Training;Learning systems;Visualization;Image color analysis;Feature extraction;Human in the loop;Detection algorithms,,6,,34,IEEE,7 Feb 2023,,,IEEE,IEEE Conferences
Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review,T. A. Bach; J. K. Kristiansen; A. Babic; A. Jacovi,"Group Research and Development, DNV, Høvik, Norway; Group Research and Development, DNV, Høvik, Norway; Group Research and Development, DNV, Høvik, Norway; Department of Computer Science, Bar Ilan University, Ramat Gan, Israel",IEEE Access,9 Aug 2024,2024,12,,106385,106414,"Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, existing research on HAII is limited, fragmented, and inconsistent. We present here a survey of that literature and recommendations for research best practices that should improve the field. We divided our investigation into the following areas: 1) terms used to describe HAII, 2) primary roles of AI-enabled systems, 3) factors that influence HAII, and 4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, seven factors influence HAII: user characteristics (e.g., user personality), user perceptions and attitudes (e.g., user biases), user expectations and experience (e.g., mismatched user expectations and experience), AI interface and features (e.g., interactive design), AI output (e.g., perceived accuracy), explainability and interpretability (e.g., level of detail, user understanding), and usage of AI (e.g., heterogeneity of environments). HAII is most measured with user-related subjective metrics (e.g., user perceptions, trust, and attitudes), and AI-assisted decision-making is the most common primary role of AI-enabled systems. Based on this review, we conclude that there are substantial research gaps in HAII. Researchers and developers need to codify HAII terminology, involve users throughout the AI lifecycle (especially during development), and tailor HAII in safety-critical industries to the users and environments.",2169-3536,,10.1109/ACCESS.2024.3437190,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10620168,Artificial intelligence;humans;measurement;methods;safety;safety-critical;society;survey;systematic literature review;technology readiness level;user,Artificial intelligence;Industries;Reviews;Task analysis;Medical services;Terminology;Safety;Human factors;Systematic literature review,,4,,145,CCBYNCND,1 Aug 2024,,,IEEE,IEEE Journals
User-Centric Explainability in Healthcare: A Knowledge-Level Perspective of Informed Machine Learning,L. Oberste; A. Heinzl,"University of Mannheim, Mannheim, Germany; University of Mannheim, Mannheim, Germany",IEEE Transactions on Artificial Intelligence,21 Jul 2023,2023,4,4,840,857,"Explaining increasingly complex machine learning will remain crucial to cope with risks, regulations, responsibilities, and human support in healthcare. However, extant explainable systems mostly provide explanations that mismatch clinical users’ conceptions and fail their expectations to leverage validated and clinically relevant information. A key to more user-centric and satisfying explanations can be seen in combining data-driven and knowledge-based systems, i.e., to utilize prior knowledge jointly with the patterns learned from data. We conduct a structured review of knowledge-informed machine learning in healthcare. In this article, we build on a framework to characterize user knowledge and prior knowledge embodied in explanations. Specifically, we explicate the types and contexts of knowledge to examine the fit between knowledge-informed approaches and users. Our results highlight that knowledge-informed machine learning is a promising paradigm to enrich former data-driven systems, yielding explanations that can increase formal understanding, convey useful medical knowledge, and are more intuitive. Although complying with medical conception, it still needs to be investigated whether knowledge-informed explanations increase medical user acceptance and trust in clinical machine learning-based information systems.",2691-4581,,10.1109/TAI.2022.3227225,OLIE; German Federal Ministry of Education and Research; Initiative “Research Campus – Public-Private Partnership for Innovation(grant numbers:13GW0387A);,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9971460,Artificial intelligence (AI) in medicine;explainable AI;human-centered AI;interpretable AI;knowledge-based systems;machine learning,Artificial intelligence;Medical diagnostic imaging;Medical services;Machine learning;Diseases;Data models;Predictive models,,12,,145,IEEE,6 Dec 2022,,,IEEE,IEEE Journals
Empowering Adaptive Human Autonomy Collaboration with Artificial Intelligence,C. Alix; D. Lafond; J. Mattioli; J. D. Heer; M. Chattington; P. -O. Robic,"Thales, Palaiseau, France; Thales Research & Technology, Quebec, Canada; Thales, Palaiseau, France; Thales Research & Technology Thales NL, Hengelo, Netherlands; Research, Technology & Innovation Thales UK, Reading, United Kingdom; Thales Global Services Thales, Velizy, France",2021 16th International Conference of System of Systems Engineering (SoSE),30 Jul 2021,2021,,,126,131,"We can now see examples emerge of intelligent distributed hybrid systems with autonomous functions pursuing a specific goal while adapting dynamically to changing environments. Such solutions are made possible by convergence of new technologies, but achieving comprehensive monitoring of the multiple interactions in their organization and functions along their life cycles, in missions and/or safety critical contexts, still challenges system (of systems) engineering practices. This paper considers the main gaps towards trusted systems of systems, including human-AI collaboration, human-machine teaming and solution effectiveness monitoring in a life cycle perspective. These gaps call for an inter-disciplinary sociotechnical approach in engineering towards Adjustable Human Autonomy Collaboration (DUAL), whose justification is outlined in this position paper.",,978-1-6654-4454-5,10.1109/SOSE52739.2021.9497497,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9497497,Human machine teaming;autonomy;adaptive automation;trust;artificial intelligence,Adaptive systems;Collaboration;Organizations;Safety;Artificial intelligence;Monitoring;Man-machine systems,,2,,25,IEEE,30 Jul 2021,,,IEEE,IEEE Conferences
Bridging Data and AIOps for Future AI Advancements with Human-in-the-Loop. The AI-DAPT Concept,S. Koussouris; T. Dalamagas; P. Figueiras; G. Pallis; N. Bountouni; V. Gkolemis; K. Perakis; D. Bibikas; C. Agostinho,"Suite5 Data Intelligence Solutions Limited, Limassol, Cyprus; Information Management Systems Institute (IMSI), Athena Research Center, ATHENA, Marousi, Greece; Intelligent Systems Associate Laboratory (LASI), Center of Technology and Systems (CTS), UNINOVA, Caparica, Portugal; University of Cyprus, Nicosia, Cyprus; Suite5 Data Intelligence Solutions Limited, Limassol, Cyprus; Information Management Systems Institute (IMSI), Athena Research Center, ATHENA, Marousi, Greece; UBITECH, Athens, Greece; Gas Supply Company Thessaloniki - Thessalia Single Member S.A., ZENITH, Thessaloniki, Greece; Intelligent Systems Associate Laboratory (LASI), Center of Technology and Systems (CTS), UNINOVA, Caparica, Portugal","2024 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)",18 Dec 2024,2024,,,1,8,"The transition of artificial intelligence (AI) from research to deployment has underscored the critical importance of leveraging data effectively in developing and evaluating AI models. Despite their pivotal role in determining performance, fairness, and robustness, data are often undervalued in AI research, lacking a data-centric focus. In response, the AI-DAPT project pioneers a data-centric approach that aligns with AI methodologies to address the pressing need for reliable and trustworthy AI systems. This paper presents the AI-DAPT project and concept, highlighting its innovative solutions. Through advanced techniques like synthetic data generation and hybrid science-guided Machine Learning (ML), AI-DAPT aims to mitigate vulnerabilities in conventional AI paradigms. The developed solutions will be demonstrated in healthcare, robotics, energy, and manufacturing scenarios, hence navigating real-world complexities.",2693-8855,979-8-3503-6243-5,10.1109/ICE/ITMC61926.2024.10794334,European Union(grant numbers:101135826);,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10794334,AI pipelines;Data pipelines;Data for AI;Hybrid Science-Guided AI,Technological innovation;Navigation;Pressing;Medical services;Machine learning;Robustness;Manufacturing;Artificial intelligence;Robots;Synthetic data,,,,36,IEEE,18 Dec 2024,,,IEEE,IEEE Conferences
Industry 5.0 and Operations Management—the Importance of Human Factors,F. Lindner; G. Reiner,"Faculty of Business Administration and Engineering, Zittau/Görlitz University of Applied Sciences, Zittau, Germany; Department of Information Systems and Operations Management, Vienna University of Economics and Business, Vienna, Austria",NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium,21 Jun 2023,2023,,,1,4,"In this position paper, we highlight the importance of human factors, especially cognition, for operations management during the transition from Industry 4.0 to 5.0 and within. We argue that the increasing prevalence of (digital) technology and data for manufacturing operations urges human-centered approaches and solutions, as well—to enable efficient and effective operations that benefit from both humans' and technologies' strengths. To stress our point, we give examples from behavioral operations management where technology may both foster or mitigate deviations from rational decision-making. In addition, we show prospects of human-AI interaction and explainable AI, specifically by using visualizations, to improve operational performance.",2374-9709,978-1-6654-7716-1,10.1109/NOMS56928.2023.10154282,Horizon Europe; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154282,human factors;behavioral manufacturing operations management;Industry 5.0;cognitive biases;visualizations,Industries;Sociotechnical systems;Software algorithms;Decision making;Bills of materials;Data visualization;Human factors,,2,,40,IEEE,21 Jun 2023,,,IEEE,IEEE Conferences
Depicting Decision-Making: A Type-2 Fuzzy Logic Based Explainable Artificial Intelligence System for Goal-Driven Simulation in the Workforce Allocation Domain,E. Ferreyra; H. Hagras; M. Kern; G. Owusu,"The Computational Intelligence Centre, School of Computer Science and Electronic Engineering, University of Essex, Colchester, UK; The Computational Intelligence Centre, School of Computer Science and Electronic Engineering, University of Essex, Colchester, UK; Applied Research, BT, Adastral Park, Martlesham Heath, Ipswich, UK; Applied Research, BT, Adastral Park, Martlesham Heath, Ipswich, UK",2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),11 Oct 2019,2019,,,1,6,"The recent years have witnessed a growing anticipation for the positive transformation of industries which adopt Artificial Intelligence (AI) for the core areas of their business activities. However, the effectiveness and reliability of such AI systems must comprise the ability to explain their data acquisition, the underlying algorithms operations and the final decisions to stakeholders, including regulators, risk managers, supervisors and end-users among others. There are plenty of areas where Explainable AI (XAI) holds the promise to be a major disruptor. Particularly, in Telecommunication Service Providers (TSPs) which is a core business activity relating to the workforce allocation domain, which, involves costly and time-consuming scheduling processes. This paper focuses on the construction of an XAI framework to assist workforce allocation based on a big bang- big crunch interval type-2 fuzzy logic system (BB-BC IT2FLS) for modelling and scaling goal-driven simulation (GDS) problems, specifically within the telecommunications industry. The obtained results reported the proposed XAI system produces similar results to opaque box models like Neural Networks (NNs) and LSTM Recurrent NNs while being able to explain the decision and operation of the employed system.",1558-4739,978-1-5386-1728-1,10.1109/FUZZ-IEEE.2019.8858933,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858933,Explainable AI;interval type-2 fuzzy logic systems;goal-driven simulation;big bang-big crunch,Artificial intelligence;Fuzzy logic;Resource management;Fuzzy sets;Uncertainty;Computational modeling,,7,,32,IEEE,11 Oct 2019,,,IEEE,IEEE Conferences
Enabling Human-Centered AI: A Methodological Perspective,W. Xu; Z. Gao,"Center for Psychological Sciences, Zhejiang University, Hangzhou, China; Dept. of Psychology and Behavioral Sciences, Zhejiang University, Hangzhou, China",2024 IEEE 4th International Conference on Human-Machine Systems (ICHMS),19 Jun 2024,2024,,,1,6,"Human-centered AI (HCAI) is a design philosophy that advocates prioritizing humans in designing, developing, and deploying intelligent systems, aiming to maximize the benefits of AI to humans and avoid potential adverse effects. While HCAI continues to influence, the lack of guidance on methodology in practice makes its adoption challenging. This paper proposes a comprehensive HCAI framework based on our previous work with integrated components, including design goals, design principles, implementation approaches, interdisciplinary teams, HCAI methods, and HCAI processes. This paper also presents a “three-layer” approach to facilitate the implementation of the framework. We believe this systematic and executable framework can overcome the weaknesses in current HCAI frameworks and the challenges currently faced in practice, putting it into action to enable HCAI further.",,979-8-3503-1579-0,10.1109/ICHMS59971.2024.10555771,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555771,Artificial intelligence;methodology;human-centered artificial intelligence;design goal;principle;process,Philosophical considerations;Systematics;Human-machine systems;Intelligent systems,,2,,26,IEEE,19 Jun 2024,,,IEEE,IEEE Conferences
Generative Adversarial Network-Based Network Intrusion Detection System for Supervisory Control and Data Acquisition System,H. N. Nguyen; T. Lan-Phan; C. -J. Song,"Department of AI and Software Engineering, School of Computing, Gachon University, Seongnam-si, Gyeonggi-do, Republic of Korea; Division of Information Technology, Viet Nam - Korea Institute of Science and Technology, Thach That, Ha Noi, Vietnam; Information Media Research Center, Korea Electronics Technology Institute, Seoul, Korea",2024 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia),10 Dec 2024,2024,,,1,3,"In the context of burgeoning cyber threats targeting Supervisory Control and Data Acquisition (SCADA) systems, this paper presents a pioneering Intrusion Detection System (IDS) grounded in Generative Adversarial Networks (GANs). The ubiquity of information technology in critical industrial processes necessitates heightened security measures, and the proposed model addresses this imperative through the fusion of GANs and IDS tailored for SCADA environments. The study delves into the evolutionary trajectory of Intrusion Detection Systems, tracing their progression from conventional signature-based methods to the adaptive capabilities afforded by artificial intelligence, particularly deep learning. Against this backdrop, the paper investigates the vulnerabilities of SCADA systems, elucidating the sophisticated attack vectors identified in recent studies. The distributed nature of SCADA systems, deployed across extensive industrial landscapes, accentuates the need for robust intrusion detection mechanisms. This research's core innovation lies in applying a GAN model to SCADA systems, leveraging the distributed network protocol version 3 (DNP3) for data access authorization. The model is trained on a dataset comprising 12 meticulously selected features from DNP3 packets, attuning it to the nuances of SCADA network traffic. Notably, the proposed GAN-based IDS demonstrates exceptional efficacy, achieving impressive accuracy. Beyond the empirical contributions, the paper envisions future directions for integrating explainable AI techniques and enhancing the interpretability of IDS models specific to SCADA environments. Moreover, collaboration between academia and industry is advocated to foster the development of comprehensive datasets that represent SCADA network traffic.",,979-8-3315-3083-9,10.1109/ICCE-Asia63397.2024.10773791,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10773791,Network Intrusion Detection System;IDS;SCADA;Generative Adversarial Networks;DNP3,Deep learning;Protocols;Accuracy;SCADA systems;Network intrusion detection;Telecommunication traffic;Generative adversarial networks;Vectors;Trajectory;Security,,1,,9,IEEE,10 Dec 2024,,,IEEE,IEEE Conferences
Colosseum: The Open RAN Digital Twin,M. Polese; L. Bonati; S. D'Oro; P. Johari; D. Villa; S. Velumani; R. Gangula; M. Tsampazi; C. Paul Robinson; G. Gemmi; A. Lacava; S. Maxenti; H. Cheng; T. Melodia,"Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA",IEEE Open Journal of the Communications Society,10 Sep 2024,2024,5,,5452,5466,"Recent years have witnessed the Open Radio Access Network (RAN) paradigm transforming the fundamental ways cellular systems are deployed, managed, and optimized. This shift is led by concepts such as openness, softwarization, programmability, interoperability, and intelligence of the network, which have emerged in wired networks through Software-defined Networking (SDN) but lag behind in cellular systems. The realization of the Open RAN vision into practical architectures, intelligent data-driven control loops, and efficient software implementations, however, is a multifaceted challenge, which requires (i) datasets to train Artificial Intelligence (AI) and Machine Learning (ML) models; (ii) facilities to test models without disrupting production networks; (iii) continuous and automated validation of the RAN software; and (iv) significant testing and integration efforts. This paper is a tutorial on how Colosseum—the world’s largest wireless network emulator with hardware in the loop—can provide the research infrastructure and tools to fill the gap between the Open RAN vision, and the deployment and commercialization of open and programmable networks. We describe how Colosseum implements an Open RAN digital twin through a high-fidelity Radio Frequency (RF) channel emulator and endto- end softwarized O-RAN and 5G-compliant protocol stacks, thus allowing users to reproduce and experiment upon topologies representative of real-world cellular deployments. Then, we detail the twinning infrastructure of Colosseum, as well as the automation pipelines for RF and protocol stack twinning. Finally, we showcase a broad range of Open RAN use cases implemented on Colosseum, including the real-time connection between the digital twin and real-world networks, and the development, prototyping, and testing of AI/ML solutions for Open RAN.",2644-125X,,10.1109/OJCOMS.2024.3447472,"O-RAN ALLIANCE; National Science Foundation(grant numbers:CNS-1925601,CNS-2112471); National Telecommunications and Information Administration (NTIA)’s Public Wireless Supply Chain Innovation Fund (PWSCIF)(grant numbers:25-60-IF011,25-60-IF054); OUSD(R&E) through Army Research Laboratory Cooperative Agreement(grant numbers:W911NF-19-2-0221);",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643670,O-RAN;Open RAN;wireless network emulation;5G;6G,Open RAN;Digital twins;Radio frequency;Containers;Testing;Software;Protocols,,9,,85,CCBY,22 Aug 2024,,,IEEE,IEEE Journals
Resilience Scores for Decision Support Using Wearable Biosignal Data with Requirements on Fair and Transparent AI,L. Paletta; H. Zeiner; M. Schneeberger; M. Pszeida; J. A. Mosbacher; J. Tschuden,"JOANNEUM RESEARCH, Forschungsgesellschaft mbH, Institute DIGITAL, Graz, Austria; JOANNEUM RESEARCH, Forschungsgesellschaft mbH, Institute DIGITAL, Graz, Austria; JOANNEUM RESEARCH, Forschungsgesellschaft mbH, Institute DIGITAL, Graz, Austria; JOANNEUM RESEARCH, Forschungsgesellschaft mbH, Institute DIGITAL, Graz, Austria; JOANNEUM RESEARCH, Forschungsgesellschaft mbH, Institute DIGITAL, Graz, Austria; JOANNEUM RESEARCH, Forschungsgesellschaft mbH, Institute DIGITAL, Graz, Austria",2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA),16 Oct 2024,2024,,,1,4,"Industry 5.0 brings human, AI, data, and robots together by supporting decision makers thus positively affecting the work balance between workers and machines. One key aspect for the daily decision-making on worker allocation in production is to consider the resilience of individual workers. Advanced manufacturing companies intend to foster well-being while avoiding illness, absenteeism and loss of resources with economic impact. We present work-in-progress on introducing human resilience as a central human factors construct for optimizing long-term effects of human-machine interaction on worker's health. For this purpose, components for computing AI-based physiological strain as well as resilience scores were developed. We also project towards adequate optimization methodology to adjust worker allocation based on resilience in time. Motivated human-centered production highly depends on the acceptance based on fairness and transparency, specifically in the context of the novel AI technologies.",1946-0759,979-8-3503-6123-0,10.1109/ETFA61755.2024.10710860,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10710860,resilience;physiological stress;optimization;decision support;fairness and transparency,Service robots;Production;Physiology;Manufacturing;Resource management;Artificial intelligence;Optimization;Manufacturing automation;Resilience;Strain,,,,26,IEEE,16 Oct 2024,,,IEEE,IEEE Conferences
Trusting Intelligent Machines: Deepening Trust Within Socio-Technical Systems,P. Andras; L. Esterle; M. Guckert; T. A. Han; P. R. Lewis; K. Milanovic; T. Payne; C. Perret; J. Pitt; S. T. Powers; N. Urquhart; S. Wells,"School of Computing and Mathematics, Keele University, Keele, Staffordshire, U.K; Department of Computer Science, Aston University, Birmingham, U.K; Fachbereich Mathematik Naturwissenschaften und Datenverarbeitung, Technische Hochschule Mittelhessen, Giessen, Hessen, Germany; Teesside University School of Computing, Middlesbrough, U.K; Department of Computer Science, Aston University, Birmingham, U.K; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K; Department of Computer Science, University of Liverpool, Liverpool, U.K; Merchiston Campus, School of Computing, Edinburgh Napier University, Edinburgh, U.K; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K; Merchiston Campus, School of Computing, Edinburgh Napier University, Edinburgh, U.K; Merchiston Campus, School of Computing, Edinburgh Napier University, Edinburgh, U.K; Merchiston Campus, School of Computing, Edinburgh Napier University, Edinburgh, U.K",IEEE Technology and Society Magazine,4 Dec 2018,2018,37,4,76,83,"Intelligent machines have reached capabilities that go beyond a level that a human being can fully comprehend without sufficiently detailed understanding of the underlying mechanisms. The choice of moves in the game Go (generated by Deep Mind?s Alpha Go Zero [1]) are an impressive example of an artificial intelligence system calculating results that even a human expert for the game can hardly retrace [2]. But this is, quite literally, a toy example. In reality, intelligent algorithms are encroaching more and more into our everyday lives, be it through algorithms that recommend products for us to buy, or whole systems such as driverless vehicles. We are delegating ever more aspects of our daily routines to machines, and this trend looks set to continue in the future. Indeed, continued economic growth is set to depend on it. The nature of human-computer interaction in the world that the digital transformation is creating will require (mutual) trust between humans and intelligent, or seemingly intelligent, machines. But what does it mean to trust an intelligent machine? How can trust be established between human societies and intelligent machines?",1937-416X,,10.1109/MTS.2018.2876107,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8558724,,Artificial intelligence;Ethics;Cognition;Intelligent systems;Game theory;Economics;Multi-agent systems,,83,,38,IEEE,4 Dec 2018,,,IEEE,IEEE Magazines
Enabling Self-Driving Networks with Machine Learning,A. S. Jacobs; R. A. Ferreira; L. Z. Granville,"Federal University of Rio Grande do Sul, Brazil; Federal University of Mato Grosso do Sul, Brazil; Federal University of Rio Grande do Sul, Brazil",NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium,21 Jun 2023,2023,,,1,6,"This work aims to enable self-driving networks by tackling the lack of trust that network operators have in Machine Learning (ML) models. We assess and scrutinize the decision-making process of ML-based classifiers used to compose a self-driving network. First, we investigate and evaluate the accuracy and credibility of classifications made by ML models used to process high-level management intents. We propose a novel conversational interface (LUMI) that allows operators to use natural language to describe how the network should behave. Second, we analyze and assess the accuracy and credibility of existing ML models’ for network security and performance. We also uncover the need to reinvent how researchers apply ML to networking problems, so we propose a new ML pipeline that introduces steps to scrutinize models using techniques from the emerging field of eXplainable Artificial Intelligence (XAI). Finally, we investigate whether there is a viable method to improve the trust of operators in the decisions made by ML models that enable self-driving networks. Our investigation led us to propose a new XAI method to extract explanations from any given black-box ML model in the form of decision trees while maintaining a manageable size, which we called TRUSTEE. Our results show that ML models widely applied to solve networking problems have not been put under proper scrutiny and can easily break when put under real-world traffic. Such models, therefore, need to be corrected to fulfill their given tasks properly.",2374-9709,978-1-6654-7716-1,10.1109/NOMS56928.2023.10154431,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154431,Self-Driving Networks;Machine Learning;Explainability;Intent-Based Networking,Natural languages;Decision making;Pipelines;Closed box;Machine learning;Network security;Human in the loop,,,,27,IEEE,21 Jun 2023,,,IEEE,IEEE Conferences
Decision Support Based on Human-Machine Collaboration Patterns: Conceptual Model and Scenario,A. Smirnov; T. Levashova,"St. Petersburg Federal Research Center of the Russian Academy of Sciences, St. Petersburg, Russia; St. Petersburg Federal Research Center of the Russian Academy of Sciences, St. Petersburg, Russia",2024 35th Conference of Open Innovations Association (FRUCT),9 May 2024,2024,,,715,724,The paper proposes a conceptual model of human-machine collaborative decision support in which humans and software agents use collaboration patterns to achieve goals of decision-making process steps. An analysis of collaboration patterns identified in multiple domains is provided and a pattern classification is offered. A sample scenario for collaborative decision support is proposed. The scenario illustrates a possible functionality of a collaborative decision support system implemented according to the conceptual model developed.,2305-7254,978-952-65246-1-0,10.23919/FRUCT61870.2024.10516361,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516361,,Decision support systems;Measurement;Analytical models;Technological innovation;Human-machine systems;Collaboration;Pattern classification,,,,25,,9 May 2024,,,IEEE,IEEE Conferences
Reliable Time Series Counterfactual Explanations Guided by ShapeDBA,P. Li; P. Hosseinzadeh; O. Bahri; S. F. Boubrahimi; S. M. Hamdi,"Department of Computer Science, Utah State University, Logan, UT; Department of Computer Science, Utah State University, Logan, UT; Department of Computer Science, Utah State University, Logan, UT; Department of Computer Science, Utah State University, Logan, UT; Department of Computer Science, Utah State University, Logan, UT",2024 IEEE International Conference on Big Data (BigData),16 Jan 2025,2024,,,1574,1579,"Artificial intelligence (AI) and algorithmic decision-making are profoundly shaping various aspects of society, with applications in healthcare, business, education, etc. As these systems become more integral to high-stakes decisions, concerns about their transparency and interpretability are growing. To address these concerns, explainable AI (XAI) methods have been developed, with counterfactual explanations emerging as a powerful tool. Counterfactuals help users understand AI decisions by demonstrating how small changes in input could alter the outcome, providing a clear and intuitive way to interpret AI behavior. Despite their potential, generating valid, interpretable, and efficient counterfactual explanations is particularly challenging in time series domains, where data points are interdependent. In this paper, we introduce a novel approach to counterfactual explanations guided by ShapeDTW Barycenter Averaging (ShapeDBA). By integrating ShapeDBA into the counterfactual generation process, we ensure that the produced explanations are not only valid and interpretable but also efficient to generate. Our approach provides counterfactuals that align closely with human intuition while maintaining the computational efficiency required for practical deployment. This work represents a significant step forward in the development of interpretable AI systems, particularly in the complex domain of time series analysis.",2573-2978,979-8-3503-6248-0,10.1109/BigData62323.2024.10825447,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825447,Explainable Artificial Intelligence (XAI);counterfactual explanations;time series classification;DTW barycenter averaging,Explainable AI;Computational modeling;Time series analysis;Education;Decision making;Medical services;Big Data;Computational efficiency;Reliability;Business,,,,25,IEEE,16 Jan 2025,,,IEEE,IEEE Conferences
User centric explanations: a breakthrough for explainable models,A. Hassan; M. A. A. Abdulhak; R. Bin Sulaiman; H. Kahtan,"Institute of IR 4.0 (IIR4.0), The National University of Malaysia, Bangi, Malaysia; Consultant & Researcher, Madinah, Saudi Arabia; Institute of IR 4.0 (IIR4.0), The National University of Malaysia, Bangi, Malaysia; Department of Software Engineering, Faculty of Computer Science and Information Technology, Universiti Malaya, Kuala Lumpur, Malaysia",2021 International Conference on Information Technology (ICIT),26 Jul 2021,2021,,,702,707,"Thanks to recent developments in explainable Deep Learning models, researchers have shown that these models can be incredibly successful and provide encouraging results. However, a lack of model interpretability can hinder the efficient implementation of Deep Learning models in real-world applications. This has encouraged researchers to develop and design a large number of algorithms to support transparency. Although studies have raised awareness of the importance of explainable artificial intelligence, the question of how to solve the needs of real users to understand artificial intelligence remains unanswered. In this paper, we provide an overview of the current state of the research field at Human-Centered Machine Learning and new methods for user-centric explanations for deep learning models. Furthermore, we outline future directions for interpretable machine learning and discuss the challenges facing this research field, as well as the importance and motivation behind developing user-centric explanations for Deep Learning models.",,978-1-6654-2870-5,10.1109/ICIT52682.2021.9491641,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491641,explainable artificial intelligence;human-AI interaction;machine learning,Deep learning;Industries;Machine learning algorithms;Social sciences;Collaboration;Medical services;Learning (artificial intelligence),,1,,51,IEEE,26 Jul 2021,,,IEEE,IEEE Conferences
Exploring Trust and Explainability of Unmanned Systems,J. R. Field,"Naval Architecture & Eng. Dept., Carderock Division, Naval Sea Systems Command, West Bethesda, MD, USA",OCEANS 2023 - Limerick,12 Sep 2023,2023,,,1,6,"Explainability (often referred to as interpretability) refers to the concept of providing context to an AI/ML model and its output, thereby assisting a human user in understanding the system’s decision-making process. The concept of explainability is especially helpful when we consider the high cognitive load and intense data management strategies that are required for current human-in-the-loop operations in use today. The work presented here aims to provide an explainability framework for autonomous systems, to provide system transparency, and enhance operator awareness. This work served to develop a novel method of sorting and evaluating data streams taken from an operational system, to filter and transmit data packages based on mission conditions. Post mission analysis yielded apparent trends in messaging hierarchy, indicating that certain health and status data streams were consistently prioritized, regardless of the pre-defined metrics. Additional data analysis was performed to evaluate sensor outputs with respect to health and status messaging. This process included conducting data correlation and data characterization, to evaluate relationships between data streams, identify data associated with nominal behavior, and perform anomaly detection. Key functional categories were developed, in which the system’s behavior is mapped to a corresponding component (and its respective data stream). Monitoring subsystem performance assists with cross-referencing sensor outputs, to confirm data projections and/or aid in identifying faulty readings. Furthermore, the application of anomaly detection algorithms is coupled with data correlation and/or pattern recognition to extract the most important and salient information.",,979-8-3503-3226-1,10.1109/OCEANSLimerick52467.2023.10244349,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10244349,Autonomy;Explainability;UUV,Measurement;Correlation;Oceans;Market research;Human in the loop;Behavioral sciences;Anomaly detection,,,,10,USGov,12 Sep 2023,,,IEEE,IEEE Conferences
Securing SCADA Systems: A Protocol-Based Intrusion Detection Approach with Shapley Analysis,S. Ustebay; B. B. Akgün; P. Gaj,"Computer Engineering, Istanbul Medeniyet University, Istanbul, Türkiye; Computer Engineering, Istanbul Medeniyet University, Istanbul, Türkiye; Istributed systems and IT devices, Silesian University of Technology, Gliwice, Poland",2024 Innovations in Intelligent Systems and Applications Conference (ASYU),28 Nov 2024,2024,,,1,7,"This paper presents an approach to enhancing the security of Supervisory Control and Data Acquisition (SCADA) systems through the development of Explainable Intrusion Detection Systems (X-IDS). SCADA systems play a crucial role in managing industrial processes and critical infrastructure, yet they are increasingly targeted by cyberattacks, posing significant risks to operational integrity. Leveraging Machine Learning (ML) techniques, particularly focused on network protocols involved in communication with SCADA HMI, this study proposes a protocol-based layered IDS framework to mitigate potential threats. Evaluation of the proposed models demonstrates promising results, achieving high accuracy rates surpassing previous studies. The performance metrics, including accuracy, precision, recall, and F1 scores, along with Brier scores, are comprehensively analyzed to gauge the effectiveness of the models. Additionally, Explainable Machine Learning (XAI) techniques such as SHAP values provide transparent insights into the model's decision-making process. The study highlights the importance of securing HMIs within SCADA systems and offers valuable insights for future research directions in enhancing overall network security.",2770-7946,979-8-3503-7943-3,10.1109/ASYU62119.2024.10756979,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10756979,IDS;SCADA (Supervisory Control and Data Acquisition);Human Machine Interface;Machine Learning;Industrial Control Systems (ICS);Explainable Intrusion Detection Systems (x-ids),Measurement;Technological innovation;Accuracy;Protocols;Decision making;SCADA systems;Intrusion detection;Machine learning;Network security;Intelligent systems,,,,17,IEEE,28 Nov 2024,,,IEEE,IEEE Conferences
Towards Model-informed Precision Dosing with Expert-in-the-loop Machine Learning,Y. Kang; Y. -W. Chiu; M. -Y. Lin; F. -Y. Su; S. -T. Huang,"Department of Information Management, National Sun Yat-sen University, Kaohsiung, Taiwan; Department of Internal Medicine, Division of Nephrology, Kaohsiung Medical University Hospital, Kaohsiung, Taiwan; Department of Internal Medicine, Division of Nephrology, Kaohsiung Medical University Hospital, Kaohsiung, Taiwan; Department of Information Management, National Sun Yat-sen University, Kaohsiung, Taiwan; Department of Information Management, National Sun Yat-sen University, Kaohsiung, Taiwan",2021 IEEE 22nd International Conference on Information Reuse and Integration for Data Science (IRI),17 Nov 2021,2021,,,342,347,"Machine Learning (ML) and its applications have been transforming our lives but it is also creating issues related to the development of fair, accountable, transparent, and ethical Artificial Intelligence. As the ML models are not fully comprehensible yet, it is obvious that we still need humans to be part of algorithmic decision-making processes. In this paper, we consider a ML framework that may accelerate model learning and improve its interpretability by incorporating human experts into the model learning loop. We propose a novel human-in-the-loop ML framework aimed at dealing with learning problems that the cost of data annotation is high and the lack of appropriate data to model the association between the target tasks and the input features. With an application to precision dosing, our experimental results show that the approach can learn interpretable rules from data and may potentially lower experts' workload by replacing data annotation with rule representation editing. The approach may also help remove algorithmic bias by introducing experts' feedback into the iterative model learning process.",,978-1-6654-3875-9,10.1109/IRI51335.2021.00053,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9599105,Human-in-the-loop Machine Learning;Rule Learning;Representation Learning;Explainable Artificial Intelligence;Model-informed Precision Dosing,Ethics;Machine learning algorithms;Costs;Annotations;Decision making;Machine learning;Data models,,2,,36,IEEE,17 Nov 2021,,,IEEE,IEEE Conferences
Visualizing and Comparing Machine Learning Predictions to Improve Human-AI Teaming on the Example of Cell Lineage,J. Hong; R. Maciejewski; A. Trubuil; T. Isenberg,"Arizona State University, Tempe, AZ, USA; Arizona State University, Tempe, AZ, USA; Université Paris-Saclay, INRAE, Villeurbanne, France; Université Paris-Saclay, CNRS, Inria, Palaiseau, France",IEEE Transactions on Visualization and Computer Graphics,28 Feb 2024,2024,30,4,1956,1969,"We visualize the predictions of multiple machine learning models to help biologists as they interactively make decisions about cell lineage—the development of a (plant) embryo from a single ovum cell. Based on a confocal microscopy dataset, traditionally biologists manually constructed the cell lineage, starting from this observation and reasoning backward in time to establish their inheritance. To speed up this tedious process, we make use of machine learning (ML) models trained on a database of manually established cell lineages to assist the biologist in cell assignment. Most biologists, however, are not familiar with ML, nor is it clear to them which model best predicts the embryo's development. We thus have developed a visualization system that is designed to support biologists in exploring and comparing ML models, checking the model predictions, detecting possible ML model mistakes, and deciding on the most likely embryo development. To evaluate our proposed system, we deployed our interface with six biologists in an observational study. Our results show that the visual representations of machine learning are easily understandable, and our tool, LineageD+, could potentially increase biologists’ working efficiency and enhance the understanding of embryos.",1941-0506,,10.1109/TVCG.2023.3302308,Inria's Naviscope Project; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10239303,Cell lineage;comparing ML predictions;human -AI teaming;machine learning;plant biology;visual analytics;visualization,Biological system modeling;Biology;Predictive models;Data models;Embryo;Three-dimensional displays;Data visualization,"Humans;Cell Lineage;Computer Graphics;Machine Learning;Databases, Genetic",1,,47,IEEE,4 Sep 2023,,,IEEE,IEEE Journals
Explainable Transformer Prototypes for Medical Diagnoses,U. Demir; D. Jha; Z. Zhang; E. Keles; B. Allen; A. K. Katsaggelos; U. Bagci,"Machine & Hybrid Intelligence Lab, Northwestern University, Chicago, IL, USA; Machine & Hybrid Intelligence Lab, Northwestern University, Chicago, IL, USA; Machine & Hybrid Intelligence Lab, Northwestern University, Chicago, IL, USA; Machine & Hybrid Intelligence Lab, Northwestern University, Chicago, IL, USA; Machine & Hybrid Intelligence Lab, Northwestern University, Chicago, IL, USA; Machine & Hybrid Intelligence Lab, Northwestern University, Chicago, IL, USA; Machine & Hybrid Intelligence Lab, Northwestern University, Chicago, IL, USA",2024 IEEE International Symposium on Biomedical Imaging (ISBI),22 Aug 2024,2024,,,1,5,"Deployments of artificial intelligence in medical diagnostics mandate not just accuracy and efficacy but also trust, emphasizing the need for explainability in machine decisions. The recent trend in automated medical image diagnostics leans towards the deployment of Transformer-based architectures, credited to their impressive capabilities. Since the self-attention feature of transformers contributes towards identifying crucial regions during the classification process, they enhance the trustability of the methods. However, the complex intricacies of these attention mechanisms may fall short of effectively pinpointing the regions of interest directly influencing AI decisions. Our research endeavors to innovate a unique attention block that underscores the correlation between ’regions’ rather than ’pixels’. To address this challenge, we introduce an innovative system grounded in prototype learning, featuring an advanced self-attention mechanism that goes beyond conventional ad-hoc visual explanation techniques by offering comprehensible visual insights. A combined quantitative and qualitative methodological approach was used to demonstrate the effectiveness of the proposed method on the large-scale NIH chest X-ray dataset. Experimental results showed that our proposed method offers a promising direction for explain-ability, which can lead to the development of more trustable systems, which can facilitate easier and rapid adoption of such technology into routine clinics. Code is available at www.github.com/NUBagcilab/r2r_proto.",1945-8452,979-8-3503-1333-8,10.1109/ISBI56570.2024.10635182,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10635182,Prototypes;Transformers;Explainability;Interpretability;Medical Diagnosis,Visualization;Prototypes;Transformers;Robustness;Classification algorithms;Medical diagnosis;Artificial intelligence,,1,,21,IEEE,22 Aug 2024,,,IEEE,IEEE Conferences
Covid-19 Computer-Aided Diagnosis through AI-Assisted CT Imaging Analysis: Deploying a Medical AI System,D. Gerogiannis; A. Arsenos; D. Kollias; D. Nikitopoulos; S. Kollias,"Dept. of Computer Science & Engineering, University of Ioannina / AIandMe SMPC, Ioannina, Greece; School of Electrical & Computer Engineering, National Technical University of Athens, Greece; School of Electronic Engineering & Computer Science, Queen Mary University of London, UK; GRNET, National Infrastructures for Research & Technology; School of Electrical & Computer Engineering, National Technical University of Athens, Greece",2024 IEEE International Symposium on Biomedical Imaging (ISBI),22 Aug 2024,2024,,,1,4,"Computer-aided diagnosis (CAD) systems stand out as potent aids for physicians in identifying the novel Coronavirus Disease 2019 (COVID-19) through medical imaging modalities. In this paper, we showcase the integration and reliable and fast deployment of a state-of-the-art AI system designed to automatically analyze CT images, offering infection probability for the swift detection of COVID-19. The suggested system, comprising both classification and segmentation components, is anticipated to reduce physicians’ detection time and enhance the overall efficiency of COVID-19 detection. We successfully surmounted various challenges, such as data discrepancy and anonymisation, testing the time-effectiveness of the model, and data security, enabling reliable and scalable deployment of the system on both cloud and edge environments. Additionally, our AI system assigns a probability of infection to each 3D CT scan and enhances explainability through anchor set similarity, facilitating timely confirmation and segregation of infected patients by physicians.",1945-8452,979-8-3503-1333-8,10.1109/ISBI56570.2024.10635484,Hellenic Foundation for Research and Innovation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10635484,Medical Imaging;COVID-19 Diagnosis;Deep Learning;Microservices;Cloud;Edge;Sandbox,COVID-19;Solid modeling;Three-dimensional displays;Computed tomography;Medical services;Reliability engineering;Computer aided diagnosis,,3,,7,IEEE,22 Aug 2024,,,IEEE,IEEE Conferences
Interpretable Classification of Pneumonia Infection Using eXplainable AI (XAI-ICP),R. -K. Sheu; M. S. Pardeshi; K. -C. Pai; L. -C. Chen; C. -L. Wu; W. -C. Chen,"Department of Computer Science, Tunghai University, Taichung, Taiwan; Artificial Intelligence (AI) Center, Tunghai University, Taichung, Taiwan; College of Engineering, Tunghai University, Taichung, Taiwan; Department of Computer Science, Tunghai University, Taichung, Taiwan; Department of Critical Care Medicine, Taichung Veterans General Hospital, Taichung, Taiwan; Department of Computer Science, Tunghai University, Taichung, Taiwan",IEEE Access,27 Mar 2023,2023,11,,28896,28919,"Open-box models in the medical domain have high acceptance and demand by many medical examiners. Even though the accuracy predicted by most of convolutional neural network (CNN) models is high, it is still not convincing as the detailed discussion regarding the outcome is semi-transparent in the functioning process. Pneumonia is known as one of the top contagious infections that makes most of the population affected due to low immunity. Therefore, the goal of this paper is to implement an interpretable classification of pneumonia infection using eXplainable AI (XAI-ICP). Thus, XAI-ICP is the highly efficient system designed to solve this challenge by adapting to the recent population health conditions. The aim is to design an interpretable deep classification and transfer learning based evaluation for pneumonia infection classification. The model is primarily pre-trained using the open Chest X-Ray (CXR) dataset from National Institutes of Health (NIH). Whereas, the training input and testing given to this system is Taichung Veterans General Hospital (TCVGH) for independent learning, Taiwan + VinDr open dataset for transfer learning of pneumonia affected patients with labeled CXR images possessing three features of infiltrate, cardiomegaly and effusion. The data labeling is performed by the medical examiners with the XAI human-in-the-loop approach. XAI-ICP demonstrates the XAI based reconfigurable DCNN with human-in-the-loop as a novel approach. The interpretable deep classification provides detailed transparency analysis and transfer learning for competitive accuracy. The purpose of this work, to design a re-configurable model that can continuously improve itself by using a feedback system and provide feasibility for the model deployment across multiple countries to provide an efficient system for the pneumonia infection classification. The designed model then provides detailed decisions taken at each step as transparency and features used within the algorithm for the pneumonia classification during the hospitalization. Thus, the scope can be given as explainable AI usage for the diagnosis classification using data preprocessing and interpretable deep convolutional neural network by the CXR evaluation. The accuracy achieved by using independent learning classification is 92.14% and is further improved based on successive transfer learning based evaluation is 93.29%. The XAI-ICP model adapts to the different populations by using transfer learning, while providing competitive results to the affected conditions.",2169-3536,,10.1109/ACCESS.2023.3255403,"Ministry of Science and Technology, Taiwan(grant numbers:MOST 111-2321-B-075A-001);",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10065420,Medical XAI;XAI pnuemonia;transfer learning;pneumonia infection,Pulmonary diseases;Transfer learning;Artificial intelligence;Solid modeling;Medical diagnostic imaging;Convolutional neural networks;Data models,,21,,58,CCBYNCND,10 Mar 2023,,,IEEE,IEEE Journals
Federated Learning-Based Explainable Anomaly Detection for Industrial Control Systems,T. T. Huong; T. P. Bac; K. N. Ha; N. V. Hoang; N. X. Hoang; N. T. Hung; K. P. Tran,"School of Electrical and Electronic Engineering, Hanoi University of Science and Technology, Hai Ba Trung, Hanoi, Vietnam; School of Electronic Engineering, Soongsil University, Seoul, South Korea; School of Electrical and Electronic Engineering, Hanoi University of Science and Technology, Hai Ba Trung, Hanoi, Vietnam; School of Electrical and Electronic Engineering, Hanoi University of Science and Technology, Hai Ba Trung, Hanoi, Vietnam; School of Electrical and Electronic Engineering, Hanoi University of Science and Technology, Hai Ba Trung, Hanoi, Vietnam; School of Electrical and Electronic Engineering, Hanoi University of Science and Technology, Hai Ba Trung, Hanoi, Vietnam; Génie et Matériaux Textiles (GEMTEX), National Higher School of Arts and Textile Industries (ENSAIT), University of Lille, Lille, France",IEEE Access,24 May 2022,2022,10,,53854,53872,"We are now witnessing the rapid growth of advanced technologies and their application, leading to Smart Manufacturing (SM). The Internet of Things (IoT) is one of the main technologies used to enable smart factories, which is connecting all industrial assets, including machines and control systems, with the information systems and the business processes. Industrial Control Systems of smart IoT-based factories are one of the top industries attacked by numerous threats, especially unknown and novel attacks. As a result, with the distributed structure of plenty of IoT front-end sensing devices in SM, an effectively distributed anomaly detection (AD) architecture for IoT-based ICSs should: achieve high detection performance, train and learn new data patterns in a fast time scale, and have lightweight to be deployed on resource-constrained edge devices. To date, most solutions for anomaly detection have not fulfilled all of these requirements. In addition, the interpretability of why an instance is predicted to be abnormal is hardly concerned. In this paper, we propose the so- called FedeX architecture to address those challenges. The experiments show that FedeX outperforms 14 other existing anomaly detection solutions on all detection metrics with the liquid storage data set. And with Recall of 1 and F1-score of 0.9857, it also outperforms those solutions on the SWAT data set. FedeX is also proven to be fast in terms of training time of about 7.5 minutes and lightweight in terms of hardware requirement with memory consumption of 14%, allowing us to deploy anomaly detection tasks on top of edge computing infrastructure and in real-time. Besides, FedeX is considered as one of the frameworks at the forefront of interpreting the predicted anomalies by using XAI, which enables experts to make quick decisions and trust the model more.",2169-3536,,10.1109/ACCESS.2022.3173288,Hanoi University of Science and Technology (HUST)(grant numbers:T2021-PC-010);,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770834,Anomaly detection;ICS;federated learning;XAI;VAE;SVDD,Anomaly detection;Integrated circuits;Training;Industrial Internet of Things;Computational modeling;Support vector machines;Edge computing,,40,,30,CCBY,9 May 2022,,,IEEE,IEEE Journals
Adaptive Autonomy in Human-on-the-Loop Vision-Based Robotics Systems,S. Abraham; Z. Carmichael; S. Banerjee; R. VidalMata; A. Agrawal; M. N. Al Islam; W. Scheirer; J. Cleland-Huang,"Computer Vision Research Lab; Computer Vision Research Lab; Computer Vision Research Lab; Computer Vision Research Lab; Department of Computer Science and Engineering, DroneResponse Lab, University of Notre Dame, South Bend, USA; Department of Computer Science and Engineering, DroneResponse Lab, University of Notre Dame, South Bend, USA; Computer Vision Research Lab; Department of Computer Science and Engineering, DroneResponse Lab, University of Notre Dame, South Bend, USA",2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN),8 Jul 2021,2021,,,113,120,"Computer vision approaches are widely used by autonomous robotic systems to sense the world around them and to guide their decision making as they perform diverse tasks such as collision avoidance, search and rescue, and object manipulation. High accuracy is critical, particularly for Human-on-the-loop (HoTL) systems where decisions are made autonomously by the system, and humans play only a supervisory role. Failures of the vision model can lead to erroneous decisions with potentially life or death consequences. In this paper, we propose a solution based upon adaptive autonomy levels, whereby the system detects loss of reliability of these models and responds by temporarily lowering its own autonomy levels and increasing engagement of the human in the decision-making process. Our solution is applicable for vision-based tasks in which humans have time to react and provide guidance. When implemented, our approach would estimate the reliability of the vision task by considering uncertainty in its model, and by performing covariate analysis to determine when the current operating environment is illmatched to the model’s training data. We provide examples from DroneResponse, in which small Unmanned Aerial Systems are deployed for Emergency Response missions, and show how the vision model’s reliability would be used in addition to confidence scores to drive and specify the behavior and adaptation of the system’s autonomy. This workshop paper outlines our proposed approach and describes open challenges at the intersection of Computer Vision and Software Engineering for the safe and reliable deployment of vision models in the decision making of autonomous systems.",,978-1-6654-4470-5,10.1109/WAIN52551.2021.00025,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474360,computer vision;adaptive autonomy;safety;uncertainty,Adaptation models;Adaptive systems;Uncertainty;Computational modeling;Decision making;Data models;Software reliability,,13,,53,IEEE,8 Jul 2021,,,IEEE,IEEE Conferences
Harmony Unleashed: Exploring the Ethical and Philosophical Aspects of Machine Learning in Human-Robot Collaboration for Industry 5.0,M. H. Zafar; F. Sanfilippo; T. Blažauskas,"Department of Engineering Sciences, University of Agder, Grimstad, Norway; Department of Engineering Sciences, University of Agder, Grimstad, Norway; Department of Software Engineering, Kaunas University of Technology, Kaunas, Lithuania",2023 IEEE Symposium Series on Computational Intelligence (SSCI),1 Jan 2024,2023,,,1775,1780,"As Industry 5.0 emerges by blending advanced technologies with human-centered approaches, the integration of machine learning (ML) in human-robot collaboration (HRC) becomes increasingly prominent. This paper explores the philosophy and ethics underlying the application of machine learning in Industry 5.0, specifically focusing on HRC. It examines the ethical considerations, philosophical implications, and potential challenges that arise in this evolving paradigm. The paper emphasises the need for a thoughtful and ethical approach to ensure the beneficial and responsible use of ML in Industry 5.0.",2472-8322,978-1-6654-3065-4,10.1109/SSCI52147.2023.10371798,University of Agder; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10371798,Industry 5.0;Human-Robot Collaboration;Artificial Intelligence;Ethics;Philosophy,Industries;Ethics;Privacy;Technological innovation;Shape;Service robots;Collaboration,,3,,12,IEEE,1 Jan 2024,,,IEEE,IEEE Conferences
LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial Search for Adaptive Arguments,P. Aryan,"Department of Computer Science, Birla Institute of Technology and Science, Pilani - Dubai Campus, Dubai, UAE",2024 IEEE Conference on Engineering Informatics (ICEI),12 Mar 2025,2024,,,1,11,"This paper introduces DebateBrawl, an innovative AI-powered debate platform that integrates Large Language Models (LLMs), Genetic Algorithms (GA), and Adversarial Search (AS) to create an adaptive and engaging debating experience. DebateBrawl addresses the limitations of traditional LLMs in strategic planning by incorporating evolutionary optimization and game-theoretic techniques. The system demonstrates remarkable performance in generating coherent, contextually relevant arguments while adapting its strategy in real-time. Experimental results involving 23 debates show balanced outcomes between AI and human participants, with the AI system achieving an average score of ${2. 7 2}$ compared to the human average of ${2. 6 7}$ out of 10. User feedback indicates significant improvements in debating skills and a highly satisfactory learning experience, with 85% of users reporting improved debating abilities and ${7 8 \%}$ finding the AI opponent appropriately challenging. The system’s ability to maintain high factual accuracy (92% compared to ${7 8 \%}$ in human-only debates) while generating diverse arguments addresses critical concerns in AI-assisted discourse. DebateBrawl not only serves as an effective educational tool but also contributes to the broader goal of improving public discourse through AI-assisted argumentation. The paper discusses the ethical implications of AI in persuasive contexts and outlines the measures implemented to ensure responsible development and deployment of the system, including robust fact-checking mechanisms and transparency in decision-making processes.",,979-8-3315-0577-6,10.1109/ICEI64305.2024.10912343,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912343,Machine Learning;Deep Learning;Generative AI;Large Language Models;Genetic Algorithms;Adversarial Search,Training;Ethics;Visualization;Large language models;Decision making;Strategic planning;Problem-solving;Artificial intelligence;Research and development;Genetic algorithms,,1,,21,IEEE,12 Mar 2025,,,IEEE,IEEE Conferences
Watch Out for Explanations: Information Type and Error Type Affect Trust and Situational Awareness in Automated Vehicles,Y. Ding; L. Jia; N. Du,"School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA; School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA; School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA",IEEE Transactions on Human-Machine Systems,,2025,PP,99,1,10,"Trust and situational awareness (SA) are critical for the acceptance and safety of automated vehicles (AVs). While AV explanations with different information types have been studied to enhance drivers' trust and SA, their effectiveness remains unclear when AVs make errors that do not trigger takeover requests. This study investigated the effects of information type, error type, and their interaction on drivers' trust in AVs, SA, and their relationships. We recruited 300 participants in an online video study with a 3 (information type: why, how, why + how) × 3 (error type: false alarm, miss, correct [no error]) mixed design. How information describes the vehicle's action, while why information refers to the reason for the vehicle's action. Linear mixed models showed that false alarms and misses were associated with lower SA compared with correct scenarios, but possibly due to different reasons. Compared with correct scenarios, both false alarms and misses were associated with lower trust, with misses even lower than false alarms, possibly due to the varying severity of potential consequences. Compared with why and why + how information, how information was generally associated with lower SA and a higher potential of overtrust in false alarms. Trust and SA had a negative linear relationship in misses and false alarms, while no correlations were found in correct scenarios. To mitigate potential overtrust and misinterpretation of situations when AVs make errors, it is crucial to maintain higher SA. We recommend including why information in AV explanations and deploying AV decision systems that are less miss-prone.",2168-2305,,10.1109/THMS.2025.3558437,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10980640,Automated vehicles (AVs);explainable artificial intelligence;human factors;human–machine interface;situational awareness (SA);trust,Vehicles;Visualization;Safety;Complexity theory;Automation;Artificial intelligence;Timing;Monitoring;Marine vehicles;Human-machine systems,,,,,IEEE,30 Apr 2025,,,IEEE,IEEE Early Access Articles
On the Role of Explainable Machine Learning for Secure Smart Vehicles,M. Scalas; G. Giacinto,"Department of Electrical and Electronic Engineering, University of Cagliari, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Cagliari, Italy",2020 AEIT International Conference of Electrical and Electronic Technologies for Automotive (AEIT AUTOMOTIVE),4 Jan 2021,2020,,,1,6,"The concept of mobility is experiencing a serious transformation due to the Mobility-as-a-Service paradigm. Accordingly, vehicles, usually referred to as smart, are seeing their architecture revamped to integrate connection to the outside environment (V2X) and autonomous driving. A significant part of these innovations is enabled by machine learning. However, deploying such systems raises some concerns. First, the complexity of the algorithms often prevents understanding what these models learn, which is relevant in the safety-critical context of mobility. Second, several studies have demonstrated the vulnerability of machine learning-based algorithms to adversarial attacks. For these reasons, research on the explainability of machine learning is raising. In this paper, we then explore the role of interpretable machine learning in the ecosystem of smart vehicles, with the goal of figuring out if and in what terms explanations help to design secure vehicles. We provide an overview of the potential uses of explainable machine learning, along with recent work in the literature that has started to investigate the topic, including from the perspectives of human-agent systems and cyber-physical systems. Our analysis highlights both benefits and criticalities in employing explanations.",,978-8-8872-3749-8,10.23919/AEITAUTOMOTIVE50086.2020.9307431,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9307431,Explainability;Cybersecurity;Machine learning;Mobility;Smart Vehicles;Automotive;Connected Cars;Autonomous Driving,Machine learning;Automobiles;Autonomous vehicles;Automotive engineering;Hardware;Standards;Security,,6,,35,,4 Jan 2021,,,IEEE,IEEE Conferences
Machine Learning/Artificial Intelligence for Sensor Data Fusion–Opportunities and Challenges,E. Blasch; T. Pham; C. -Y. Chong; W. Koch; H. Leung; D. Braines; T. Abdelzaher,"Air Force Office of Scientific Research, Arlington, VA, USA; Army Research Laboratory, Adelphi, MD, USA; Independent Consultant, San Jose, CA, USA; Fraunhofer FKIE, Wachtberg, Germany; University of Calgary, Calgary, Canada; IBM, Hants, U.K.; University of Illinois at Urbana-Champaign, Urbana, IL, USA",IEEE Aerospace and Electronic Systems Magazine,7 Jul 2021,2021,36,7,80,93,"During Fusion 2019 Conference (https://www.fusion2019.org/program.html), leading experts presented ideas on the historical, contemporary, and future coordination of artificial intelligence/machine learning (AI/ML) with sensor data fusion (SDF). While AI/ML and SDF concepts have had a rich history since the early 1900s—emerging from philosophy and psychology—it was not until the dawn of computers that both AI/ML and SDF researchers initiated discussions on how mathematical techniques could be implemented for real-time analysis. ML, and in particular deep learning, has demonstrated tremendous success in computer vision, natural language understanding, and data analytics. As a result, ML has been proposed as the solution to many problems that inherently include multi-modal data. For example, success in autonomous vehicles has validated the promise of ML with SDF, but additional research is needed to explain, understand, and coordinate heterogeneous data analytics for situation awareness. The panel identified opportunities for merging AI/ML and SDF such as computational efficiency, improved decision making, expanding knowledge, and providing security; while highlighting challenges for multi-domain operations, human-machine teaming, and ethical deployment strategies.",1557-959X,,10.1109/MAES.2020.3049030,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9475913,,Uncertainty;Data analysis;Psychology;Reinforcement learning;Big Data;Data models;Robustness,,88,,38,USGov,7 Jul 2021,,,IEEE,IEEE Magazines
Trustworthy AI Development Guidelines for Human System Interaction,C. S. Wickramasinghe; D. L. Marino; J. Grandio; M. Manic,"Virginia Commonwealth University, Richmond, Virginia; Virginia Commonwealth University, Richmond, Virginia; Virginia Commonwealth University, Richmond, Virginia; Virginia Commonwealth University, Richmond, Virginia",2020 13th International Conference on Human System Interaction (HSI),17 Jul 2020,2020,,,130,136,"Artificial Intelligence (AI) is influencing almost all areas of human life. Even though these AI-based systems frequently provide state-of-the-art performance, humans still hesitate to develop, deploy, and use AI systems. The main reason for this is the lack of trust in AI systems caused by the deficiency of transparency of existing AI systems. As a solution, “Trustworthy AI” research area merged with the goal of defining guidelines and frameworks for improving user trust in AI systems, allowing humans to use them without fear. While trust in AI is an active area of research, very little work exists where the focus is to build human trust to improve the interactions between human and AI systems. In this paper, we provide a concise survey on concepts of trustworthy AI. Further, we present trustworthy AI development guidelines for improving the user trust to enhance the interactions between AI systems and humans, that happen during the AI system life cycle.",2158-2254,978-1-7281-7392-4,10.1109/HSI49210.2020.9142644,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142644,Trustworthy AI;Transparency;Explainable AI;Human System Interactions;Human Machine Interactions;AI Life Cycle,Artificial intelligence;Guidelines;Data models;Organizations;Robustness;Optimization;Testing,,27,,32,IEEE,17 Jul 2020,,,IEEE,IEEE Conferences
Scalable Interactive Machine Learning for Future Command and Control,A. Madison; E. Novoseller; V. G. Goecks; B. T. Files; N. Waytowich; A. Yu; V. J. Lawhern; S. Thurman; C. Kelshaw; K. McDowell,"Humans in Complex Systems, U.S. DEVCOM Army Research Laboratory, Aberdeen Proving Ground, MD, USA; Humans in Complex Systems, U.S. DEVCOM Army Research Laboratory, Aberdeen Proving Ground, MD, USA; Humans in Complex Systems, U.S. DEVCOM Army Research Laboratory, Aberdeen Proving Ground, MD, USA; Humans in Complex Systems, U.S. DEVCOM Army Research Laboratory, Aberdeen Proving Ground, MD, USA; Humans in Complex Systems, U.S. DEVCOM Army Research Laboratory, Aberdeen Proving Ground, MD, USA; Humans in Complex Systems, U.S. DEVCOM Army Research Laboratory, Aberdeen Proving Ground, MD, USA; Humans in Complex Systems, U.S. DEVCOM Army Research Laboratory, Aberdeen Proving Ground, MD, USA; Humans in Complex Systems, U.S. DEVCOM Army Research Laboratory, Aberdeen Proving Ground, MD, USA; U.S. Mission Command Battle Lab, Futures Branch, Ft. Leavenworth, KS, USA; Humans in Complex Systems, U.S. DEVCOM Army Research Laboratory, Aberdeen Proving Ground, MD, USA",2024 International Conference on Military Communication and Information Systems (ICMCIS),5 Jun 2024,2024,,,1,10,"Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in complex, dynamic situations; 2) fostering resilient human-AI teams through optimizing roles, configurations, and trust; and 3) scaling algorithms and human-AI teams for flexibility across a range of potential contexts and situations.",,979-8-3503-7319-6,10.1109/ICMCIS61231.2024.10540933,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10540933,Scalable Interactive Machine Learning;Artificial Intelligence;Human-AI Teaming;Command and Control,Command and control systems;Military communication;Machine learning algorithms;Heuristic algorithms;Human intelligence;Decision making;Machine learning,,3,,126,USGov,5 Jun 2024,,,IEEE,IEEE Conferences
Using Decision Support in Human-in-the-Loop Experimental Design Toward Building Trustworthy Autonomous Systems,J. M. Gregory; F. Sanchez; E. Lancaster; A. -A. Agha-Mohammadi; S. K. Gupta,"DEVCOM Army Research Laboratory, Adelphi, MD, USA; Booz Allen Hamilton, Washington, D.C., USA; Booz Allen Hamilton, Washington, D.C., USA; NASA Jet Propulsion Laboratory, Pasadena, CA, USA; Viterbi School of Engineering, University of Southern California, CA, USA",2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),13 Nov 2023,2023,,,205,212,"Experimental design of autonomous systems involves defining experimental inputs to maximize the experimenter’s information gained, minimize costs, and balance risk. This effectively leads to improved understanding and trustworthiness, which are necessary for deployment in realworld settings. Since experimental design is inherently a human-in-the-loop, sequential decision making problem, and decisions are being made about complex systems, an investigation into decision-making quality and decision-supporting methods is warranted. In this work, we investigate a decision support system (DSS) to augment the human’s experimental design decision making abilities, and conduct an exploratory user study to investigate the potential for decision support. Our findings show that experimenters, including experienced field roboticists, make suboptimal decisions and mistakes during the experimental design process, which suggests robotics research could benefit from DSSs. Our proposed DSS shows promise in some select aspects of experimental design, including helping to reduce suboptimal decisions, and participants in the user study reported favorable opinions of using such a system, including a sense of usefulness and lack of burden. The broader implication of this work is the identification of decision support in experimental design as one way to help bridge the gap between academia and industry by way of accelerated, informative experimentation and increased system explainability.",1944-9437,979-8-3503-3670-2,10.1109/RO-MAN57019.2023.10309571,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10309571,,Decision support systems;Industries;Costs;Autonomous systems;Service robots;Buildings;Spread spectrum communication,,,,31,IEEE,13 Nov 2023,,,IEEE,IEEE Conferences
Cryptomining Detection in Container Clouds Using System Calls and Explainable Machine Learning,R. R. Karn; P. Kudva; H. Huang; S. Suneja; I. M. Elfadel,"Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE; IBM Research, Yorktown Heights, NY, USA; IBM Research, Yorktown Heights, NY, USA; IBM Research, Yorktown Heights, NY, USA; Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE",IEEE Transactions on Parallel and Distributed Systems,26 Oct 2020,2021,32,3,674,691,"The use of containers in cloud computing has been steadily increasing. With the emergence of Kubernetes, the management of applications inside containers (or pods) is simplified. Kubernetes allows automated actions like self-healing, scaling, rolling back, and updates for the application management. At the same time, security threats have also evolved with attacks on pods to perform malicious actions. Out of several recent malware types, cryptomining has emerged as one of the most serious threats with its hijacking of server resources for cryptocurrency mining. During application deployment and execution in the pod, a cryptomining process, started by a hidden malware executable can be run in the background, and a method to detect malicious cryptomining software running inside Kubernetes pods is needed. One feasible strategy is to use machine learning (ML) to identify and classify pods based on whether or not they contain a running process of cryptomining. In addition to such detection, the system administrator will need an explanation as to the reason(s) of the ML's classification outcome. The explanation will justify and support disruptive administrative decisions such as pod removal or its restart with a new image. In this article, we describe the design and implementation of an ML-based detection system of anomalous pods in a Kubernetes cluster by monitoring Linux-kernel system calls (syscalls). Several types of cryptominers images are used as containers within an anomalous pod, and several ML models are built to detect such pods in the presence of numerous healthy cloud workloads. Explainability is provided using SHAP, LIME, and a novel auto-encoding-based scheme for LSTM models. Seven evaluation metrics are used to compare and contrast the explainable models of the proposed ML cryptomining detection engine.",1558-2183,,10.1109/TPDS.2020.3029088,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9215018,Cryptomining;docker;kubernetes;containers;machine learning;explainability;pod;anomaly,Containers;Cloud computing;Malware;Machine learning;Cryptocurrency;Data mining,,59,,93,CCBY,6 Oct 2020,,,IEEE,IEEE Journals
Effects of Robot Competency and Motion Legibility on Human Correction Feedback,S. Wang; A. Wang; S. Goncharova; B. Scassellati; T. Fitzgerald,"Yale University, New Haven, CT, USA; Yale University, New Haven, CT, USA; Phillips Exeter Academy, Exeter, NH, USA; Yale University, New Haven, CT, USA; Yale University, New Haven, CT, USA",2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI),30 Apr 2025,2025,,,789,799,"As robot deployments become more commonplace, people are likely to take on the role of supervising robots (i.e., correcting their mistakes) rather than directly teaching them. Prior works on Learning from Corrections (LfC) have relied on three key assumptions to interpret human feedback: (1) people correct the robot only when there is significant task objective divergence; (2) people can accurately predict if a correction is necessary; and (3) people trade off precision and physical effort when giving corrections. In this work, we study how two key factors (robot competency and motion legibility) affect how people provide correction feedback and their implications on these existing assumptions. We conduct a user study $(N=60)$ under an LfC setting where participants supervise and correct a robot performing pick-and-place tasks. We find that people are more sensitive to suboptimal behavior by a highly competent robot compared to an incompetent robot when the motions are legible $(p=0.0015)$ and predictable $(p=0.0055)$. In addition, people also tend to withhold necessary corrections $(p < 0.0001)$ when supervising an incompetent robot and are more prone to offering unnecessary ones $(p=0.0171)$ when supervising a highly competent robot. We also find that physical effort positively correlates with correction precision, providing empirical evidence to support this common assumption. We also find that this correlation is significantly weaker for an incompetent robot with legible motions than an incompetent robot with predictable motions $(p=0.0075)$. Our findings offer insights for accounting for competency and legibility when designing robot interaction behaviors and learning task objectives from corrections.",,979-8-3503-7893-1,10.1109/HRI61500.2025.10974241,"National Science Foundation(grant numbers:IIS-2106690,IIS-1955653); Office of Naval Research(grant numbers:N00014-24-1-2124);",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10974241,Interactive Robot Learning;Learning from Corrections;Kinesthetic Teaching,Correlation;Education;Human-robot interaction;Robot sensing systems;Robot learning;Behavioral sciences;Reliability;Robots,,,,98,IEEE,30 Apr 2025,,,IEEE,IEEE Conferences
BeGREEN: Beyond 5G Energy Efficient Networking by Hardware Acceleration and AI-Driven Management of Network Functions,M. Ghoraishi; J. O. Sallent; M. Catalan-Cid; G. Bielsa; J. -F. Esteban-Rivas; V. Sark; J. G. Teran; S. Pryor,"Gigasys Solutions Ltd, UK; Universitat Politècnica de Catalunya, Spain; i2CAT Foundation, Barcelona, Spain; RAN Innovation Department, Telefonica, Spain; RAN Innovation Department, Telefonica, Spain; IHP GmbH - Innovations for High Performance Microelectronics, Germany; IHP GmbH - Innovations for High Performance Microelectronics, Germany; Accelleran, Belgium",2023 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),26 Jul 2023,2023,,,717,722,"This paper presents a technical overview of BeGREEN project, a Horizon Europe, Smart Networks and Services Joint-Undertaking (SNS-JU) Phase 1 project kicked off on January 1, 2023 [1]. This paper is intended to describe BeGREEN's technical scope and objectives. These objectives aim at improving energy efficiency of the beyond 5G (B5G) networks. BeGREEN technical agenda includes analysis of the combined energy and spectrum efficiency of the B5G networks, based on massive multiple-input-multiple-output (mMIMO) scenarios. The project proposes a novel architecture that includes several innovative solutions. An offloading engine is used for hardware acceleration that is a solution for compute-heavy physical layer processing in 5G new radio (5G NR) mMIMO and beyond to improve the processing performance and energy efficiency. The architecture also includes joint communication and sensing (JCAS) for improving energy efficiency of the physical layer functions by, e.g., efficient beam-search and beam tracking, and uses reconfigurable intelligent surfaces (RIS) as an enabler for JCAS. BeGreenproposes an artificial intelligence (AI)-assisted energy-aware “Intelligent Plane” as an additional plane along with user plane and data plane, that allows the data, model, and inference to be seamlessly exchanged between network functions. The project also proposes an AI Engine that is consist of an execution environment that can host AI models and will manage their lifecycle and access to data.",2575-4912,979-8-3503-1102-0,10.1109/EuCNC/6GSummit58263.2023.10188307,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10188307,Energy Efficiency;Beyond 5G;Hardware Acceleration;Intelligent Plane;O-RAN Based Interface;AI Engine,5G mobile communication;Computer architecture;Physical layer;Energy efficiency;Data models;Sensors;Artificial intelligence,,,,10,IEEE,26 Jul 2023,,,IEEE,IEEE Conferences
Holistic Explainability Requirements for End-to-End Machine Learning in IoT Cloud Systems,M. -L. Nguyen; T. Phung; D. -H. Ly; H. -L. Truong,"Department of Computer Science, Aalto University, Finland; Department of Computer Science, Aalto University, Finland; Department of Computer Science, Aalto University, Finland; Department of Computer Science, Aalto University, Finland",2021 IEEE 29th International Requirements Engineering Conference Workshops (REW),27 Oct 2021,2021,,,188,194,"End-to-end machine learning (ML) in Internet of Things (IoT) Cloud systems consists of multiple processes, covering data, model, and service engineering, and involves multiple stakeholders. Therefore, to be able to explain ML to relevant stakeholders, it is important to identify explainability requirements in a holistic manner. In this paper, we present our methodology to address explainability requirements for end-to-end ML in developing ML services to be deployed within IoT Cloud systems. We identify and classify explainability requirements engineering through (i) involvement of relevant stakeholders, (ii) end-to-end data, model, and service engineering processes, and (iii) multiple explainability aspects. We present our work with a case study of predictive maintenance for Base Transceiver Stations (BTS) in the telco domain.",,978-1-6654-1898-0,10.1109/REW53955.2021.00034,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582347,,Cloud computing;Conferences;Machine learning;Data models;Transceivers;Stakeholders;Requirements engineering,,7,,41,IEEE,27 Oct 2021,,,IEEE,IEEE Conferences
SAFEXPLAIN: Safe and Explainable Critical Embedded Systems Based on AI,J. Abella; J. Perez; C. Englund; B. Zonooz; G. Giordana; C. Donzella; F. J. Cazorla; E. Mezzetti; I. Serra; A. Brando; I. Agirre; F. Eizaguirre; T. H. Bui; E. Arani; F. Sarfraz; A. Balasubramaniam; A. Badar; I. Bloise; L. Feruglio; I. Cinelli; D. Brighenti; D. Cunial,"Barcelona Supercomputing Center, Spain; Ikerlan Technology Research Centre, Basque Research and Technology Alliance (BRTA), Spain; RISE Research Institutes of Sweden, Sweden; Navinfo Europe, The Netherlands; AIKO s.r.l., Italy; Exida Development s.r.l., Italy; Barcelona Supercomputing Center, Spain; Barcelona Supercomputing Center, Spain; Barcelona Supercomputing Center, Spain; Barcelona Supercomputing Center, Spain; Ikerlan Technology Research Centre, Basque Research and Technology Alliance (BRTA), Spain; Ikerlan Technology Research Centre, Basque Research and Technology Alliance (BRTA), Spain; RISE Research Institutes of Sweden, Sweden; Navinfo Europe, The Netherlands; Navinfo Europe, The Netherlands; Navinfo Europe, The Netherlands; Navinfo Europe, The Netherlands; AIKO s.r.l., Italy; AIKO s.r.l., Italy; AIKO s.r.l., Italy; Exida Engineering s.r.l., Italy; Exida Engineering s.r.l., Italy","2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2 Jun 2023,2023,,,1,6,"Deep Learning (DL) techniques are at the heart of most future advanced software functions in Critical Autonomous AI-based Systems (CAIS), where they also represent a major competitive factor. Hence, the economic success of CAIS industries (e.g., automotive, space, railway) depends on their ability to design, implement, qualify, and certify DL-based software products under bounded effort/cost. However, there is a fundamental gap between Functional Safety (FUSA) requirements on CAIS and the nature of DL solutions. This gap stems from the development process of DL libraries and affects high-level safety concepts such as (1) explainability and traceability, (2) suitability for varying safety requirements, (3) FUSA-compliant implementations, and (4) real-time constraints. As a matter of fact, the data-dependent and stochastic nature of DL algorithms clashes with current FUSA practice, which instead builds on deterministic, verifiable, and pass/fail test-based software. The SAFEXPLAIN project tackles these challenges and targets by providing a flexible approach to allow the certification - hence adoption - of DL-based solutions in CAIS building on: (1) DL solutions that provide end-to-end traceability, with specific approaches to explain whether predictions can be trusted and strategies to reach (and prove) correct operation, in accordance to certification standards; (2) alternative and increasingly sophisticated design safety patterns for DL with varying criticality and fault tolerance requirements; (3) DL library implementations that adhere to safety requirements; and (4) computing platform configurations, to regain determinism, and probabilistic timing analyses, to handle the remaining non-determinism.",1558-1101,979-8-3503-9624-9,10.23919/DATE56975.2023.10137128,Spanish Ministry of Science and Innovation(grant numbers:PID2019-107255GBC21/AEI/10.13039/501100011033);,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10137128,,Buildings;Software algorithms;Software;Rail transportation;Libraries;Safety;Timing,,3,,31,,2 Jun 2023,,,IEEE,IEEE Conferences
Safe Explainable Agents for Autonomous Navigation using Evolving Behavior Trees,N. Potteiger; X. Koutsoukos,"Department of Computer Science, Vanderbilt University, Nashville, TN, USA; Department of Computer Science, Vanderbilt University, Nashville, TN, USA",2023 IEEE International Conference on Assured Autonomy (ICAA),10 Aug 2023,2023,,,44,52,"Machine learning and reinforcement learning are increasingly used to solve complex tasks in autonomous systems. However, autonomous agents represented by large neural networks are not transparent leading to their assurability and trustworthiness becoming critical challenges. Large models also result in a lack of interpretability which causes severe obstacles related to trust in autonomous agents and human-machine teaming. In this paper, we leverage the hierarchical structure of behavior trees and hierarchical reinforcement learning to develop a neurosymbolic model architecture for autonomous agents. The proposed model, referred to as Evolving Behavior Trees (EBTs), integrates the required components to represent the learning tasks as well as the switching between tasks to achieve complex long-term goals. We design an agent for autonomous navigation and we evaluate the approach against a state-of-the-art hierarchical reinforcement learning method using a Maze Simulation Environment. The results show autonomous agents represented by EBTs can be trained efficiently. The approach incorporates explicit safety constraints into the model and incurs significantly fewer safety violations during training and execution. Further, the model provides explanations for the behavior of the autonomous agent by associating the state of the executing EBT with agent actions.",,979-8-3503-2601-7,10.1109/ICAA58325.2023.00014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10207600,explainable AI;behavior trees;hierarchical reinforcement learning;autonomous navigation,Training;Navigation;Neural networks;Reinforcement learning;Switches;Autonomous agents;Safety,,2,,27,IEEE,10 Aug 2023,,,IEEE,IEEE Conferences
Synchronizing Innovation and Accountability in the Ethical Consequences of Artificial Intelligence,S. Yadav; N. Singh; K. V. R. Devi; G. Nijhawan; R. S. Zabibah; A. K,"CSE, Axis Institute of Technology and Management, Kanpur, U.P; IILM Institute of Higher Education, New Delhi, India; Institute of Aeronautical Engineering, Hyderabad; Lovely Professional University, Phagwara; Medical Laboratory Technology Department, College of Medical Technology, The Islamic University, Najaf Iraq, College of medicine, Jabir Ibn Hayyan medical university, Najaf; Department of Electronics and Communication Engineering, New Horizon College of Engineering, Bangalore","2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)",26 Feb 2024,2023,10,,397,402,"The technology known as artificial intelligence (AI) has great promise for transforming several facets of human existence. But the creation and application of AI can bring up certain ethical issues. Unfair results may result from AI systems that reflect or magnify societal prejudices already in place. An AI system that evaluates loan applications, for instance, may have prejudices against particular racial or ethnic groupings. AI-powered data collecting and surveillance gives rise to worries about possible abuse and invasions of personal privacy. AI systems may be used, for instance, to monitor our online and offline activities without our permission. AI's capacity to make decisions raises concerns about responsibility and unforeseen consequences. AI systems may be used, for instance, to decide who should be imprisoned or granted parole-decisions that might literally mean the difference between life and death. Many AI systems are opaque, making it challenging to evaluate their fairness, dependability, and potential for harm. For instance, humans could not be aware of the reasoning behind an AI system's decision-making, which might breed mistrust and abuse. AI has significant societal ramifications that might lead to social instability, economic inequality, and job displacement. AI may, for instance, result in widespread unemployment as robots replace humans in the workforce. The research can guarantee that AI is created and deployed in a responsible and ethical manner by addressing these ethical problems.",2687-7767,979-8-3503-8247-1,10.1109/UPCON59197.2023.10434644,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434644,Artificial intelligence (AI);Ethics;Bias and fairness;Privacy and surveillance;Autonomy and control;Transparency and explainability;Social impact and societal responsibility;Job displacement;Economic inequality;Social unrest,Ethics;Technological innovation;Surveillance;Synchronization;Stakeholders;Artificial intelligence;Unemployment,,4,,22,IEEE,26 Feb 2024,,,IEEE,IEEE Conferences
VADR: Discriminative Multimodal Explanations for Situational Understanding,H. Taylor; L. Hiley; J. Furby; A. Preece; D. Braines,"Crime and Security Research Institute, Cardiff University, Cardiff, UK; Crime and Security Research Institute, Cardiff University, Cardiff, UK; Crime and Security Research Institute, Cardiff University, Cardiff, UK; Crime and Security Research Institute, Cardiff University, Cardiff, UK; IBM Research, Hursley Park, Winchester, UK",2020 IEEE 23rd International Conference on Information Fusion (FUSION),10 Sep 2020,2020,,,1,8,"The focus of this paper is on the generation of multimodal explanations for information fusion tasks performed on multimodal data. We propose that separating modal components in saliency map explanations provides users with a better understanding of how convolutional neural networks process multimodal data. We adapt established state-of-the-art explainability techniques to mid-level fusion networks in order to better understand (a) which modality of the input contributes most to a model's decision and (b) which parts of the input data are most relevant to that decision. Our method separates temporal from non-temporal information to allow a user to focus their attention on salient elements of the scene that are changing in multiple modalities. The work is experimentally tested on an activity recognition task using video and audio data. In view of the fact that explanations need to be tailored to the type of user in a User Fusion context, we focus on meeting explanation requirements for system creators and operators respectively.",,978-0-578-64709-8,10.23919/FUSION45008.2020.9190215,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9190215,multimodal fusion;user fusion;explanation;audiovisual;situational understanding;human-agent teams,Task analysis;Data models;Streaming media;Activity recognition;Feature extraction;Government;Laboratories,,2,,26,,10 Sep 2020,,,IEEE,IEEE Conferences
Sensor Data Based System-Level Anomaly Prediction for Smart Manufacturing,J. Wang; C. Liu; M. Zhu; P. Guo; Y. Hu,"Department of Information Systems, University of Maryland, Baltimore County, Baltimore, MD, U.S.A; Cloud Computing Research Center, North China University of Technology, Beijing, China; Cloud Computing Research Center, North China University of Technology, Beijing, China; Department of Information Systems, University of Maryland, Baltimore County, Baltimore, MD, U.S.A; Cloud Computing Research Center, North China University of Technology, Beijing, China",2018 IEEE International Congress on Big Data (BigData Congress),11 Sep 2018,2018,,,158,165,"With the popularity of Supervisory Information System (SIS), Supervisory Control and Data Acquisition (SCADA) system and Internet of Things (IoT) sensors, we can easily obtain abundant sensor data in manufacturing. We could save manufacturing maintenance costs and prevent further damages if we can accurately predict system anomalies from the sensor data. Yet learning from individual sensors often cannot directly determine whether the system will have anomaly because each sensor only measures a partial state of a big system. By detecting events across sensors collectively and their temporal dependencies, this paper proposes a new system-level anomaly prediction framework by mining anomaly dependency graph from sensor data. The advantages of the approach include explainability, collective prediction and temporal sensitivity. We applied our approach with a real-world power plant dataset to evaluate its feasibility.",,978-1-5386-7232-7,10.1109/BigDataCongress.2018.00028,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457744,"Smart Manufacturing, Anomaly Prediction, Predictive Maintenance, Data Stream Mining, Sensor Data Driven",Predictive maintenance;Manufacturing;Anomaly detection;Time series analysis;Power generation,,22,,32,IEEE,11 Sep 2018,,,IEEE,IEEE Conferences
Working Alongside Non-Human Agents,A. H. Duin; I. Pedersen,"Writing Studies Department, College of Liberal Arts, University of Minnesota, Minneapolis, USA; Writing Studies Department, College of Liberal Arts, University of Minnesota, Minneapolis, USA",2021 IEEE International Professional Communication Conference (ProComm),13 Jun 2022,2021,,,1,5,"We coexist with non-human AI agents, and we now must plan for human and non-human-agent teaming, for cooperation and collaboration, as a means to expand collaborative intelligence in our ongoing quest for user advocacy. For practice and experimentation, we provide links to current non-human agents. We then distinguish automation and autonomy, and discuss humanness design, teaming. A deeper understanding of usability and ethical considerations for working alongside these systems, deploying robots and building bonds and trust with nonhuman agents, begins with differentiation of automation and autonomy, human-autonomy teaming, and a humanness design approach as a means to prevent undesirable autonomy. While TPC scholarship attends to privacy, accountability, safety and security, and transparency and explainability, we need additional vigilance regarding fairness and non-discrimination, human control of technology, TPC professional responsibility, and continued promotion of human values as we work alongside non-human agents.",2158-1002,978-1-6654-4327-2,10.1109/ProComm52174.2021.00005,Canada Research Chairs; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793412,artificial intelligence;autonomous agents;collaboration;non-human agents,Automation;Scholarships;Collaboration;Safety;Security;Artificial intelligence;Usability,,,,28,IEEE,13 Jun 2022,,,IEEE,IEEE Conferences
A Novel Interval Type-2 Fuzzy Classifier Based on Explainable Neural Network for Surface Electromyogram Gesture Recognition,S. Lv; Z. Li; J. Huang; P. Shi,"Institute of Advanced Technology, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; School of Electrical and Mechanical Engineering, The University of Adelaide, Adelaide, SA, Australia",IEEE Transactions on Human-Machine Systems,14 Dec 2023,2023,53,6,955,964,"The existing hand gesture classification research based on surface electromyogram (sEMG) faces the challenges of low classification accuracy, weak real-time ability, weak robustness, few categories, and lack of explainability. In this article, we investigate how to classify sEMG signals for grasp recognition and human–robot interaction to consider these issues. A novel interval type-2 (IT2) fuzzy classifier based on explainable neural network is proposed for sEMG gesture recognition. Based on fully connected neural network, the adaptive moment estimation is applied to tune the antecedent parameters. The Ninapro data is adopted to test the performance of the proposed model, which realizes recognition of 52 gestures and achieves 95.04% categorization accuracy. Moreover, grasping experiments are conducted on computer, communication, and consumer electronics (3C) experiment platform to test the ability of the classifier in real scenarios. The experiment recognizes six gestures. The results of the 3C grasping experiment show that the proposed method achieves 99.4% offline training accuracy as well as 96.07% online test accuracy. Meanwhile, 89.4% of the classification results can be obtained within 0.5 s. The overall results demonstrate great potential for real-world applications, such as human intent detection and manipulator control.",2168-2305,,10.1109/THMS.2023.3310524,"National Key Research and Development Program of China(grant numbers:2018AAA0102900,2021YFF0501600); National Natural Science Foundation of China(grant numbers:U22A2060); Major Science and Technology Projects of Anhui Province(grant numbers:202103a05020004);",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247642,Hand gesture classification;interval type-2 (IT2) fuzzy logic system;surface electromyogram (SEMG);teleoperation,Electromyography;Manipulators;Fuzzy sets;Man-machine systems;Linguistics;Input variables;Gesture recognition;Classification algorithms;Fuzzy logic;Teleoperators,,6,,32,IEEE,12 Sep 2023,,,IEEE,IEEE Journals
Toward Robots’ Behavioral Transparency of Temporal Difference Reinforcement Learning With a Human Teacher,M. Matarese; A. Sciutti; F. Rea; S. Rossi,"University of Genova and Italian Institute of Technology, Genova, Italy; Italian Institute of Technology, Genova, Italy; Italian Institute of Technology, Genova, Italy; University of Napoli Federico II, Napoli, Italy",IEEE Transactions on Human-Machine Systems,15 Nov 2021,2021,51,6,578,589,"The high request for autonomous human–robot interaction (HRI), combined with the potential of machine learning (ML) techniques, allow us to deploy ML mechanisms in robot control. However, the use of ML can make robots’ behavior unclear to the observer during the learning phase. Recently, transparency in HRI has been investigated to make such interactions more comprehensible. In this work, we propose a model to improve the transparency during reinforcement learning (RL) tasks for HRI scenarios: the model supports transparency by having the robot show nonverbal emotional-behavioral cues. Our model considered human feedback as the reward of the RL algorithm and it presents emotional-behavioral responses based on the progress of the robot learning. The model is managed only by the temporal-difference error. We tested the architecture in a teaching scenario with the iCub humanoid robot. The results highlight that when the robot expresses its emotional-behavioral response, the human teacher is able to understand its learning process better. Furthermore, people prefer to interact with an expressive robot as compared to a mechanical one. Movement-based signals proved to be more effective in revealing the internal state of the robot than facial expressions. In particular, gaze movements were effective in showing the robot's next intentions. In contrast, communicating uncertainty through robot movements sometimes led to action misinterpretation, highlighting the importance of balancing transparency and the legibility of the robot goal. We also found a reliable temporal window in which to register teachers’ feedback that can be used by the robot as a reward.",2168-2305,,10.1109/THMS.2021.3116119,Starting Grant from the European Research Council; European Union's Horizon 2020 research and innovation(grant numbers:804388);,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9580746,Human–robot interaction;humanoid robot;reinforcement learning (RL);social robotics;transparency,Humanoid robots;Reinforcement learning;Human-robot interaction,,18,,53,CCBY,19 Oct 2021,,,IEEE,IEEE Journals
A Novel Graphical Lasso Based Approach Towards Segmentation Analysis in Energy Game-Theoretic Frameworks,H. P. Das; I. C. Konstantakopoulos; A. B. Manasawala; T. Veeravalli; H. Liu; C. J. Spanos,"Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; Department of Industrial Engineering and Operations Research, University of California, Berkeley; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley",2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),17 Feb 2020,2019,,,1702,1709,"Energy game-theoretic frameworks have emerged to be a successful strategy to encourage energy efficient behavior in large scale by leveraging human-in-the-loop strategy. A number of such frameworks have been introduced over the years which formulate the energy saving process as a competitive game with appropriate incentives for energy efficient players. However, prior works involve an incentive design mechanism which is dependent on knowledge of utility functions for all the players in the game, which is hard to compute especially when the number of players is high, common in energy game-theoretic frameworks. Our research proposes that the utilities of players in such a framework can be grouped together to a relatively small number of clusters, and the clusters can then be targeted with tailored incentives. The key to above segmentation analysis is to learn the features leading to human decision making towards energy usage in competitive environments. We propose a novel graphical lasso based approach to perform such segmentation, by studying the feature correlations in a real-world energy social game dataset. To further improve the explainability of the model, we perform causality study using grangers causality. Proposed segmentation analysis results in characteristic clusters demonstrating different energy usage behaviors. We also present avenues to implement intelligent incentive design using proposed segmentation method.",,978-1-7281-4550-1,10.1109/ICMLA.2019.00277,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999336,"Segmentation Analysis, Energy Game-Theoretic Frameworks, Graphical Lasso, Smart Building",Games;Buildings;Decision making;Hidden Markov models;Clustering algorithms;Elbow;Distortion,,8,,32,IEEE,17 Feb 2020,,,IEEE,IEEE Conferences
Chapter 13 Leveraging Artificial Intelligence for Enhanced Risk Management in Banking: A Systematic Literature Review,N. J. Dewasiri; D. G. Dharmarathna; M. Choudhary,NA; NA; NA,Artificial Intelligence Enabled Management: An Emerging Economy Perspective,,2024,,,197,214,"This systematic review delves into the transformative role of Artificial Intelligence (AI) in the banking industry’s risk management practices. AI, encompassing machine learning, data analytics, and natural language processing, has enhanced risk assessment, mitigation, and decision-making processes. The findings emphasise AI’s capacity to identify and assess risks, enabling proactive risk management effectively. Applications like credit scoring models, fraud detection systems, and stress testing tools play instrumental roles in optimising risk management processes. At the same time, the importance of data quality, governance, and transparency cannot be overstated in successfully implementing AI-driven risk management strategies. The implications of AI in banking are profound, offering data-driven procedures, equitable lending practices, and enhanced operational efficiency. However, data privacy concerns, model interpretability issues, and regulatory compliance complexities must be addressed carefully. Emerging trends in AI for risk management encompass Explainable AI, AI-enabled regulatory Compliance, AI for Cybersecurity Risk Management, and Natural Language Processing for Unstructured Data Analysis, along with the optimisation of efficiency through Robotic Process Automation in Risk Operations. Future research should focus on ethical considerations, dynamic stress testing models, AI’s role in climate-related risk analysis, human-AI collaboration, cybersecurity risk prediction, and the development of robust regulatory frameworks for AI integration in risk management. AI stands poised to revolutionise banking risk management. Still, responsible and ethical integration is paramount, necessitating collaborative efforts to harness its full potential while ensuring trust and stability within the sector.",,9.78311E+12,,,https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10790827.pdf&bkn=10780983&pdfType=chapter,,Risk management;Artificial intelligence;Banking;Industries;Economic indicators;Machine learning;Complexity theory;Natural language processing;Guidelines;Fraud,,,,,,14 Jan 2025,,,De Gruyter,De Gruyter eBook Chapters
Laban head-motions convey robot state: A call for robot body language,H. Knight; R. Simmons,"Robotics Institute, Carnegie Mellon University, Pittsburgh, PA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA",2016 IEEE International Conference on Robotics and Automation (ICRA),9 Jun 2016,2016,,,2881,2888,"Functional robots are an increasing presence in shared human-machine environments. Humans efficiently parse motion expressions, gaining an immediate impression of an agent's current action and state. Past work has shown that motion can effectively reveal a robot's current task objective to bystanders and collaborators, however, the layering of expression on pre-existing robot task motions has yet to be explored. Rather than showing us what the robot is doing, these layered motion characteristics leverage the how of the task motions to convey additional robot attitudes, e.g., confidence, adherence to deadline or flexibility of attention. To lay the foundations for this objective, we adapt the Laban Efforts, a system from dance and acting training in use for over 50 years. We operationalize features representing the four Laban Efforts (Time, Space, Weight, and Flow) to the movements of a 2-DOF Nao head and a 4-DOF Keepon robot during simple dance and look-for-someone behaviors. Using online survey, we collect 1028 motion ratings for 72 robot motion videos depicting contrasting Effort motion examples. We achieve statistically significant legibility results for all four Effort implementations. Even without human degrees of freedom, we find that robot motion patterns can convey complex expressions to people.",,978-1-4673-8026-3,10.1109/ICRA.2016.7487451,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487451,,Acceleration;Robot motion;Man-machine systems;Videos;Timing;Aerospace electronics,,41,,21,IEEE,9 Jun 2016,,,IEEE,IEEE Conferences
Graph Spatio-Temporal Networks for Condition Monitoring of Wind Turbine,X. Jin; S. Lv; Z. Kong; H. Yang; Y. Zhang; Y. Guo; Z. Xu,"College of Mechanical Engineering, Zhejiang University of Technology, Hangzhou, China; School of Design and Architecture, Zhejiang University of Technology, Hangzhou, China; College of Control Science and Engineering, Zhejiang University, Hangzhou, China; School of Design and Architecture, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Zhijiang College, Zhejiang University of Technology, Shaoxing, China; College of Control Science and Engineering, Zhejiang University, Hangzhou, China",IEEE Transactions on Sustainable Energy,1 Oct 2024,2024,15,4,2276,2286,"Condition monitoring of wind turbines (WTs) is essential for advancing wind energy. Existing data-driven methods heavily rely on deep learning and big data, leading to challenges in distinguishing true faults from false alarms, impacting operational decisions negatively. Thus, this paper proposes a spatio-temporal graph neural network framework that incorporates prior knowledge. Prior WT knowledge is utilized by establishing a spatially structured directed graph embedded in a graph attention network (GAT). The features in WTs’ supervisory control and data acquisition system are indicated by the nodes in GAT. Then, the global and local attention embedding layers as well as long short-term memory layers are employed to combine spatio-temporal information from each node. Finally, the condition monitoring in WTs’ graph and node-level are established, and a fault propagation chain at node-level is constructed for explaining condition monitoring results. To demonstrate the explainability, robustness and sensitivity of the proposed approach, a comparative analysis between a true fault case and a false alarm case are given, and anomaly detection results are also reported.",1949-3037,,10.1109/TSTE.2024.3411884,National Key Research and Development Program of China(grant numbers:2022YFE0198900); Open Research Project of the State Key Laboratory of Mechanical Transmission for Advanced Equipment; Chongqing University(grant numbers:SKLMT-MSKFKT-202315);,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10552429,Wind turbine (WT);prior knowledge;graph spatio-temporal neural networks;condition monitoring,Condition monitoring;Feature extraction;Monitoring;Sensors;Long short term memory;Wind turbines;SCADA systems;Knowledge graphs;Spatiotemporal phenomena;Neural networks,,,,34,IEEE,10 Jun 2024,,,IEEE,IEEE Journals