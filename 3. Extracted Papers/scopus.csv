scopus,,,,,,,,,,,,,,,,,,,,,,
Authors,Author full names,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Abstract,Author Keywords,Index Keywords,Document Type,Publication Stage,Open Access,Source,EID
Wu Z.; Guo C.; Chen J.; Ding S.; Zheng Y.,"Wu, Zhuoqing (57226779736); Guo, Chonghui (12775741500); Chen, Jingfeng (57195714980); Ding, Suying (57217362957); Zheng, Yunchao (59897142000)",57226779736; 12775741500; 57195714980; 57217362957; 59897142000,Integrating temporal association rules into intelligent prediction system for metabolic dysfunction-associated fatty liver disease,2025,Decision Support Systems,195,,114467,,,,0,10.1016/j.dss.2025.114467,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005109273&doi=10.1016%2fj.dss.2025.114467&partnerID=40&md5=b891891b13f70b4cb1db8bc0e97cc883,"Healthcare big data provides trajectory data on chronic disease onset, progression, and outcomes, essential for understanding metabolic dysfunction-associated fatty liver disease (MAFLD) patient health dynamics. However, constructing explainable predictive models for MAFLD using longitudinal healthcare big data remains challenging due to its complexity. While several high-performance machine learning models have shown promise, their “black box” nature limits interpretability and trust among clinical healthcare professionals. Most studies also rely on cross-sectional data, which lacks the depth of longitudinal data, hindering accurate health status tracking. This paper proposes an intelligent MAFLD prediction system integrating temporal association rules (TARs) through a “human-in-the-loop” approach. By analyzing TARs that capture disease dynamics, the system incorporates high-quality domain knowledge into its predictive model. To enhance explainability, we use the SHapley Additive exPlanations framework alongside clinically significant TARs. The system's effectiveness was validated on real-world data, showing improved MAFLD outcome prediction. Sensitivity analysis identified optimal TARs and robust model configurations. Finally, the online-deployed explainable prototype system demonstrates potential to boost trust and adoption among clinical healthcare professionals. Additionally, the system's effectiveness and their willingness to use it were further evaluated through the “human-on-the-loop” method. These findings suggest the system could serve as a valuable tool for clinical applications and advance information systems design. © 2025 Elsevier B.V.",Explainability; Healthcare big data; Intelligent prediction system; Metabolic dysfunction-associated fatty liver disease; Temporal association rule,Diseases; mHealth; Nutrition; Explainability; Fatty liver disease; Health care professionals; Healthcare big data; Intelligent prediction; Intelligent prediction system; Metabolic dysfunction-associated fatty liver disease; Prediction systems; Predictive models; Temporal association rule; Patient treatment,Article,Final,,Scopus,2-s2.0-105005109273
Jha D.; Durak G.; Sharma V.; Keles E.; Cicek V.; Zhang Z.; Srivastava A.; Rauniyar A.; Hagos D.H.; Tomar N.K.; Miller F.H.; Topcu A.; Yazidi A.; Håkegård J.E.; Bagci U.,"Jha, Debesh (57194764980); Durak, Gorkem (57219047943); Sharma, Vanshali (57216315107); Keles, Elif (57200083115); Cicek, Vedat (57211592619); Zhang, Zheyuan (57712650100); Srivastava, Abhishek (57224567257); Rauniyar, Ashish (56653500000); Hagos, Desta Haileselassie (55818981400); Tomar, Nikhil Kumar (57222073120); Miller, Frank H. (57265760400); Topcu, Ahmet (57207987244); Yazidi, Anis (36623068400); Håkegård, Jan Erik (6507511138); Bagci, Ulas (24176491700)",57194764980; 57219047943; 57216315107; 57200083115; 57211592619; 57712650100; 57224567257; 56653500000; 55818981400; 57222073120; 57265760400; 57207987244; 36623068400; 6507511138; 24176491700,A Conceptual Framework for Applying Ethical Principles of AI to Medical Practice,2025,Bioengineering,12,2,180,,,,0,10.3390/bioengineering12020180,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219200212&doi=10.3390%2fbioengineering12020180&partnerID=40&md5=1c943f063a75d5af2ad07c65713ae8cf,"Artificial Intelligence (AI) is reshaping healthcare through advancements in clinical decision support and diagnostic capabilities. While human expertise remains foundational to medical practice, AI-powered tools are increasingly matching or exceeding specialist-level performance across multiple domains, paving the way for a new era of democratized healthcare access. These systems promise to reduce disparities in care delivery across demographic, racial, and socioeconomic boundaries by providing high-quality diagnostic support at scale. As a result, advanced healthcare services can be affordable to all populations, irrespective of demographics, race, or socioeconomic background. The democratization of such AI tools can reduce the cost of care, optimize resource allocation, and improve the quality of care. In contrast to humans, AI can potentially uncover complex relationships in the data from a large set of inputs and generate new evidence-based knowledge in medicine. However, integrating AI into healthcare raises several ethical and philosophical concerns, such as bias, transparency, autonomy, responsibility, and accountability. In this study, we examine recent advances in AI-enabled medical image analysis, current regulatory frameworks, and emerging best practices for clinical integration. We analyze both technical and ethical challenges inherent in deploying AI systems across healthcare institutions, with particular attention to data privacy, algorithmic fairness, and system transparency. Furthermore, we propose practical solutions to address key challenges, including data scarcity, racial bias in training datasets, limited model interpretability, and systematic algorithmic biases. Finally, we outline a conceptual algorithm for responsible AI implementations and identify promising future research and development directions. © 2025 by the authors.",artificial intelligence (AI); ethical AI; philosophical AI; trustworthy AI,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85219200212
Upadhyay S.; Lakkaraju H.; Gajos K.Z.,"Upadhyay, Sohini (57222360735); Lakkaraju, Himabindu (37104369800); Gajos, Krzysztof Z. (8375653300)",57222360735; 37104369800; 8375653300,Counterfactual Explanations May Not Be the Best Algorithmic Recourse Approach,2025,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,446,462,16,0,10.1145/3708359.3712095,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001919802&doi=10.1145%2f3708359.3712095&partnerID=40&md5=fd295bbe61f9ffea612ceef6a7fb8f6e,"Algorithmic recourse is a rapidly developing subfield in explainable AI (XAI) concerned with providing individuals subject to adverse high-stakes algorithmic outcomes with explanations indicating how to reverse said outcomes. While XAI research in the machine learning community doesn't confine itself to counterfactual explanations, its algorithmic recourse subfield does, adopting the assumption that the optimal way to provide recourse is through counterfactual explanations. Though there has been extensive human-AI interaction research on explanations, translating these findings to the algorithmic recourse setting is non-obvious due to meaningful problem setting differences, leaving the question of whether counterfactuals are the most optimal explanation paradigm for recourse unanswered. While intuitively satisfying, the prescriptive nature of counterfactuals makes them vulnerable to poor outcomes when circumstances unknown to the decision-making and explanation generating algorithms affect re-application strategies. With these concerns in mind, we designed a series of experiments comparing different explanation methods in the recourse setting, explicitly incorporating scenarios where circumstances unknown to the decision-making and explanation algorithms affect re-application strategies. In Experiment 1, we compared counterfactuals with reason codes, a simple feature-based explanation, finding that they both yield comparable re-application success, and that reason codes led to better user outcomes when unknown circumstances had a high impact on re-application strategies. In Experiment 2, we sought to improve on reason code outcomes, comparing them to feature attributions, a more informative feature-based explanation, but found no improvements. Finally, in Experiment 3, we aimed to improve on reason code outcomes with a multiple counterfactual explanation condition, finding that multiple counterfactuals led to higher re-application success but still resulted in comparatively worse user outcomes in the face of high impact unknown circumstances. Taken together, these findings call into question whether the standard counterfactual paradigm is the best approach for the algorithmic recourse problem setting.  © 2025 Copyright held by the owner/author(s).",AI explanations; algorithmic recourse; counterfactual explanations,Adversarial machine learning; AI explanation; Algorithmic recourse; Algorithmics; Application strategies; Counterfactual explanation; Counterfactuals; Decisions makings; Feature-based; Subfields; Unknown circumstances; Contrastive Learning,Conference paper,Final,,Scopus,2-s2.0-105001919802
Wang H.W.; Birnbaum L.; Setlur V.,"Wang, Huichen Will (59299956200); Birnbaum, Larry (7103144019); Setlur, Vidya (21743848500)",59299956200; 7103144019; 21743848500,Jupybara: Operationalizing a Design Space for Actionable Data Analysis and Storytelling with LLMs,2025,Conference on Human Factors in Computing Systems - Proceedings ,,,1005,,,,1,10.1145/3706598.3713913,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005747639&doi=10.1145%2f3706598.3713913&partnerID=40&md5=77202d525fd3e9c355322406bf2f9861,"Mining and conveying actionable insights from complex data is a key challenge of exploratory data analysis (EDA) and storytelling. To address this challenge, we present a design space for actionable EDA and storytelling. Synthesizing theory and expert interviews, we highlight how semantic precision, rhetorical persuasion, and pragmatic relevance underpin effective EDA and storytelling. We also show how this design space subsumes common challenges in actionable EDA and storytelling, such as identifying appropriate analytical strategies and leveraging relevant domain knowledge. Building on the potential of LLMs to generate coherent narratives with commonsense reasoning, we contribute Jupybara, an AI-enabled assistant for actionable EDA and storytelling implemented as a Jupyter Notebook extension. Jupybara employs two strategies - design-space-aware prompting and multi-agent architectures - to operationalize our design space. An expert evaluation confirms Jupybara's usability, steerability, explainability, and reparability, as well as the effectiveness of our strategies in operationalizing the design space framework with LLMs. © 2025 Copyright held by the owner/author(s).",Actionable Insights; Data Science; Data Storytelling; Exploratory Data Analysis; Human-AI Collaboration; Large Language Model; Multi-Agent System; Pragmatics; Rhetoric; Semantics,Data reduction; Data Science; Information retrieval; Information theory; Actionable insight; Data storytelling; Design spaces; Exploratory data analysis; Human-AI collaboration; Language model; Large language model; Multiagent systems (MASs); Pragmatic; Rhetoric; Data mining,Conference paper,Final,,Scopus,2-s2.0-105005747639
Kayser M.; Menzat B.; Emde C.; Bercean B.; Novak A.; Espinosa A.; Papiez B.W.; Gaube S.; Lukasiewicz T.; Camburu O.-M.,"Kayser, Maxime (57209110868); Menzat, Bayar (55809032500); Emde, Cornelius (57224572567); Bercean, Bogdan (57217588594); Novak, Alex (57201418545); Espinosa, Abdala (58887885100); Papiez, Bartlomiej W. (36460161500); Gaube, Susanne (57202200441); Lukasiewicz, Thomas (7004581222); Camburu, Oana-Maria (57063266900)",57209110868; 55809032500; 57224572567; 57217588594; 57201418545; 58887885100; 36460161500; 57202200441; 7004581222; 57063266900,Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting,2024,"EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",,,,18891,18919,28,0,10.18653/v1/2024.emnlp-main.1051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217771428&doi=10.18653%2fv1%2f2024.emnlp-main.1051&partnerID=40&md5=f7283bfa04746abb74681980b88b3e9c,"The growing capabilities of AI models are leading to their wider use, including in safety-critical domains.Explainable AI (XAI) aims to make these models safer to use by making their inference process more transparent.However, current explainability methods are seldom evaluated in the way they are intended to be used: by real-world end users.To address this, we conducted a large-scale user study with 85 healthcare practitioners in the context of human-AI collaborative chest X-ray analysis.We evaluated three types of explanations: visual explanations (saliency maps), natural language explanations, and a combination of both modalities.We specifically examined how different explanation types influence users depending on whether the AI advice and explanations are factually correct.We find that text-based explanations lead to significant over-reliance, which is alleviated by combining them with saliency maps.We also observe that the quality of explanations, that is, how much factually correct information they entail, and how much this aligns with AI correctness, significantly impacts the usefulness of the different explanation types. © 2024 Association for Computational Linguistics.",,Visual languages; 'current; Clinical decision support; End-users; Inference process; Large-scales; Natural language explanations; Real-world; Safety-critical domain; Saliency map; User study; Computational linguistics,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85217771428
Schmude T.; Koesten L.; Möller T.; Tschiatschek S.,"Schmude, Timothée (58114864600); Koesten, Laura (57190130539); Möller, Torsten (7103010114); Tschiatschek, Sebastian (54917447400)",58114864600; 57190130539; 7103010114; 54917447400,Information that matters: Exploring information needs of people affected by algorithmic decisions,2025,International Journal of Human Computer Studies,193,,103380,,,,0,10.1016/j.ijhcs.2024.103380,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205992227&doi=10.1016%2fj.ijhcs.2024.103380&partnerID=40&md5=70bfe884417416cff76aa9b70c97a589,"Every AI system that makes decisions about people has a group of stakeholders that are personally affected by these decisions. However, explanations of AI systems rarely address the information needs of this stakeholder group, who often are AI novices. This creates a gap between conveyed information and information that matters to those who are impacted by the system's decisions, such as domain experts and decision subjects. To address this, we present the “XAI Novice Question Bank”, an extension of the XAI Question Bank (Liao et al., 2020) containing a catalog of information needs from AI novices in two use cases: employment prediction and health monitoring. The catalog covers the categories of data, system context, system usage, and system specifications. We gathered information needs through task based interviews where participants asked questions about two AI systems to decide on their adoption and received verbal explanations in response. Our analysis showed that participants’ confidence increased after receiving explanations but that their understanding faced challenges. These included difficulties in locating information and in assessing their own understanding, as well as attempts to outsource understanding. Additionally, participants’ prior perceptions of the systems’ risks and benefits influenced their information needs. Participants who perceived high risks sought explanations about the intentions behind a system's deployment, while those who perceived low risks rather asked about the system's operation. Our work aims to support the inclusion of AI novices in explainability efforts by highlighting their information needs, aims, and challenges. We summarize our findings as five key implications that can inform the design of future explanations for lay stakeholder audiences. © 2024 The Authors",Affected stakeholders; Explainable AI; Information needs; Qualitative methods; Question-driven explanations; Understanding,Affected stakeholder; AI systems; Algorithmics; Explainable AI; Information need; Qualitative method; Question banks; Question-driven explanation; Stakeholder groups; Understanding,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85205992227
Schmitt V.; Csomor B.P.; Meyer J.; Villa-Areas L.-F.; Jakob C.; Polzehl T.; Möller S.,"Schmitt, Vera (57225044629); Csomor, Balázs Patrik (59179277200); Meyer, Joachim (7406100627); Villa-Areas, Luis-Felipe (59182493500); Jakob, Charlott (58972688800); Polzehl, Tim (34969523200); Möller, Sebastian (24076818400)",57225044629; 59179277200; 7406100627; 59182493500; 58972688800; 34969523200; 24076818400,Evaluating Human-Centered AI Explanations: Introduction of an XAI Evaluation Framework for Fact-Checking,2024,ACM International Conference Proceeding Series,,,,91,100,9,4,10.1145/3643491.3660283,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196375447&doi=10.1145%2f3643491.3660283&partnerID=40&md5=8891d866de5533b08b3730b8994bf302,"The rapidly increasing amount of online information and the advent of Generative Artificial Intelligence (GenAI) make the manual verification of information impractical. Consequently, AI systems are deployed to detect disinformation and deepfakes. Prior studies have indicated that combining AI and human capabilities yields enhanced performance in detecting disinformation. Furthermore, the European Union (EU) AI Act mandates human supervision for AI applications in areas impacting essential human rights, like freedom of speech, necessitating that AI systems be transparent and provide adequate explanations to ensure comprehensibility. Extensive research has been conducted on incorporating explainability (XAI) attributes to augment AI transparency, yet these often miss a human-centric assessment. The effectiveness of such explanations also varies with the user's prior knowledge and personal attributes. Therefore, we developed a framework for validating XAI features for the collaborative human-AI fact-checking task. The framework allows the testing of XAI features with objective and subjective evaluation dimensions and follows human-centric design principles when displaying information about the AI system to the users. The framework was tested in a crowdsourcing experiment with 433 participants, including 406 crowdworkers and 27 journalists for the collaborative disinformation detection task. The tested XAI features increase the AI system's perceived usefulness, understandability, and trust. With this publication, the XAI evaluation framework is made open source. © 2024 Owner/Author.",blind trust in AI systems; Human-centered eXplanations; objective and subjective evaluation of eXplanations,Crowdsourcing; AI systems; Blind trust in AI system; European union; Evaluation framework; Human capability; Human-centered explanation; Objective and subjective evaluation of explanation; Objective and subjective evaluations; Online information; Performance; Subjective testing,Conference paper,Final,,Scopus,2-s2.0-85196375447
Bae S.W.; Chung T.; Zhang T.; Dey A.K.; Islam R.,"Bae, Sang Won (57191539544); Chung, Tammy (7401571086); Zhang, Tongze (58242959500); Dey, Anind K. (7101701731); Islam, Rahul (57257182400)",57191539544; 7401571086; 58242959500; 7101701731; 57257182400,"Enhancing Interpretable, Transparent, and Unobtrusive Detection of Acute Marijuana Intoxication in Natural Environments: Harnessing Smart Devices and Explainable AI to Empower Just-In-Time Adaptive Interventions: Longitudinal Observational Study",2025,JMIR AI,4,,e52270,,,,0,10.2196/52270,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216964363&doi=10.2196%2f52270&partnerID=40&md5=f98ed1787527d160ce9d10b7247ea2c3,"Background: Acute marijuana intoxication can impair motor skills and cognitive functions such as attention and information processing. However, traditional tests, like blood, urine, and saliva, fail to accurately detect acute marijuana intoxication in real time. Objective: This study aims to explore whether integrating smartphone-based sensors with readily accessible wearable activity trackers, like Fitbit, can enhance the detection of acute marijuana intoxication in naturalistic settings. No previous research has investigated the effectiveness of passive sensing technologies for enhancing algorithm accuracy or enhancing the interpretability of digital phenotyping through explainable artificial intelligence in real-life scenarios. This approach aims to provide insights into how individuals interact with digital devices during algorithmic decision-making, particularly for detecting moderate to intensive marijuana intoxication in real-world contexts. Methods: Sensor data from smartphones and Fitbits, along with self-reported marijuana use, were collected from 33 young adults over a 30-day period using the experience sampling method. Participants rated their level of intoxication on a scale from 1 to 10 within 15 minutes of consuming marijuana and during 3 daily semirandom prompts. The ratings were categorized as not intoxicated (0), low (1-3), and moderate to intense intoxication (4-10). The study analyzed the performance of models using mobile phone data only, Fitbit data only, and a combination of both (MobiFit) in detecting acute marijuana intoxication. Results: The eXtreme Gradient Boosting Machine classifier showed that the MobiFit model, which combines mobile phone and wearable device data, achieved 99% accuracy (area under the curve=0.99; F1-score=0.85) in detecting acute marijuana intoxication in natural environments. The F1-score indicated significant improvements in sensitivity and specificity for the combined MobiFit model compared to using mobile or Fitbit data alone. Explainable artificial intelligence revealed that moderate to intense self-reported marijuana intoxication was associated with specific smartphone and Fitbit metrics, including elevated minimum heart rate, reduced macromovement, and increased noise energy around participants. Conclusions: This study demonstrates the potential of using smartphone sensors and wearable devices for interpretable, transparent, and unobtrusive monitoring of acute marijuana intoxication in daily life. Advanced algorithmic decision-making provides valuable insight into behavioral, physiological, and environmental factors that could support timely interventions to reduce marijuana-related harm. Future real-world applications of these algorithms should be evaluated in collaboration with clinical experts to enhance their practicality and effectiveness. © Sang Won Bae, Tammy Chung, Tongze Zhang, Anind K Dey, Rahul Islam.",algorithmic decision-making process; artificial intelligence; cannabis; data collection; decision support; digital phenotyping; experience sampling; explainable artificial intelligence; eXtreme Gradient Boosting Machine classifier; Fitbit; intoxication; JITAI; just-in-time adaptive interventions; machine learning; marijuana; mHealth; passive sensing; smart devices; smartphone-based sensors; wearables; XAI; XGBoost,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85216964363
Zace D.; Semeraro F.; Schnaubelt S.; Montomoli J.; Ristagno G.; Fijačko N.; Gamberini L.; Bignami E.G.; Greif R.; Monsieurs K.G.; Scapigliati A.,"Zace, Drieda (57203149302); Semeraro, Federico (6604045883); Schnaubelt, Sebastian (57200533421); Montomoli, Jonathan (35337224900); Ristagno, Giuseppe (14056769000); Fijačko, Nino (56801806500); Gamberini, Lorenzo (56446635100); Bignami, Elena G. (6507436865); Greif, Robert (57218181374); Monsieurs, Koenraad G. (7004522350); Scapigliati, Andrea (22954512900)",57203149302; 6604045883; 57200533421; 35337224900; 14056769000; 56801806500; 56446635100; 6507436865; 57218181374; 7004522350; 22954512900,Artificial intelligence in resuscitation: a scoping review,2025,Resuscitation Plus,24,,100973,,,,0,10.1016/j.resplu.2025.100973,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004939487&doi=10.1016%2fj.resplu.2025.100973&partnerID=40&md5=34ce60ae5e570d112d7ef54b77d1d1f2,"Background: Artificial intelligence (AI) is increasingly applied in medicine, with growing interest in its potential to improve outcomes in cardiac arrest (CA). However, the scope and characteristics of current AI applications in resuscitation remain unclear. Methods: This scoping review aims to map the existing literature on AI applications in CA and resuscitation and identify research gaps for further investigation. PRISMA-ScR framework and ILCOR guidelines were followed. A systematic literature search across PubMed, EMBASE, and Cochrane identified AI applications in resuscitation. Articles were screened and classified by AI methodology, study design, outcomes, and implementation settings. AI-assisted data extraction was manually validated for accuracy. Results: Out of 4046 records, 197 studies met inclusion criteria. Most were retrospective (90%), with only 16 prospective studies and 2 randomised controlled trials. AI was predominantly applied in prediction of CA, rhythm classification, and post-resuscitation outcome prognostication. Machine learning was the most commonly used method (50% of studies), followed by deep learning and, less frequently, natural language processing. Reported performance was generally high, with AUROC values often exceeding 0.85; however, external validation was rare and real-world implementation limited. Conclusions: While AI applications in resuscitation demonstrate encouraging performance in prediction and decision support tasks, clear evidence of improved patient outcomes or routine clinical use remains limited. Future research should focus on prospective validation, equity in data sources, explainability, and seamless integration of AI tools into clinical workflows. © 2025 The Author(s)",Artificial intelligence; Cardiac arrest; Deep learning; Large language model; Machine learning; Resuscitation; Scoping review,,Review,Final,,Scopus,2-s2.0-105004939487
Duke T.; Giudici P.,"Duke, Toju (57383828800); Giudici, Paolo (23491813000)",57383828800; 23491813000,Responsible AI in Practice: A Practical Guide to Safe and Human AI,2025,Responsible AI in Practice: A Practical Guide to Safe and Human AI,,,,1,212,211,0,10.1007/979-8-8688-1166-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003352337&doi=10.1007%2f979-8-8688-1166-1&partnerID=40&md5=6178aa612a627eecc3fd97076690606b,"This book is the first practical book on AI risk assessment and management. It will enable you to evaluate and implement safe and accurate AI models and applications. The book features risk assessment frameworks, statistical metrics and code, a risk taxonomy curated from real-world case studies, and insights into AI regulation and policy, and is an essential tool for AI governance teams, AI auditors, AI ethicists, machine learning (ML) practitioners, Responsible AI practitioners, and computer science and data science students building safe and trustworthy AI systems across businesses, organizations, and universities. The centerpiece of this book is a risk management and assessment framework titled “Safe Human-centered AI (SAFE-HAI),” which highlights AI risks across the following Responsible AI principles: accuracy, sustainability and robustness, explainability, transparency and accountability, fairness, privacy and human rights, human-centered AI, and AI governance. Using several statistical metrics such as Area Under Curve (AUC), Rank Graduation Accuracy, and Shapley values, you will learn to apply Lorenz curves to measure risk and inequality across the different principles and will be equipped with a taxonomy/scoring rubric to identify and mitigate identified risks. This book is a true practical guide and covers a real-world case study using the proposed SAFE-HAI framework. The book will help you adopt standards and voluntary codes of conduct in compliance with AI risk and safety policies and regulations, including those from the NIST (National Institute of Standards and Technology) and EU AI Act (European Commission). What You Will Learn • Know the key principles behind Responsible AI and associated risks • Become familiar with risk assessment frameworks, statistical metrics, and mitigation measures for identified risks • Be aware of the fundamentals of AI regulations and policies and how to adopt them • Understand AI governance basics and implementation guidelines Who This Book Is For AI governance teams, AI auditors, AI ethicists, machine learning (ML) practitioners, Responsible AI practitioners, and computer science and data science students building safe and trustworthy AI systems across businesses, organizations, and universities. © 2025 by Toju Duke and Paolo Giudici.",AI Governance; AI Risk Impact Assessment; AI Risk Mitigation; AI Safety; AI Safety Metrics; Artificial Intelligence; Data Ethics; Responsible AI,,Book,Final,,Scopus,2-s2.0-105003352337
Stodt J.; Reich C.; Knahl M.,"Stodt, Jan (57215365788); Reich, Christoph (57210686845); Knahl, Martin (6507944656)",57215365788; 57210686845; 6507944656,Demystifying XAI: Requirements for Understandable XAI Explanations,2024,Studies in Health Technology and Informatics,316,,,565,569,4,0,10.3233/SHTI240477,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202007827&doi=10.3233%2fSHTI240477&partnerID=40&md5=580c1b1eac99ebf2fcc968787d92d8ed,"This paper establishes requirements for assessing the usability of Explainable Artificial Intelligence (XAI) methods, focusing on non-AI experts like healthcare professionals. Through a synthesis of literature and empirical findings, it emphasizes achieving optimal cognitive load, task performance, and task time in XAI explanations. Key components include tailoring explanations to user expertise, integrating domain knowledge, and using non-propositional representations for comprehension. The paper highlights the critical role of relevance, accuracy, and truthfulness in fostering user trust. Practical guidelines are provided for designing transparent and user-friendly XAI explanations, especially in high-stakes contexts like healthcare. Overall, the paper's primary contribution lies in delineating clear requirements for effective XAI explanations, facilitating human-AI collaboration across diverse domains. © 2024 The Authors.",Explanations; Non-AI Experts; Understandability Requirements; XAI,Artificial Intelligence; Comprehension; Humans; Economic and social effects; Cognitive loads; Domain knowledge; Empirical findings; Explanation; Health care professionals; Non-AI expert; Task performance; Understandability; Understandability requirement; XAI; artificial intelligence; comprehension; human; Requirements engineering,Conference paper,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85202007827
Smirnov A.; Ponomarev A.; Levashova T.; Teslya N.; Shilov N.,"Smirnov, Alexander (55725403900); Ponomarev, Andrew (55858984800); Levashova, Tatiana (6603894696); Teslya, Nikolay (56946917500); Shilov, Nikolay (55932123000)",55725403900; 55858984800; 6603894696; 56946917500; 55932123000,Platform Architecture for Human-AI Collaborative Decision Support,2024,Lecture Notes in Networks and Systems,1209 LNNS,,,334,345,11,0,10.1007/978-3-031-77688-5_32,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214233879&doi=10.1007%2f978-3-031-77688-5_32&partnerID=40&md5=0d9809b7b2be44dc04c5262841cc3d99,"Modern decision-maker typically uses several AI-based tools to obtain more information about the problem at hand and to evaluate possible solutions, besides, decision support in complex dynamic environment typically requires knowledge of several experts, therefore, collaboration between them. These trends naturally merge in human-AI collaborative systems, providing the means of collaboration of heterogeneous participants. However, collaborative human-AI decision support is connected with many challenges, both theoretical and technological. This paper addresses the technological side of the problem by presenting a platform for human-AI collaborative decision support systems. The platform provides a set of mechanisms and interfaces, simplifying the development of such systems: team formation and collaboration features, interfaces to define, deploy and manage AI agents, and a set of structured representations facilitating interaction between human experts and AI agents. Possible application of the platform is discussed on a use case in road safety analysis. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Collaborative Decision Support Systems; Explainable AI; Human-AI Interaction; Neuro-Symbolic AI; Ontology; Service Ecosystem,Decision making; Collaborative decision support system; Collaborative decisions; Decision supports; Explainable AI; Human-AI interaction; Neuro-symbolic AI; Ontology's; Platform architecture; Service ecosystems; Support systems; Ontology,Conference paper,Final,,Scopus,2-s2.0-85214233879
Nadizar G.; Rovito L.; De Lorenzo A.; Medvet E.; Virgolin M.,"Nadizar, Giorgia (57226276857); Rovito, Luigi (57746811200); De Lorenzo, Andrea (38461011100); Medvet, Eric (15073896000); Virgolin, Marco (57188824850)",57226276857; 57746811200; 38461011100; 15073896000; 57188824850,An Analysis of the Ingredients for Learning Interpretable Symbolic Regression Models with Human-in-The-loop and Genetic Programming,2024,ACM Transactions on Evolutionary Learning and Optimization,4,1,5,,,,7,10.1145/3643688,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187802221&doi=10.1145%2f3643688&partnerID=40&md5=0bae125cc568ddd03928ccdc26bd391f,"Interpretability is a critical aspect to ensure a fair and responsible use of machine learning (ML) in high-stakes applications. Genetic programming (GP) has been used to obtain interpretable ML models because it operates at the level of functional building blocks: if these building blocks are interpretable, there is a chance that their composition (i.e., the entire ML model) is also interpretable. However, the degree to which a model is interpretable depends on the observer. Motivated by this, we study a recently-introduced human-in-The-loop system that allows the user to steer GP's generation process to their preferences, which shall be online-learned by an artificial neural network (ANN). We focus on the generation of ML models as analytical functions (i.e., symbolic regression) as this is a key problem in interpretable ML, and propose a two-fold contribution. First, we devise more general representations for the ML models for the ANN to learn upon, to enable the application of the system to a wider range of problems. Second, we delve into a deeper analysis of the system's components. To this end, we propose an incremental experimental evaluation, aimed at (1) studying the effectiveness by which an ANN can capture the perceived interpretability for simulated users, (2) investigating how the GP's outcome is affected across different simulated user feedback profiles, and (3) determining whether humans participants would prefer models that were generated with or without their involvement. Our results pose clarity on pros and cons of using a human-in-The-loop approach to discover interpretable ML models with GP. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",active learning; deep learning; evolutionary algorithms; evolutionary computation; Explainable artificial intelligence; explainable evolutionary computation; genetic programming; interpretable machine learning; neural networks,Deep learning; Functional programming; Genetic algorithms; Learning systems; Neural networks; Regression analysis; Reinforcement learning; Active Learning; Deep learning; Explainable artificial intelligence; Explainable evolutionary computation; Human-in-the-loop; Interpretable machine learning; Machine learning models; Machine-learning; Neural-networks; Symbolic regression; Genetic programming,Article,Final,,Scopus,2-s2.0-85187802221
Reis M.I.; Gonçalves J.N.C.; Cortez P.; Carvalho M.S.; Fernandes J.M.,"Reis, Marcelo I. (57577889000); Gonçalves, João N.C. (57193505412); Cortez, Paulo (7003574407); Carvalho, M. Sameiro (25122113000); Fernandes, João M. (7201540270)",57577889000; 57193505412; 7003574407; 25122113000; 7201540270,A context-aware decision support system for selecting explainable artificial intelligence methods in business organizations,2025,Computers in Industry,165,,104233,,,,0,10.1016/j.compind.2024.104233,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213207820&doi=10.1016%2fj.compind.2024.104233&partnerID=40&md5=d9d852cc9cf2ab9f5ae845213406e6fe,"Explainable Artificial Intelligence (XAI) methods are valuable tools for promoting understanding, trust, and efficient use of Artificial Intelligence (AI) systems in business organizations. However, the question of how organizations should select suitable XAI methods for a given task and business context remains a challenge, particularly when the number of methods available in the literature continues to increase. Here, we propose a context-aware decision support system (DSS) to select, from a given set of XAI methods, those with higher suitability to the needs of stakeholders operating in a given AI-based business problem. By including the human-in-the-loop, our DSS comprises an application-grounded analytical metric designed to facilitate the selection of XAI methods that align with the business stakeholders’ desiderata and promote a deeper understanding of the results generated by a given machine learning model. The proposed system was tested on a real supply chain demand problem, using real data and real users. The results provide evidence on the usefulness of our metric in selecting XAI methods based on the feedback and analytical maturity of stakeholders from the deployment context. We believe that our DSS is sufficiently flexible and understandable to be applied in a variety of business contexts, with stakeholders with varying degrees of AI literacy. © 2024",Business analytics; Decision support system; Explainable AI; Machine learning; Supply chain,Artificial intelligence methods; Artificial intelligence systems; Business analytics; Business contexts; Business organizations; Context-Aware; Decision supports; Explainable artificial intelligence; Machine-learning; Support systems; Adversarial machine learning,Article,Final,,Scopus,2-s2.0-85213207820
Holzinger A.; Zatloukal K.; Müller H.,"Holzinger, Andreas (23396282000); Zatloukal, Kurt (23989863900); Müller, Heimo (7404944998)",23396282000; 23989863900; 7404944998,Is human oversight to AI systems still possible?,2025,New Biotechnology,85,,,59,62,3,8,10.1016/j.nbt.2024.12.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212069001&doi=10.1016%2fj.nbt.2024.12.003&partnerID=40&md5=6a1a81a65b7df5a73eed09a94b0d646c,"The rapid proliferation of artificial intelligence (AI) systems across diverse domains raises critical questions about the feasibility of meaningful human oversight, particularly in high-stakes domains such as new biotechnology. As AI systems grow increasingly complex, opaque, and autonomous, ensuring responsible use becomes a formidable challenge. During our editorial work for the special issue “Artificial Intelligence for Life Sciences”, we placed increasing emphasis on the topic of “human oversight”. Consequently, in this editorial we briefly discuss the evolving role of human oversight in AI governance, focusing on the practical, technical, and ethical dimensions of maintaining control. It examines how the complexity of contemporary AI architectures, such as large-scale neural networks and generative AI applications, undermine human understanding and decision-making capabilities. Furthermore, it evaluates emerging approaches—such as explainable AI (XAI), human-in-the-loop systems, and regulatory frameworks—that aim to enable oversight while acknowledging their limitations. Through a comprehensive analysis, the picture emerged while complete oversight may no longer be viable in certain contexts, strategic interventions leveraging human-AI collaboration and trustworthy AI design principles can preserve accountability and safety. The discussion highlights the urgent need for interdisciplinary efforts to rethink oversight mechanisms in an era where AI may outpace human comprehension. © 2024 The Authors",Artificial intelligence; Biotechnology; Deep learning; Digital transformation; Machine learning,"Artificial Intelligence; Biotechnology; Humans; Neural Networks, Computer; artificial intelligence; decision making; deep learning; Editorial; ethics; explainable artificial intelligence; generative artificial intelligence; human; human oversight; information processing; artificial neural network; biotechnology",Editorial,Final,,Scopus,2-s2.0-85212069001
Swaroop S.; Buçinca Z.; Gajos K.Z.; Doshi-Velez F.,"Swaroop, Siddharth (57218716133); Buçinca, Zana (57195217062); Gajos, Krzysztof Z. (8375653300); Doshi-Velez, Finale (34874672900)",57218716133; 57195217062; 8375653300; 34874672900,Personalising AI Assistance Based on Overreliance Rate in AI-Assisted Decision Making,2025,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,1107,1122,15,1,10.1145/3708359.3712128,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001917996&doi=10.1145%2f3708359.3712128&partnerID=40&md5=fed388b7492f62098e7869b4fefe5e0b,"Personalising decision-making assistance to different users and tasks can improve human-AI team performance, such as by appropriately impacting reliance on AI assistance. However, people are different in many ways, with many hidden qualities, and adapting AI assistance to these hidden qualities is difficult. In this work, we consider a hidden quality previously identified as important: overreliance on AI assistance. We would like to (i) quickly determine the value of this hidden quality, and (ii) personalise AI assistance based on this value. In our first study, we introduce a few probe questions (where we know the true answer) to determine if a user is an overrelier or not, finding that correctly-chosen probe questions work well. In our second study, we improve human-AI team performance, personalising AI assistance based on users' overreliance quality. Exploratory analysis indicates that people learn different strategies of using AI assistance depending on what AI assistance they saw previously, indicating that we may need to take this into account when designing adaptive AI assistance. We hope that future work will continue exploring how to infer and personalise to other important hidden qualities.  © 2025 Copyright held by the owner/author(s).",adaptive AI; AI-assisted decision-making; decision support systems; explainable AI; human-AI interaction; human-centered AI; overreliance; reinforcement learning; time pressure,Adaptive AI; AI-assisted decision-making; Decision supports; Decisions makings; Explainable AI; Human-AI interaction; Human-centered AI; Overreliance; Reinforcement learnings; Support systems; Time pressures; Reinforcement learning,Conference paper,Final,,Scopus,2-s2.0-105001917996
Park J.; Kang D.,"Park, Jiyoung (57218891881); Kang, Dongheon (56767427600)",57218891881; 56767427600,Artificial Intelligence and Smart Technologies in Safety Management: A Comprehensive Analysis Across Multiple Industries,2024,Applied Sciences (Switzerland),14,24,11934,,,,4,10.3390/app142411934,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213257089&doi=10.3390%2fapp142411934&partnerID=40&md5=c8b26628f3ae790fa40391dfc6c843b6,"The integration of Artificial Intelligence (AI) and smart technologies into safety management is a pivotal aspect of the Fourth Industrial Revolution or Industry 4.0. This study conducts a systematic literature review to identify and analyze how AI and smart technologies enhance safety management across various sectors within the Safety 4.0 paradigm. Focusing on peer-reviewed journal articles that explicitly mention “Smart”, “AI”, or “Artificial Intelligence” in their titles, the research examines key safety management factors, such as accident prevention, risk management, real-time monitoring, and ethical implementation, across sectors, including construction, industrial safety, disaster and public safety, transport and logistics, energy and power, health, smart home and living, and other diverse industries. AI-driven solutions, such as predictive analytics, machine learning algorithms, IoT sensor integration, and digital twin models, are shown to proactively identify and mitigate potential hazards, optimize energy consumption, and enhance operational efficiency. For instance, in the energy and power sector, intelligent gas meters and automated fire suppression systems manage gas-related risks effectively, while in the health sector, AI-powered health monitoring devices and mental health support applications improve patient and worker safety. The analysis reveals a significant trend towards shifting from reactive to proactive safety management, facilitated by the convergence of AI with IoT and Big Data analytics. Additionally, ethical considerations and data privacy emerge as critical challenges in the adoption of AI technologies. The study highlights the transformative role of AI in enhancing safety protocols, reducing accident rates, and improving overall safety outcomes across industries. It underscores the need for standardized protocols, robust AI governance frameworks, and interdisciplinary research to address existing challenges and maximize the benefits of AI in safety management. Future research directions include developing explainable AI models, enhancing human–AI collaboration, and fostering global standardization to ensure the responsible and effective implementation of AI-driven safety solutions. © 2024 by the authors.",artificial intelligence; Industry 4.0; proactive safety; Safety 4.0; safety management; smart technologies,Ethical technology; Fire extinguishers; Fire hazards; Health hazards; International cooperation; Risk management; Artificial intelligence technologies; Comprehensive analysis; Industrial revolutions; Journal articles; Management IS; Proactive safety; Safety 4.0; Safety management; Smart technology; Systematic literature review; Gas hazards,Review,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85213257089
Skarzyńska E.; Paliszkiewicz J.; Dabrowski I.; Mendel M.,"Skarzyńska, Edyta (57891712600); Paliszkiewicz, Joanna (23005756100); Dabrowski, Ireneusz (57221759149); Mendel, Marta (55140650500)",57891712600; 23005756100; 57221759149; 55140650500,"Risks, failures, and ethical dilemmas of AI technologies and trust",2025,Trust in Generative Artificial Intelligence: Human-Robot Interaction and Ethical Considerations,,,,3,11,8,0,10.4324/9781003586937-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000006005&doi=10.4324%2f9781003586937-2&partnerID=40&md5=8b36efa3cc5879ea6a38fb6b0f1fde7c,"This chapter explores the risks, failures, and ethical dilemmas associated with artificial intelligence (AI) technologies, with a focus on their implications for public trust. While AI promises significant benefits across sectors, from health care to commerce, its rapid adoption also introduces risks at both individual and systemic levels. Issues such as data quality, algorithmic bias, and privacy concerns can lead to malfunctions and injustices that undermine the reliability of AI systems. Additionally, the inherent complexity of AI models, often operating as ""black boxes,"" complicates accountability and transparency, intensifying public concerns about trust and fairness. Ethical dilemmas arise in areas like algorithmic decision-making, where the lack of explainability and accountability questions the moral integrity of AI systems. This chapter emphasizes the need for sustainable and ethical AI design that aligns with societal values and legal standards to foster trust. International guidelines and ethical frameworks, such as those proposed by the European Union, highlight the importance of transparency, privacy, and accountability in AI. By addressing these ethical and operational challenges, stakeholders can contribute to a more responsible and trust-based integration of AI into society, ensuring its benefits while mitigating associated risks. © 2025 selection and editorial matter, Joanna Paliszkiewicz, Ireneusz Dabrowski and Leila Halawi. All rights reserved.",,,Book chapter,Final,,Scopus,2-s2.0-105000006005
Vasileiou S.L.; Kumar A.; Yeoh W.; Son T.C.; Toni F.,"Vasileiou, Stylianos Loukas (57220053434); Kumar, Ashwin (57670852600); Yeoh, William (57202488768); Son, Tran Cao (7006447691); Toni, Francesca (6603756423)",57220053434; 57670852600; 57202488768; 7006447691; 6603756423,Dialectical Reconciliation via Structured Argumentative Dialogues,2024,Proceedings of the International Conference on Knowledge Representation and Reasoning,,,,777,787,10,2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210509975&partnerID=40&md5=f57a039e1cb926c8f257d8bbc5c6b471,"We present a novel framework designed to extend model reconciliation approaches, commonly used in human-aware planning, for enhanced human-AI interaction. By adopting a structured argumentation-based dialogue paradigm, our framework enables dialectical reconciliation to address knowledge discrepancies between an explainer (AI agent) and an explainee (human user), where the goal is for the explainee to understand the explainer’s decision. We formally describe the operational semantics of our proposed framework, providing theoretical guarantees. We then evaluate the framework’s efficacy “in the wild” via computational and human-subject experiments. Our findings suggest that our framework offers a promising direction for fostering effective human-AI interactions in domains where explainability is important. © 2024 Proceedings of the International Conference on Knowledge Representation and Reasoning. All rights reserved.",,Semantics; Argumentative dialogues; Human subject experiments; Human users; Human-aware; Model reconciliation; Operational semantics; Theoretical guarantees; Knowledge representation,Conference paper,Final,,Scopus,2-s2.0-85210509975
Rong Y.; Scheerer D.; Kasneci E.,"Rong, Yao (57215968958); Scheerer, David (59193554800); Kasneci, Enkelejda (56059892600)",57215968958; 59193554800; 56059892600,Faithful Attention Explainer: Verbalizing Decisions Based on Discriminative Features,2024,CEUR Workshop Proceedings,3793,,,33,40,7,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208289099&partnerID=40&md5=ebc90b8dba5c7d316c6f9f2f4f81faf4,"In recent years, model explanation methods have been designed to interpret model decisions faithfully and intuitively so that users can easily understand them. In this paper, we propose a framework, Faithful Attention Explainer (FAE), capable of generating faithful textual explanations regarding the attended-to features. Towards this goal, we deploy an attention module that takes the visual feature maps from the classifier for sentence generation. Furthermore, our method successfully learns the association between features and words, which allows a novel attention enforcement module for attention explanation. Our model achieves promising performance in caption quality metrics and a faithful decision-relevance metric on two datasets (CUB and ACT-X). In addition, we show that FAE can interpret gaze-based human attention, as human gaze indicates the discriminative features that humans use for decision-making, demonstrating the potential of deploying human gaze for advanced human-AI interaction. © 2022 Copyright for this paper by its authors.",Explainable AI (XAI); Faithfulness; Saliency Map; Textual Explanations; Visual Explanation,Decision-based; Discriminative features; Explainable AI (XAI); Faithfulness; Feature map; Modeling decisions; Saliency map; Textual explanation; Visual explanation; Visual feature; Decision making,Conference paper,Final,,Scopus,2-s2.0-85208289099
Alami J.; El Iskandarani M.; Riggs S.L.,"Alami, Jawad (57828306500); El Iskandarani, Mohamad (58995655900); Riggs, Sara Lu (57004526300)",57828306500; 58995655900; 57004526300,The Effect of Workload and Task Priority on Multitasking Performance and Reliance on Level 1 Explainable AI (XAI) Use,2025,Human Factors,,,,,,,0,10.1177/00187208251323478,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000178288&doi=10.1177%2f00187208251323478&partnerID=40&md5=714eba7be42e2132ef998599f11c00f8,"Objective: This study investigates the effects of workload and task priority on multitasking performance and reliance on Level 1 Explainable Artificial Intelligence (XAI) systems in high-stakes decision environments. Background: Operators in critical settings manage multiple tasks under varying levels of workload and priority, potentially leading to performance degradation. XAI offers opportunities to support decision making by providing insights into AI’s reasoning, yet its adoption and effectiveness in multitasking scenarios remain underexplored. Method: Thirty participants engaged in a simulated multitasking environment, involving UAV command and control tasks, with the assistance of a Level 1 (i.e., basic perceptual information) XAI system on one of the tasks. The study utilized a within-subjects experimental design, manipulating workload (low, medium, and high) and AI-supported-task priority (low and high) across six conditions. Participants’ accuracy, use of automatic rerouting, AI miss detection, false alert identification, and use of AI explanations were measured and analyzed across the different experimental conditions. Results: Workload significantly hindered performance on the AI-assisted task and increased reliance on the AI system especially when the AI-assisted task was given low priority. The use of AI explanations was significantly affected by task priority only. Conclusion: An increase in workload led to proper offloading by relying on the AI’s alerts, but it also led to a lower rate of alert verification despite the alert feature’s high false alert rate. Application: The findings from the present work help inform AI system designers on how to design their systems for high-stakes environments such that reliance on AI is properly calibrated. © 2025 Human Factors and Ergonomics Society.",automation; explanation; multitasking; performance; reliance; workload,Artificial intelligence; Decision making; AI systems; Decision environment; Explanation; Level-1; Multiple tasks; Performance; Performance degradation; Reliance; Task priorities; Workload; Ergonomics,Article,Article in press,,Scopus,2-s2.0-105000178288
Correa N.; Correa A.; Zadrozny W.,"Correa, Nelson (58068955300); Correa, Antonio (58069163100); Zadrozny, Wlodek (6603116034)",58068955300; 58069163100; 6603116034,"Generative AI for Consumer Communications: Classification, Summarization, Response Generation",2024,"IEEE Andescon, ANDESCON 2024 - Proceedings",,,,,,,1,10.1109/ANDESCON61840.2024.10755794,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211963129&doi=10.1109%2fANDESCON61840.2024.10755794&partnerID=40&md5=a1e94f01496c7c7f1162d0ac0b9af204,"Generative AI showed the unexpected power of large language models (LLMs) for understanding and generation of natural language text and other modalities at the end of 2022. This paper presents a novel generative AI system for text classification, summarization and response generation of consumer communications. The system uses the same foundation model and a uniform pipeline for the tasks proposed. Consumer communications are massive and served mainly via voice and text, and until recently could be handled only with human agents (customer service representatives). However, they must be handled with quality, consistency, speed and low cost, at scale. We limit our attention to financial consumer communications from the U.S. Consumer Financial Protection Bureau (CFPB), publicly available in a dataset of over 4.7 million complaints. Performance reaches 88 % accuracy (without fine-tuning) for classification and over 72 % for summarization and response generation. Artificial intelligence has great positive impacts for business and society, but its application and deployment also poses risks and unknowns. We thus address the important questions of risk, bias, interpretability, explainability, safety and regulatory compliance with the emerging legal frameworks. © 2024 IEEE.",AI safety; Generative AI; large language models; natural language processing; text classification; text summarization; vector embeddings,AI safety; Embeddings; Generative AI; Language model; Language processing; Large language model; Natural language processing; Natural languages; Text classification; Text Summarisation; Vector embedding; Natural language processing systems,Conference paper,Final,,Scopus,2-s2.0-85211963129
Soldatos J.,"Soldatos, John (8662280800)",8662280800,"Artificial intelligence in manufacturing: Enabling intelligent, flexible and cost-effective production through AI",2024,"Artificial Intelligence in Manufacturing: Enabling Intelligent, Flexible and Cost-Effective Production Through AI",,,,1,505,504,4,10.1007/978-3-031-46452-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201858561&doi=10.1007%2f978-3-031-46452-2&partnerID=40&md5=c04ead5b85b72172e3f3dd18867267ce,"This open access book presents a rich set of innovative solutions for artificial intelligence (AI) in manufacturing. The various chapters of the book provide a broad coverage of AI systems for state of the art flexible production lines including both cyber-physical production systems (Industry 4.0) and emerging trustworthy and human-centered manufacturing systems (Industry 5.0). From a technology perspective, the book addresses a wide range of AI paradigms such as deep learning, reinforcement learning, active learning, agent-based systems, explainable AI, industrial robots, and AI-based digital twins. Emphasis is put on system architectures and technologies that foster human-AI collaboration based on trusted interactions between workers and AI systems. From a manufacturing applications perspective, the book illustrates the deployment of these AI paradigms in a variety of use cases spanning production planning, quality control, anomaly detection, metrology, workers' training, supply chain management, as well as various production optimization scenarios. This is an open access book. © The Author(s) 2024. All rights reserved.",Explainable AI; Industry 4.0; Industry 5.0; Intelligent agents; Open access; Open access; Reinforcement learning,,Book,Final,,Scopus,2-s2.0-85201858561
Hua D.; Petrina N.; Sacks A.J.; Young N.; Cho J.-G.; Smith R.; Poon S.K.,"Hua, David (58145423100); Petrina, Neysa (37075697300); Sacks, Alan J. (58714950600); Young, Noel (7402413046); Cho, Jin-Gun (39061078100); Smith, Ross (59521106200); Poon, Simon K. (16301806100)",58145423100; 37075697300; 58714950600; 7402413046; 39061078100; 59521106200; 16301806100,Towards human-AI collaboration in radiology: a multidimensional evaluation of the acceptability of AI for chest radiograph analysis in supporting pulmonary tuberculosis diagnosis,2025,JAMIA Open,8,1,ooae151,,,,0,10.1093/jamiaopen/ooae151,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217842449&doi=10.1093%2fjamiaopen%2fooae151&partnerID=40&md5=1f6b229e6e45acf4b4d5b108161795d3,"Objective: Artificial intelligence (AI) technology promises to be a powerful tool in addressing the global health challenges posed by tuberculosis (TB). However, evidence for its real-world impact is lacking, which may hinder safe, responsible adoption. This case study addresses this gap by assessing the technical performance, usability and workflow aspects, and health impact of implementing a commercial AI system (qXR by Qure.ai) to support Australian radiologists in diagnosing pulmonary TB. Materials and Methods: A retrospective diagnostic accuracy evaluation was conducted to establish the technical performance of qXR in detecting TB compared to a human radiologist and microbiological reference standard. A qualitative human factors assessment was performed to investigate the user experience and clinical decision-making process of radiologists using qXR. A task productivity analysis was completed to quantify how the radiological screening turnaround time is impacted. Results: qXR displays near-human performance satisfying the World Health Organization’s suggested accuracy profile. Radiologists reported high satisfaction with using qXR based on minimal workflow disruptions, respect for their professional autonomy, and limited increases in workload burden despite poor algorithm explainability. qXR delivers considerable productivity gains for normal cases and optimizes resource allocation through redistributing time from normal to abnormal cases. Discussion and Conclusion: This study provides preliminary evidence of how an AI system with reasonable diagnostic accuracy and a human-centered user experience can meaningfully augment the TB diagnostic workflow. Future research needs to investigate the impact of AI on clinician accuracy, its relationship with efficiency, and best practices for optimizing the impact of clinician-AI collaboration. © The Author(s) 2025. Published by Oxford University Press on behalf of the American Medical Informatics Association.",artificial intelligence; evaluation; human factors; pulmonary tuberculosis; technical performance; translational impact,algorithm; Article; artificial intelligence; Australian; biomedical technology assessment; case study; clinical decision making; clinician; comparative study; diagnostic accuracy; diagnostic test accuracy study; health impact assessment; human; image analysis; job experience; lung tuberculosis; microbiological examination; performance; predictive value; productivity; professional practice; quantitative analysis; radiologist; resource allocation; retrospective study; satisfaction; sensitivity and specificity; standard; thematic analysis; thorax radiography; turnaround time; usability; workflow; workload; World Health Organization,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85217842449
Buschmeyer K.; Zenner J.; Hatfield S.,"Buschmeyer, Katharina (57729227600); Zenner, Julie (58640222800); Hatfield, Sarah (57729679300)",57729227600; 58640222800; 57729679300,Effectiveness of AI-based decision support systems in work environment: a systematic literature review,2024,International Journal of Human Factors and Ergonomics,11,5,,1,54,53,0,10.1504/IJHFE.2024.142761,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210291373&doi=10.1504%2fIJHFE.2024.142761&partnerID=40&md5=f5cfa63c1738d0037c813af0218982f9,"Artificial intelligence (AI) is being increasingly used in high-stakes working areas to augment experts in challenging decision-making situations. The AI support is intended to reduce the cognitive load on experts, which should ideally be reflected both in a greater sense of well-being when working on demanding tasks and in joint performance exceeding that of both the humans and AI alone. However, the extent and conditions of achievement (such as the AI accuracy and explainability) of these intended effects have not been systematically investigated. Therefore, we identified and reviewed 44 articles published since 2018 that have investigated the effects of AI-based decision support systems on experts in controlled experimental settings. The results suggest that, for optimal human-AI performance, which surpasses the performance of either alone, both must operate at similar and high levels. However, the effect on the psychological load remains unclear owing to limited research.  © The Author(s) 2024.",AI-based decision support systems; artificial intelligence; cognitive relief; decision making; task performance,,Review,Final,,Scopus,2-s2.0-85210291373
Al E'mari S.; Sanjalawe Y.; Fataftah F.; Hajjaj R.,"Al E'mari, Salam (59566680700); Sanjalawe, Yousef (57194709597); Fataftah, Fuad (59901819300); Hajjaj, Rula (59902014300)",59566680700; 57194709597; 59901819300; 59902014300,Foundations of autonomous cyber defense systems,2025,AI-Driven Security Systems and Intelligent Threat Response Using Autonomous Cyber Defense,,,,1,33,32,0,10.4018/979-8-3373-0954-5.ch001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005382708&doi=10.4018%2f979-8-3373-0954-5.ch001&partnerID=40&md5=d43f677d12328488b0db1eb229e7dce1,"Traditional cybersecurity methods are becoming inadequate due to the growing complexity and frequency of cyber threats. This chapter explores autonomous cyber defense systems-self-sustaining, intelligent frameworks that detect, analyze, and respond to threats in real time without human intervention. Leveraging AI, Machine Learning, Reinforcement Learning, NLP, and Explainable AI, these systems enable adaptive and scalable security operations. The chapter analyzes system architectures across varying autonomy levels-human-in-the-loop, on-the-loop, and out-of-the-loop-and discusses enabling technologies such as Cyber Threat Intelligence. It reviews modern threats including zero-day exploits, AI-driven malware, and APTs, highlighting the advantages of autonomous systems in resilience and responsiveness. Practical frameworks, deployment strategies, and real-world case studies are presented with performance and ethical evaluation. The chapter concludes with future directions such as quantum-resilient architectures and sustainable cybersecurity strategies. © 2025, IGI Global Scientific Publishing. All rights reserved.",,Artificial life; Computer viruses; Computer worms; Cyber attacks; Embedded systems; Intelligent systems; Robotics; Adaptive security; Cyber security; Cyber threats; Cyber-defense; Defence systems; Human intervention; Learning reinforcements; Machine-learning; Real- time; Reinforcement learnings; Reinforcement learning,Book chapter,Final,,Scopus,2-s2.0-105005382708
Tiwari A.; Masilamani M.; Rao T.S.; Zope S.; Deepak S.A.; Karthick L.,"Tiwari, Ajay (59562599200); Masilamani, M. (59561916800); Rao, T. Srinivasa (57191434082); Zope, Sharmila (57205026399); Deepak, S.A. (56644420000); Karthick, L. (57225086019)",59562599200; 59561916800; 57191434082; 57205026399; 56644420000; 57225086019,Shaping the future: Emerging trends and strategic predictions in big data and AI,2025,AI and the Revival of Big Data,,,,125,153,28,1,10.4018/979-8-3693-8472-5.ch006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218162219&doi=10.4018%2f979-8-3693-8472-5.ch006&partnerID=40&md5=f7791f709e3ff5de8127accfe5889d43,"Big Data and AI are changing global enterprises, cultures, and human- machine interaction. This chapter discusses this fast- changing world and the intricate interplay between these two massive forces. Industry- wide AI-powered automation, real- time data analytics for agile decision- making, quantum computing's disruptive potential and ramifications for AI capabilities, edge AI's impact on dispersed intelligence, and explainable AI (XAI)'s role in trust and transparency are future topics and Fast technological breakthroughs can innovate and solve global issues, but they also present huge challenges. This chapter forecasts data privacy and robust security in an increasingly interconnected world, algorithmic bias mitigation and AI system fairness and equity, the complex ethical issues surrounding massive AI adoption and its potential societal impacts, the changing regulatory landscape and the need for adaptive governance frameworks, and human- in- the- loop using cutting- edge research, industry best practices, and real- world case studies. © 2025, IGI Global Scientific Publishing. All rights reserved.",,,Book chapter,Final,,Scopus,2-s2.0-85218162219
Wood N.G.,"Wood, Nathan Gabriel (57210274373)",57210274373,Explainable AI in the military domain,2024,Ethics and Information Technology,26,2,29,,,,2,10.1007/s10676-024-09762-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190401777&doi=10.1007%2fs10676-024-09762-w&partnerID=40&md5=35f96aaf468aba4f30c0e72f20244b3d,"Artificial intelligence (AI) has become nearly ubiquitous in modern society, from components of mobile applications to medical support systems, and everything in between. In societally impactful systems imbued with AI, there has been increasing concern related to opaque AI, that is, artificial intelligence where it is unclear how or why certain decisions are reached. This has led to a recent boom in research on “explainable AI” (XAI), or approaches to making AI more explainable and understandable to human users. In the military domain, numerous bodies have argued that autonomous and AI-enabled weapon systems ought not incorporate unexplainable AI, with the International Committee of the Red Cross and the United States Department of Defense both explicitly including explainability as a relevant factor in the development and use of such systems. In this article, I present a cautiously critical assessment of this view, arguing that explainability will be irrelevant for many current and near-future autonomous systems in the military (which do not incorporate any AI), that it will be trivially incorporated into most military systems which do possess AI (as these generally possess simpler AI systems), and that for those systems with genuinely opaque AI, explainability will prove to be of more limited value than one might imagine. In particular, I argue that explainability, while indeed a virtue in design, is a virtue aimed primarily at designers and troubleshooters of AI-enabled systems, but is far less relevant for users and handlers actually deploying these systems. I further argue that human–machine teaming is a far more important element of responsibly using AI for military purposes, adding that explainability may undermine efforts to improve human–machine teamings by creating a prima facie sense that the AI, due to its explainability, may be utilized with little (or less) potential for mistakes. I conclude by clarifying that the arguments are not against XAI in the military, but are instead intended as a caution against over-inflating the value of XAI in this domain, or ignoring the limitations and potential pitfalls of this approach. © The Author(s) 2024.",AI; Artificial intelligence; Autonomous weapon systems; Explainability; Human–machine interaction,Autonomous weapon system; Explainability; Human machine interaction; Human users; Human-machine; International committee of the red cross; Military domains; Mobile applications; Support systems; Weapon system; Artificial intelligence,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85190401777
Bergomi L.,"Bergomi, Laura (58971133100)",58971133100,Fostering Human-AI interaction: development of a Clinical Decision Support System enhanced by eXplainable AI and Natural Language Processing,2024,CEUR Workshop Proceedings,3793,,,321,328,7,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208286837&partnerID=40&md5=71f74ced0cdd204f075646a5d0344846,"Artificial Intelligence (AI) is increasingly integrated into Decision Support Systems (DSS). The explainability of AI-based systems becomes crucial in sensitive and critical domains, such as healthcare, where ethical considerations and reliability are paramount concerns. In the clinical setting, it is important to evaluate how humans and AI can collaborate on cognitive tasks. Collaboration protocols (HAI-CP) allow for the investigation of the usefulness of AI models and their impact on users (both positive and negative). Although research on the application of these methods is blooming, there is little understanding of the impact on clinical decision-making, especially for eXplainable AI (XAI) systems, due to the lack of user studies. Therefore, the goal of this proposal is to develop a clinical DSS enhanced by XAI and Natural Language Processing (NLP): their synergy can add value to the interaction between users and AI, fostering a more linguistically natural, comprehensible, trustworthy, and supporting interfacing, that blends into the existing workflows. This proposal explores potential solutions to tailor natural language explanations and data visualizations to the end-user, improving the comprehensibility of the reasons behind a decision, and increasing the user’s confidence in the decision; investigates and tests possible strategies to “get the patient-in-the-loop”; explores uncertainty quantification and counterfactual approaches, and finally assesses the impact on naturalistic (i.e., real-world) decision-making and long-term effects and biases. © 2024 Copyright for this paper by its authors.",Clinical decision making; Explainable artificial intelligence; Human-AI collaboration protocol; Natural language interaction,Clinical research; Decision making; Natural language processing systems; Visual languages; Clinical decision making; Clinical decision support systems; Collaboration protocols; Decision supports; Explainable artificial intelligence; Human-artificial intelligence collaboration protocol; Language processing; Natural language interaction; Natural languages; Support systems; Decision support systems,Conference paper,Final,,Scopus,2-s2.0-85208286837
Guzzi J.; Giusti A.,"Guzzi, Jérôme (55857399600); Giusti, Alessandro (23392613000)",55857399600; 23392613000,Human-in-the-loop testing of the explainability of robot navigation algorithms in extended reality,2024,CEUR Workshop Proceedings,3793,,,297,304,7,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208285748&partnerID=40&md5=d38e67e24dac709843e4247e6b876415,"Developing robots to be deployed in spaces shared with people requires testing with humans-in-the-loop. In fact, only humans co-located with the robots are in a suitable position to judge the robots’ behaviors. This is especially important when people participate in the same task as the robots, like when navigating a shared environment: the actions of people and robots influence each other. To test the resulting dynamic system, we need to let real people experience the interaction, which, when done in simulation, translates to immersing real users using Virtual or Mixed Reality. We implement and demonstrate this solution where users experience variable legibility, predictability, and explainability of the robot navigation algorithms, depending on if and how the robots explicitly communicate their intentions. © 2024 Copyright for this paper by its authors.",Human-in-the loop; Social robotic navigation; Virtual Reality,Microrobots; Mixed reality; Social robots; Co-located; Human-in-the-loop; Mixed reality; Navigation algorithms; Robot behavior; Robot navigation; Robotic navigation; Social robotic navigation; Social robotics; Users' experiences; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-85208285748
Herrera F.,"Herrera, Francisco (7102347190)",7102347190,Reflections and attentiveness on eXplainable Artificial Intelligence (XAI). The journey ahead from criticisms to human–AI collaboration,2025,Information Fusion,121,,103133,,,,1,10.1016/j.inffus.2025.103133,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001229308&doi=10.1016%2fj.inffus.2025.103133&partnerID=40&md5=b07f7a8e854fe51b3ab1de35026413c7,"The emergence of deep learning over the past decade has driven the development of increasingly complex AI models, amplifying the need for Explainable Artificial Intelligence (XAI). As AI systems grow in size and complexity, ensuring interpretability and transparency becomes essential, especially in high-stakes applications. With the rapid expansion of XAI research, addressing emerging debates and criticisms requires a comprehensive examination. This paper explores the complexities of XAI from multiple perspectives, proposing six key axes that shed light on its role in human–AI interaction and collaboration. First, it examines the imperative of XAI under the dominance of black-box AI models. Given the lack of definitional cohesion, the paper argues that XAI must be framed through the lens of audience and understanding, highlighting its different uses in AI–human interaction. The recent BLUE vs. RED XAI distinction is analyzed through this perspective. The study then addresses the criticisms of XAI, evaluating its maturity, current trajectory, and limitations in handling complex problems. The discussion then shifts to explanations as a bridge between AI models and human understanding, emphasizing the importance of usability of explanations in human–AI decision making. Key aspects such as AI reliance, human intuition, and emerging collaboration theories — including the human-algorithm centaur and co-intelligence paradigms — are explored in connection with XAI. The medical field is considered as a case study, given its extensive research on collaboration between doctors and AI through explainability. The paper proposes a framework to evaluate the maturity of XAI using three dimensions: practicality, auditability, and AI governance. Provide the final lessons learned focused on trends and questions to tackle in the near future. This is an in-depth exploration of the impact and urgency of XAI in the era of pervasive expansion of AI. Three Key reflections from this study include: (a) XAI must enhance cognitive engagement with explanations, (b) it must evolve to fully address why, what, and for what purpose explanations are needed, and (c) it plays a crucial role in building societal trust in AI. By advancing XAI in these directions, we can ensure that AI remains transparent, auditable, and accountable, and aligned with human needs. © 2025 Elsevier B.V.",AI governance; AI safety; Auditability; eXplainable Artificial Intelligence; Explanations; Human–AI collaboration; Human–AI decision-making; Maturity level of explainability; XAI audience; XAI criticisms,Decision making; Deep learning; AI governance; AI safety; Auditability; Decisions makings; Explainable artificial intelligence; Explanation; Human–AI collaboration; Human–AI decision-making; Maturity level of explainability; Maturity levels; XAI audience; XAI criticism; Economic and social effects,Article,Final,,Scopus,2-s2.0-105001229308
Francis A.; Pérez-D'Arpino C.; Li C.; Xia F.; Alahi A.; Alami R.; Bera A.; Biswas A.; Biswas J.; Chandra R.; Chiang H.-T.L.; Everett M.; Ha S.; Hart J.; How J.P.; Karnan H.; Lee T.-W.E.; Manso L.J.; Mirsky R.; Pirk S.; Singamaneni P.T.; Stone P.; Taylor A.V.; Trautman P.; Tsoi N.; Vázquez M.; Xiao X.; Xu P.; Yokoyama N.; Toshev A.; Martín-Martín R.,"Francis, Anthony (57207571685); Pérez-D'Arpino, Claudia (27267865200); Li, Chengshu (57214363595); Xia, Fei (57191034548); Alahi, Alexandre (34869135400); Alami, Rachid (7003803131); Bera, Aniket (56462306900); Biswas, Abhijat (57197845136); Biswas, Joydeep (35774160500); Chandra, Rohan (57211498752); Chiang, Hao-Tien Lewis (56742715400); Everett, Michael (57195425375); Ha, Sehoon (51261111200); Hart, Justin (25031145200); How, Jonathan P. (57203030489); Karnan, Haresh (57200625157); Lee, Tsang-Wei Edward (57215561854); Manso, Luis J. (56602601300); Mirsky, Reuth (57192390008); Pirk, Sören (37091447900); Singamaneni, Phani Teja (57216331576); Stone, Peter (7203001213); Taylor, Ada V. (57214767457); Trautman, Peter (36803082100); Tsoi, Nathan (57214458789); Vázquez, Marynel (37017787700); Xiao, Xuesu (56742595000); Xu, Peng (57606508900); Yokoyama, Naoki (57222866446); Toshev, Alexander (22235568300); Martín-Martín, Roberto (36996641700)",57207571685; 27267865200; 57214363595; 57191034548; 34869135400; 7003803131; 56462306900; 57197845136; 35774160500; 57211498752; 56742715400; 57195425375; 51261111200; 25031145200; 57203030489; 57200625157; 57215561854; 56602601300; 57192390008; 37091447900; 57216331576; 7203001213; 57214767457; 36803082100; 57214458789; 37017787700; 56742595000; 57606508900; 57222866446; 22235568300; 36996641700,Principles and Guidelines for Evaluating Social Robot Navigation Algorithms,2025,ACM Transactions on Human-Robot Interaction,14,2,34,,,,1,10.1145/3700599,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003641956&doi=10.1145%2f3700599&partnerID=40&md5=46df44bda948ff8c9c3f089b67ce4774,"A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this article, we pave the road toward common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributions include (a) a definition of a socially navigating robot as one that respects the principles of safety, comfort, legibility, politeness, social competency, agent understanding, proactivity, and responsiveness to context, (b) guidelines for the use of metrics, development of scenarios, benchmarks, datasets, and simulators to evaluate social navigation, and (c) a design of a social navigation metrics framework to make it easier to compare results from different simulators, robots, and datasets.  © 2025 Copyright held by the owner/author(s).",benchmarks; datasets; robot navigation; simulators; social robotics,Industrial robots; Intelligent robots; Benchmark; Dataset; Human agent; Navigation algorithms; Robot navigation; Robotic agents; Social navigation; Social robotics; Social robots; Static environment; Social robots,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-105003641956
Rheem H.; Lee J.; Lee J.D.; Szczerba J.F.; Tsimhoni O.,"Rheem, Hansol (57208275267); Lee, Joonbum (57189577286); Lee, John D. (55957582900); Szczerba, Joseph F. (55687916900); Tsimhoni, Omer (7801582027)",57208275267; 57189577286; 55957582900; 55687916900; 7801582027,Explaining Automated Vehicle Behavior at an Appropriate Abstraction Level and Timescale to Maintain Common Ground,2025,Journal of Cognitive Engineering and Decision Making,19,2,,174,199,25,0,10.1177/15553434251318477,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002267951&doi=10.1177%2f15553434251318477&partnerID=40&md5=f362e36f02e93d456142d2077dedc4ca,"Automation is becoming increasingly complex, playing a larger role in driving and expanding its operational design domain to dynamic urban roads. Explainable AI (XAI) research in computer science aims to craft explanations of automation that help people understand the behavior of complex algorithms. However, many XAI approaches rely on fixed-format explanations, which may not effectively support drivers with varying levels of automation knowledge and tasks with different timescales. Maintaining common ground is a multilevel process, in which individuals and automation must adjust communication format and abstraction based on knowledge and time constraints. We first draw on existing research to suggest that common ground is a shared understanding between drivers and automation that requires constant maintenance. We applied the abstraction hierarchy (AH) modeling method, which describes complex systems across multiple abstraction levels to match drivers’ cognitive capacity. We modified it to translate vehicle and traffic data into multilevel explanations of automation behavior. We expanded the model into the abstraction–decomposition space, naming it the Driver–Automation Teaming model, designed to generate explanations that account for task timescale. With this modified model, we developed three human–machine interface concepts to demonstrate how it can improve XAI’s support for driver–automation collaboration. © 2025, Human Factors and Ergonomics Society.",abstraction hierarchy; automated driving; common ground; communication; eXplainable AI; human-autonomy teaming; human–machine interface; work domain analysis,Cognitive systems; Ergonomics; Hierarchical systems; Large scale systems; Man machine systems; Abstraction hierarchy; Abstraction level; Automated driving; Automated vehicles; Common ground; Explainable AI; Human Machine Interface; Human-autonomy teaming; Time-scales; Work domain analysis,Article,Final,,Scopus,2-s2.0-105002267951
Bhakte A.; Kumar Kumawat P.; Srinivasan R.,"Bhakte, Abhijit (57396331700); Kumar Kumawat, Piyush (59226615800); Srinivasan, Rajagopalan (23981378700)",57396331700; 59226615800; 23981378700,Explainable AI methodology for understanding fault detection results during Multi-Mode operations,2024,Chemical Engineering Science,299,,120493,,,,3,10.1016/j.ces.2024.120493,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199017993&doi=10.1016%2fj.ces.2024.120493&partnerID=40&md5=3c51ab8dbbb58054f34cd72e4e0bf48c,"Multi-mode operations are prevalent in the chemical industry. Various methods have been proposed for monitoring multi-mode operations. Of these, AI-based approaches such as Deep Neural Networks (DNN) are becoming popular due to their higher accuracy. However, the lack of transparency of DNNs hinders their widespread adoption in safety–critical applications such as process monitoring. This work addresses this limitation by proposing an Explainable AI (XAI) methodology for multi-mode operations. The proposed methodology encompasses a supervisory system that identifies the current operational mode. This information is used by an Integrated Gradient (IG) based XAI method to configure mode-specific baselines and thus generate DNN explanations corresponding to each operational mode. The ability of this methodology to generate reliable explanations to aid plant operators is illustrated through a simulated CSTR process, the Tennessee-Eastman process, and the pilot-scale Multiphase Flow Facility case study. © 2024 Elsevier Ltd",Deep Learning; Explainable Artificial Intelligence; Multi-mode operations; Process Monitoring,Chemical industry; Deep neural networks; Fault detection; Process control; 'current; Deep learning; Explainable artificial intelligence; Faults detection; Gradient based; High-accuracy; Multimode operations; Operational modes; Safety critical applications; Supervisory systems; Process monitoring,Article,Final,,Scopus,2-s2.0-85199017993
Moghadasi N.; Valdez R.S.; Piran M.; Moghaddasi N.; Linkov I.; Polmateer T.L.; Loose D.C.; Lambert J.H.,"Moghadasi, Negin (57245262900); Valdez, Rupa S. (34769424300); Piran, Misagh (57126138600); Moghaddasi, Negar (57224492459); Linkov, Igor (6701532645); Polmateer, Thomas L. (57192069217); Loose, Davis C. (57712828500); Lambert, James H. (7401837124)",57245262900; 34769424300; 57126138600; 57224492459; 6701532645; 57192069217; 57712828500; 7401837124,Risk Analysis of Artificial Intelligence in Medicine with a Multilayer Concept of System Order,2024,Systems,12,2,47,,,,3,10.3390/systems12020047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185656044&doi=10.3390%2fsystems12020047&partnerID=40&md5=1811cd6d3f43ddd8acaad33822bbb357,"Artificial intelligence (AI) is advancing across technology domains including healthcare, commerce, the economy, the environment, cybersecurity, transportation, etc. AI will transform healthcare systems, bringing profound changes to diagnosis, treatment, patient care, data, medicines, devices, etc. However, AI in healthcare introduces entirely new categories of risk for assessment, management, and communication. For this topic, the framing of conventional risk and decision analyses is ongoing. This paper introduces a method to quantify risk as the disruption of the order of AI initiatives in healthcare systems, aiming to find the scenarios that are most and least disruptive to system order. This novel approach addresses scenarios that bring about a re-ordering of initiatives in each of the following three characteristic layers: purpose, structure, and function. In each layer, the following model elements are identified: 1. Typical research and development initiatives in healthcare. 2. The ordering criteria of the initiatives. 3. Emergent conditions and scenarios that could influence the ordering of the AI initiatives. This approach is a manifold accounting of the scenarios that could contribute to the risk associated with AI in healthcare. Recognizing the context-specific nature of risks and highlighting the role of human in the loop, this study identifies scenario s.06—non-interpretable AI and lack of human–AI communications—as the most disruptive across all three layers of healthcare systems. This finding suggests that AI transparency solutions primarily target domain experts, a reasonable inclination given the significance of “high-stakes” AI systems, particularly in healthcare. Future work should connect this approach with decision analysis and quantifying the value of information. Future work will explore the disruptions of system order in additional layers of the healthcare system, including the environment, boundary, interconnections, workforce, facilities, supply chains, and others. © 2024 by the authors.",interpretable and explainable AI; risk communication; risk management; scenario-based preferences; systems engineering,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85185656044
Schneider L.; Grote G.; Boos D.; Cakir Y.,"Schneider, Lena (59907241400); Grote, Gudela (7004080235); Boos, Daniel (35221233700); Cakir, Yaren (59907442500)",59907241400; 7004080235; 35221233700; 59907442500,Designing Targeted Explanations for Heterogeneous Stakeholders - AI for Collaborative Use in the Railways,2025,Conference on Human Factors in Computing Systems - Proceedings ,,,180,,,,0,10.1145/3706599.3719927,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005733282&doi=10.1145%2f3706599.3719927&partnerID=40&md5=98cc3782160fdc3a65aaa12771bec3fb,"The opaqueness of AI continues to represent a significant challenge when designing safe and reliable high risk-AI-systems, particularly for collaborative use by multiple heterogeneous stakeholders. Explainability approaches are used to address this opacity by increasing transparency and interpretability of AI-based systems and thereby establishing human control over the AI-system. In order to be effective, these explainability approaches need to be targeted to needs of all involved stakeholders. We show with an example use case from the railway sector how work design can be combined with technology design for successful human-system integration. Through observation of work processes and interviews with stakeholders at an early stage of system development we can support the design of targeted explanations for high-risk AI systems used in complex collaborative settings by gaining in depth understanding of the respective work systems. We identified five relevant explainability approaches for a visual train inspection decision support system. Exploratory testing then showed preferences for some explanations over others, fostering the discussion on choosing appropriate and effective explanations for visual tasks and informing the design of a final visual train inspection interface prototype for future research and application in the railways. © 2025 Copyright held by the owner/author(s).",Accountability; AI Ethics; AI in high-risk domains; Control; Explainability; Human-in-the-loop; Socio-technical systems design; Stakeholder Networks; Work Design; XAI,Economics; Human engineering; Man machine systems; Accountability; AI ethic; AI in high-risk domain; Explainability; High-risk domains; Human-in-the-loop; Socio-technical system design; Sociotechnical systems; Stakeholder network; Work design; XAI; Railroad transportation,Conference paper,Final,,Scopus,2-s2.0-105005733282
El Arab R.A.; Almoosa Z.; Alkhunaizi M.; Abuadas F.H.; Somerville J.,"El Arab, Rabie Adel (57204688079); Almoosa, Zainab (57195739812); Alkhunaizi, May (59414343900); Abuadas, Fuad H. (57193453829); Somerville, Joel (58244259300)",57204688079; 57195739812; 59414343900; 57193453829; 58244259300,Artificial intelligence in hospital infection prevention: an integrative review,2025,Frontiers in Public Health,13,,1547450,,,,0,10.3389/fpubh.2025.1547450,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003294593&doi=10.3389%2ffpubh.2025.1547450&partnerID=40&md5=9dd8b8faffed79f158f94908458e3500,"Background: Hospital-acquired infections (HAIs) represent a persistent challenge in healthcare, contributing to substantial morbidity, mortality, and economic burden. Artificial intelligence (AI) offers promising potential for improving HAIs prevention through advanced predictive capabilities. Objective: To evaluate the effectiveness, usability, and challenges of AI models in preventing, detecting, and managing HAIs. Methods: This integrative review synthesized findings from 42 studies, guided by the SPIDER framework for inclusion criteria. We assessed the quality of included studies by applying the TRIPOD checklist to individual predictive studies and the AMSTAR 2 tool for reviews. Results: AI models demonstrated high predictive accuracy for the detection, surveillance, and prevention of multiple HAIs, with models for surgical site infections and urinary tract infections frequently achieving area-under-the-curve (AUC) scores exceeding 0.80, indicating strong reliability. Comparative data suggest that while both machine learning and deep learning approaches perform well, some deep learning models may offer slight advantages in complex data environments. Advanced algorithms, including neural networks, decision trees, and random forests, significantly improved detection rates when integrated with EHRs, enabling real-time surveillance and timely interventions. In resource-constrained settings, non-real-time AI models utilizing historical EHR data showed considerable scalability, facilitating broader implementation in infection surveillance and control. AI-supported surveillance systems outperformed traditional methods in accurately identifying infection rates and enhancing compliance with hand hygiene protocols. Furthermore, Explainable AI (XAI) frameworks and interpretability tools such as Shapley additive explanations (SHAP) values increased clinician trust and facilitated actionable insights. AI also played a pivotal role in antimicrobial stewardship by predicting the emergence of multidrug-resistant organisms and guiding optimal antibiotic usage, thereby reducing reliance on second-line treatments. However, challenges including the need for comprehensive clinician training, high integration costs, and ensuring compatibility with existing workflows were identified as barriers to widespread adoption. Discussion: The integration of AI in HAI prevention and management represents a potentially transformative shift in enhancing predictive capabilities and supporting effective infection control measures. Successful implementation necessitates standardized validation protocols, transparent data reporting, and the development of user-friendly interfaces to ensure seamless adoption by healthcare professionals. Variability in data sources and model validations across studies underscores the necessity for multicenter collaborations and external validations to ensure consistent performance across diverse healthcare environments. Innovations in non-real-time AI frameworks offer viable solutions for scaling AI applications in low- and middle-income countries (LMICs), addressing the higher prevalence of HAIs in these regions. Conclusions: Artificial Intelligence stands as a transformative tool in the fight against hospital-acquired infections, offering advanced solutions for prevention, surveillance, and management. To fully realize its potential, the healthcare sector must prioritize rigorous validation standards, comprehensive data quality reporting, and the incorporation of interpretability tools to build clinician confidence. By adopting scalable AI models and fostering interdisciplinary collaborations, healthcare systems can overcome existing barriers, integrating AI seamlessly into infection control policies and ultimately enhancing patient safety and care quality. Further research is needed to evaluate cost-effectiveness, real-world applications, and strategies (e.g., clinician training and the integration of explainable AI) to improve trust and broaden clinical adoption. Copyright © 2025 El Arab, Almoosa, Alkhunaizi, Abuadas and Somerville.",artificial intelligence; explainable AI; hospital-acquired infections; infection control; infection prevention; infection surveillance; predictive analytics,Artificial Intelligence; Cross Infection; Humans; Infection Control; artificial intelligence; cross infection; human; infection control; prevention and control; procedures,Review,Final,,Scopus,2-s2.0-105003294593
Pyle C.; Roemmich K.; Andalibi N.,"Pyle, Cassidy (57223998362); Roemmich, Kat (57313096600); Andalibi, Nazanin (36968038900)",57223998362; 57313096600; 36968038900,U.S. Job-Seekers' Organizational Justice Perceptions of Emotion AI-Enabled Interviews,2024,Proceedings of the ACM on Human-Computer Interaction,8,CSCW2,454,,,,4,10.1145/3686993,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209351616&doi=10.1145%2f3686993&partnerID=40&md5=2b237b49b906da6a163229e97664b981,"Emotion AI is increasingly used to automatically evaluate asynchronous hiring interviews. Although touted for increasing hiring fit and reducing bias, it is unclear how job-seekers perceive emotion AI-enabled asynchronous interviews. This gap is striking, given job-seekers' marginalized position in hiring and how job-seekers with marginalized identities may be particularly vulnerable to this technology's potential harms. Addressing this gap, we conducted exploratory interviews with 14 U.S.-based participants with direct, recent experience with emotion AI-enabled asynchronous interviews. While participants acknowledged the asynchronous, virtual modality's potential benefits to employers and job-seekers, they perceived harms to job-seekers associated with automatic emotion inferences that our analysis maps to distributive, procedural, and interactional injustices. We find that social identity can inform job-seekers' perceptions of emotion AI, extending prior work's understandings of the factors contributing to job-seekers' perceptions of AI (broadly) in hiring. Moreover, our results suggest that emotion AI use may reconfigure demands for emotional labor in hiring and that deploying this technology in its current state may unjustly risk harmful outcomes for job-seekers - or, at the very least, perceptions thereof, which shape behaviors and attitudes. Accordingly, we recommend against the present adoption of emotion AI in hiring, identifying opportunities for the design of future asynchronous hiring interview platforms to be meaningfully transparent, contestable, and privacy-preserving. We emphasize that only a subset of perceived harms we surface may be alleviated by these efforts; some injustices may only be resolved by removing emotion AI-enabled features.  © 2024 ACM.",affective computing; algorithmic decision-making; algorithms; contestability; emotion recognition; employment; explainability; future of work; hiring; justice; organizational justice; privacy; privacy harms; relational ethics; transparency; workplace,Decision making; Emotion Recognition; Affective Computing; Algorithmic decision-making; Algorithmics; Contestability; Decisions makings; Emotion recognition; Explainability; Future of works; Hiring; Justice; Organisational; Organizational justice; Privacy; Privacy harm; Relational ethic; Workplace; Differential privacy,Article,Final,,Scopus,2-s2.0-85209351616
Chevalier O.; Dubey G.; Benkabbou A.; Majbar M.A.; Souadka A.,"Chevalier, Olivia (59700501900); Dubey, Gérard (7102913288); Benkabbou, Amine (16063004400); Majbar, Mohammed Anass (57219127875); Souadka, Amine (57116300900)",59700501900; 7102913288; 16063004400; 57219127875; 57116300900,Comprehensive overview of artificial intelligence in surgery: a systematic review and perspectives,2025,Pflugers Archiv European Journal of Physiology,477,4,,617,626,9,0,10.1007/s00424-025-03076-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000449165&doi=10.1007%2fs00424-025-03076-6&partnerID=40&md5=9a6429b4acbff6963b2684450d76015c,"The rapid integration of artificial intelligence (AI) into surgical practice necessitates a comprehensive evaluation of its applications, challenges, and physiological impact. This systematic review synthesizes current AI applications in surgery, with a particular focus on machine learning (ML) and its role in optimizing preoperative planning, intraoperative decision-making, and postoperative patient management. Using PRISMA guidelines and PICO criteria, we analyzed key studies addressing AI’s contributions to surgical precision, outcome prediction, and real-time physiological monitoring. While AI has demonstrated significant promise—from enhancing diagnostics to improving intraoperative safety—many surgeons remain skeptical due to concerns over algorithmic unpredictability, surgeon autonomy, and ethical transparency. This review explores AI’s physiological integration into surgery, discussing its role in real-time hemodynamic assessments, AI-guided tissue characterization, and intraoperative physiological modeling. Ethical concerns, including algorithmic opacity and liability in high-stakes scenarios, are critically examined alongside AI’s potential to augment surgical expertise. We conclude that longitudinal validation, improved AI explainability, and adaptive regulatory frameworks are essential to ensure safe, effective, and ethically sound integration of AI into surgical decision-making. Future research should focus on bridging AI-driven analytics with real-time physiological feedback to refine precision surgery and patient safety strategies. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.",Decision support systems in surgery; Ethical implications of AI; Human-AI collaboration; Machine learning in surgery; Surgical artificial intelligence,Artificial Intelligence; Humans; Machine Learning; artificial intelligence; augmented reality; automation; decision making; decision support system; deep learning; digital twin; error; explainable artificial intelligence; follow up; information security; intraoperative monitoring; machine learning; mortality risk; patient care; postoperative care; postoperative period; Preferred Reporting Items for Systematic Reviews and Meta-Analyses; Review; surgery; surgical technology; systematic review; tissue characterization; human; machine learning,Review,Final,,Scopus,2-s2.0-105000449165
Wen Y.; Li S.; Zuo R.; Yuan L.; Mao H.; Liu P.,"Wen, Yongyan (59465609300); Li, Siyuan (57205550729); Zuo, Rongchang (58680415100); Yuan, Lei (57289008600); Mao, Hangyu (57195585397); Liu, Peng (55574230218)",59465609300; 57205550729; 58680415100; 57289008600; 57195585397; 55574230218,SkillTree: Explainable Skill-Based Deep Reinforcement Learning for Long-Horizon Control Tasks,2025,Proceedings of the AAAI Conference on Artificial Intelligence,39,20,,21491,21500,9,0,10.1609/aaai.v39i20.35451,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003996345&doi=10.1609%2faaai.v39i20.35451&partnerID=40&md5=4ff4ce71fc37744ca3ffdb64d58ec49b,"Deep reinforcement learning (DRL) has achieved remarkable success in various domains, yet its reliance on neural networks results in a lack of transparency, which limits its practical applications in safety-critical and human-agent interaction domains. Decision trees, known for their notable explainability, have emerged as a promising alternative to neural networks. However, decision trees often struggle in long-horizon continuous control tasks with high-dimensional observation space due to their limited expressiveness. To address this challenge, we propose SkillTree, a novel hierarchical framework that reduces the complex continuous action space of challenging control tasks into discrete skill space. By integrating the differentiable decision tree within the high-level policy, SkillTree generates discrete skill embeddings that guide low-level policy execution. Furthermore, through distillation, we obtain a simplified decision tree model that improves performance while further reducing complexity. Experiment results validate SkillTree’s effectiveness across various robotic manipulation tasks, providing clear skill-level insights into the decision-making process. The proposed approach not only achieves performance comparable to neural network based methods in complex long-horizon control tasks but also significantly enhances the transparency and explainability of the decision-making process. © 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Deep neural networks; Reinforcement learning; Robots; Continuous control; Control task; Decision-making process; High-dimensional; Higher-dimensional; Horizon control; Human-agent interaction; Interaction domain; Neural-networks; Reinforcement learnings; Deep reinforcement learning,Conference paper,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-105003996345
Shi H.; Li S.; Huang Z.; Li S.; Li A.; Zhou Y.,"Shi, Hanyue (57219547361); Li, Shaowei (57016010000); Huang, Zian (59407174100); Li, Shida (59215668900); Li, Ang (57203385867); Zhou, Yaoming (55152900500)",57219547361; 57016010000; 59407174100; 59215668900; 57203385867; 55152900500,Manned and Unmanned Aerial Vehicles Cooperative Combat Framework Based on Large Language Models,2024,ICAS Proceedings,,,,,,23,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208791772&partnerID=40&md5=d2c9fbfebd7c89d9ba78b9232dc7074b,"The collaborative combat of manned and unmanned aerial vehicles is one of the principal directions for the future development of aerial combat systems, facing the challenge of excessive decision-making and operational burdens on pilots during UAV control. This paper proposes a novel framework for the collaborative combat of manned and unmanned aerial vehicles, incorporating large language model(LLM) into the process of aircraft pilots commanding and controlling multiple unmanned aerial vehicles (UAVs). The framework utilizes LLM for complex semantic understanding and monitoring of task instruction execution. It allows pilots to issue task instructions to UAVs using non-standard natural language. The received natural language task instructions are matched with the preloaded policy library of the designed task executor in UAVs, and an appropriate policy is selected for execution. During task execution, UAVs provide feedback on the task execution status to manned aircraft at key nodes, and continue task execution upon confirmation by manned aircraft until task completion or receipt of new task instructions. The framework is tested in typical beyond-visual-range combat scenarios of manned and unmanned aerial vehicle collaboration. It exhibits good human-machine interaction, robustness, trustworthiness, explainability, and effectively reducing the decision-making and operational burdens on pilots. The research findings of this paper can be widely applied to various task scenarios where humans and robots collaborate to accomplish tasks, providing a feasible technical route for the collaborative combat of manned and unmanned aerial vehicles. © 2024, International Council of the Aeronautical Sciences. All rights reserved.",Human- machine cooperation; Large language model agent; Large language model control; Manned and unmanned aerial vehicles cooperative; Natural language contro,Collaborative robots; Fighter aircraft; Man machine systems; Military helicopters; Problem oriented languages; Unmanned aerial vehicles (UAV); Aerial vehicle; Human-machine cooperation; Language model; Large language model agent; Large language model control; Manned and unmanned aerial vehicle cooperative; Model agents; Modelling controls; Natural language contro; Natural languages; Semantics,Conference paper,Final,,Scopus,2-s2.0-85208791772
Fregosi C.; Cabitza F.,"Fregosi, Caterina (59251224000); Cabitza, Federico (16199544700)",59251224000; 16199544700,A Frictional Design Approach: Towards Judicial AI and its Possible Applications,2024,CEUR Workshop Proceedings,3825,,,23,28,5,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210318631&partnerID=40&md5=57237a3a4d79500d5013d2a24a1b6b7a,"Decision support systems (DSS) are increasingly being integrated into high-stakes domains like healthcare, law, and finance, where critical decisions have significant consequences. Traditional DSS often provide a single, clear-cut recommendation, which can lead to automation bias and diminish the user’s sense of agency. However, there is a growing concern about the over-reliance on these systems and the potential for deskilling among users. The knowledge gap we aim to address is the development of decision support systems that effectively encourage critical reflection and maintain user engagement and responsibility in decision-making processes. In this workshop contribution, we report on the development of Judicial AI, a novel approach inspired by Frictional AI. Judicial AI diverges from traditional DSS by offering multiple, contrasting explanations to support different potential outcomes. This design encourages users to engage in deeper cognitive processing, thereby promoting critical reflection, reducing automation bias, and preserving the user’s sense of agency. This ongoing study employs a two-arm experiment to investigate the effects of this approach in the context of content classification tasks, comparing it with the traditional protocol. The expected outcomes of this ongoing study suggest that the Judicial protocol could not only mitigate automation bias but also safeguard users’ sense of agency and promote long-term skill retention. © 2024 Copyright for this paper by its authors.",eXplainable AI (XAI); Frictional AI; Human-AI Decision making process; Judicial AI,Artificial intelligence; Decision making; Automation bias; Critical reflections; Decision supports; Decision-making process; Explainable AI (XAI); Frictional AI; Human-AI decision making process; Judicial AI; Sense of agencies; Support systems; Decision support systems,Conference paper,Final,,Scopus,2-s2.0-85210318631
Senoner J.; Schallmoser S.; Kratzwald B.; Feuerriegel S.; Netland T.,"Senoner, Julian (57216863672); Schallmoser, Simon (57234617300); Kratzwald, Bernhard (56470814500); Feuerriegel, Stefan (53881265200); Netland, Torbjørn (36667063700)",57216863672; 57234617300; 56470814500; 53881265200; 36667063700,Explainable AI improves task performance in human–AI collaboration,2024,Scientific Reports,14,1,31150,,,,2,10.1038/s41598-024-82501-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213519314&doi=10.1038%2fs41598-024-82501-9&partnerID=40&md5=82e9bf118cc37b240571021c3ee2c53a,"Artificial intelligence (AI) provides considerable opportunities to assist human work. However, one crucial challenge of human–AI collaboration is that many AI algorithms operate in a black-box manner where the way how the AI makes predictions remains opaque. This makes it difficult for humans to validate a prediction made by AI against their own domain knowledge. For this reason, we hypothesize that augmenting humans with explainable AI improves task performance in human–AI collaboration. To test this hypothesis, we implement explainable AI in the form of visual heatmaps in inspection tasks conducted by domain experts. Visual heatmaps have the advantage that they are easy to understand and help to localize relevant parts of an image. We then compare participants that were either supported by (a) black-box AI or (b) explainable AI, where the latter supports them to follow AI predictions when the AI is accurate or overrule the AI when the AI predictions are wrong. We conducted two preregistered experiments with representative, real-world visual inspection tasks from manufacturing and medicine. The first experiment was conducted with factory workers from an electronics factory, who performed assessments of whether electronic products have defects. The second experiment was conducted with radiologists, who performed assessments of chest X-ray images to identify lung lesions. The results of our experiments with domain experts performing real-world tasks show that task performance improves when participants are supported by explainable AI with heatmaps instead of black-box AI. We find that explainable AI as a decision aid improved the task performance by 7.7 percentage points (95% confidence interval [CI]: 3.3% to 12.0%,) in the manufacturing experiment and by 4.7 percentage points (95% CI: 1.1% to 8.3%,) in the medical experiment compared to black-box AI. These gains represent a significant improvement in task performance. © The Author(s) 2024.",Decision-making; Explainable AI; Human-centered AI; Human–AI collaboration; Task performance,Adult; Algorithms; Artificial Intelligence; Female; Humans; Male; Task Performance and Analysis; adult; algorithm; artificial intelligence; female; human; male; task performance,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85213519314
Alt B.; Zahn J.; Kienle C.; Dvorak J.; May M.; Katic D.; Jäkel R.; Kopp T.; Beetz M.; Lanza G.,"Alt, Benjamin (57222956861); Zahn, Johannes (59147846600); Kienle, Claudius (58343756500); Dvorak, Julia (57741276500); May, Marvin (57654529100); Katic, Darko (36610339700); Jäkel, Rainer (23568193700); Kopp, Tobias (57201388519); Beetz, Michael (36818071500); Lanza, Gisela (23100137400)",57222956861; 59147846600; 58343756500; 57741276500; 57654529100; 36610339700; 23568193700; 57201388519; 36818071500; 23100137400,Human-AI Interaction in Industrial Robotics: Design and Empirical Evaluation of a User Interface for Explainable AI-Based Robot Program Optimization,2024,IFAC-PapersOnLine,58,27,,591,596,5,0,10.1016/j.procir.2024.10.134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213028346&doi=10.1016%2fj.procir.2024.10.134&partnerID=40&md5=5646b09d73b1828b533e3392837b42a1,"While recent advances in deep learning have demonstrated its transformative potential, its adoption for real-world manufacturing applications remains limited. We present an Explanation User Interface (XUI) for a state-of-the-art deep learning-based robot program optimizer which provides both naive and expert users with different user experiences depending on their skill level, as well as Explainable AI (XAI) features to facilitate the application of deep learning methods in real-world applications. To evaluate the impact of the XUI on task performance, user satisfaction and cognitive load, we present the results of a preliminary user survey and propose a study design for a large-scale follow-up study. © 2024 The Authors.",deep learning; explainable artificial intelligence; explanation user interfaces; industrial robotics; manufacturing; user study,Contrastive Learning; Deep learning; Human robot interaction; Industrial robots; Microrobots; Robot applications; Robot learning; Robot programming; Deep learning; Design evaluation; Empirical evaluations; Explainable artificial intelligence; Explanation user interface; Industrial robotics; Real-world; Robot programs; Robotic design; User study; Smart manufacturing,Conference paper,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85213028346
Masaeid T.A.; Alkhalidi M.M.; Ali A.A.A.A.; Almaazmi S.M.G.A.; Alami R.,"Masaeid, Turki Al (57216928871); Alkhalidi, Marwan Mohamed (59649662500); Ali, Ahmed Abdulla Ahmed Al (59652176800); Almaazmi, Saeed Mohamed Ghuloum Aldoobi (59649662600); Alami, Rachid (58065710900)",57216928871; 59649662500; 59652176800; 59649662600; 58065710900,Artificial Intelligence-Augmented Decision-Making: Examining the Interplay Between Machine Learning Algorithms and Human Judgment in Organizational Leadership,2025,Journal of Ecohumanism,4,1,,4683,4699,16,0,10.62754/joe.v4i1.6364,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218781082&doi=10.62754%2fjoe.v4i1.6364&partnerID=40&md5=e46d6bd01c86d5bcb4358ac65da3fd66,"The paper discusses the bright and dark sides of the relationship between human judgment and AI-driven machine learning (ML) algorithms. While discussing important issues, such as algorithm aversion, automation bias, and trust, it probes into how AI improves decision-making efficiency through predictive accuracy, resource optimisation, and data-driven insights. Even as AI can revolutionise decision-making, its effective integration must balance algorithmic output and human judgment. The most critical challenges include automation bias resulting from over-reliance on advice given by AI and algorithm aversion driven by concerns related to AI failures. Open systems, explainable AI (XAI) frameworks, and user-centered design can help to engender confidence in AI systems and alleviate these issues. Accountability, equity, and prejudice concerns raise further ethical considerations with AI. The study proposed several tactics that might mitigate such challenges: audits of ethics, adherence to legal policy, and integration of the AI systems with the company’s values. It underlines the human-AI collaboration that will be increasingly necessary, as well as hybrid models for decision-making that bring algorithmic accuracy to human intuition. It follows the case study review and empirical findings with practical lessons for organisational leaders on ethics, best deployment practices for AI, and tactical ways to engender better collaboration and trust. The conclusion outlines the need to enhance the explainability features of AI, study cognitive dynamics in decision processes, and work out ethical schemata guiding leading positions for AI. Beyond providing a roadmap for organisations to leverage the interaction of human judgment and machine intelligence to drive and achieve more ethical and effective leadership outcomes, this paper tries to contribute to the ongoing debate on AI-augmented decision-making. © 2025, Creative Publishing House. All rights reserved.",AI-augmented decision-making; algorithm aversion; algorithm machine learning (ML) algorithms; and human-AI collaboration; Cognitive Bias; Data-driven insights; Ethical AI; ethical frameworks; Explainable AI (XAI); human judgment; human-AI collaboration; Hybrid Decision-Making Models; strategies of leaders,,Article,Final,,Scopus,2-s2.0-85218781082
Kathrine Kollerup N.; Johansen S.S.; Tolsgaard M.G.; Lønborg Friis M.; Skov M.B.; van Berkel N.,"Kathrine Kollerup, Naja (57947860400); Johansen, Stine S. (50361380300); Tolsgaard, Martin Grønnebæk (22956300900); Lønborg Friis, Mikkel (58978290500); Skov, Mikael B. (7004242735); van Berkel, Niels (56032304300)",57947860400; 50361380300; 22956300900; 58978290500; 7004242735; 56032304300,Clinical needs and preferences for AI-based explanations in clinical simulation training,2025,Behaviour and Information Technology,44,5,,954,974,20,4,10.1080/0144929X.2024.2334852,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001929911&doi=10.1080%2f0144929X.2024.2334852&partnerID=40&md5=8b213c5f791534515483335248862ad2,"Medical training is a key element in maintaining and improving today's healthcare standards. Given the nature of medical work, students must master not only theory but also develop their hands-on abilities and skills in clinical practice. Medical simulators play an increasing role in supporting the active learning of these students due to their ability to present a large variety of tasks allowing students to train and experiment indefinitely without causing any patient harm. While the criticality of explainable AI systems has been extensively discussed in the literature, the medical training context presents unique user needs for explanations. In this paper, we explore the potential gap of current limitations within simulation-based training, and the role Artificial Intelligence (AI) holds in supporting the needs of medical students in training. Through contextual inquiries and interviews with clinicians in training (N = 9) and subsequent validation with medical experts (N = 4), we obtain an understanding of the shortcomings in current simulation-based training and offer recommendations for future AI-driven training. Our results stress the need for continuous and actionable feedback that resembles the interaction between clinical supervisor and resident in real-world training scenarios while adjusting training material to the residents' skills and prior performance. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",AI support systems; clinical; Explanations; feedback; learning; simulation,Artificial intelligence; Artificial intelligence support system; Clinical; Clinical simulation; Explanation; Intelligence-support systems; Learning; Medical training; Simulation; Simulation training; Simulation-based training; Students,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-105001929911
Paramasivan P.; Rajest S.S.; Chinnusamy K.; Regin R.; Joseph F.J.J.,"Paramasivan, P. (57190001854); Rajest, S. Suman (57204111477); Chinnusamy, Karthikeyan (57210935104); Regin, R. (57208682999); Joseph, Ferdin Joe John (38661195200)",57190001854; 57204111477; 57210935104; 57208682999; 38661195200,Cross-industry AI applications,2024,Cross-Industry AI Applications,,,,1,389,388,2,10.4018/979-8-3693-5951-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199666214&doi=10.4018%2f979-8-3693-5951-8&partnerID=40&md5=f56330ee69a6dd84e3145f7ea5e1c97b,"The rise of Artificial Intelligence (AI) amidst the backdrop of a world that is changing at lightning speed presents a whole new set of challenges. One of our biggest hurdles is more transparency in AI solutions. It's a complex issue, but one that we need to address if we want to ensure that the benefits of AI are accessible to everyone. Across diverse sectors such as healthcare, surveillance, and human-computer interaction, the inability to understand and evaluate AI's decision-making processes hinders progress and raises concerns about accountability. Cross-Industry AI Applications is a groundbreaking solution to illuminate the mysteries of AI-driven human behavior analysis. This pioneering book addresses the necessity of transparency and explainability in AI systems, particularly in analyzing human behavior. Compiling insights from leading experts and academics offers a comprehensive exploration of state-of-the-art methodologies, benchmarks, and systems for understanding and predicting human behavior using AI. Each chapter delves into novel approaches and real-world applications, from facial expressions to gesture recognition, providing invaluable knowledge for scholars and professionals alike. With its focus on explainable AI and its implications for decision-making, Cross-Industry AI Applications serves as a roadmap for navigating the complexities of human behavior analysis in the age of AI. By offering clarity and insight into AI algorithms and their impact on diverse industries, this book empowers readers to harness the full potential of AI while ensuring accountability and ethical practice. Join the forefront of AI research and innovation - order your copy today and embark on a transformative journey into the future of human-AI interaction. © 2024 by IGI Global. All rights reserved.",,,Book,Final,,Scopus,2-s2.0-85199666214
Dewasiri N.J.; Dharmarathna D.G.; Choudhary M.,"Dewasiri, Narayanage Jayantha (57142582200); Dharmarathna, Dunusinghe G. (59137776000); Choudhary, Mrinalini (58076371100)",57142582200; 59137776000; 58076371100,Leveraging artificial intelligence for enhanced risk management in banking: A systematic literature review,2024,Artificial Intelligence Enabled Management: An Emerging Economy Perspective,,,,197,213,16,5,10.1515/9783111172408013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193833743&doi=10.1515%2f9783111172408013&partnerID=40&md5=9e88db9ae791d08b3a3910e5677f34d0,"This systematic review delves into the transformative role of Artificial Intelligence (AI) in the banking industry's risk management practices. AI, encompassing machine learning, data analytics, and natural language processing, has enhanced risk assessment, mitigation, and decision-making processes. The findings emphasise AI's capacity to identify and assess risks, enabling proactive risk management effectively. Applications like credit scoring models, fraud detection systems, and stress testing tools play instrumental roles in optimising risk management processes. At the same time, the importance of data quality, governance, and transparency cannot be overstated in successfully implementing AI-driven risk management strategies. The implications of AI in banking are profound, offering data-driven procedures, equitable lending practices, and enhanced operational efficiency. However, data privacy concerns, model interpretability issues, and regulatory compliance complexities must be addressed carefully. Emerging trends in AI for risk management encompass Explainable AI, AI-enabled regulatory Compliance, AI for Cybersecurity Risk Management, and Natural Language Processing for Unstructured Data Analysis, along with the optimisation of efficiency through Robotic Process Automation in Risk Operations. Future research should focus on ethical considerations, dynamic stress testing models, AI's role in climate-related risk analysis, human-AI collaboration, cybersecurity risk prediction, and the development of robust regulatory frameworks for AI integration in risk management. AI stands poised to revolutionise banking risk management. Still, responsible and ethical integration is paramount, necessitating collaborative efforts to harness its full potential while ensuring trust and stability within the sector. © 2024 Walter de Gruyter GmbH, Berlin/Boston.",Artificial Intelligence (AI); Banking Industry; Data Analytics; Regulatory Compliance; Risk Management,Artificial intelligence; Climate models; Data Analytics; Decision making; Efficiency; Ethical technology; Information management; Learning algorithms; Natural language processing systems; Regulatory compliance; Risk analysis; Risk assessment; Artificial intelligence; Banking industry; Cyber security; Data analytics; Language processing; Natural languages; Risks management; Stress Testing; Systematic literature review; Systematic Review; Risk management,Book chapter,Final,,Scopus,2-s2.0-85193833743
Alt B.; Zahn J.; Kienle C.; Dvorak J.; May M.; Katic D.; Jäkel R.; Kopp T.; Beetz M.; Lanza G.,"Alt, Benjamin (57222956861); Zahn, Johannes (59147846600); Kienle, Claudius (58343756500); Dvorak, Julia (57741276500); May, Marvin (57654529100); Katic, Darko (36610339700); Jäkel, Rainer (23568193700); Kopp, Tobias (57201388519); Beetz, Michael (36818071500); Lanza, Gisela (23100137400)",57222956861; 59147846600; 58343756500; 57741276500; 57654529100; 36610339700; 23568193700; 57201388519; 36818071500; 23100137400,Human-AI Interaction in Industrial Robotics: Design and Empirical Evaluation of a User Interface for Explainable AI-Based Robot Program Optimization,2024,Procedia CIRP,130,,,591,596,5,0,10.1016/j.procir.2024.10.134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215011175&doi=10.1016%2fj.procir.2024.10.134&partnerID=40&md5=1789c67601d69f87e07eb33e98bf5243,"While recent advances in deep learning have demonstrated its transformative potential, its adoption for real-world manufacturing applications remains limited. We present an Explanation User Interface (XUI) for a state-of-the-art deep learning-based robot program optimizer which provides both naive and expert users with different user experiences depending on their skill level, as well as Explainable AI (XAI) features to facilitate the application of deep learning methods in real-world applications. To evaluate the impact of the XUI on task performance, user satisfaction and cognitive load, we present the results of a preliminary user survey and propose a study design for a large-scale follow-up study. © 2024 The Authors. Published by Elsevier B.V.",deep learning; explainable artificial intelligence; explanation user interfaces; industrial robotics; manufacturing; user study,Contrastive Learning; Industrial robots; Microrobots; Robot programming; Deep learning; Design evaluation; Empirical evaluations; Explainable artificial intelligence; Explanation user interface; Industrial robotics; Real-world; Robot programs; Robotic design; User study; Smart manufacturing,Conference paper,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85215011175
Adhvaryu R.; Agal S.; Odedra N.D.; Rusho M.A.; Pande S.D.,"Adhvaryu, Rachit (56593220000); Agal, Sanjay (57214911342); Odedra, Niyati Dhirubhai (59208764400); Rusho, Maher Ali (59126288100); Pande, Sagar Dhanraj (57213160540)",56593220000; 57214911342; 59208764400; 59126288100; 57213160540,An in-depth exploration of data analysis and processing through the prism of explainable artificial intelligence paradigms,2024,Explainable Artificial Intelligence for Biomedical and Healthcare Applications,,,,249,266,17,0,10.1201/9781003220107-15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210683744&doi=10.1201%2f9781003220107-15&partnerID=40&md5=bc9bdd3e4b9c7eedf6943404d6cfcd50,"This extensive study investigates the complex problem of data processing and analysis through the lens of explainable artificial intelligence (XAI), a highly advanced kind of artificial intelligence. The purpose of this study is to perform a detailed examination of the many functions that XAI plays in shining light on the often-obscure processes that underpin AI-driven data analytics. The major premise of this study is that the openness and understandability given by XAI may significantly improve the performance and accuracy of data interpretation for sophisticated artificial intelligence models. Although the abstract covers a wide range of XAI applications, its major focus is on the relevance of these applications in terms of democratizing data-driven insights and encouraging human–AI trust. The current discussion primarily focuses on the ethical implications, issues associated with the deployment of XAI in various settings, and the prognosis for this rapidly growing business. This article introduces XAI as a cutting-edge technology that has the potential to change data analysis. People and organizations navigating the contemporary world's data-rich environment depend on XAI. © 2025 selection and editorial matter, Aditya Khamparia and Deepak Gupta; individual chapters, the contributors.",,,Book chapter,Final,,Scopus,2-s2.0-85210683744
Zhang Z.T.; Argın S.K.; Bilen M.B.; Urgun D.; Deniz S.M.; Liu Y.; Hassib M.,"Zhang, Zelun Tony (57226128028); Argın, Seniha Ketenci (59445073800); Bilen, Mustafa Baha (59445316100); Urgun, Doğan (57203837845); Deniz, Sencer Melih (34267512700); Liu, Yuanting (57207882809); Hassib, Mariam (56150903100)",57226128028; 59445073800; 59445316100; 57203837845; 34267512700; 57207882809; 56150903100,Measuring the effect of mental workload and explanations on appropriate AI reliance using EEG,2024,Behaviour and Information Technology,,,,,,,0,10.1080/0144929X.2024.2431055,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210474451&doi=10.1080%2f0144929X.2024.2431055&partnerID=40&md5=ba19d0ef535559167d943a8b7df57fb8,"AI is anticipated to improve human decision-making across various domains, often in high-stakes, difficult tasks. However, human reliance on AI recommendations is often inappropriate. A common approach to address this is to provide explanations about the AI output to decision makers, but results have been mixed so far. It often remains unclear when people can rely appropriately on AI and when explanations can help. In this work, we conducted a lab experiment (N = 34) to investigate how the appropriateness of human reliance on (explainable) AI depends on the mental workload induced by different decision difficulties. Instead of self-assessments, we used EEG (Emotiv Epoc Flex head cap, 32 wet electrodes) to more directly measure participants' mental workload. We found that the difficulty of a decision, indicated by the induced mental workload, strongly influences participants' ability to rely appropriately on AI, as assessed through relative self-reliance, relative AI reliance, and decision accuracy with and without AI. While reliance was appropriate for low mental workload decisions, participants were prone to overreliance in high mental workload decisions. Explanations had no significant effect in either case. Our results imply that alternatives to the common ‘recommend-and-explain’ approach should be explored to assist human decision-making in challenging tasks. © 2024 Informa UK Limited, trading as Taylor & Francis Group.",appropriate reliance; decision difficulty; EEG; explainable AI; Human-AI decision-making; mental workload,Appropriate reliance; Decision difficulty; Decision makers; Decisions makings; Explainable AI; Human decision-making; Human-AI decision-making; Lab. experiment; Mental workload; Self-assessment,Article,Article in press,,Scopus,2-s2.0-85210474451
Ghela S.; Bhagavatula A.; Tripathy B.K.,"Ghela, Shrusti (59295831900); Bhagavatula, Anuhya (59295605500); Tripathy, B.K. (7006285374)",59295831900; 59295605500; 7006285374,Unveiling the Power of Explainable AI: Real-World Applications and Implications,2024,"Explainable, Interpretable, and Transparent AI Systems",,,,1,19,18,0,10.1201/9781003442509-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201888075&doi=10.1201%2f9781003442509-1&partnerID=40&md5=436b8693897d7a3c22398a09d6a09dfd,"Artificial intelligence (AI), a mainstay of modern decision-making, is a black box that is widely used but sometimes poorly understood. Despite the numerous potential advantages of AI in algorithmic decision-making in various domains, the significance of trusting these systems has just recently come to light. The field of explainable artificial intelligence (XAI) tries to make a model less mysterious by explaining its behavior and/or predictions and enables the primary stakeholders to examine the model and place more trust in it. “If algorithm results are low-impact, like the songs recommended by a music service, society probably doesn’t need regulators plumbing the depths of how those recommendations are made. However, the primary stakeholders may not be able to live with much more important decisions borne out of AI systems, perhaps literally in the case of a recommended medical treatment or a rejected application for a mortgage loan.” Through this chapter, we aim to dispel the magic behind this black box by exploring and understanding various applications of XAI. © 2024 CRC Press.",,,Book chapter,Final,,Scopus,2-s2.0-85201888075
Tutul A.A.; Nirjhar E.H.; Chaspari T.,"Tutul, Abdullah Aman (57221598740); Nirjhar, Ehsanul Haque (57207912830); Chaspari, Theodora (55351228300)",57221598740; 57207912830; 55351228300,Investigating Trust in Human-AI Collaboration for a Speech-Based Data Analytics Task,2025,International Journal of Human-Computer Interaction,41,5,,2936,2954,18,3,10.1080/10447318.2024.2328910,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000377267&doi=10.1080%2f10447318.2024.2328910&partnerID=40&md5=e1dc9dc890242b65d3eaf0dedd70a0fc,"Complex real-world problems can benefit from the collaboration between humans and artificial intelligence (AI) to achieve reliable decision-making. We investigate trust in a human-in-the-loop decision-making task, in which participants with background on psychological sciences collaborate with an explainable AI system for estimating one’s anxiety level from speech. The AI system relies on the explainable boosting machine (EBM) model which takes prosodic features as the input and estimates the anxiety level. Trust in AI is quantified via self-reported (i.e., administered via a questionnaire) and behavioral (i.e., computed as user-AI agreement) measures, which are positively correlated with each other. Results indicate that humans and AI depict differences in performance depending on the characteristics of the specific case under review. Overall, human annotators’ trust in the AI increases over time, with momentary decreases after the AI partner makes an error. Annotators further differ in terms of appropriate trust calibration in the AI system, with some annotators over-trusting and some under-trusting the system. Personality characteristics (i.e., agreeableness, conscientiousness) and overall propensity to trust machines further affect the level of trust in the AI system, with these findings approaching statistical significance. Results from this work will lead to a better understanding of human-AI collaboration and will guide the design of AI algorithms toward supporting better calibration of user trust. © 2024 Taylor & Francis Group, LLC.",Explainable AI; human trust; transparency; trust calibration,Behavioral research; Data Analytics; Decision making; Anxiety levels; Artificial intelligence systems; Data analytics; Decisions makings; Explainable artificial intelligence; Human trust; Human-in-the-loop; Machine modelling; Real-world problem; Trust calibration; Calibration,Article,Final,,Scopus,2-s2.0-86000377267
Jain B.; Mamodiya U.,"Jain, Bindiya (59169152000); Mamodiya, Udit (57215363727)",59169152000; 57215363727,Al-Enhanced Security Information and Event Management,2025,Deep Learning Innovations for Securing Critical Infrastructures,,,,109,118,9,0,10.4018/979-8-3373-0563-9.ch007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005131847&doi=10.4018%2f979-8-3373-0563-9.ch007&partnerID=40&md5=9429a6aa3e590c058c9dc06bb99385c7,"AI-enhanced Security Information and Event Management (SIEM) systems are the new paradigm of cybersecurity. SIEM itself is combined with a full extent of power in terms of sophisticated artificial intelligence through machine learning, natural language processing, and predictive analytics, so now the system offers real-time monitoring, further advancement of anomaly detection, fully automated incident response, as well as minimization in detection as well as in response with reduced false positives. These AI-augmented SIEM systems will, in a proactive way, defend organizations from both known as well as unknown threats through integrating user behaviour analytics, context-aware insights, and predictive modelling. The architecture, primary features, and the benefits of AI-based SIEM have been explored by the authors considering data quality problems, computational overheads, and explainability issues. Some examples of use cases like cloud security, IoT protection, and phishing prevention are given below to validate real-world applicability. © 2025 by IGI Global Scientific Publishing. All rights reserved.",,Anomaly detection; Cyber attacks; Data Analytics; Natural language processing systems; Prediction models; Anomaly detection; Cyber security; Fully automated; Language processing; Machine-learning; Natural languages; Power; Real time monitoring; Security information and event management systems; Security information and event managements; Information management,Book chapter,Final,,Scopus,2-s2.0-105005131847
Kirkby A.; Baumgarth C.; Henseler J.,"Kirkby, Alexandra (58035993400); Baumgarth, Carsten (16028109000); Henseler, Jörg (29067736100)",58035993400; 16028109000; 29067736100,"Welcome, new brand colleague! A conceptual framework for efficient and effective human–AI co-creation for creative brand voice",2025,Journal of Brand Management,,,,,,,0,10.1057/s41262-025-00387-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005096759&doi=10.1057%2fs41262-025-00387-y&partnerID=40&md5=33752cfc2e1071856c177218623b93a7,"The rapid advancement of artificial intelligence (AI) capabilities has extended into creative realms, presenting opportunities for creative collaboration between human brand professionals and AI in support of brand voice efforts. However, there remains little clarity regarding the implementation of this creative interaction. With a conceptual approach, the current research proposes a three-level framework of human–AI co-creation for creative brand voice that highlights key factors that can facilitate brand efficiency and effectiveness at the individual (AI task roles, co-creation teaming, knowledge and skills), organisational (infrastructure and brand voice database, socialisation), and societal (responsibility and accountability, AI transparency, brand voice copyright) levels. Each level presents different challenges and insights. At the individual level, it is critical to consider operational processes; at the organisational level, managing the interactions is key; and at the societal level, external influences must be accounted for, to manage the brand. This research contribution in turn offers theoretical guidance, aligned with a high-level brand management perspective, on how to pursue efficiency and effectiveness at three defined levels, as well as relevant avenues for further research. © The Author(s) 2025.",Artificial intelligence; Brand management; Brand voice; Co-creation; Creativity,,Article,Article in press,,Scopus,2-s2.0-105005096759
Huang B.; Niyomsilp E.,"Huang, Baoyu (59746054100); Niyomsilp, Eksiri (57226101298)",59746054100; 57226101298,The impact of artificial intelligence on organizational decision-making processes,2025,Edelweiss Applied Science and Technology,9,4,,794,808,14,0,10.55214/25768484.v9i4.6081,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003212632&doi=10.55214%2f25768484.v9i4.6081&partnerID=40&md5=6a3f5e76a4e1d8f329ea949f9560e96a,"Using a mixed-methods approach, data was collected through quantitative surveys (N=258) and qualitative interviews with AI practitioners and decision-makers across multiple industries. Findings indicate that AI significantly improves decision efficiency by automating analytical tasks, reducing human cognitive biases, and enabling real-time insights. However, challenges persist, particularly in algorithmic transparency, ethical governance, and compliance with regulatory standards. Key findings reveal that AI integration positively influences decision effectiveness (β=0.156, p=0.031), but human oversight (β=0.381, p<0.001) and regulatory compliance (β=0.314, p<0.001) play crucial mediating roles. Ethical and security challenges necessitate stronger AI governance frameworks, as organizations struggle with bias mitigation, legal accountability, and AI explainability. Industry experts emphasize the need for a hybrid Human-AI collaboration model, ensuring AI remains an augmentation rather than a replacement for human decision-makers. This study contributes to AI governance literature by highlighting the importance of ethical AI deployment, transparent decision systems, and regulatory adherence. Future research should explore AI’s impact in high-risk sectors, develop proactive AI compliance strategies, and examine cross-national AI regulatory frameworks to enhance responsible AI adoption globally. © 2025 by the authors.",AI-driven decision-making; Artificial intelligence; Ethical AI; Human oversight; Organizational strategy; Regulatory compliance,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-105003212632
Gardezi M.; Joshi B.; Rizzo D.M.; Ryan M.; Prutzer E.; Brugler S.; Dadkhah A.,"Gardezi, Maaz (57198450694); Joshi, Bhavna (57222131904); Rizzo, Donna M. (7005922424); Ryan, Mark (57211005946); Prutzer, Edward (57486291900); Brugler, Skye (58295749900); Dadkhah, Ali (58294931700)",57198450694; 57222131904; 7005922424; 57211005946; 57486291900; 58295749900; 58294931700,Artificial intelligence in farming: Challenges and opportunities for building trust,2024,Agronomy Journal,116,3,,1217,1228,11,27,10.1002/agj2.21353,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160595564&doi=10.1002%2fagj2.21353&partnerID=40&md5=0e212f1b94a6b3c4266bc0b761dc0b51,"Artificial intelligence (AI) represents technologies with human-like cognitive abilities to learn, perform, and make decisions. AI in precision agriculture (PA) enables farmers and farm managers to deploy highly targeted and precise farming practices based on site-specific agroclimatic field measurements. The foundational and applied development of AI has matured considerably over the last 30 years. The time is now right to engage seriously with the ethics and responsible practice of AI for the well-being of farmers and farm managers. In this paper, we identify and discuss both challenges and opportunities for improving farmers’ trust in those providing AI solutions for PA. We highlight that farmers’ trust can be moderated by how the benefits and risks of AI are perceived, shared, and distributed. We propose four recommendations for improving farmers’ trust. First, AI developers should improve model transparency and explainability. Second, clear responsibility and accountability should be assigned to AI decisions. Third, concerns about the fairness of AI need to be overcome to improve human-machine partnerships in agriculture. Finally, regulation and voluntary compliance of data ownership, privacy, and security are needed, if AI systems are to become accepted and used by farmers. © 2023 The Authors. Agronomy Journal published by Wiley Periodicals LLC on behalf of American Society of Agronomy.",,,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85160595564
Wang K.; Hou W.; Hong L.; Guo J.,"Wang, Keran (57218311204); Hou, Wenjun (14627737100); Hong, Leyi (59173337000); Guo, Jinyu (59530933500)",57218311204; 14627737100; 59173337000; 59530933500,Smart Transparency: A User-Centered Approach to Improving Human–Machine Interaction in High-Risk Supervisory Control Tasks,2025,Electronics (Switzerland),14,3,420,,,,1,10.3390/electronics14030420,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217748678&doi=10.3390%2felectronics14030420&partnerID=40&md5=f5fe1ee1b352439fd7c34c1f436eb569,"In supervisory control tasks, particularly in high-risk fields, operators need to collaborate with automated intelligent agents to manage dynamic, time-sensitive, and uncertain information. Effective human–agent collaboration relies on transparent interface communication to align with the operator’s cognition and enhance trust. This paper proposes a human-centered adaptive transparency information design framework (ATDF), which dynamically adjusts the display of transparency information based on the operator’s needs and the task type. This ensures that information is accurately conveyed at critical moments, thereby enhancing trust, task performance, and interface usability. Additionally, the paper introduces a novel user research method, Heu–Kano, to explore the prioritization of transparency needs and presents a model based on eye-tracking and machine learning to identify different types of human–agent interactions. This research provides new insights into human-centered explainability in supervisory control tasks. © 2025 by the authors.",activity recognition; adaptive design; eye tracking; supervisory control tasks; transparency; user research,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85217748678
de Brito Duarte R.,"de Brito Duarte, Regina (58590460800)",58590460800,Explainable AI as a Crucial Factor for Improving Human-AI Decision-Making Processes,2024,CEUR Workshop Proceedings,3793,,,345,352,7,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208285005&partnerID=40&md5=bf4190e06bf68832eb03cfa2221f266c,"A crucial aspect of AI-assisted decision making involves providing explanations for AI recommendations and predictions. Despite the optimism surrounding eXplainable AI (XAI) to improve transparency and trustworthiness, several studies have highlighted its shortcomings. My doctoral research aims to develop and validate a framework for human-AI decision making where explanations are central, serving as an enhancement factor for AI-assisted decision tasks. I hypothesize that a robust framework will elucidate underlying mechanisms and investigate the effects of AI explanations on decision outcomes. This research will advance our understanding of the combination of AI and human capabilities, informing the design of AI-assisted decision tasks for real-world scenarios. © 2024 Copyright for this paper by its authors.",eXplainable AI; Human-AI Decision Making; Human-AI Interaction,Decision outcome; Decision task; Decision-making process; Decisions makings; Doctoral research; Enhancement factor; Explainable AI; Human capability; Human-AI decision making; Human-AI interaction,Conference paper,Final,,Scopus,2-s2.0-85208285005
Naiseh M.; Webb C.; Underwood T.; Ramchurn G.; Walters Z.; Thavanesan N.; Vigneswaran G.,"Naiseh, Mohammad (57217108046); Webb, Catherine (59397648600); Underwood, Tim (23978947100); Ramchurn, Gopal (23135559900); Walters, Zoe (41361936300); Thavanesan, Navamayooran (53980612300); Vigneswaran, Ganesh (57208137593)",57217108046; 59397648600; 23978947100; 23135559900; 41361936300; 53980612300; 57208137593,XAI for Group-AI Interaction: Towards Collaborative and Inclusive Explanations,2024,CEUR Workshop Proceedings,3793,,,249,256,7,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208284036&partnerID=40&md5=035ba61682ad3e0add621ac5c7bcab8d,"The increasing integration of Machine Learning (ML) into decision-making across various sectors has raised concerns about ethics, legality, explainability, and safety, highlighting the necessity of human oversight. In response, eXplainable AI (XAI) has emerged as a means to enhance transparency by providing insights into ML model decisions and offering humans an understanding of the underlying logic. Despite its potential, existing XAI models often lack practical usability and fail to improve human-AI performance, as they may introduce issues such as overreliance. This underscores the need for further research in Human-Centered XAI to improve the usability of current XAI methods. Notably, much of the current research focuses on one-to-one interactions between the XAI and individual decision-makers, overlooking the dynamics of many-to-one relationships in real-world scenarios where groups of humans collaborate using XAI in collective decision-making. In this late-breaking work, we draw upon current work in Human-Centered XAI research and discuss how XAI design could be transitioned to group-AI interaction. We discuss four potential challenges in the transition of XAI from human-AI interaction to group-AI interaction. This paper contributes to advancing the field of Human-Centered XAI and facilitates the discussion on group-XAI interaction, calling for further research in this area. © 2024 Copyright for this paper by its authors.",Explainable AI; Group-AI Interaction; Interaction Design,Usability engineering; 'current; Decisions makings; Explainable AI; Group-AI interaction; Human oversight; Interaction design; Machine learning models; Machine-learning; Modeling decisions; Performance; Adversarial machine learning,Conference paper,Final,,Scopus,2-s2.0-85208284036
Lash M.T.,"Lash, Michael T. (56565273400)",56565273400,HEX: Human-in-the-loop explainability via deep reinforcement learning,2024,Decision Support Systems,187,,114304,,,,0,10.1016/j.dss.2024.114304,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207096278&doi=10.1016%2fj.dss.2024.114304&partnerID=40&md5=1f4595348adc8a14a0bcc3f608948df5,"The use of machine learning (ML) models in decision-making contexts, particularly those used in high-stakes decision-making, are fraught with issue and peril since a person – not a machine – must ultimately be held accountable for the consequences of decisions made using such systems. Machine learning explainability (MLX) promises to provide decision-makers with prediction-specific rationale, assuring them that the model-elicited predictions are made for the right reasons and are thus reliable. Few works explicitly consider this key human-in-the-loop (HITL) component, however. In this work we propose HEX, a human-in-the-loop deep reinforcement learning approach to MLX. HEX incorporates 0-distrust projection to synthesize decider-specific explainers that produce explanations strictly in terms of a decider's preferred explanatory features using any classification model. Our formulation explicitly considers the decision boundary of the ML model in question using a proposed explanatory point mode of explanation, thus ensuring explanations are specific to the ML model in question. We empirically evaluate HEX against other competing methods, finding that HEX is competitive with the state-of-the-art and outperforms other methods in human-in-the-loop scenarios. We conduct a randomized, controlled laboratory experiment utilizing actual explanations elicited from both HEX and competing methods. We causally establish that our method increases decider's trust and tendency to rely on trusted features. © 2024 The Author(s)",Behavioral machine learning; Decision support; Deep reinforcement learning; Explainability; Human-in-the-loop; Interpretability; Machine learning,Adversarial machine learning; Contrastive Learning; Federated learning; Reinforcement learning; Behavioral machine learning; Decision supports; Decisions makings; Explainability; Human-in-the-loop; Interpretability; Machine learning models; Machine-learning; Reinforcement learnings; Deep reinforcement learning,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85207096278
Swaroop S.; Buçinca Z.; Gajos K.Z.; Doshi-Velez F.,"Swaroop, Siddharth (57218716133); Buçinca, Zana (57195217062); Gajos, Krzysztof Z. (8375653300); Doshi-Velez, Finale (34874672900)",57218716133; 57195217062; 8375653300; 34874672900,Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure,2024,ACM International Conference Proceeding Series,,,,138,154,16,9,10.1145/3640543.3645206,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190963645&doi=10.1145%2f3640543.3645206&partnerID=40&md5=c49de85436de6cf2a0706957a570f022,"In settings where users both need high accuracy and are time-pressured, such as doctors working in emergency rooms, we want to provide AI assistance that both increases decision accuracy and reduces decision-making time. Current literature focusses on how users interact with AI assistance when there is no time pressure, finding that different AI assistances have different benefits: some can reduce time taken while increasing overreliance on AI, while others do the opposite. The precise benefit can depend on both the user and task. In time-pressured scenarios, adapting when we show AI assistance is especially important: relying on the AI assistance can save time, and can therefore be beneficial when the AI is likely to be right. We would ideally adapt what AI assistance we show depending on various properties (of the task and of the user) in order to best trade off accuracy and time. We introduce a study where users have to answer a series of logic puzzles. We find that time pressure affects how users use different AI assistances, making some assistances more beneficial than others when compared to no-time-pressure settings. We also find that a user's overreliance rate is a key predictor of their behaviour: overreliers and not-overreliers use different AI assistance types differently. We find marginal correlations between a user's overreliance rate (which is related to the user's trust in AI recommendations) and their personality traits (Big Five Personality traits). Overall, our work suggests that AI assistances have different accuracy-time tradeoffs when people are under time pressure compared to no time pressure, and we explore how we might adapt AI assistances in this setting. © 2024 ACM.",AI-assisted decision-making; decision support systems; explainable AI; human-AI interaction; human-centered AI; overreliance; time pressure,Artificial intelligence; Behavioral research; Decision support systems; Economic and social effects; AI-assisted decision-making; Decision accuracies; Decisions makings; Explainable AI; High-accuracy; Human-AI interaction; Human-centered AI; Overreliance; Personality traits; Time pressures; Decision making,Conference paper,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85190963645
Li B.; Liu Y.,"Li, Bo (59767446500); Liu, Yuan (59767647800)",59767446500; 59767647800,Ethical Issues In Translation Education in the AI Era,2025,Translation Studies in the Age of Artificial Intelligence,,,,210,229,19,0,10.4324/9781003482369-11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004130250&doi=10.4324%2f9781003482369-11&partnerID=40&md5=9f65c0d5cfa7ae96e94869707ad1eba0,"This chapter critically examines the ethical challenges arising from the integration of artificial intelligence (AI), particularly large language models (LLMs) and generative AI (GenAI), into translation education. It highlights how rapid technological advancements have outpaced the development of comprehensive ethical frameworks and pedagogical approaches within the field. The authors discuss the digital inequalities that result from disparities in access to AI tools, which can lead to unfair advantages among students and exacerbate existing socioeconomic and geographical divides. Issues of transparency in AI-assisted assessment are explored, emphasizing the need to shift from product-focused evaluation to process-oriented approaches that consider the human–AI interaction. The chapter proposes strategies for teaching AI-related ethics, including developing digital reflexivity, critical thinking, and ethical decision-making skills. It advocates for an integrated approach to professional ethics training, incorporating real-world scenarios, collaboration between academia and industry, and the development of discipline-specific AI guidelines. By addressing these ethical complexities, the chapter aims to prepare future translators for the evolving demands of an AI-augmented professional landscape. © 2025 selection and editorial matter, Sanjun Sun, Kanglong Liu and Riccardo Moratto.",,Decision making; Personnel training; Teaching; Artificial intelligence tools; Critical thinking; Digital inequalities; Ethical decision making; Ethical issues; Language model; Pedagogical approach; Process-oriented approaches; Socio-economics; Technological advancement; Students,Book chapter,Final,,Scopus,2-s2.0-105004130250
Chen Y.; Liu Z.,"Chen, Yuexi (58746156600); Liu, Zhicheng (55714445500)",58746156600; 55714445500,WordDecipher: Enhancing Digital Workspace Communication with Explainable AI for Non-native English Speakers,2024,ACM International Conference Proceeding Series,,,,7,10,3,1,10.1145/3690712.3690715,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209357161&doi=10.1145%2f3690712.3690715&partnerID=40&md5=9ec092ab17621d8bc88bcb6614dfa2d0,"Non-native English speakers (NNES) face challenges in digital workspace communication (e.g., emails, Slack messages), often inadvertently translating expressions from their native languages, which can lead to awkward or incorrect usage. Current AI-assisted writing tools are equipped with fluency enhancement and rewriting suggestions; however, NNES may struggle to grasp the subtleties among various expressions, making it challenging to choose the one that accurately reflects their intent. Such challenges are exacerbated in high-stake text-based communications, where the absence of non-verbal cues heightens the risk of misinterpretation. By leveraging the latest advancements in large language models (LLM) and word embeddings, we propose WordDecipher, an explainable AI-assisted writing tool to enhance digital workspace communication for NNES. WordDecipher not only identifies the perceived social intentions detected in users' writing, but also generates rewriting suggestions aligned with users' intended messages, either numerically or by inferring from users' writing in their native language. Then, WordDecipher provides an overview of nuances to help NNES make selections. Through a usage scenario, we demonstrate how WordDecipher can significantly enhance an NNES's ability to communicate her request, showcasing its potential to transform workspace communication for NNES. © 2024 Owner/Author.",AI-assisted writing tools; Explainable AI; Human-AI interaction; Large language models,Technical writing; Translation (languages); 'current; AI-assisted writing tool; Explainable AI; Human-AI interaction; Language model; Large language model; Native language; Non-native; Text-based communication; Writing tools; Digital elevation model,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85209357161
Bach T.A.; Kristiansen J.K.; Babic A.; Jacovi A.,"Bach, Tita A. (59229722200); Kristiansen, Jenny K. (58657064600); Babic, Aleksandar (58647926500); Jacovi, Alon (57210642311)",59229722200; 58657064600; 58647926500; 57210642311,Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review,2024,IEEE Access,12,,,106385,106414,29,3,10.1109/ACCESS.2024.3437190,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200246892&doi=10.1109%2fACCESS.2024.3437190&partnerID=40&md5=183c41ba0a8496dd303ff57e9c446d8c,"Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, existing research on HAII is limited, fragmented, and inconsistent. We present here a survey of that literature and recommendations for research best practices that should improve the field. We divided our investigation into the following areas: 1) terms used to describe HAII, 2) primary roles of AI-enabled systems, 3) factors that influence HAII, and 4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, seven factors influence HAII: user characteristics (e.g., user personality), user perceptions and attitudes (e.g., user biases), user expectations and experience (e.g., mismatched user expectations and experience), AI interface and features (e.g., interactive design), AI output (e.g., perceived accuracy), explainability and interpretability (e.g., level of detail, user understanding), and usage of AI (e.g., heterogeneity of environments). HAII is most measured with user-related subjective metrics (e.g., user perceptions, trust, and attitudes), and AI-assisted decision-making is the most common primary role of AI-enabled systems. Based on this review, we conclude that there are substantial research gaps in HAII. Researchers and developers need to codify HAII terminology, involve users throughout the AI lifecycle (especially during development), and tailor HAII in safety-critical industries to the users and environments. © 2013 IEEE.",Artificial intelligence; humans; measurement; methods; safety; safety-critical; society; survey; systematic literature review; technology readiness level; user,Accident prevention; Artificial intelligence; Decision making; Ergonomics; Job analysis; User interfaces; Human; Medical services; Method; Safety-critical; Society; Systematic literature review; Task analysis; Technology readiness levels; User; Terminology,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85200246892
Chen H.; Dreizin D.; Gomez C.; Zapaishchykova A.; Unberath M.,"Chen, Haomin (57218874723); Dreizin, David (9234377000); Gomez, Catalina (57199860392); Zapaishchykova, Anna (57219692045); Unberath, Mathias (56893868600)",57218874723; 9234377000; 57199860392; 57219692045; 56893868600,Interpretable Severity Scoring of Pelvic Trauma Through Automated Fracture Detection and Bayesian Inference,2025,IEEE Transactions on Medical Imaging,44,1,,130,141,11,1,10.1109/TMI.2024.3428836,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199332242&doi=10.1109%2fTMI.2024.3428836&partnerID=40&md5=66c2bbbc772ba42fabceb8ba614e5ab0,"Pelvic ring disruptions result from blunt injury mechanisms and are potentially lethal mainly due to associated injuries and massive pelvic hemorrhage. The severity of pelvic fractures in trauma victims is frequently assessed by grading the fracture according to the Tile AO/OTA classification in whole-body Computed Tomography (CT) scans. Due to the high volume of whole-body CT scans generated in trauma centers, the overall information content of a single whole-body CT scan and low manual CT reading speed, an automatic approach to Tile classification would provide substantial value, e.g., to prioritize the reading sequence of the trauma radiologists or enable them to focus on other major injuries in multi-trauma patients. In such a high-stakes scenario, an automated method for Tile grading should ideally be transparent such that the symbolic information provided by the method follows the same logic a radiologist or orthopedic surgeon would use to determine the fracture grade. This paper introduces an automated yet interpretable pelvic trauma decision support system to assist radiologists in fracture detection and Tile grading. To achieve interpretability despite processing high-dimensional whole-body CT images, we design a neurosymbolic algorithm that operates similarly to human interpretation of CT scans. The algorithm first detects relevant pelvic fractures on CTs with high specificity using Faster-RCNN. To generate robust fracture detections and associated detection (un)certainties, we perform test-time augmentation of the CT scans to apply fracture detection several times in a self-ensembling approach. The fracture detections are interpreted using a structural causal model based on clinical best practices to infer an initial Tile grade. We apply a Bayesian causal model to recover likely co-occurring fractures that may have been rejected initially due to the highly specific operating point of the detector, resulting in an updated list of detected fractures and corresponding final Tile grade. Our method is transparent in that it provides fracture location and types, as well as information on important counterfactuals that would invalidate the system's recommendation. Our approach achieves an AUC of 0.89/0.74 for translational and rotational instability,which is comparable to radiologist performance. Despite being designed for human-machine teaming, our approach does not compromise on performance compared to previous black-box methods.  © 1982-2012 IEEE.",Bayesian inference; deep learning; explainable machine learning; human-computer interaction,"Algorithms; Bayes Theorem; Fractures, Bone; Humans; Pelvic Bones; Tomography, X-Ray Computed; Whole Body Imaging; Automation; Bayesian networks; Classification (of information); Decision support systems; Deep learning; Grading; Human computer interaction; Inference engines; Three dimensional displays; Bayes method; Bayesian inference; Computed tomography; Deep learning; Explainable machine learning; Features extraction; Injury; Machine-learning; Three-dimensional display; age; Article; automation; Bayes theorem; Bayesian learning; best practice; causality; clinical significance; comparative study; computer assisted tomography; convolutional neural network; cross validation; data interpretation; decision support system; diastatic sacral fracture; disease severity; explainable artificial intelligence; false negative result; false positive result; frequency analysis; human; pelvis fracture; prediction; sacrum fracture; whole body CT; algorithm; diagnostic imaging; fracture; injury; pelvic girdle; procedures; whole body imaging; x-ray computed tomography; Feature extraction",Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85199332242
Teixeira B.; Valina L.; Pinto T.; Reis A.; Barroso J.; Vale Z.,"Teixeira, Brígida (56565068900); Valina, Luís (58131030100); Pinto, Tiago (35219107600); Reis, Arsénio (55803825800); Barroso, João (20435746800); Vale, Zita (57203219788)",56565068900; 58131030100; 35219107600; 55803825800; 20435746800; 57203219788,Exploring Clustering to Improve Interpretability in Complex Energy Forecasting Models,2024,"2024 International Conference on Smart Energy Systems and Technologies: Driving the Advances for Future Electrification, SEST 2024 - Proceedings",,,,,,,0,10.1109/SEST61601.2024.10694413,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207641777&doi=10.1109%2fSEST61601.2024.10694413&partnerID=40&md5=3321b2c79a5409af0349c742679e1113,"Explainable Artificial Intelligence (XAI) aims to enhance the interpretability of Artificial Intelligence (AI) systems for humans. The goal is to ensure that algorithmic decisions and underlying data are understandable to non-technical stakeholders. Advanced Machine Learning (ML) models, such as deep neural networks, enable AI systems to process vast data and extract intricate patterns, akin to the human brain, but this complicates XAI development. Complex ML models require substantial data for training, exacerbating the challenge. Consequently, this paper proposes a novel approach to improve XAI for complex ML models, particularly those with large data needs. Using K-Means clustering, the paper proposes to identify relevant data instances to create similarity clusters. This filtering process focuses XAI on essential information, even with complex models, reducing the data set to find patterns and explanations, so that, using the same approach, only the best explanations are filtered efficiently. The paper proposes to implement and test this model with a case study on ML for PV generation forecasting in buildings. Results show that the proposed approach is able to generate explanations that are very similar to those generated when using the entire available data, in only a portion of the execution time, leveraging from the identification of a small number of representative data points. This approach, therefore, enhances the efficiency of XAI by achieving promising results with a smaller dataset. It also facilitates the development of more understandable and fastly provided solutions, which is essential for real-world XAI users such as electric mobility users that need PV forecasting explanations as support for their vehicles charging management. © 2024 IEEE.",Artificial intelligence; clustering; explainable artificial intelligence; large data sets,Artificial intelligence systems; Clusterings; Complex energy; Complex machines; Energy forecasting; Explainable artificial intelligence; Forecasting models; Interpretability; Large datasets; Machine learning models; Deep neural networks,Conference paper,Final,,Scopus,2-s2.0-85207641777
Ling S.; Zhang Y.; Du N.,"Ling, Shihong (58237250400); Zhang, Yutong (58237740400); Du, Na (57201549821)",58237250400; 58237740400; 57201549821,More Is Not Always Better: Impacts of AI-Generated Confidence and Explanations in Human–Automation Interaction,2024,Human Factors,66,12,,2606,2620,14,2,10.1177/00187208241234810,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187416241&doi=10.1177%2f00187208241234810&partnerID=40&md5=dad37dcdec9bb41b5514269b0f2a4f90,"Objective: The study aimed to enhance transparency in autonomous systems by automatically generating and visualizing confidence and explanations and assessing their impacts on performance, trust, preference, and eye-tracking behaviors in human–automation interaction. Background: System transparency is vital to maintaining appropriate levels of trust and mission success. Previous studies presented mixed results regarding the impact of displaying likelihood information and explanations, and often relied on hand-created information, limiting scalability and failing to address real-world dynamics. Method: We conducted a dual-task experiment involving 42 university students who operated a simulated surveillance testbed with assistance from intelligent detectors. The study used a 2 (confidence visualization: yes vs. no) × 3 (visual explanations: none, bounding boxes, bounding boxes and keypoints) mixed design. Task performance, human trust, preference for intelligent detectors, and eye-tracking behaviors were evaluated. Results: Visual explanations using bounding boxes and keypoints improved detection task performance when confidence was not displayed. Meanwhile, visual explanations enhanced trust and preference for the intelligent detector, regardless of the explanation type. Confidence visualization did not influence human trust in and preference for the intelligent detector. Moreover, both visual information slowed saccade velocities. Conclusion: The study demonstrated that visual explanations could improve performance, trust, and preference in human–automation interaction without confidence visualization partially by changing the search strategies. However, excessive information might cause adverse effects. Application: These findings provide guidance for the design of transparent automation, emphasizing the importance of context-appropriate and user-centered explanations to foster effective human–machine collaboration. © 2024 Human Factors and Ergonomics Society.",explainable artificial intelligence; eye-tracking analysis; human–automation interaction; task performance; transparency; trust,Adult; Artificial Intelligence; Eye-Tracking Technology; Female; Humans; Male; Man-Machine Systems; Task Performance and Analysis; Trust; Young Adult; Automation; Behavioral research; Eye movements; Eye tracking; Visualization; Bounding-box; Explainable artificial intelligence; Eye-tracking; Eye-tracking analysis; Human-automation interactions; Keypoints; Performance tracking; Task performance; Tracking behavior; Trust; adult; artificial intelligence; eye-tracking technology; female; human; male; man machine interaction; task performance; trust; young adult; Transparency,Article,Final,,Scopus,2-s2.0-85187416241
Ehsan U.; Liao Q.V.; Passi S.; Riedl M.O.; Daumé H.I.I.I.,"Ehsan, Upol (57195223484); Liao, Q. Vera (36095944800); Passi, Samir (57193544205); Riedl, Mark O. (7004421643); Daumé, Hal I.I.I. (57210198346)",57195223484; 36095944800; 57193544205; 7004421643; 57210198346,Seamful XAI: Operationalizing Seamful Design in Explainable AI,2024,Proceedings of the ACM on Human-Computer Interaction,8,CSCW1,119,,,,5,10.1145/3637396,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193368846&doi=10.1145%2f3637396&partnerID=40&md5=4effec7a23b279cc9d5ee5d8c11d7a1b,"Mistakes in AI systems are inevitable, arising from both technical limitations and sociotechnical gaps. While black-boxing AI systems can make the user experience seamless, hiding the seams risks disempowering users to mitigate fallouts from AI mistakes. Instead of hiding these AI imperfections, can we leverage them to help the user? While Explainable AI (XAI) has predominantly tackled algorithmic opaqueness, we propose that seamful design can foster AI explainability by revealing and leveraging sociotechnical and infrastructural mismatches. We introduce the concept of Seamful XAI by (1) conceptually transferring “seams” to the AI context and (2) developing a design process that helps stakeholders anticipate and design with seams. We explore this process with 43 AI practitioners and real end-users, using a scenario-based co-design activity informed by real-world use cases. We found that the Seamful XAI design process helped users foresee AI harms, identify underlying reasons (seams), locate them in the AI's lifecycle, learn how to leverage seamful information to improve XAI and user agency. We share empirical insights, implications, and reflections on how this process can help practitioners anticipate and craft seams in AI, how seamfulness can improve explainability, empower end-users, and facilitate Responsible AI. © 2024 Copyright held by the owner/author(s).",explainable AI; human-AI interaction; responsible AI; Seamful design,Design; User interfaces; AI systems; Design-process; End-users; Explainable AI; Human-AI interaction; Responsible AI; Seamful designs; Sociotechnical; Technical limitations; Users' experiences; Life cycle,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85193368846
Durlik I.; Miller T.; Kostecka E.; Kozlovska P.; Ślączka W.,"Durlik, Irmina (58077891000); Miller, Tymoteusz (56583223500); Kostecka, Ewelina (55644024200); Kozlovska, Polina (58512715700); Ślączka, Wojciech (57195086774)",58077891000; 56583223500; 55644024200; 58512715700; 57195086774,Enhancing Safety in Autonomous Maritime Transportation Systems with Real-Time AI Agents,2025,Applied Sciences (Switzerland),15,9,4986,,,,0,10.3390/app15094986,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004899698&doi=10.3390%2fapp15094986&partnerID=40&md5=b901117d459fd7ab933550ee5238e884,"The maritime transportation sector is undergoing a profound shift with the emergence of autonomous vessels powered by real-time artificial intelligence (AI) agents. This article investigates the pivotal role of these agents in enhancing the safety, efficiency, and sustainability of autonomous maritime systems. Following a structured literature review, we examine the architecture of real-time AI agents, including sensor integration, communication systems, and computational infrastructure. We distinguish maritime AI agents from conventional systems by emphasizing their specialized functions, real-time processing demands, and resilience in dynamic environments. Key safety mechanisms—such as collision avoidance, anomaly detection, emergency coordination, and fail-safe operations—are analyzed to demonstrate how AI agents contribute to operational reliability. The study also explores regulatory compliance, focusing on emission control, real-time monitoring, and data governance. Implementation challenges, including limited onboard computational power, legal and ethical constraints, and interoperability issues, are addressed with practical solutions such as edge AI and modular architectures. Finally, the article outlines future research directions involving smart port integration, scalable AI models, and emerging technologies like federated and explainable AI. This work highlights the transformative potential of AI agents in advancing autonomous maritime transportation. © 2025 by the authors.",AI agents; anomaly detection; autonomous maritime systems; collision avoidance; computational infrastructure; cybersecurity; data governance; emergency response; ethical AI; human-machine interaction; maritime safety; real-time processing; regulatory compliance; smart port integration; sustainability,Autonomous vehicles; Ethical technology; Inland waterways; Interoperability; Marine safety; Anomaly detection; Artificial intelligence agent; Autonomous maritime system; Collisions avoidance; Computational infrastructure; Cyber security; Data governances; Emergency response; Ethical artificial intelligence; Human machine interaction; Maritime safety; Maritime systems; Realtime processing; Smart port integration; Emergency services,Review,Final,,Scopus,2-s2.0-105004899698
Xu K.; Shi J.,"Xu, Kun (57193532942); Shi, Jingyuan (56297090300)",57193532942; 56297090300,Visioning a two-level human-machine communication framework: initiating conversations between explainable AI and communication,2024,Communication Theory,34,4,,216,229,13,3,10.1093/ct/qtae016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208079357&doi=10.1093%2fct%2fqtae016&partnerID=40&md5=c27000af0b72a218c27af8c1fab74c9d,"Amid mounting interest in artificial intelligence (AI) technology, communication scholars have sought to understand humans' perceptions of and attitudes toward AI's predictions, recommendations, and decisions. Meanwhile, scholars in the nascent but growing field of explainable AI (XAI) have aimed to clarify AI's operational mechanisms and make them interpretable, visible, and transparent. In this conceptual article, we suggest that a conversation between human-machine communication (HMC) and XAI is advantageous and necessary. Following the introduction of these two areas, we demonstrate how research on XAI can inform the HMC scholarship regarding the human-in-the-loop approach and the message production explainability. Next, we expound upon how communication scholars' focuses on message sources, receivers, features, and effects can reciprocally benefit XAI research. At its core, this article proposes a two-level HMC framework and posits that bridging the two fields can guide future AI research and development. © 2024 The Author(s). Published by Oxford University Press on behalf of International Communication Association. All rights reserved.",artificial intelligence; explainable AI; human-AI interaction; human-in-the-loop approach; human-machine communication,artificial intelligence; future prospect; machine learning; perception; prediction; research and development,Article,Final,,Scopus,2-s2.0-85208079357
Akhtar M.; Nehal N.; Gull A.; Parveen R.; Khan S.; Khan S.; Ali J.,"Akhtar, Masheera (59698910400); Nehal, Nida (57219902144); Gull, Azka (57191417718); Parveen, Rabea (35084132400); Khan, Sana (59907855300); Khan, Saba (55703564100); Ali, Javed (25641028400)",59698910400; 57219902144; 57191417718; 35084132400; 59907855300; 55703564100; 25641028400,Explicating the transformative role of artificial intelligence in designing targeted nanomedicine,2025,Expert Opinion on Drug Delivery,,,,,,,0,10.1080/17425247.2025.2502022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005808512&doi=10.1080%2f17425247.2025.2502022&partnerID=40&md5=6014989a250924a74848cd50b0daef00,"Introduction: Artificial intelligence (AI) has emerged as a transformative force in nanomedicine, revolutionizing drug delivery, diagnostics, and personalized treatment. While nanomedicine offers precise targeted drug delivery and reduced toxic effects, its clinical translation is hindered by biological complexity, unpredictable in vivo behavior, and inefficient trial-and-error approaches. Areas covered: This review covers the application of AI and Machine Learning (ML) across the nanomedicine development pipeline, starting from drug and target identification to nanoparticle design, toxicity prediction, and personalized dosing. Different AI/ML models like QSAR, MTK-QSBER, and Alchemite, along with data sources and high-throughput screening methods, have been explored. Real-world applications are critically discussed, including AI-assisted drug repurposing, controlled-release formulations, and cancer-specific delivery systems. Expert opinion: AI has emerged as an essential component in designing next-generation nanomedicine. Efficiently handling multidimensional datasets, optimizing formulations, and personalizing treatment regimens, it has sped up the innovation process. However, challenges like data heterogeneity, model transparency, and regulatory gaps remain. Addressing these hurdles through interdisciplinary efforts and emerging innovations like explainable AI and federated learning will pave the way for the clinical translation of AI-driven nanomedicine. © 2025 Informa UK Limited, trading as Taylor & Francis Group.",Artificial intelligence; deep learning; machine learning; nanomedicine; personalized treatments,nanoparticle; artificial intelligence; controlled release formulation; deep learning; drug delivery system; drug development; drug repositioning; explainable artificial intelligence; federated learning; high throughput screening; human; machine learning; nanomedicine; nonhuman; personalized medicine; prediction; quantitative structure activity relation; review,Review,Article in press,,Scopus,2-s2.0-105005808512
Ding Y.; Jia L.; Du N.,"Ding, Yaohan (57221473551); Jia, Lesong (57219549255); Du, Na (57201549821)",57221473551; 57219549255; 57201549821,Watch Out for Explanations: Information Type and Error Type Affect Trust and Situational Awareness in Automated Vehicles,2025,IEEE Transactions on Human-Machine Systems,,,,,,,0,10.1109/THMS.2025.3558437,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004029939&doi=10.1109%2fTHMS.2025.3558437&partnerID=40&md5=4fb457bb40672d28aa2f284880af76d3,"Trust and situational awareness (SA) are critical for the acceptance and safety of automated vehicles (AVs). While AV explanations with different information types have been studied to enhance drivers' trust and SA, their effectiveness remains unclear when AVs make errors that do not trigger takeover requests. This study investigated the effects of information type, error type, and their interaction on drivers' trust in AVs, SA, and their relationships. We recruited 300 participants in an online video study with a 3 (information type: why, how, why + how) × 3 (error type: false alarm, miss, correct [no error]) mixed design. How information describes the vehicle's action, while why information refers to the reason for the vehicle's action. Linear mixed models showed that false alarms and misses were associated with lower SA compared with correct scenarios, but possibly due to different reasons. Compared with correct scenarios, both false alarms and misses were associated with lower trust, with misses even lower than false alarms, possibly due to the varying severity of potential consequences. Compared with why and why + how information, how information was generally associated with lower SA and a higher potential of overtrust in false alarms. Trust and SA had a negative linear relationship in misses and false alarms, while no correlations were found in correct scenarios. To mitigate potential overtrust and misinterpretation of situations when AVs make errors, it is crucial to maintain higher SA. We recommend including why information in AV explanations and deploying AV decision systems that are less miss-prone. © 2013 IEEE.",Automated vehicles (AVs); explainable artificial intelligence; human factors; human–machine interface; situational awareness (SA); trust,,Article,Article in press,,Scopus,2-s2.0-105004029939
Bhat M.,"Bhat, Maalvika (59210181600)",59210181600,Designing AI Interfaces for Transparent Decision-Making and Ethical Reflection,2025,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,211,214,3,0,10.1145/3708557.3716150,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001674262&doi=10.1145%2f3708557.3716150&partnerID=40&md5=89f46aa0ab298d97ba11c4a6b594bb79,"As artificial intelligence (AI) systems increasingly mediate high-stakes decisions in domains such as healthcare, finance, and education, ensuring transparency and ethical accountability in AI interfaces is critical. However, existing interfaces often obscure algorithmic processes, leading to overtrust, disengagement, or misinterpretation of AI-generated outputs. My research explores how interface design—including presentation modes, interactive explainability tools, and speculative design interventions—can shape user perceptions, foster critical reflection, and enhance AI literacy. By integrating controlled experiments, participatory design, and qualitative analysis, my work aims to develop AI interfaces that not only communicate algorithmic decisions effectively but also encourage users to critically assess the ethical implications of AI technologies. I examine the trade-offs between transparency, usability, and engagement, investigating how interfaces can balance cognitive load while making ethical considerations more salient. Through this doctoral consortium, I seek mentorship and feedback on designing adaptive transparency mechanisms and mitigating overtrust in engaging AI interfaces. © 2025 Copyright held by the owner/author(s).",Agency; AI Literacy; AI Transparency; Cognitive Interaction; Control; Design Theory; Explainable AI; Interaction Design; Technology-Mediated Learning; Trust; User Research,Decision making; Agency; Artificial intelligence literacy; Artificial intelligence transparency; Cognitive interaction; Design theory; Explainable artificial intelligence; Interaction design; Technology-mediated learning; Trust; User research; Ethical technology,Conference paper,Final,,Scopus,2-s2.0-105001674262
Kabir M.N.,"Kabir, Mitt Nowshade (59522788600)",59522788600,Unleashing Human Potential: A Framework for Augmenting Co-Creation with Generative AI,2024,"Proceedings of the 4th International Conference on AI Research, ICAIR 2024",,,,183,193,10,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215684757&partnerID=40&md5=ff69a3dee02e00bc61fe3cb623c98a75,"This paper redefines the traditional view of automation as a threat to human labor, advocating for human-AI co-creation as a strategic imperative for organizations. We propose a comprehensive six-stage framework for co-creation, emphasizing iterative feedback loops and continuous improvement to integrate generative AI into workflows effectively. Drawing from a multi-disciplinary perspective, we explore critical enablers of successful human-AI collaboration, including user-centered interface design, explainable AI systems, and fostering a culture of trust and experimentation. Real-world case studies, such as AI-enhanced visual design and creative writing, illustrate the transformative potential of co-creation across various sectors. We also propose a multifaceted measurement framework encompassing quantitative metrics (e.g., productivity gains, time-to-market acceleration) and qualitative indicators (e.g., employee well-being, skill development) to assess the impact of co-creation comprehensively. This research offers a strategic roadmap for organizations to embrace generative AI as a tool for collaboration and augmentation, thereby unlocking new levels of creativity, productivity, and employee empowerment. © Proceedings of the 4th International Conference on AI Research, ICAIR 2024.",Augmentation; Collaboration; Future of work; Generative AI; Human-AI co-creation; Innovation,Empowerment of personnel; Augmentation; Co-creation; Collaboration; Future of works; Generative AI; Human labor; Human potential; Human-AI co-creation; Innovation; Strategic imperative; User centered design,Conference paper,Final,,Scopus,2-s2.0-85215684757
Lee C.P.; Lee M.K.; Mutlu B.,"Lee, Christine P. (57476245100); Lee, Min Kyung (35573951200); Mutlu, Bilge (15060220700)",57476245100; 35573951200; 15060220700,The AI-DEC: A Card-based Design Method for User-centered AI Explanations,2024,"Proceedings of the 2024 ACM Designing Interactive Systems Conference, DIS 2024",,,,1010,1028,18,5,10.1145/3643834.3661576,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200342630&doi=10.1145%2f3643834.3661576&partnerID=40&md5=a72a8356a87061be4791ae4264632231,"Increasing evidence suggests that many deployed AI systems do not sufficiently support end-user interaction and information needs. Engaging end-users in the design of these systems can reveal user needs and expectations, yet effective ways of engaging end-users in the AI explanation design remain under-explored. To address this gap, we developed a design method, called AI-DEC, that defines four dimensions of AI explanations that are critical for the integration of AI systems—communication content, modality, frequency, and direction—and offers design examples for end-users to design AI explanations that meet their needs. We evaluated this method through co-design sessions with workers in healthcare, finance, and management industries who regularly use AI systems in their daily work. Findings indicate that the AI-DEC effectively supported workers in designing explanations that accommodated diverse levels of performance and autonomy needs, which varied depending on the AI system’s workplace role and worker values. We discuss the implications of using the AI-DEC for the user-centered design of AI explanations in real-world systems. © 2024 Copyright held by the owner/author(s).",Design cards; human-AI interaction; user-centered design,Human engineering; Human resource management; AI systems; Design card; Design method; End-users; Human-AI interaction; User information; User interaction; User need; User-centred; Workers'; User centered design,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85200342630
Olsen H.P.; Hildebrandt T.T.; Wiesener C.; Larsen M.S.; Ammitzbøll Flügge A.W.,"Olsen, Henrik Palmer (15921633400); Hildebrandt, Thomas Troels (12753331300); Wiesener, Cornelius (55824187000); Larsen, Matthias Smed (57733561000); Ammitzbøll Flügge, Asbjørn William (57214154567)",15921633400; 12753331300; 55824187000; 57733561000; 57214154567,The Right to Transparency in Public Governance: Freedom of Information and the Use of Artificial Intelligence by Public Agencies,2024,Digital Government: Research and Practice,5,1,8,,,,3,10.1145/3632753,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187807869&doi=10.1145%2f3632753&partnerID=40&md5=5792158c41ccf799f9327f5887c2f171,"What information should and can be transparent for artificial intelligence (AI) algorithms? This article examines the socio- technical and legal perspectives of transparency in relation to algorithmic decision-making in public administration. We show how transparency in AI can be understood in light of the various technologies and the challenges one may encounter. Despite some first steps in that direction, there exists so far no mature standard for documenting AI models. From a legal perspective, this article examined the applicable freedom of information (FOI) regimes across different jurisdictions, with a particular focus on Denmark and other Scandinavian countries. Despite notable differences, our findings show that the FOI regimes generally only grant access to existing documents, and that access can be denied on the basis of the wide proprietary interests and internal documents exemptions. This is why we ultimately conclude that the European data-protection framework and the proposed EU AI Act -with their far-reaching duties to document the functioning of AI systems -provide promising new avenues for research and insights into transparency in AI.  Copyright © 2024 held by the owner/author(s).",administrative decision-making; algorithm; freedom of information; Transparency,Artificial intelligence; Decision making; Public administration; Administrative decision making; Algorithmics; Artificial intelligence algorithms; Decisions makings; Denmark; Freedom of informations; Intelligence models; Public agencies; Sociotechnical; Various technologies; Transparency,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85187807869
Baron S.; Latham A.J.; Varga S.,"Baron, Sam (46460889000); Latham, Andrew J. (55890011600); Varga, Somogy (36449999000)",46460889000; 55890011600; 36449999000,Explainable AI and stakes in medicine: A user study,2025,Artificial Intelligence,340,,104282,,,,0,10.1016/j.artint.2025.104282,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215221249&doi=10.1016%2fj.artint.2025.104282&partnerID=40&md5=698931cb5d5e166bca5edfb0ca5952ed,"The apparent downsides of opaque algorithms have led to a demand for explainable AI (XAI) methods by which a user might come to understand why an algorithm produced the particular output it did, given its inputs. Patients, for example, might find that the lack of explanation of the process underlying the algorithmic recommendations for diagnosis and treatment hinders their ability to provide informed consent. This paper examines the impact of two factors on user perceptions of explanations for AI systems in medical contexts. The factors considered were the stakes of the decision—high versus low—and the decision source—human versus AI. 484 participants were presented with vignettes in which medical diagnosis and treatment plan recommendations were made by humans or by AI. Separate vignettes were used for high stakes scenarios involving life-threatening diseases, and low stakes scenarios involving mild diseases. In each vignette, an explanation for the decision was given. Four explanation types were tested across separate vignettes: no explanation, counterfactual, causal and a novel ‘narrative-based’ explanation, not previously considered. This yielded a total of 16 conditions, of which each participant saw only one. Individuals were asked to evaluate the explanations they received based on helpfulness, understanding, consent, reliability, trust, interests and likelihood of undergoing treatment. We observed a main effect for stakes on all factors and a main effect for decision source on all factors except for helpfulness and likelihood to undergo treatment. While we observed effects for explanation on helpfulness, understanding, consent, reliability, trust and interests, we by and large did not see any differences between the effects of explanation types. This suggests that the effectiveness of explanations may not depend on type of explanation but instead, on the stakes and decision source. © 2025",Causation; Explainable AI (XAI); Explainable ML; Human-AI interaction; Human-centered XAI; User study,AI systems; Algorithmics; Causation; Explainable AI (XAI); Explainable ML; Human-AI interaction; Human-centered XAI; Main effect; User perceptions; User study,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85215221249
Festor P.; Nagendran M.; Gordon A.C.; Faisal A.A.; Komorowski M.,"Festor, Paul (57222216170); Nagendran, Myura (36139697800); Gordon, Anthony C. (57206874913); Faisal, Aldo A. (6602900233); Komorowski, Matthieu (55639773100)",57222216170; 36139697800; 57206874913; 6602900233; 55639773100,Safety of human-AI cooperative decision-making within intensive care: A physical simulation study,2025,PLOS Digital Health,4,2,e0000726,,,,0,10.1371/journal.pdig.0000726,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219010067&doi=10.1371%2fjournal.pdig.0000726&partnerID=40&md5=6ec44ec1e817d51d9df9510d5ff4b7c3,"The safety of Artificial Intelligence (AI) systems is as much one of human decision-making as a technological question. In AI-driven decision support systems, particularly in high-stakes settings such as healthcare, ensuring the safety of human-AI interactions is paramount, given the potential risks of following erroneous AI recommendations. To explore this question, we ran a safety-focused clinician-AI interaction study in a physical simulation suite. Physicians were placed in a simulated intensive care ward, with a human nurse (played by an experimenter), an ICU data chart, a high-fidelity patient mannequin and an AI recommender system on a display. Clinicians were asked to prescribe two drugs for the simulated patients suffering from sepsis and wore eye-tracking glasses to allow us to assess where their gaze was directed. We recorded clinician treatment plans before and after they saw the AI treatment recommendations, which could be either ‘safe’ or ‘unsafe’. 92% of clinicians rejected unsafe AI recommendations vs 29% of safe ones. Physicians paid increased attention (+37% gaze fixations) to unsafe AI recommendations vs safe ones. However, visual attention on AI explanations was not greater in unsafe scenarios. Similarly, clinical information (patient monitor, patient chart) did not receive more attention after an unsafe versus safe AI reveal suggesting that the physicians did not look back to these sources of information to investigate why the AI suggestion might be unsafe. Physicians were only successfully persuaded to change their dose by scripted comments from the bedside nurse 5% of the time. Our study emphasises the importance of human oversight in safety-critical AI and the value of evaluating human-AI systems in high-fidelity settings that more closely resemble real world practice. © 2025 Festor et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85219010067
Hemalatha P.; Manikandan J.; Balaji B.; Sujitha V.,"Hemalatha, P. (57201494602); Manikandan, J. (57210765037); Balaji, B. (57209013453); Sujitha, V. (59716208300)",57201494602; 57210765037; 57209013453; 59716208300,Enlightened XAI: Illuminating Ethics and Equitable Explainability,2025,Explainable Artificial Intelligence in the Healthcare Industry,,,,593,617,24,0,10.1002/9781394249312.ch25,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001368718&doi=10.1002%2f9781394249312.ch25&partnerID=40&md5=51e2ad00ea791d96645366f5b2dbdcf1,"As artificial intelligence (AI) algorithms become increasingly integral parts of society, their opaqueness and inaccessibility raise significant ethical concerns. Explainable AI (XAI) addresses these challenges by providing insight into how AI models make decisions. In this chapter, we explore some ethical considerations surrounding XAI systems and the associated fairness concerns. This paper highlights the significance of transparency and interpretability for artificial intelligence algorithms and their associated risks such as biased decision making. Additionally, this chapter addresses the challenges of attaining fairness within XAI systems and their need to address algorithmic biases. Furthermore, frameworks and guidelines that ensure such systems’ responsible development and deployment uphold ethical principles while providing greater fairness are also reviewed. This chapter focuses on understanding the impact of artificial intelligence (AI) on society and individuals and emphasizes ethical guidelines in AI development. Additionally, its contents examine the potential consequences of biased AI decision making and real-world examples involving fairness issues related to AI development. This study investigates the trade-off between fairness and interpretability, assessing fairness in explainable AI models and successful implementations of ethical XAI through case studies and best practices. Also included are practical measures for incorporating ethics and fairness into XAI projects and public concerns about AI systems used for healthcare, finance, or criminal justice purposes. This chapter discusses emerging trends and research directions in ethical AI, placing special emphasis on interdisciplinary when developing ethical XAI applications while emphasizing fairness and transparency as vital ingredients of shaping its future, calling for responsible use and development of XAI so that its beneficial societal effects may be maximized. © 2025 Scrivener Publishing LLC.",Artificial intelligence; biased decision making; compliance; ethical XAI; explainable AI; human-AI collaboration; interpretability; k-nearest neighbors,,Book chapter,Final,,Scopus,2-s2.0-105001368718
Bertrand A.; Eagan J.R.; Maxwell W.; Brand J.,"Bertrand, Astrid (57226068202); Eagan, James R. (55966189500); Maxwell, Winston (57218952429); Brand, Joshua (57874215500)",57226068202; 55966189500; 57218952429; 57874215500,AI is Entering Regulated Territory: Understanding the Supervisors' Perspective on Model Justifiability in Financial Crime Detection,2024,Conference on Human Factors in Computing Systems - Proceedings,,,480,,,,0,10.1145/3613904.3642326,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194900040&doi=10.1145%2f3613904.3642326&partnerID=40&md5=566d068d921812b1dade8d701c693dfe,"Artificial intelligence (AI) has the potential to bring significant benefits to highly regulated industries such as healthcare or banking. Adoption, however, remains low. AI's entry into complex sociotechno-legal systems raises issues of transparency, specifically for regulators. However, the perspective of supervisors, regulators who monitor compliance with applicable financial regulations, has rarely been studied. This paper focuses on understanding the needs of supervisors in anti-money laundering (AML) to better inform the design of AI justifications and explanations in highly regulated fields. Through scenario-based workshops with 13 supervisors and 6 banking professionals, we outline the auditing practices and socio-technical context of the supervisor. By combining the workshops' insights with an analysis of compliance requirements, we identify the AML obligations that conflict with AI opacity. We then formulate seven needs that supervisors have for model justifiability. We discuss the role of explanations as reliable evidence on which to base justifications. © 2024 Copyright held by the owner/author(s)",AI regulation; anti-money laundering; explainability; highly-regulated environment; justifiability,Artificial intelligence; Laundering; Regulatory compliance; Anti-money laundering; Artificial intelligence regulation; Explainability; Financial crime detection; Financial regulations; Highly-regulated environment; Justifiability; Legal system; Scenario-based; Sociotechnical; Supervisory personnel,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85194900040
Marmolejo-Ramos F.; Marrone R.; Korolkiewicz M.; Gabriel F.; Siemens G.; Joksimovic S.; Yamada Y.; Mori Y.; Rahwan T.; Sahakyan M.; Sonna B.; Meirmanov A.; Bolatov A.; Som B.; Ndukaihe I.; Arinze N.C.; Kundrát J.; Skanderová L.; Ngo V.-G.; Nguyen G.; Lacia M.; Kung C.-C.; Irmayanti M.; Muktadir A.; Samosir F.T.; Liuzza M.T.; Giorgini R.; Khatin-Zadeh O.; Banaruee H.; Özdoğru A.A.; Ariyabuddhiphongs K.; Rakchai W.; Trujillo N.; Valencia S.M.; Janyan A.; Kostov K.; Montoro P.R.; Hinojosa J.; Medeiros K.; Hunt T.E.; Posada J.; Freitag R.M.K.; Tejada J.,"Marmolejo-Ramos, Fernando (31767581800); Marrone, Rebecca (57203360344); Korolkiewicz, Malgorzata (24332207000); Gabriel, Florence (57220338344); Siemens, George (36844717000); Joksimovic, Srecko (55837206800); Yamada, Yuki (55375527500); Mori, Yuki (57219519323); Rahwan, Talal (10239828100); Sahakyan, Maria (57295613800); Sonna, Belona (58181736200); Meirmanov, Assylbek (59563734200); Bolatov, Aidos (57219945238); Som, Bidisha (57201912372); Ndukaihe, Izuchukwu (57214221479); Arinze, Nwadiogo C. (57219930040); Kundrát, Josef (57226283420); Skanderová, Lenka (55234957700); Ngo, Van-Giang (59563580600); Nguyen, Giang (59563656800); Lacia, Michelle (59563580700); Kung, Chun-Chia (55268654300); Irmayanti, Meiselina (59342420400); Muktadir, Abdul (59342280200); Samosir, Fransiska Timoria (58756562100); Liuzza, Marco Tullio (39861663300); Giorgini, Roberto (58044684100); Khatin-Zadeh, Omid (57200438697); Banaruee, Hassan (57200442717); Özdoğru, Asil Ali (29068063500); Ariyabuddhiphongs, Kris (57221305502); Rakchai, Wachirawit (57223622945); Trujillo, Natalia (36928617300); Valencia, Stella Maris (57193610373); Janyan, Armina (6507529361); Kostov, Kiril (55311367100); Montoro, Pedro R. (25960230700); Hinojosa, Jose (35237108000); Medeiros, Kelsey (55356480500); Hunt, Thomas E. (54393367500); Posada, Julian (57219816505); Freitag, Raquel Meister Ko (58242257900); Tejada, Julian (58098065100)",31767581800; 57203360344; 24332207000; 57220338344; 36844717000; 55837206800; 55375527500; 57219519323; 10239828100; 57295613800; 58181736200; 59563734200; 57219945238; 57201912372; 57214221479; 57219930040; 57226283420; 55234957700; 59563580600; 59563656800; 59563580700; 55268654300; 59342420400; 59342280200; 58756562100; 39861663300; 58044684100; 57200438697; 57200442717; 29068063500; 57221305502; 57223622945; 36928617300; 57193610373; 6507529361; 55311367100; 25960230700; 35237108000; 55356480500; 54393367500; 57219816505; 58242257900; 58098065100,Factors influencing trust in algorithmic decision-making: an indirect scenario-based experiment,2024,Frontiers in Artificial Intelligence,7,,1465605,,,,1,10.3389/frai.2024.1465605,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218185895&doi=10.3389%2ffrai.2024.1465605&partnerID=40&md5=f769d97ad80e7c133e065080735bfcb9,"Algorithms are involved in decisions ranging from trivial to significant, but people often express distrust toward them. Research suggests that educational efforts to explain how algorithms work may help mitigate this distrust. In a study of 1,921 participants from 20 countries, we examined differences in algorithmic trust for low-stakes and high-stakes decisions. Our results suggest that statistical literacy is negatively associated with trust in algorithms for high-stakes situations, while it is positively associated with trust in low-stakes scenarios with high algorithm familiarity. However, explainability did not appear to influence trust in algorithms. We conclude that having statistical literacy enables individuals to critically evaluate the decisions made by algorithms, data and AI, and consider them alongside other factors before making significant life decisions. This ensures that individuals are not solely relying on algorithms that may not fully capture the complexity and nuances of human behavior and decision-making. Therefore, policymakers should consider promoting statistical/AI literacy to address some of the complexities associated with trust in algorithms. This work paves the way for further research, including the triangulation of data with direct observations of user interactions with algorithms or physiological measures to assess trust more accurately. Copyright © 2025 Marmolejo-Ramos, Marrone, Korolkiewicz, Gabriel, Siemens, Joksimovic, Yamada, Mori, Rahwan, Sahakyan, Sonna, Meirmanov, Bolatov, Som, Ndukaihe, Arinze, Kundrát, Skanderová, Ngo, Nguyen, Lacia, Kung, Irmayanti, Muktadir, Samosir, Liuzza, Giorgini, Khatin-Zadeh, Banaruee, Özdoğru, Ariyabuddhiphongs, Rakchai, Trujillo, Valencia, Janyan, Kostov, Montoro, Hinojosa, Medeiros, Hunt, Posada, Freitag and Tejada.",AI; algorithms; data; explainability; statistical literacy; trust,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85218185895
Qian P.; Unhelkar V.,"Qian, Peizhu (57806716500); Unhelkar, Vaibhav (56086482900)",57806716500; 56086482900,Interactively Explaining Robot Policies to Humans in Integrated Virtual and Physical Training Environments,2024,ACM/IEEE International Conference on Human-Robot Interaction,,,,847,851,4,0,10.1145/3610978.3640656,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188086000&doi=10.1145%2f3610978.3640656&partnerID=40&md5=5b6debbfeacabce47138a84f6f1a5847,"Policy summarization is a computational paradigm for explaining the behavior and decision-making processes of autonomous robots to humans. It summarizes robot policies via exemplary demonstrations, aiming to improve human understanding of robotic behaviors. This understanding is crucial, especially since users often make critical decisions about robot deployment in the real world. Previous research in policy summarization has predominantly focused on simulated robots and environments, overlooking its application to physically embodied robots. Our work fills this gap by combining current policy summarization methods with a novel, interactive user interface that involves physical interaction with robots. We conduct human-subject experiments to assess our explanation system, focusing on the impact of different explanation modalities in policy summarization. Our findings underscore the unique advantages of combining virtual and physical training environments to effectively communicate robot behavior to human users. © 2024 Copyright held by the owner/author(s)",AI-Assisted Human Training; Explainable AI; Value Alignment,Behavioral research; Decision making; E-learning; Robots; Virtual reality; AI-assisted human training; Computational paradigm; Decision-making process; Explainable AI; Human understanding; Physical training; Real-world; Robotic behavior; Value alignment; Virtual training; User interfaces,Conference paper,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85188086000
Bennett S.J.; Catanzariti B.; Tollon F.,"Bennett, S.J. (59036961200); Catanzariti, Benedetta (57324460200); Tollon, Fabio (57214150838)",59036961200; 57324460200; 57214150838,“Everybody knows what a pothole is”: representations of work and intelligence in AI practice and governance,2025,AI and Society,,,,,,,1,10.1007/s00146-024-02162-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217191777&doi=10.1007%2fs00146-024-02162-0&partnerID=40&md5=954d7f430adc3167b79a83382ad768d4,"In this paper, we empirically and conceptually examine how distributed human–machine networks of labour comprise a form of underlying intelligence within Artificial Intelligence (AI), considering the implications of this for Responsible Artificial Intelligence (R-AI) innovation. R-AI aims to guide AI research, development and deployment in line with certain normative principles, for example fairness, privacy, and explainability; notions implicitly shaped by comparisons of AI with individualised notions of human intelligence. However, as critical scholarship on AI demonstrates, this is a limited framing of the nature of intelligence, both of humans and AI. Furthermore, it dismisses the skills and labour central to developing AI systems, involving a distributed network of human-directed practices and reasoning. We argue that inequities in the agency and recognition of different types of practitioners across these networks of AI development have implications beyond RAI, with narrow framings concealing considerations which are important within broader discussions of AI intelligence. Drawing from interactive workshops conducted with AI practitioners, we explore practices of data acquisition, cleaning, and annotation, as the point where practitioners interface with domain experts and data annotators. Despite forming a crucial part of AI design and development, this type of data work is frequently framed as a tedious, unskilled, and low-value process. In exploring these practices, we examine the political role of the epistemic framings that underpin AI development and how these framings can shape understandings of distributed intelligence, labour practices, and annotators’ agency within data structures. Finally, we reflect on the implications of our findings for developing more participatory and equitable approaches to machine learning applications in the service of R-AI. © The Author(s) 2025.",Artificial intelligence; Automation; Intelligence; Labour; Machine learning; Responsible AI,,Article,Article in press,,Scopus,2-s2.0-85217191777
Fischer I.,"Fischer, Isabel (58339638000)",58339638000,Evaluating the ethics of machines assessing humans,2024,Journal of Information Technology Teaching Cases,14,2,,273,281,8,2,10.1177/20438869231178844,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162942545&doi=10.1177%2f20438869231178844&partnerID=40&md5=3b1154ec5dabd1d6bcb8ba51c3c5fbac,"The case focusses on AQA (formerly known as Assessment and Qualification Alliance), one of England’s largest exam boards, and its evaluation of whether Artificial Intelligence (AI) could be deployed, in principle, for marking high-stakes assessments. At a time when generative AI, such as ChatGPT, has gained popularity, the case offers insights into the challenges and risks of algorithmic decision making and algorithmic fairness, such as accuracy and explainability. The case allows students to explore the role of ethics when developing an AI-based tool in an area that they all know very well: Most students will have had to sit high-stakes assessments in the past and are still likely to be assessed on an ongoing basis as a current student. Students will thus be able to relate to the case and have their own stake(s) and opinion(s) about whether they would want to be assessed by AI. The case is also more broadly applicable in raising general awareness of the challenges and potential risks involved when using AI for decision making and it encourages students to consider the wider consequences for all stakeholders that are triggered by the use of digital technology. © Association for Information Technology Trust 2023.",algorithmic decision-making; artificial intelligence; artificial intelligence ethics; assessments; automated essay scoring; education,,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85162942545
Selimovic M.; Almisreb A.A.; Ismail N.; Amanzholova S.,"Selimovic, Mirza (59514088600); Almisreb, Ali Abd (50460937600); Ismail, Nurlaila (35092702500); Amanzholova, Saule (55624044900)",59514088600; 50460937600; 35092702500; 55624044900,"AI in Cancer Research: Challenges, Applications, and Future Directions",2025,Lecture Notes in Networks and Systems,1273,,,201,216,15,0,10.1007/978-3-031-82881-2_13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004638353&doi=10.1007%2f978-3-031-82881-2_13&partnerID=40&md5=467d4809cd21291312b7aed608f4d966,"The era of cancer diagnosis and treatment can change dramatically through Artificial Intelligence (AI). The present paper looks into AI's transformative attributes alongside the barriers and constraints that need to be solved for clinical integration to be successful. We discuss how AI-assisted solutions can help target therapeutic regime investigation and speed up medical decision-making in different medical fields related to oncology. Next, we highlight the challenges that harnessing AI faces like data standardization, ethical matters, and the level of accuracy of the data and adaptability of AI models. In summary, we underline the necessity of international cooperation, the importance of XAI (explainable AI) for establishing trust, and the conduct of the research in a manner that involves clinical trials and real-time demonstration in patients to guarantee database safety and usefulness in cancer care. AI can change the treatment world of cancers by combatting these obstacles and moving onto new ways. Such possibilities entail more accurate diagnosis, effective therapy, and hopefully, improved outcomes for the patients in the end. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",Artificial intelligence (AI); Cancer diagnosis; Clinical decision support systems; Clinical trials; Data standardization; Ethical considerations; Explainable AI (XAI); Personalized medicine; Real-world validation,Biopsy; Cardiography; Diseases; Ethical technology; Artificial intelligence; Cancer diagnosis; Clinical decision support systems; Clinical trial; Data standardization; Ethical considerations; Explainable artificial intelligence (XAI); Personalized medicines; Real-world; Real-world validation; Theranostics,Conference paper,Final,,Scopus,2-s2.0-105004638353
Mohammadi A.; Maghsoudi M.,"Mohammadi, Amirmahdi (58587308100); Maghsoudi, Mehrdad (57766558600)",58587308100; 57766558600,Bridging perspectives on artificial intelligence: a comparative analysis of hopes and concerns in developed and developing countries,2025,AI and Society,,,114155,,,,0,10.1007/s00146-025-02331-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002085433&doi=10.1007%2fs00146-025-02331-9&partnerID=40&md5=18a021830a2be5284c21331fb200a867,"Artificial intelligence (AI) is transforming industries, generating both enthusiasm and concern. While AI-driven innovations enhance productivity, healthcare, and education, significant ethical issues persist, including misinformation, algorithmic bias, and job displacement. This study examines public perceptions of AI by analyzing large-scale social media discourse and integrating sentiment analysis with expert insights via the Delphi method to assess global perspectives. Findings reveal notable differences across socio-economic contexts. In high-income countries, discussions emphasize AI ethics, governance, and automation risks, whereas in low-income regions, economic challenges and accessibility barriers dominate concerns. Public trust in AI is significantly influenced by governance frameworks, transparency in algorithmic decision-making, and regulatory oversight. Recent advancements in AI governance highlight the increasing role of explainable AI (XAI) and algorithmic fairness, alongside regulatory developments tailored to societal needs. Algorithmic nudging has emerged as a tool for guiding user behavior while maintaining autonomy, and research on user sensemaking in fairness and transparency underscores the importance of interpretability tools in fostering trust and acceptance of AI-driven decisions. These insights emphasize the need for adaptive, context-specific policies that ensure ethical AI deployment while mitigating risks. By bridging public sentiment analysis with governance research, this study provides a comprehensive understanding of AI’s societal impact. Findings offer practical implications for policymakers, industry leaders, and researchers, contributing to the development of inclusive, transparent, and accountable AI governance frameworks that align with public expectations. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.",AI governance; Ethical AI; Global AI strategies; Public sentiment; Socio-economic contexts,Algorithmics; Artificial intelligence governance; Comparative analyzes; Ethical artificial intelligence; Ethical issues; Global artificial intelligence strategy; Public sentiments; Sentiment analysis; Socio-economic context; Socio-economics; Decision making,Review,Article in press,,Scopus,2-s2.0-105002085433
Weng L.; Liu S.; Zhu H.; Sun J.; Kam-Kwai W.; Han D.; Zhu M.; Chen W.,"Weng, Luoxuan (57920303400); Liu, Shi (58090883300); Zhu, Hang (58220546100); Sun, Jiashun (58221637800); Kam-Kwai, Wong (57221150576); Han, Dongming (57203680623); Zhu, Minfeng (57149322700); Chen, Wei (55613230656)",57920303400; 58090883300; 58220546100; 58221637800; 57221150576; 57203680623; 57149322700; 55613230656,Towards an understanding and explanation for mixed-initiative artificial scientific text detection,2024,Information Visualization,23,3,,272,291,19,0,10.1177/14738716241240156,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190283229&doi=10.1177%2f14738716241240156&partnerID=40&md5=527c67b16a4903503e3d8396576d24d3,"Large language models (LLMs) have gained popularity in various fields for their exceptional capability of generating human-like text. Their potential misuse has raised social concerns about plagiarism in academic contexts. However, effective artificial scientific text detection is a non-trivial task due to several challenges, including (1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, (2) the poor generalization performance of existing methods caused by out-of-distribution issues, and (3) the limited support for human-machine collaboration with sufficient interpretability during the detection process. In this paper, we first identify the critical distinctions between machine-generated and human-written scientific text through a quantitative experiment. Then, we propose a mixed-initiative workflow that combines human experts’ prior knowledge with machine intelligence, along with a visual analytics system to facilitate efficient and trustworthy scientific text detection. Finally, we demonstrate the effectiveness of our approach through two case studies and a controlled user study. We also provide design implications for interactive artificial text detection tools in high-stakes decision-making scenarios. © The Author(s) 2024.",explainable artificial intelligence; Large language models; mixed-initiative; visual analytics,Artificial intelligence; Computational linguistics; Visualization; Explainable artificial intelligence; Human like; Language model; Large language model; Mixed-initiative; Non-trivial tasks; Scientific texts; Social concerns; Text detection; Visual analytics; Decision making,Article,Final,,Scopus,2-s2.0-85190283229
Mittal D.; Parashar A.R.; Thawkar S.; Katta V.S.,"Mittal, Deepti (59153830900); Parashar, Ajay Raj (58197212700); Thawkar, Shankar (57193201983); Katta, Vijay Subhash (56594615400)",59153830900; 58197212700; 57193201983; 56594615400,Human-machine interaction for knowledge discovery and management,2024,"Modern Technology in Healthcare and Medical Education: Blockchain, IoT, AR, and VR",,,,88,105,17,0,10.4018/979-8-3693-5493-3.ch006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194997246&doi=10.4018%2f979-8-3693-5493-3.ch006&partnerID=40&md5=d729a4f3cde06a3cfd80291c0f5cf4c9,"The ongoing data explosion driven by digital information growth has led to the emergence of human-machine interaction (HMI) as a vital tool in knowledge discovery and management across various disciplines. The exponential growth of knowledge discovery and management due to the influx of digital data and investigates into HMI applications, encompassing data visualization, natural language processing, machine learning, and augmented reality, with real-world examples demonstrating their ability in deciphering complex data and extracting insights. Ethical concerns and future directions, including issues of bias, privacy, security, and research prospects like explainable AI and personalized interfaces, are also discussed. The societal and workforce implications of HMI are explored, highlighting its potential benefits and challenges while advocating for responsible development and policies to ensure a harmonious interaction between humans and machines in the realm of knowledge discovery and management. © 2024, IGI Global.",,,Book chapter,Final,,Scopus,2-s2.0-85194997246
Schrank A.; Kettwich C.; Oehl M.,"Schrank, Andreas (57224536172); Kettwich, Carmen (24724601400); Oehl, Michael (23470366100)",57224536172; 24724601400; 23470366100,Aiding Automated Shuttles with Their Driving Tasks as an On-Board Operator: A Case Study on Different Automated Driving Systems in Three Living Labs,2024,Applied Sciences (Switzerland),14,8,3336,,,,2,10.3390/app14083336,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192506801&doi=10.3390%2fapp14083336&partnerID=40&md5=4cf0fc2eece67f77ec9f611789d6a236,"Highly automated shuttle vehicles (SAE Level 4) have the potential to enhance public transport services by decreasing the demand for drivers, enabling more frequent and flexible ride options. However, at least in a transitionary phase, safety operators that supervise and support the shuttles with their driving tasks may be required on board the vehicle from a technical or legal point of view. A crucial component for executing supervisory and intervening tasks is the human–machine interface between an automated vehicle and its on-board operator. This research presents in-depth case studies from three heterogenous living laboratories in Germany that deployed highly automated shuttle vehicles with on-board operators on public roads. The living labs differed significantly regarding the on-board operators’ tasks and the design of the human–machine interfaces. Originally considered a provisional solution until the vehicle automation is fully capable of running without human support, these interfaces were, in general, not designed in a user-centered way. However, since technological progress has been slower than expected, on-board operator interfaces are likely to persist in the mid-term at least. Hence, this research aims to assess the aptitude of interfaces that are in practical use for the on-board operators’ tasks, in order to determine the user-centered design of future interfaces. Completing questionnaires and undergoing comprehensive, semi-structured interviews, nine on-board operators evaluated their human–machine interfaces in light of the respective tasks they complete regarding user variables such as work context, acceptance, system transparency, and trust. The results were highly diverse across laboratories and underlined that the concrete system setup, encompassing task and interface design, has a considerable impact on these variables. Ergonomics, physical demand, and system transparency were identified as the most significant deficits. These findings and derived recommendations may inform the design of on-board operator workspaces, and bear implications for remote operation workstations as well. © 2024 by the authors.",automated driving; highly automated vehicles; human–machine interface; living lab; on-board operators; public transport; shuttles; workplace analysis,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85192506801
Yu H.; Kamat V.R.; Menassa C.C.,"Yu, Hongrui (58142004100); Kamat, Vineet R. (7004477339); Menassa, Carol C. (59887515600)",58142004100; 7004477339; 59887515600,Cloud-Based Hierarchical Imitation Learning for Scalable Transfer of Construction Skills from Human Workers to Assisting Robots,2024,Journal of Computing in Civil Engineering,38,4,4024019,,,,1,10.1061/JCCEE5.CPENG-5731,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191248891&doi=10.1061%2fJCCEE5.CPENG-5731&partnerID=40&md5=6248de17957b46907db2f26a59033263,"Assigning repetitive and physically demanding construction tasks to robots can alleviate human workers’ exposure to occupational injuries, which often result in significant downtime or premature retirement. However, the successful delegation of construction tasks and the achievement of high-quality robot-constructed work requires transferring necessary dexterous and adaptive construction craft skills from workers to robots. Predefined motion planning scripts tend to generate rigid and collision-prone robotic behaviors in unstructured construction site environments. In contrast, imitation learning (IL) offers a more robust and flexible skill transfer scheme. However, the majority of IL algorithms rely on human workers repeatedly demonstrating task performance at full scale, which can be counterproductive and infeasible in the case of construction work. To address this concern, in this paper, we propose an immersive and Cloud Robotics-based virtual demonstration framework that serves two primary purposes. First, it digitalizes the demonstration process, eliminating the need for repetitive physical manipulation of heavy construction objects. Second, it employs a federated collection of reusable demonstrations that are transferable for similar tasks in the future and can, consequently, reduce the requirement for repetitive illustration of tasks by human agents. In addition, to enhance the trustworthiness, explainability, and ethical soundness of the robot training, this framework utilizes a hierarchical imitation learning (HIL) model to decompose human manipulation skills into sequential and reactive subskills. These two layers of skills are represented by deep generative models; these models enable adaptive control of robot action. The proposed framework has the potential to mitigate technical adoption barriers and facilitate the practical deployment of full-scale construction robots to perform a variety of tasks with human supervision. By delegating the physical strains of construction work to human-trained robots, this framework promotes the inclusion of workers with diverse physical capabilities and educational backgrounds within the construction industry. © 2024 American Society of Civil Engineers.",,Demonstrations; Motion planning; Occupational risks; Personnel training; Robot programming; Virtual addresses; Cloud-based; Construction skills; Construction works; Craft skills; High quality; Imitation learning; Motion-planning; Occupational injury; Workers'; Workers' exposures; Construction industry,Article,Final,,Scopus,2-s2.0-85191248891
Li P.; Hosseinzadeh P.; Bahri O.; Boubrahimi S.F.; Hamdi S.M.,"Li, Peiyu (57468838200); Hosseinzadeh, Pouya (57900601700); Bahri, Omar (57866817400); Boubrahimi, Soukaïna Filali (57192204690); Hamdi, Shah Muhammad (57192205976)",57468838200; 57900601700; 57866817400; 57192204690; 57192205976,Reliable Time Series Counterfactual Explanations Guided by ShapeDBA,2024,"Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024",,,,1574,1579,5,0,10.1109/BigData62323.2024.10825447,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217991038&doi=10.1109%2fBigData62323.2024.10825447&partnerID=40&md5=642de01887135cdd0c8f23704350c9bc,"Artificial intelligence (AI) and algorithmic decision-making are profoundly shaping various aspects of society, with applications in healthcare, business, education, etc. As these systems become more integral to high-stakes decisions, concerns about their transparency and interpretability are growing. To address these concerns, explainable AI (XAI) methods have been developed, with counterfactual explanations emerging as a powerful tool. Counterfactuals help users understand AI decisions by demonstrating how small changes in input could alter the outcome, providing a clear and intuitive way to interpret AI behavior. Despite their potential, generating valid, interpretable, and efficient counterfactual explanations is particularly challenging in time series domains, where data points are interdependent. In this paper, we introduce a novel approach to counterfactual explanations guided by ShapeDTW Barycenter Averaging (ShapeDBA). By integrating ShapeDBA into the counterfactual generation process, we ensure that the produced explanations are not only valid and interpretable but also efficient to generate. Our approach provides counterfactuals that align closely with human intuition while maintaining the computational efficiency required for practical deployment. This work represents a significant step forward in the development of interpretable AI systems, particularly in the complex domain of time series analysis. © 2024 IEEE.",counterfactual explanations; DTW barycenter averaging; Explainable Artificial Intelligence (XAI); time series classification,Economic and social effects; Algorithmics; Barycenters; Business educations; Counterfactual explanation; Counterfactuals; Decisions makings; DTW barycenter averaging; Explainable artificial intelligence (XAI); Time series classifications; Times series; Decision making,Conference paper,Final,,Scopus,2-s2.0-85217991038
Sarker I.H.,"Sarker, Iqbal H. (56997358700)",56997358700,"AI-Driven Cybersecurity and Threat Intelligence: Cyber Automation, Intelligent Decision-Making and Explainability",2024,"AI-Driven Cybersecurity and Threat Intelligence: Cyber Automation, Intelligent Decision-Making and Explainability",,,,1,200,199,32,10.1007/978-3-031-54497-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200166453&doi=10.1007%2f978-3-031-54497-2&partnerID=40&md5=e1a1546ee712d46205910fb07a262e45,"This book explores the dynamics of how AI (Artificial Intelligence) technology intersects with cybersecurity challenges and threat intelligence as they evolve. Integrating AI into cybersecurity not only offers enhanced defense mechanisms, but this book introduces a paradigm shift illustrating how one conceptualize, detect and mitigate cyber threats. An in-depth exploration of AI-driven solutions is presented, including machine learning algorithms, data science modeling, generative AI modeling, threat intelligence frameworks and Explainable AI (XAI) models. As a roadmap or comprehensive guide to leveraging AI/XAI to defend digital ecosystems against evolving cyber threats, this book provides insights, modeling, real-world applications and research issues. Throughout this journey, the authors discover innovation, challenges, and opportunities. It provides a holistic perspective on the transformative role of AI in securing the digital world. Overall, the useof AI can transform the way one detects, responds and defends against threats, by enabling proactive threat detection, rapid response and adaptive defense mechanisms. AI-driven cybersecurity systems excel at analyzing vast datasets rapidly, identifying patterns that indicate malicious activities, detecting threats in real time as well as conducting predictive analytics for proactive solution. Moreover, AI enhances the ability to detect anomalies, predict potential threats, and respond swiftly, preventing risks from escalated. As cyber threats become increasingly diverse and relentless, incorporating AI/XAI into cybersecurity is not just a choice, but a necessity for improving resilience and staying ahead of ever-changing threats. This book targets advanced-level students in computer science as a secondary textbook. Researchers and industry professionals working in various areas, such as Cyber AI, Explainable and Responsible AI, Human-AI Collaboration, Automation and Intelligent Systems, Adaptive and Robust Security Systems, Cybersecurity Data Science and Data-Driven Decision Making will also find this book useful as reference book. © All rights reserved.","Artificial intelligence; Automation; Cyber data analytics; Cyber threat intelligence; Cybersecurity; Data science; Deep learning; Explainable AI; Generative AI; Intelligent decision-making; Large language modeling; Machine learning,; Next-generation cybersecurity applications; Responsible AI",,Book,Final,,Scopus,2-s2.0-85200166453
Giavina Bianchi M.; D'adario A.; Giavina Bianchi P.; Machado B.S.,"Giavina Bianchi, Mara (16241736400); D'adario, Andrew (59468332200); Giavina Bianchi, Pedro (6506219239); Machado, Birajara Soares (7005886478)",16241736400; 59468332200; 6506219239; 7005886478,"Three versions of an atopic dermatitis case report written by humans, artificial intelligence, or both: Identification of authorship and preferences",2025,Journal of Allergy and Clinical Immunology: Global,4,1,100373,,,,0,10.1016/j.jacig.2024.100373,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211716448&doi=10.1016%2fj.jacig.2024.100373&partnerID=40&md5=d73e655c4a2472ecaddf29913b3be5ed,"Background: The use of artificial intelligence (AI) in scientific writing is rapidly increasing, raising concerns about authorship identification, content quality, and writing efficiency. Objectives: This study investigates the real-world impact of ChatGPT, a large language model, on those aspects in a simulated publication scenario. Methods: Forty-eight individuals representing 3 medical expertise levels (medical students, residents, and experts in allergy or dermatology) evaluated 3 blinded versions of an atopic dermatitis case report: one each human written (HUM), AI generated (AI), and combined written (COM). The survey assessed authorship, ranked their preference, and graded 13 quality criteria for each text. Time taken to generate each manuscript was also recorded. Results: Authorship identification accuracy mirrored the odds at 33%. Expert participants (50.9%) demonstrated significantly higher accuracy compared to residents (27.7%) and students (19.6%, P < .001). Participants favored AI-assisted versions (AI and COM) over HUM (P < .001), with COM receiving the highest quality scores. COM and AI achieved 83.8% and 84.3% reduction in writing time, respectively, compared to HUM, while showing 13.9% (P < .001) and 11.1% improvement in quality (P < .001), respectively. However, experts assigned the lowest score for the references of the AI manuscript, potentially hindering its publication. Conclusion: AI can deceptively mimic human writing, particularly for less experienced readers. Although AI-assisted writing is appealing and offers significant time savings, human oversight remains crucial to ensure accuracy, ethical considerations, and optimal quality. These findings underscore the need for transparency in AI use and highlight the potential of human-AI collaboration in the future of scientific writing. © 2024 The Author(s)",artificial intelligence; ChatGPT; Generative Pre-training Transformer (GPT); large language model (LLM); medical survey; scientific writing,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85211716448
Din M.; Daga K.; Saoud J.; Wood D.; Kierkegaard P.; Brex P.; Booth T.C.,"Din, Munaib (57199235932); Daga, Karan (57221089194); Saoud, Jihad (59508373100); Wood, David (57219466235); Kierkegaard, Patrick (36910697000); Brex, Peter (6701510522); Booth, Thomas C (57219470277)",57199235932; 57221089194; 59508373100; 57219466235; 36910697000; 6701510522; 57219470277,Clinicians’ perspectives on the use of artificial intelligence to triage MRI brain scans,2025,European Journal of Radiology,183,,111921,,,,1,10.1016/j.ejrad.2025.111921,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214584451&doi=10.1016%2fj.ejrad.2025.111921&partnerID=40&md5=fa8e8324e642d6746863e38140c54325,"Artificial intelligence (AI) tools can triage radiology scans to streamline the patient pathway and also relieve clinician workload. Validated AI tools can mitigate the delays in reporting scans by flagging time-sensitive and actionable findings. In this study, we aim to investigate current stakeholder perspectives and identify obstacles to integrating AI in clinical pathways. We created a survey to ascertain the perspectives of 133 clinicians across the United Kingdom regarding the acceptability of an AI tool that triages MRI brain scans into ‘normal’ and ‘abnormal’. As part of this survey, we supplied clinicians with information on training and validation case numbers, model performance, validation using unseen data, and explainability saliency maps. With regards to the specific use case of AI in MRI brain scans, 71% of respondents preferred the use of an AI-assisted triage compared to the current system without triage, typically chronologically. Notably, information that explained and helped visualise the AI model's decision making was found to improve clinician confidence. When shown a heatmap, 60% of participants felt more confident in the AI's decision. The results of this short communication demonstrate a positive support for the implementation of AI-assistive tools in triage. © 2025 The Authors",Artificial Intelligence; Brain; MRI; Triage,Artificial Intelligence; Attitude of Health Personnel; Brain; Humans; Magnetic Resonance Imaging; Surveys and Questionnaires; Triage; United Kingdom; adult; Article; artificial intelligence; brain scintiscanning; clinical pathway; clinical practice; clinician; decision making; demographics; female; follow up; health care; human; Likert scale; male; neuroimaging; neuroradiologist; normal human; nuclear magnetic resonance imaging; patient triage; questionnaire; training; brain; diagnostic imaging; health personnel attitude; patient triage; procedures; United Kingdom,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85214584451
Widjaja G.; Wahid N.; Raha S.; Pande S.D.; Manerkar S.G.V.,"Widjaja, Gunawan (59346590800); Wahid, Nazia (57210185714); Raha, Shrinwantu (57211181982); Pande, Sagar Dhanraj (57213160540); Manerkar, Shri Ganesh Vasudeo (58943304300)",59346590800; 57210185714; 57211181982; 57213160540; 58943304300,"An exhaustive exploration of explainable AI-driven applications in healthcare, enhancing diagnostic accuracy, treatment efficacy, and patient trust",2024,Explainable Artificial Intelligence for Biomedical and Healthcare Applications,,,,230,248,18,0,10.1201/9781003220107-14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210641547&doi=10.1201%2f9781003220107-14&partnerID=40&md5=a1abf906a23b751d84dd65448f009e23,"The purpose of this abstract is to offer a comprehensive assessment of the methodologies. Explainable artificial intelligence (XAI) is transforming medical treatment and patient care. It focuses on the several ways that XAI-powered apps are transforming the healthcare environment by enhancing treatment effectiveness, diagnostic accuracy, and patient confidence in AI-assisted healthcare systems. The goal of this research is to look at the use of XAI in diagnosis, therapeutic planning, and continuous patient monitoring. The capacity of XAI to provide a clear rationale for AI judgments is critical in sensitive healthcare situations where trust, openness, and lives are at risk. According to various research studies, artificial intelligence may enhance medical results by leveraging real-world case studies and settings. It also investigates the ethical implications of XAI in healthcare. Patient privacy, algorithm fairness, and legal compliance are all critical considerations here. © 2025 selection and editorial matter, Aditya Khamparia and Deepak Gupta; individual chapters, the contributors.",,,Book chapter,Final,,Scopus,2-s2.0-85210641547
Sobrie L.; Verschelde M.,"Sobrie, Léon (58192438400); Verschelde, Marijn (55255524600)",58192438400; 55255524600,Real-time decision support for human–machine interaction in digital railway control rooms,2024,Decision Support Systems,181,,114216,,,,4,10.1016/j.dss.2024.114216,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189566946&doi=10.1016%2fj.dss.2024.114216&partnerID=40&md5=05fe7c51cf9aedbe645d0f84978cfec9,"This study proposes a real-time Decision Support System (DSS) using machine learning to enhance proactive management of Human–Machine Interaction (HMI) in safety–critical digital control rooms. The DSS provides explainable predictions and recommendations regarding near-future automation usage, customized for the railway control room management, who supervise the operations of traffic controllers (TCs). In this setting, TCs decide on the spot whether to manually or automatically open signals to regulate railway traffic, a critical aspect of ensuring punctuality and safety. This time-setting specific HMI differs across TCs and is not yet supported by a data-driven tool. The proposed DSS includes agreement levels for predictions among different modeling paradigms: linear models, tree-based models, and deep neural networks. SHAP (SHapley Additive exPlanations) values are deployed to assess the agreement level in explainability between these different modeling paradigms. The prescriptions are based on the HMI of well-performing peers. We implement the DSS as proof of concept at the Belgian railway infrastructure company and report end-user feedback on the perception, the operational impact, and the inclusion of agreement levels. © 2024 Elsevier B.V.",Behavioral prescription; Decision support systems; End-user feedback; Explainable prediction; Human–machine interaction; Real-time railway traffic management,Decision making; Deep neural networks; Digital control systems; Feedback; Forecasting; Railroad transportation; Railroads; Rails; Real time systems; Behavioral prescription; End-user feedback; Explainable prediction; Human machine interaction; Modeling paradigms; Railway control; Railway traffic management; Real- time; Real-time railway traffic management; Traffic controllers; Decision support systems,Article,Final,,Scopus,2-s2.0-85189566946
Sloane M.; Wüllhorst E.,"Sloane, Mona (57211216452); Wüllhorst, Elena (58134571200)",57211216452; 58134571200,"A systematic review of regulatory strategies and transparency mandates in AI regulation in Europe, the United States, and Canada",2025,Data and Policy,7,,e11,,,,1,10.1017/dap.2024.54,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000818351&doi=10.1017%2fdap.2024.54&partnerID=40&md5=fed8a8e010afc89161af793627c2ea17,"In this paper, we provide a systematic review of existing artificial intelligence (AI) regulations in Europe, the United States, and Canada. We build on the qualitative analysis of 129 AI regulations (enacted and not enacted) to identify patterns in regulatory strategies and in AI transparency requirements. Based on the analysis of this sample, we suggest that there are three main regulatory strategies for AI: AI-focused overhauls of existing regulation, the introduction of novel AI regulation, and the omnibus approach. We argue that although these types emerge as distinct strategies, their boundaries are porous as the AI regulation landscape is rapidly evolving. We find that across our sample, AI transparency is effectively treated as a central mechanism for meaningful mitigation of potential AI harms. We therefore focus on AI transparency mandates in our analysis and identify six AI transparency patterns: human in the loop, assessments, audits, disclosures, inventories, and red teaming. We contend that this qualitative analysis of AI regulations and AI transparency patterns provides a much needed bridge between the policy discourse on AI, which is all too often bound up in very detailed legal discussions and applied sociotechnical research on AI fairness, accountability, and transparency. © The Author(s), 2025.",AI regulation; compliance; sociotechnical research; transparency,,Conference paper,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-105000818351
Lee C.P.,"Lee, Christine P. (57476245100)",57476245100,"Design, Development, and Deployment of Context-Adaptive AI Systems for Enhanced User Adoption",2024,Conference on Human Factors in Computing Systems - Proceedings,,,429,,,,1,10.1145/3613905.3638195,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194167021&doi=10.1145%2f3613905.3638195&partnerID=40&md5=1b3f94bf12fa6b7cadae2fc07928095b,"My research centers on the development of context-adaptive AI systems to improve end-user adoption through the integration of technical methods. I deploy these AI systems across various interaction modalities, including user interfaces and embodied agents like robots, to expand their practical applicability. My research unfolds in three key stages: design, development, and deployment. In the design phase, user-centered approaches were used to understand user experiences with AI systems and create design tools for user participation in crafting AI explanations. In the ongoing development stage, a safety-guaranteed AI system for a robot agent was created to automatically provide adaptive solutions and explanations for unforeseen scenarios. The next steps will involve the implementation and evaluation of context-adaptive AI systems in various interaction forms. I seek to prioritize human needs in technology development, creating AI systems that tangibly benefit end-users in real-world applications and enhance interaction experiences. © 2024 Association for Computing Machinery. All rights reserved.",Human-AI interaction; human-robot interaction; user-centered design,Human robot interaction; Machine design; Software design; User interfaces; AI systems; Design development; Design phase; Embodied agent; End-users; Human-AI interaction; Humans-robot interactions; Research center; User adoptions; User interface agents; User centered design,Conference paper,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85194167021
Finkelstein J.; Gabriel A.; Schmer S.; Truong T.-T.; Dunn A.,"Finkelstein, Joseph (7201815643); Gabriel, Aileen (58265134000); Schmer, Susanna (59332587700); Truong, Tuyet-Trinh (55561269700); Dunn, Andrew (57197633891)",7201815643; 58265134000; 59332587700; 55561269700; 57197633891,Identifying Facilitators and Barriers to Implementation of AI-Assisted Clinical Decision Support in an Electronic Health Record System,2024,Journal of Medical Systems,48,1,89,,,,8,10.1007/s10916-024-02104-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204309975&doi=10.1007%2fs10916-024-02104-9&partnerID=40&md5=57cb4d04e4773e5eaf3837faf7eb4b87,"Recent advancements in computing have led to the development of artificial intelligence (AI) enabled healthcare technologies. AI-assisted clinical decision support (CDS) integrated into electronic health records (EHR) was demonstrated to have a significant potential to improve clinical care. With the rapid proliferation of AI-assisted CDS, came the realization that a lack of careful consideration of socio-technical issues surrounding the implementation and maintenance of these tools can result in unanticipated consequences, missed opportunities, and suboptimal uptake of these potentially useful technologies. The 48-h Discharge Prediction Tool (48DPT) is a new AI-assisted EHR CDS to facilitate discharge planning. This study aimed to methodologically assess the implementation of 48DPT and identify the barriers and facilitators of adoption and maintenance using the validated implementation science frameworks. The major dimensions of RE-AIM (Reach, Effectiveness, Adoption, Implementation, Maintenance) and the constructs of the Consolidated Framework for Implementation Research (CFIR) frameworks have been used to analyze interviews of 24 key stakeholders using 48DPT. The systematic assessment of the 48DPT implementation allowed us to describe facilitators and barriers to implementation such as lack of awareness, lack of accuracy and trust, limited accessibility, and transparency. Based on our evaluation, the factors that are crucial for the successful implementation of AI-assisted EHR CDS were identified. Future implementation efforts of AI-assisted EHR CDS should engage the key clinical stakeholders in the AI tool development from the very inception of the project, support transparency and explainability of the AI models, provide ongoing education and onboarding of the clinical users, and obtain continuous input from clinical staff on the CDS performance. © The Author(s) 2024.",Artificial Intelligence; Clinical Decision Support; Electronic Health Record; Implementation Science; Socio-Technical Factors,"Artificial Intelligence; Decision Support Systems, Clinical; Electronic Health Records; Humans; Patient Discharge; article; artificial intelligence; awareness; clinical decision support system; drug therapy; electronic health record; hospital discharge; human; implementation science; interview; major clinical study; pilot study; prediction; organization and management",Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85204309975
Schemmer M.; Kuehl N.; Benz C.; Bartos A.; Satzger G.,"Schemmer, Max (57226651200); Kuehl, Niklas (57196220660); Benz, Carina (57202586715); Bartos, Andrea (57225147786); Satzger, Gerhard (25825625400)",57226651200; 57196220660; 57202586715; 57225147786; 25825625400,Appropriate Reliance on AI Advice: Conceptualization and the Effect of Explanations,2023,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,410,422,12,74,10.1145/3581641.3584066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152119120&doi=10.1145%2f3581641.3584066&partnerID=40&md5=ae468e21ff729bc86d2af226e0301e17,"AI advice is becoming increasingly popular, e.g., in investment and medical treatment decisions. As this advice is typically imperfect, decision-makers have to exert discretion as to whether actually follow that advice: they have to ""appropriately""rely on correct and turn down incorrect advice. However, current research on appropriate reliance still lacks a common definition as well as an operational measurement concept. Additionally, no in-depth behavioral experiments have been conducted that help understand the factors influencing this behavior. In this paper, we propose Appropriateness of Reliance (AoR) as an underlying, quantifiable two-dimensional measurement concept. We develop a research model that analyzes the effect of providing explanations for AI advice. In an experiment with 200 participants, we demonstrate how these explanations influence the AoR, and, thus, the effectiveness of AI advice. Our work contributes fundamental concepts for the analysis of reliance behavior and the purposeful design of AI advisors.  © 2023 ACM.",Appropriate Reliance; Explainable AI; Human-AI Collaboration; Human-AI Complementarity,Decision making; 'current; Appropriate reliance; Behavioral experiment; Decision makers; Explainable AI; Human-AI collaboration; Human-AI complementarity; Measurement concepts; Medical treatment; Two-dimensional measurement; Human engineering,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85152119120
Victoria A.H.; Tiwari R.S.; Ghulam A.K.,"Victoria, A. Helen (58121257400); Tiwari, Ravi Shekhar (57224778545); Ghulam, Ayaan Khadir (59180031900)",58121257400; 57224778545; 59180031900,Libraries for Explainable Artificial Intelligence (EXAI) Python,2024,Explainable AI (XAI) for Sustainable Development: Trends and Applications,,,,211,232,21,2,10.1201/9781003457176-13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196457231&doi=10.1201%2f9781003457176-13&partnerID=40&md5=ebfd141ed84779d7bd0192debb39e9c2,"Artificial Intelligence (AI) was first coined by Sir John McCarthy in the 1960s, and many of the algorithms that are used today were invented by researchers several decades ago. But due to restrictions on storage and computing power, they were not able to be implemented in real-world applications. Around 2010, the implementation pace increased drastically because of the breakthroughs in computing as well as storage. Since then, AI has been implemented in almost every field. Healthcare is one of the few fields where AI is taking its baby steps. AI is dependent on the dataset though healthcare has existed for several centuries. But we have a dataset shortage when it comes to train the AI model, especially deep learning (DL) models. Somehow, researchers are able to solve the dataset shortage problem by augmenting the original dataset or by generating synthetic dataset – considering several parameters from the original dataset such as mean, dispersion, mode range, and a variety of parameters to train/test and deploy the model. Machine learning (ML) and DL are subsets of AI, which is dependent on quality data for training. In recent years, researchers have succeeded in training a state-of-the-art model that can predict fatigue, wear-tear, life cycle, and other numerous properties of materials. But with recent breakthroughs, one problem of bias is persistent, and it has made a catastrophic, i.e., biased model. Quality of model output is clearly dependent on the quality of the dataset, and if the dataset is biased, then the model will become biased about certain features that can lead to incorrect results. We have several epitomes of biased models; for instance, in the USA, black skin colored people were categorized as criminals, women were considered as homemakers, and men were considered as breadwinners by the NLP word-embedding model and several other models which became biased on certain features after training. So, there is a need to explain the model behavior with respect to the dataset by considering each specific feature as well as by considering the full dataset. So Explainable Artificial Intelligence (XAI) comes into existence which provides the reason why the model predicted this output. Healthcare is a very sensitive area where small error can cost numerous human lives and early detection of diseases can save numerous lives. ML is holding its ground but the reason why the respective ML model comes to certain decisions is still unknown; hence, it is known as black-box models. ML products and algorithms have been utilized in humongous amounts by our environment, and a single error or wrong prediction can cost numerous human lives. So, there is a need to explain why the model predicted this specific value of class so that the human supervisor can be satisfied as well as the chance of error can be minimized. Recently, XAI has explained the reason behind the model prediction satisfactorily by employing algorithms such as Feature Importance, Permutation of Features, SHAP Value, and Activation at each layer which are used by various libraries to visually represent the reason behind the prediction. Nowadays, there are a variety of frameworks and libraries to justify the prediction of the model such as What-If Tool by TensorFlow, ELI 5, Shapley, Lime, IBM 360 Explainability, Deep LIFT, Skater, etc. © 2024 selection and editorial matter, Lakshmi D., Ravi Shekhar Tiwari, Rajesh Kumar Dhanaraj and Seifedine Kadry; individual chapters, the contributors.",,,Book chapter,Final,,Scopus,2-s2.0-85196457231
Kottinger J.; Almagor S.; Lahijanian M.,"Kottinger, Justin (57224008572); Almagor, Shaull (36627051400); Lahijanian, Morteza (35105134400)",57224008572; 36627051400; 35105134400,Conflict-Based Search for Explainable Multi-Agent Path Finding,2022,"Proceedings International Conference on Automated Planning and Scheduling, ICAPS",32,,,692,700,8,11,10.1609/icaps.v32i1.19859,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142631834&doi=10.1609%2ficaps.v32i1.19859&partnerID=40&md5=91baf553137034fe489d840701c5b3db,"The goal of the Multi-Agent Path Finding (MAPF) problem is to find non-colliding paths for agents in an environment, such that each agent reaches its goal from its initial location. In safety-critical applications, a human supervisor may want to verify that the plan is indeed collision-free. To this end, a recent work introduces a notion of explainability for MAPF based on a visualization of the plan as a short sequence of images representing time segments, where in each time segment the trajectories of the agents are disjoint. Then, the problem of Explainable MAPF via Segmentation asks for a set of non-colliding paths that admit a short-enough explanation. Explainable MAPF adds a new difficulty to MAPF, in that it is NP-hard with respect to the size of the environment, and not just the number of agents. Thus, traditional MAPF algorithms are not equipped to directly handle Explainable MAPF. In this work, we adapt Conflict Based Search (CBS), a well-studied algorithm for MAPF, to handle Explainable MAPF. We show how to add explainability constraints on top of the standard CBS tree and its underlying A∗ search. We examine the usefulness of this approach and, in particular, the trade-off between planning time and explainability. © 2022, Association for the Advancement of Artificial Intelligence.",,Electric circuit breakers; Image segmentation; Multi agent systems; Safety engineering; Collision-free; Human supervisors; Multi agent; NP-hard; Path finding; Path finding problems; Safety critical applications; Sequence of images; Short sequences; Time segments; Economic and social effects,Conference paper,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85142631834
Mualla Y.; Tchappi I.; Kampik T.; Najjar A.; Calvaresi D.; Abbas-Turki A.; Galland S.; Nicolle C.,"Mualla, Yazan (57196025561); Tchappi, Igor (57203995511); Kampik, Timotheus (56728735900); Najjar, Amro (55821733600); Calvaresi, Davide (56358489700); Abbas-Turki, Abdeljalil (55971619000); Galland, Stéphane (23008496500); Nicolle, Christophe (6701366851)",57196025561; 57203995511; 56728735900; 55821733600; 56358489700; 55971619000; 23008496500; 6701366851,The quest of parsimonious XAI: A human-agent architecture for explanation formulation,2022,Artificial Intelligence,302,,103573,,,,32,10.1016/j.artint.2021.103573,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113998530&doi=10.1016%2fj.artint.2021.103573&partnerID=40&md5=709fb1f7eec11394a7b634e3f9e1519b,"With the widespread use of Artificial Intelligence (AI), understanding the behavior of intelligent agents and robots is crucial to guarantee successful human-agent collaboration since it is not straightforward for humans to understand an agent's state of mind. Recent empirical studies have confirmed that explaining a system's behavior to human users fosters the latter's acceptance of the system. However, providing overwhelming or unnecessary information may also confuse the users and cause failure. For these reasons, parsimony has been outlined as one of the key features allowing successful human-agent interaction with parsimonious explanation defined as the simplest explanation (i.e. least complex) that describes the situation adequately (i.e. descriptive adequacy). While parsimony is receiving growing attention in the literature, most of the works are carried out on the conceptual front. This paper proposes a mechanism for parsimonious eXplainable AI (XAI). In particular, it introduces the process of explanation formulation and proposes HAExA, a human-agent explainability architecture allowing to make it operational for remote robots. To provide parsimonious explanations, HAExA relies on both contrastive explanations and explanation filtering. To evaluate the proposed architecture, several research hypotheses are investigated in an empirical user study that relies on well-established XAI metrics to estimate how trustworthy and satisfactory the explanations provided by HAExA are. The results are analyzed using parametric and non-parametric statistical testing. © 2021 Elsevier B.V.",Empirical user studies; Explainable artificial intelligence; Human-computer interaction; Multi-agent systems; Statistical testing,Artificial intelligence; Behavioral research; Intelligent agents; Empirical studies; Human agent interactions; Human users; Key feature; Non-parametric; Proposed architectures; Remote robot; Statistical testing; Social robots,Article,Final,,Scopus,2-s2.0-85113998530
Sipos L.; Schäfer U.; Glinka K.; Müller-Birn C.,"Sipos, Lars (57926152300); Schäfer, Ulrike (58521323300); Glinka, Katrin (57194556142); Müller-Birn, Claudia (35243519200)",57926152300; 58521323300; 57194556142; 35243519200,Identifying Explanation Needs of End-users: Applying and Extending the XAI Question Bank,2023,ACM International Conference Proceeding Series,,,,492,497,5,6,10.1145/3603555.3608551,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171176071&doi=10.1145%2f3603555.3608551&partnerID=40&md5=f872b7700f8a37b3fc3d3e8a71beec5c,"Explainable Artificial Intelligence (XAI) is concerned with making the decisions of AI systems interpretable to humans. Explanations are typically developed by AI experts and focus on algorithmic transparency and the inner workings of AI systems. Research has shown that such explanations do not meet the needs of users who do not have AI expertise. As a result, explanations are often ineffective in making system decisions interpretable and understandable. We aim to strengthen a socio-technical view of AI by following a Human-Centered Explainable Artificial Intelligence (HC-XAI) approach, which investigates the explanation needs of end-users (i.e., subject matter experts and lay users) in specific usage contexts. One of the most influential works in this area is the XAI Question Bank (XAIQB) by Liao et al. The authors propose a set of questions that end-users might ask when using an AI system, which in turn is intended to help developers and designers identify and address explanation needs. Although the XAIQB is widely referenced, there are few reports of its use in practice. In particular, it is unclear to what extent the XAIQB sufficiently captures the explanation needs of end-users and what potential problems exist in the practical application of the XAIQB. To explore these open questions, we used the XAIQB as the basis for analyzing 12 think-aloud software explorations with subject matter experts, i.e., art historians. We investigated the suitability of the XAIQB as a tool for identifying explanation needs in a specific usage context. Our analysis revealed a number of explanation needs that were missing from the question bank, but that emerged repeatedly as our study participants interacted with an AI system. We also found that some of the XAIQB questions were difficult to distinguish and required interpretation during use. Our contribution is an extension of the XAIQB with 11 new questions. In addition, we have expanded the descriptions of all new and existing questions to facilitate their use. We hope that this extension will enable HCI researchers and practitioners to use the XAIQB in practice and may provide a basis for future studies on the identification of explanation needs in different contexts.  © 2023 Owner/Author.",Explainable AI; Explanation needs; Human-AI collaboration; User study,User interfaces; AI systems; Algorithmics; End-users; Explainable AI; Explanation need; Human-AI collaboration; Question banks; Subject matter experts; Usage context; User study; Artificial intelligence,Conference paper,Final,,Scopus,2-s2.0-85171176071
Naiseh M.; Al-Thani D.; Jiang N.; Ali R.,"Naiseh, Mohammad (57217108046); Al-Thani, Dena (56444337800); Jiang, Nan (56038315600); Ali, Raian (56038311800)",57217108046; 56444337800; 56038315600; 56038311800,How the different explanation classes impact trust calibration: The case of clinical decision support systems,2023,International Journal of Human Computer Studies,169,,102941,,,,65,10.1016/j.ijhcs.2022.102941,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139859152&doi=10.1016%2fj.ijhcs.2022.102941&partnerID=40&md5=87e55eeea60ef783e02b78186514493b,"Machine learning has made rapid advances in safety-critical applications, such as traffic control, finance, and healthcare. With the criticality of decisions they support and the potential consequences of following their recommendations, it also became critical to provide users with explanations to interpret machine learning models in general, and black-box models in particular. However, despite the agreement on explainability as a necessity, there is little evidence on how recent advances in eXplainable Artificial Intelligence literature (XAI) can be applied in collaborative decision-making tasks, i.e., human decision-maker and an AI system working together, to contribute to the process of trust calibration effectively. This research conducts an empirical study to evaluate four XAI classes for their impact on trust calibration. We take clinical decision support systems as a case study and adopt a within-subject design followed by semi-structured interviews. We gave participants clinical scenarios and XAI interfaces as a basis for decision-making and rating tasks. Our study involved 41 medical practitioners who use clinical decision support systems frequently. We found that users perceive the contribution of explanations to trust calibration differently according to the XAI class and to whether XAI interface design fits their job constraints and scope. We revealed additional requirements on how explanations shall be instantiated and designed to help a better trust calibration. Finally, we build on our findings and present guidelines for designing XAI interfaces. © 2022",Clinical decision support systems; Explainable AI; Human-AI Interaction; Trust Calibration,Decision making; Decision support systems; Machine learning; Safety engineering; Traffic control; Black box modelling; Clinical decision support systems; Collaborative decision making; Explainable AI; Human decisions; Human-AI interaction; Machine learning models; Machine-learning; Safety critical applications; Trust calibration; Calibration,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85139859152
Ellenrieder S.; Kallina E.; Pumplun L.; Gawlitza J.F.; Ziegelmayer S.; Buxmann P.,"Ellenrieder, Sara (58176751900); Kallina, Emma (57218764038); Pumplun, Luisa (57217045826); Gawlitza, Joshua Felix (57189056020); Ziegelmayer, Sebastian (57202266583); Buxmann, Peter (6603259233)",58176751900; 57218764038; 57217045826; 57189056020; 57202266583; 6603259233,Promoting Learning Through Explainable Artificial Intelligence: An Experimental Study in Radiology,2023,"International Conference on Information Systems, ICIS 2023: ""Rising like a Phoenix: Emerging from the Pandemic and Reshaping Human Endeavors with Digital Technologies""",,,,,,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192570517&partnerID=40&md5=b5a1efccf1a48359852d4f953934d5bf,"The deployment of machine learning (ML)-based decision support systems (DSSs) in high-risk environments such as radiology is increasing. Despite having achieved high decision accuracy, they are prone to errors. Thus, they are primarily used to assist radiologists in their decision making. However, collaborative decision making poses risks to the decision maker, e.g. automation bias and long-term performance degradation. To address these issues, we propose combining findings of the research streams of explainable artificial intelligence and education to promote human learning through interaction with ML-based DSSs. We provided radiologists with explainable vs non-explainable decision support that was high- vs low-performing in a between-subject experimental study to support manual segmentation of 690 brain tumor scans. Our results show that explainable ML-based DSSs improved human learning outcomes and prevented false learning triggered by incorrect decision support. In fact, radiologists were able to learn from errors made by the low-performing explainable ML-based DSS. © 2023 International Conference on Information Systems, ICIS 2023: ""Rising like a Phoenix: Emerging from the Pandemic and Reshaping Hu. All Rights Reserved.",experimental study; Explainable artificial intelligence; human learning; human-AI collaboration; machine learning; radiology,Decision making; Information systems; Information use; Machine learning; Radiology; Collaborative decision making; Decision accuracies; Decision supports; Decisions makings; Experimental study; Explainable artificial intelligence; High risk environment; Human learning; Human-AI collaboration; Machine-learning; Decision support systems,Conference paper,Final,,Scopus,2-s2.0-85192570517
Retzlaff C.O.; Das S.; Wayllace C.; Mousavi P.; Afshari M.; Yang T.; Saranti A.; Angerschmid A.; Taylor M.E.; Holzinger A.,"Retzlaff, Carl Orge (57226654783); Das, Srijita (57213459594); Wayllace, Christabel (57192386042); Mousavi, Payam (58194977000); Afshari, Mohammad (58439456600); Yang, Tianpei (57193340370); Saranti, Anna (36761552200); Angerschmid, Alessa (57575470700); Taylor, Matthew E. (10339318000); Holzinger, Andreas (23396282000)",57226654783; 57213459594; 57192386042; 58194977000; 58439456600; 57193340370; 36761552200; 57575470700; 10339318000; 23396282000,"Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities",2024,Journal of Artificial Intelligence Research,79,,,359,415,56,42,10.1613/jair.1.15348,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184656775&doi=10.1613%2fjair.1.15348&partnerID=40&md5=7d43b0336a7075eb678e93ab7d56a7bf,"Artificial intelligence (AI) and especially reinforcement learning (RL) have the potential to enable agents to learn and perform tasks autonomously with superhuman performance. However, we consider RL as fundamentally a Human-in-the-Loop (HITL) paradigm, even when an agent eventually performs its task autonomously. In cases where the reward function is challenging or impossible to define, HITL approaches are considered particularly advantageous. The application of Reinforcement Learning from Human Feedback (RLHF) in systems such as ChatGPT demonstrates the effectiveness of optimizing for user experience and integrating their feedback into the training loop. In HITL RL, human input is integrated during the agent’s learning process, allowing iterative updates and fine-tuning based on human feedback, thus enhancing the agent’s performance. Since the human is an essential part of this process, we argue that human-centric approaches are the key to successful RL, a fact that has not been adequately considered in the existing literature. This paper aims to inform readers about current explainability methods in HITL RL. It also shows how the application of explainable AI (xAI) and specific improvements to existing explainability approaches can enable a better human-agent interaction in HITL RL for all types of users, whether for lay people, domain experts, or machine learning specialists. Accounting for the workflow in HITL RL and based on software and machine learning methodologies, this article identifies four phases for human involvement for creating HITL RL systems: (1) Agent Development, (2) Agent Learning, (3) Agent Evaluation, and (4) Agent Deployment. We highlight human involvement, explanation requirements, new challenges, and goals for each phase. We furthermore identify low-risk, high-return opportunities for explainability research in HITL RL and present long-term research goals to advance the field. Finally, we propose a vision of human-robot collaboration that allows both parties to reach their full potential and cooperate effectively. ©2024 The Authors. Published by AI Access Foundation under Creative Commons Attribution License CC BY 4.0.",,Autonomous agents; Iterative methods; Learning systems; Robots; Fine tuning; Human-in-the-loop; Iterative update; Learn+; Learning process; Machine-learning; Performance; Reinforcement learnings; Reward function; Users' experiences; Reinforcement learning,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85184656775
Linja A.; Mamun T.I.; Mueller S.T.,"Linja, Anne (57939992900); Mamun, Tauseef Ibne (57222238564); Mueller, Shane T. (24765109700)",57939992900; 57222238564; 24765109700,When Self-Driving Fails: Evaluating Social Media Posts Regarding Problems and Misconceptions about Tesla’s FSD Mode,2022,Multimodal Technologies and Interaction,6,10,86,,,,5,10.3390/mti6100086,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140474124&doi=10.3390%2fmti6100086&partnerID=40&md5=b7eac6332adcd381cb0ea9b12d2c8c3e,"With the recent deployment of the latest generation of Tesla’s Full Self-Driving (FSD) mode, consumers are using semi-autonomous vehicles in both highway and residential driving for the first time. As a result, drivers are facing complex and unanticipated situations with an unproven technology, which is a central challenge for cooperative cognition. One way to support cooperative cognition in such situations is to inform and educate the user about potential limitations. Because these limitations are not always easily discovered, users have turned to the internet and social media to document their experiences, seek answers to questions they have, provide advice on features to others, and assist other drivers with less FSD experience. In this paper, we explore a novel approach to supporting cooperative cognition: Using social media posts can help characterize the limitations of the automation in order to get information about the limitations of the system and explanations and workarounds for how to deal with these limitations. Ultimately, our goal is to determine the kinds of problems being reported via social media that might be useful in helping users anticipate and develop a better mental model of an AI system that they rely on. To do so, we examine a corpus of social media posts about FSD problems to identify (1) the typical problems reported, (2) the kinds of explanations or answers provided by users, and (3) the feasibility of using such user-generated information to provide training and assistance for new drivers. The results reveal a number of limitations of the FSD system (e.g., lane-keeping and phantom braking) that may be anticipated by drivers, enabling them to predict and avoid the problems, thus allowing better mental models of the system and supporting cooperative cognition of the human-AI system in more situations. © 2022 by the authors.",cooperative cognition; Explainable AI; Tesla FSD; user-centered AI,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85140474124
Banf M.; Steinhagen G.,"Banf, Michael (26422756200); Steinhagen, Gregor (57425239800)",26422756200; 57425239800,Supervising The Supervisor – Model Monitoring In Production Using Deep Feature Embeddings With Applications To Workpiece Inspection,2022,Proceedings of the World Congress on Electrical Engineering and Computer Systems and Science,,,MVML 104,,,10,0,10.11159/mhci22.104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141827266&doi=10.11159%2fmhci22.104&partnerID=40&md5=b828c923ce57fcfbf157903a0dc62119,"The automation of condition monitoring and workpiece inspection plays an essential role in maintaining high quality as well as high throughput of the manufacturing process. To this end, the recent rise of developments in machine learning has lead to vast improvements in the area of autonomous process supervision. However, the more complex and powerful these models become, the less transparent and explainable they generally are as well. One of the main challenges is the monitoring of live deployments of these machine learning systems and raising alerts when encountering events that might impact model performance. In particular, supervised classifiers are typically build under the assumption of stationarity in the underlying data distribution. For example, a visual inspection system trained on a set of material surface defects generally does not adapt or even recognize gradual changes in the data distribution-an issue known as ""data drift""-such as the emergence of new types of surface defects. This, in turn, may lead to detrimental mispredictions, e.g. samples from new defect classes being classified as non-defective. To this end, it is desirable to provide real-time tracking of a classifier’s performance to inform about the putative onset of additional error classes and the necessity for manual intervention with respect to classifier re-training. Here, we propose an unsupervised framework that acts on top of a supervised classification system, thereby harnessing its internal deep feature representations as a proxy to track changes in the data distribution during deployment and, hence, to anticipate classifier performance degradation. © 2022, Avestia Publishing. All rights reserved.",condition monitoring; data drift detection; deep feature learning; explainable artificial intelligence; model performance monitoring; transfer learning,,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85141827266
Morris J.; Liu Z.; Liang H.; Nagala S.; Hong X.,"Morris, Jamie (58726759400); Liu, Zehao (57339903600); Liang, Huizhi (26422285000); Nagala, Sidhartha (6506103353); Hong, Xia (7201551690)",58726759400; 57339903600; 26422285000; 6506103353; 7201551690,ThyExp: An explainable AI-assisted Decision Making Toolkit for Thyroid Nodule Diagnosis based on Ultra-sound Images,2023,"International Conference on Information and Knowledge Management, Proceedings",,,,5371,5375,4,2,10.1145/3583780.3615131,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178133932&doi=10.1145%2f3583780.3615131&partnerID=40&md5=a98a65bf67321db23c092309eecefc1b,"Radiologists have an important task of diagnosing thyroid nodules present in ultra sound images. Although reporting systems exist to aid in the diagnosis process, these systems do not provide explanations about the diagnosis results. We present ThyExp - a web based toolkit for it use by medical professionals, allowing for accurate diagnosis with explanations of thyroid nodules present in ultrasound images utilising artificial intelligence models. The proposed web-based toolkit can be easily incorporated into current medical workflows, and allows medical professionals to have the confidence of a highly accurate machine learning model with explanations to provide supplementary diagnosis data. The solution provides classification results with their probability accuracy, as well as the explanations in the form of presenting the key features or characteristics that contribute to the classification results. The experiments conducted on a real-world UK NHS hospital patient dataset demonstrate the effectiveness of the proposed approach. This toolkit can improve the trust of medical professional to understand the confidence of the model in its predictions. This toolkit can improve the trust of medical professionals in understanding the models reasoning behind its predictions. © 2023 Copyright held by the owner/author(s).",Artificial Intelligence Assisted disease diagnosis; Thyroid Nodule,Artificial intelligence; Computer aided diagnosis; Decision making; Medical imaging; Artificial intelligence assisted disease diagnose; Classification results; Decisions makings; Disease diagnosis; Medical professionals; Reporting systems; Sound image; Thyroid nodule; Ultrasound images; Web based; Websites,Conference paper,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85178133932
Ding Y.; Jia L.; Du N.,"Ding, Yaohan (57221473551); Jia, Lesong (57219549255); Du, Na (57201549821)",57221473551; 57219549255; 57201549821,Designing for Trust and Situational Awareness in Automated Vehicles: Effects of Information Type and Error Type,2023,Proceedings of the Human Factors and Ergonomics Society,67,1,,1176,1177,1,2,10.1177/21695067231192406,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191010449&doi=10.1177%2f21695067231192406&partnerID=40&md5=e084cecdc3d7a406bb25e90332a3875e,"Trust and situational awareness (SA) are crucial to the adoption and safety of automated vehicles (AVs). Appropriate design of AV explanations could promote drivers’ acceptance, trust, and SA, enabling drivers to get more benefits from the technology. This study investigated the effects of error type and information type of AV explanations on drivers’ trust and SA. We recruited 300 participants for an online video study with a 3 (information type) × 2 (error type) mixed design. Linear mixed model analyses showed that compared with false alarms, misses were associated with more trust decrease after the error and more trust decrease after the post-error recovery. Compared with why information, how information was associated with lower SA generally and risked potential over-trust in false alarms. Therefore, we recommend deploying AV decision systems that are less miss-prone and including why information in AV explanations. © 2023 Human Factors and Ergonomics Society.",Automated Vehicles; Explainable Artificial Intelligence; Human Factors; Human-Machine Interface; Situational Awareness; Trust,Automation; Human engineering; Vehicles; Appropriate designs; Automated vehicles; Error types; Explainable artificial intelligence; Falsealarms; Human Machine Interface; Information types; Situational awareness; Trust; Trust-awareness; Errors,Conference paper,Final,,Scopus,2-s2.0-85191010449
Manca G.; Bhattacharya N.; MacZey S.; Ziobro D.; Brorsson E.; Bång M.,"Manca, Gianluca (57222573400); Bhattacharya, Nilavra (56785508000); MacZey, Sylvia (54938580900); Ziobro, Dawid (57223037655); Brorsson, Emmanuel (58589990300); Bång, Magnus (7004092161)",57222573400; 56785508000; 54938580900; 57223037655; 58589990300; 7004092161,XAIProcessLens: A Counterfactual-Based Dashboard for Explainable AI in Process Industries,2023,Frontiers in Artificial Intelligence and Applications,368,,,401,403,2,2,10.3233/FAIA230110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171476418&doi=10.3233%2fFAIA230110&partnerID=40&md5=bbe7f575c5be745013383e081f0ba748,"We present a novel counterfactual-based dashboard for explainable artificial intelligence (XAI) in process industries, aimed at enhancing the understanding and adoption of machine learning (ML) models by providing transparency, explainability, and performance evaluation. Our dashboard comprises two modules: a statistical analysis module for data visualization and model performance assessment, and an XAI module for exploring counterfactual explanations at varying levels of abstraction. Through a case study of an industrial batch process, we demonstrate the dashboard's applicability and potential to increase trust in ML models among stakeholders, paving the way for confident deployment in process industries. © 2023 The Authors.",Counterfactual explanations; dashboard; explainable AI; human-in-the-loop; machine learning; process industries,Batch data processing; Data visualization; Counterfactual explanation; Counterfactuals; Dashboard; Explainable AI; Human-in-the-loop; In-process; Machine learning models; Machine-learning; Performances evaluation; Process industries; Machine learning,Conference paper,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85171476418
Berretta S.; Tausch A.; Ontrup G.; Gilles B.; Peifer C.; Kluge A.,"Berretta, Sophie (58309043900); Tausch, Alina (57219250178); Ontrup, Greta (56888728700); Gilles, Björn (58647761200); Peifer, Corinna (56104297200); Kluge, Annette (23397227500)",58309043900; 57219250178; 56888728700; 58647761200; 56104297200; 23397227500,Defining human-AI teaming the human-centered way: a scoping review and network analysis,2023,Frontiers in Artificial Intelligence,6,,1250725,,,,24,10.3389/frai.2023.1250725,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174188953&doi=10.3389%2ffrai.2023.1250725&partnerID=40&md5=c68c9632825b83d85fb4119b5f789226,"Introduction: With the advancement of technology and the increasing utilization of AI, the nature of human work is evolving, requiring individuals to collaborate not only with other humans but also with AI technologies to accomplish complex goals. This requires a shift in perspective from technology-driven questions to a human-centered research and design agenda putting people and evolving teams in the center of attention. A socio-technical approach is needed to view AI as more than just a technological tool, but as a team member, leading to the emergence of human-AI teaming (HAIT). In this new form of work, humans and AI synergistically combine their respective capabilities to accomplish shared goals. Methods: The aim of our work is to uncover current research streams on HAIT and derive a unified understanding of the construct through a bibliometric network analysis, a scoping review and synthetization of a definition from a socio-technical point of view. In addition, antecedents and outcomes examined in the literature are extracted to guide future research in this field. Results: Through network analysis, five clusters with different research focuses on HAIT were identified. These clusters revolve around (1) human and (2) task-dependent variables, (3) AI explainability, (4) AI-driven robotic systems, and (5) the effects of AI performance on human perception. Despite these diverse research focuses, the current body of literature is predominantly driven by a technology-centric and engineering perspective, with no consistent definition or terminology of HAIT emerging to date. Discussion: We propose a unifying definition combining a human-centered and team-oriented perspective as well as summarize what is still needed in future research regarding HAIT. Thus, this work contributes to support the idea of the Frontiers Research Topic of a theoretical and conceptual basis for human work with AI systems. Copyright © 2023 Berretta, Tausch, Ontrup, Gilles, Peifer and Kluge.",artificial intelligence; bibliometric analysis; bibliometric coupling; human-AI teaming; human-centered AI; humane work; network analysis; work psychology,,Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85174188953
Krinkin K.; Shichkina Y.; Ignatyev A.,"Krinkin, Kirill (35868014600); Shichkina, Yulia (57144627300); Ignatyev, Andrey (57930333500)",35868014600; 57144627300; 57930333500,Co-evolutionary hybrid intelligence is a key concept for the world intellectualization,2023,Kybernetes,52,9,,2907,2923,16,9,10.1108/K-03-2022-0472,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139970979&doi=10.1108%2fK-03-2022-0472&partnerID=40&md5=6e177ab56f4f344fb19961df94488f91,"Purpose: This study aims to show the inconsistency of the approach to the development of artificial intelligence as an independent tool (just one more tool that humans have developed); to describe the logic and concept of intelligence development regardless of its substrate: a human or a machine and to prove that the co-evolutionary hybridization of the machine and human intelligence will make it possible to reach a solution for the problems inaccessible to humanity so far (global climate monitoring and control, pandemics, etc.). Design/methodology/approach: The global trend for artificial intelligence development (has been) was set during the Dartmouth seminar in 1956. The main goal was to define characteristics and research directions for artificial intelligence comparable to or even outperforming human intelligence. It should be able to acquire and create new knowledge in a highly uncertain dynamic environment (the real-world environment is an example) and apply that knowledge to solving practical problems. Nowadays artificial intelligence overperforms human abilities (playing games, speech recognition, search, art generation, extracting patterns from data etc.), but all these examples show that developers have come to a dead end. Narrow artificial intelligence has no connection to real human intelligence and even cannot be successfully used in many cases due to lack of transparency, explainability, computational ineffectiveness and many other limits. A strong artificial intelligence development model can be discussed unrelated to the substrate development of intelligence and its general properties that are inherent in this development. Only then it is to be clarified which part of cognitive functions can be transferred to an artificial medium. The process of development of intelligence (as mutual development (co-development) of human and artificial intelligence) should correspond to the property of increasing cognitive interoperability. The degree of cognitive interoperability is arranged in the same way as the method of measuring the strength of intelligence. It is stronger if knowledge can be transferred between different domains on a higher level of abstraction (Chollet, 2018). Findings: The key factors behind the development of hybrid intelligence are interoperability – the ability to create a common ontology in the context of the problem being solved, plan and carry out joint activities; co-evolution – ensuring the growth of aggregate intellectual ability without the loss of subjectness by each of the substrates (human, machine). The rate of co-evolution depends on the rate of knowledge interchange and the manufacturability of this process. Research limitations/implications: Resistance to the idea of developing co-evolutionary hybrid intelligence can be expected from agents and developers who have bet on and invested in data-driven artificial intelligence and machine learning. Practical implications: Revision of the approach to intellectualization through the development of hybrid intelligence methods will help bridge the gap between the developers of specific solutions and those who apply them. Co-evolution of machine intelligence and human intelligence will ensure seamless integration of smart new solutions into the global division of labor and social institutions. Originality/value: The novelty of the research is connected with a new look at the principles of the development of machine and human intelligence in the co-evolution style. Also new is the statement that the development of intelligence should take place within the framework of integration of the following four domains: global challenges and tasks, concepts (general hybrid intelligence), technologies and products (specific applications that satisfy the needs of the market). © 2022, Emerald Publishing Limited.",AI ethics; Artificial intelligence; Co-evolution; Cognitive functions; Division of labor; Human–machine hybridization; Hybrid intelligence,Artificial intelligence; Brain; Cognitive systems; Computation theory; Interoperability; Speech recognition; AI ethic; Co-evolution; Co-evolutionary; Cognitive functions; Division of labor; Human intelligence; Human-machine; Human–machine hybridization; Hybrid intelligence; Hybridisation; Substrates,Article,Final,,Scopus,2-s2.0-85139970979
Turner C.J.; Garn W.,"Turner, Chris J (23490513700); Garn, Wolfgang (36138706800)",23490513700; 36138706800,Next generation DES simulation: A research agenda for human centric manufacturing systems,2022,Journal of Industrial Information Integration,28,,100354,,,,49,10.1016/j.jii.2022.100354,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134201179&doi=10.1016%2fj.jii.2022.100354&partnerID=40&md5=6e8eab20ae2d49b51bfac2bb3a9ee9a1,"In this paper we introduce a research agenda to guide the development of the next generation of Discrete Event Simulation (DES) systems. Interfaces to digital twins are projected to go beyond physical representations to become blueprints for the actual “objects” and an active dashboard for their control. The role and importance of real-time interactive animations presented in an Extended Reality (XR) format will be explored. The need for using game engines, particularly their physics engines and AI within interactive simulated Extended Reality is expanded on. Importing and scanning real-world environments is assumed to become more efficient when using AR. Exporting to VR and AR is recommended to be a default feature. A technology framework for the next generation simulators is presented along with a proposed set of implementation guidelines. The need for more human centric technology approaches, nascent in Industry 4.0, are now central to the emerging Industry 5.0 paradigm; an agenda that is discussed in this research as part of a human in the loop future, supported by DES. The potential role of Explainable Artificial Intelligence is also explored along with an audit trail approach to provide a justification of complex and automated decision-making systems with relation to DES. A technology framework is proposed, which brings the above together and can serve as a guide for the next generation of holistic simulators for manufacturing. © 2022",Agent based simulation; Discrete event simulation (DES); Explainable artificial intelligence (XAI); Extended reality (XR); Human centric manufacturing; Human in the loop; Industry 4.0; Industry 5.0,Animation; Artificial intelligence; Decision making; Discrete event simulation; Industrial research; Agent based simulation; Discrete event simulation; Discrete-event simulations; Explainable artificial intelligence (XAI); Extended reality (XR); Human centric manufacturing; Human-centric; Human-in-the-loop; Industry 5.0; Research agenda; Industry 4.0,Review,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85134201179
Cabour G.; Morales-Forero A.; Ledoux É.; Bassetto S.,"Cabour, Garrick (57216995923); Morales-Forero, Andrés (57188976801); Ledoux, Élise (22135537900); Bassetto, Samuel (23974307300)",57216995923; 57188976801; 22135537900; 23974307300,An explanation space to align user studies with the technical development of Explainable AI,2023,AI and Society,38,2,,869,887,18,9,10.1007/s00146-022-01536-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134830553&doi=10.1007%2fs00146-022-01536-6&partnerID=40&md5=4fd352ddc41443ef17eccf3f893f65aa,"Providing meaningful and actionable explanations for end-users is a situated problem requiring the intersection of multiple disciplines to address social, operational, and technical challenges. However, the explainable artificial intelligence community has not commonly adopted or created tangible design tools that allow interdisciplinary work to develop reliable AI-powered solutions. This paper proposes a formative architecture that defines the explanation space from a user-inspired perspective. The architecture comprises five intertwined components to outline explanation requirements for a task: (1) the end-users’ mental models, (2) the end-users’ cognitive process, (3) the user interface, (4) the Human-Explainer Agent, and (5) the agent process. We first define each component of the architecture. Then, we present the Abstracted Explanation Space, a modeling tool that aggregates the architecture’s components to support designers in systematically aligning explanations with end-users’ work practices, needs, and goals. It guides the specifications of what needs to be explained (content: end-users’ mental model), why this explanation is necessary (context: end-users’ cognitive process), to delimit how to explain it (format: Human-Explainer Agent and user interface), and when the explanations should be given. We then exemplify the tool’s use in an ongoing case study in the aircraft maintenance domain. Finally, we discuss possible contributions of the tool, known limitations or areas for improvement, and future work to be done. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Explainable AI; Human–AI teaming; Interdisciplinary study; User-centered design,Architecture; Cognitive systems; Software design; User interfaces; Cognitive process; End-users; Explainable AI; Human–AI teaming; Inter-disciplinary studies; Mental model; Multiple disciplines; Social challenges; Technical development; User study; User centered design,Article,Final,,Scopus,2-s2.0-85134830553
Wilchek M.; Hanley W.; Lim J.; Luther K.; Batarseh F.A.,"Wilchek, Matthew (57442330100); Hanley, Will (58321315700); Lim, Jude (58244909600); Luther, Kurt (23051459200); Batarseh, Feras A. (55579745600)",57442330100; 58321315700; 58244909600; 23051459200; 55579745600,Human-in-the-loop for computer vision assurance: A survey,2023,Engineering Applications of Artificial Intelligence,123,,106376,,,,10,10.1016/j.engappai.2023.106376,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159075824&doi=10.1016%2fj.engappai.2023.106376&partnerID=40&md5=eacdedfc2fced6869d3efeece7ecb080,"Human-in-the-loop (HITL), a key branch of Human–Computer Interaction (HCI), is increasingly proposed in the research literature as a key assurance method for automated analyses and predictive application designs. As the need increases to improve methods in Artificial Intelligence (AI) model training, optimize systems performance, provide AI explainability, and monitor AI system operations, the concept of HITL is gaining traction due to its value in solving these challenges. This survey of existing works on HITL from a computer vision system design perspective focuses on the following AI assurance principles: (1) improved data assurance, such as data preparation or automated data labeling; (2) algorithmic assurance, such as managing uncertainty and AI trustworthiness; and (3) critical limitations and capabilities introduced by HITL into a system's operational efficiency. We survey prior work within these foci, including technical strengths and weaknesses of novel approaches and ongoing research. This review of the state of the art in HITL computer vision research supports an informed discussion of considerations and future opportunities in this critical space. © 2023 Elsevier Ltd",AI assurance; Computer vision; Human-in-the-loop; Human–computer interaction; Learning systems; Object detection,Computer vision; Human computer interaction; Learning systems; Application design; Artificial intelligence assurance; Artificial intelligence systems; Automated analysis; Human-in-the-loop; Intelligence models; Model training; Objects detection; Systems operation; Systems performance; Object detection,Short survey,Final,,Scopus,2-s2.0-85159075824
Patil M.S.; Främling K.,"Patil, Minal Suresh (57944557900); Främling, Kary (6506103412)",57944557900; 6506103412,Do Intermediate Feature Coalitions Aid Explainability of Black-Box Models?,2023,Communications in Computer and Information Science,1901 CCIS,,,115,130,15,0,10.1007/978-3-031-44064-9_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176954534&doi=10.1007%2f978-3-031-44064-9_7&partnerID=40&md5=ccc61257ca776322622814d0f2589bde,"This work introduces the notion of intermediate concepts based on levels structure to aid explainability for black-box models. The levels structure is a hierarchical structure in which each level corresponds to features of a dataset (i.e., a player-set partition). The level of coarseness increases from the trivial set, which only comprises singletons, to the set, which only contains the grand coalition. In addition, it is possible to establish meronomies, i.e., part-whole relationships, via a domain expert that can be utilised to generate explanations at an abstract level. We illustrate the usability of this approach in a real-world car model example and the Titanic dataset, where intermediate concepts aid in explainability at different levels of abstraction. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Coalition Formation; Explainability; Trust in Human-Agent Systems,Agent systems; Black box modelling; Coalition formations; Concept-based; Explainability; Hierarchical structures; Human agent; Intermediate concept; Level structure; Trust in human-agent system; Model automobiles,Conference paper,Final,,Scopus,2-s2.0-85176954534
Sikdar S.; Bhattacharya P.,"Sikdar, Sandipan (57220520249); Bhattacharya, Parantapa (55954244800)",57220520249; 55954244800,Interpretability of Deep Neural Models,2023,Studies in Computational Intelligence,1123,,,131,143,12,0,10.1007/978-981-99-7184-8_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182477687&doi=10.1007%2f978-981-99-7184-8_8&partnerID=40&md5=ce4f8759ea81e1b14dc0b68497bbd14e,"The rise of deep neural networks in machine learning has been remarkable, leading to their deployment in algorithmic decision-making. However, this has raised questions about the explainability and interpretability of these models, given their growing importance in society. To address this, the field of interpretability in machine learning has been developed, with the goal of creating frameworks that can explain the decisions of a machine learning model in a way that is comprehensible to humans. This could be essential in building trust in the system, as well as debugging models for potential errors and meeting legal requirements (e.g., GDPR). Even though the success of deep neural network is attributed to its ability to capture higher level feature interactions, most of existing frameworks still focus on highlighting important individual features (e.g., words in text or pixels in images). Hence, to further improve interpretability, we propose to quantify the importance of feature interactions in addition to individual features. In this work, we introduce integrated directional gradients (IDG), a game-theory inspired method for assigning importance scores to higher level feature interactions. Our experiments with DNN-based text classifiers on the task of sentiment classification demonstrate that IDG is able to effectively capture the importance of feature interactions. © The Institution of Engineers (India) 2023.",,,Book chapter,Final,,Scopus,2-s2.0-85182477687
Veale M.; Silberman M.‘.; Binns R.,"Veale, Michael (57073209400); Silberman, Michael ‘Six’ (59819021000); Binns, Reuben (57189002981)",57073209400; 59819021000; 57189002981,Fortifying the algorithmic management provisions in the proposed Platform Work Directive,2023,European Labour Law Journal,14,2,,308,332,24,6,10.1177/20319525231167983,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175214916&doi=10.1177%2f20319525231167983&partnerID=40&md5=6021eea71f98f4235fbcb05f3c226ff2,"The European Commission proposed a Directive on Platform Work at the end of 2021. While much attention has been placed on its effort to address misclassification of the employed as self-employed, it also contains ambitious provisions for the regulation of the algorithmic management prevalent on these platforms. Overall, these provisions are well-drafted, yet they require extra scrutiny in light of the fierce lobbying and resistance they will likely encounter in the legislative process, in implementation and in enforcement. In this article, we place the proposal in its sociotechnical context, drawing upon wide cross-disciplinary scholarship to identify a range of tensions, potential misinterpretations, and perversions that should be pre-empted and guarded against at the earliest possible stage. These include improvements to ex ante and ex post algorithmic transparency; identifying and strengthening the standard against which human reviewers of algorithmic decisions review; anticipating challenges of representation and organising in complex platform contexts; creating realistic ambitions for digital worker communication channels; and accountably monitoring and evaluating impacts on workers while limiting data collection. We encourage legislators and regulators at both European and national levels to act to fortify these provisions in the negotiation of the Directive, its potential transposition, and in its enforcement. © The Author(s) 2023.",algorithmic decisions; algorithmic explanations; algorithmic management; platform work; workplace surveillance,,Note,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85175214916
Natali C.; Famiglini L.; Campagner A.; La Maida G.A.; Gallazzi E.; Cabitza F.,"Natali, Chiara (57916058900); Famiglini, Lorenzo (57222408968); Campagner, Andrea (57195064969); La Maida, Giovanni Andrea (8762866400); Gallazzi, Enrico (56878946300); Cabitza, Federico (16199544700)",57916058900; 57222408968; 57195064969; 8762866400; 56878946300; 16199544700,Color Shadows 2: Assessing the Impact of XAI on Diagnostic Decision-Making,2023,Communications in Computer and Information Science,1901 CCIS,,,618,629,11,4,10.1007/978-3-031-44064-9_33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176926415&doi=10.1007%2f978-3-031-44064-9_33&partnerID=40&md5=98420ae9dd7a9054f6fe45351a164f80,"A comprehensive assessment of the impact of eXplainable AI (XAI) on diagnostic decision-making should adopt a socio-technical perspective. Our study focuses on Decision Support Systems (DSS) that provide explanations in the form of Activation Maps, assessing their impact in terms of automation bias and algorithmic aversion. Specifically, we focus on the XAI-assisted task of detecting thoraco-lumbar fractures from X-rays by radiologists, taking into account the complexity of the cases and the experience level of users. Our results show how XAI support has a clear and positive impact on diagnostic performance. By introducing the concepts of technology impact, reliance patterns, and the white box paradox, we highlight the importance of designing Human-AI Collaboration Protocols (HAI-CP) that are specific to the task at hand to optimize the integration of XAI into diagnostic decision-making. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Decision Support Systems (DSS); eXplainable AI (XAI); Human-AI Collaboration Protocol (HAI-CP),Artificial intelligence; Decision making; Activation maps; Algorithmics; Automation bias; Collaboration protocols; Comprehensive assessment; Decision support system; Diagnostic decision makings; Explainable AI (XAI); Human-AI collaboration protocol; Socio-technical perspective; Decision support systems,Conference paper,Final,,Scopus,2-s2.0-85176926415
Liang P.P.; Lyu Y.; Chhablani G.; Jain N.; Deng Z.; Wang X.; Morency L.-P.; Salakhutdinov R.,"Liang, Paul Pu (57203051940); Lyu, Yiwei (57211207258); Chhablani, Gunjan (57215680967); Jain, Nihal (57295667200); Deng, Zihao (57350186500); Wang, Xingbo (57211999463); Morency, Louis-Philippe (6603047400); Salakhutdinov, Ruslan (57203057355)",57203051940; 57211207258; 57215680967; 57295667200; 57350186500; 57211999463; 6603047400; 57203057355,MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models,2023,Conference on Human Factors in Computing Systems - Proceedings,,,214,,,,2,10.1145/3544549.3585604,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158076483&doi=10.1145%2f3544549.3585604&partnerID=40&md5=55392ca22e88f5084b899e0779c0b185,"The nature of human and computer interactions are inherently multimodal, which has led to substantial interest in building interpretable, interactive, and reliable multimodal interfaces. However, modern multimodal models and interfaces are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize their internal workings in order to empower stakeholders to visualize model behavior, perform model debugging, and promote trust in these models? Our paper proposes MultiViz, a method for analyzing the behavior of multimodal models via 4 stages: (1) unimodal importance, (2) cross-modal interactions, (3) multimodal representations and (4) multimodal prediction. MultiViz includes modular visualization tools for each stage before combining outputs from all stages through an interactive and human-in-the-loop API. Through user studies with 21 participants on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available at https://github.com/pliang279/MultiViz, will be regularly updated with new visualization tools and metrics, and welcomes input from the community1. © 2023 Owner/Author.",explainable AI; human-in-the-loop; interpretability; model analysis and debugging; multimodal machine learning; visualization,Error analysis; Learning systems; Machine learning; Neural networks; Program debugging; User interfaces; Explainable AI; Human-in-the-loop; Interpretability; Machine-learning; Model analyse and debugging; Modeling analyzes; Multi-modal; Multi-modal interfaces; Multimodal machine learning; Multimodal models; Visualization,Conference paper,Final,,Scopus,2-s2.0-85158076483
"Ehsan U.; Wintersberger P.; Watkins E.A.; Manger C.; Ramos G.; Weisz J.D.; Daumé H., II; Riener A.; Riedl M.O.","Ehsan, Upol (57195223484); Wintersberger, Philipp (55485458100); Watkins, Elizabeth A. (57193833255); Manger, Carina (57223025938); Ramos, Gonzalo (55993680200); Weisz, Justin D. (14036587800); Daumé, Hal (57210198346); Riener, Andreas (23012938100); Riedl, Mark O. (7004421643)",57195223484; 55485458100; 57193833255; 57223025938; 55993680200; 14036587800; 57210198346; 23012938100; 7004421643,Human-Centered Explainable AI (HCXAI): Coming of Age,2023,Conference on Human Factors in Computing Systems - Proceedings,,,353,,,,14,10.1145/3544549.3573832,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158123516&doi=10.1145%2f3544549.3573832&partnerID=40&md5=f45781f736c33aca8cdccddc3e641b64,"Explainability is an essential pillar of Responsible AI that calls for equitable and ethical Human-AI interaction. Explanations are essential to hold AI systems and their producers accountable, and can serve as a means to ensure humans' right to understand and contest AI decisions. Human-centered XAI (HCXAI) argues that there is more to making AI explainable than algorithmic transparency. Explainability of AI is more than just ""opening""the black box - who opens it matters just as much, if not more, as the ways of opening it. In this third CHI workshop on Human-centered XAI (HCXAI), we build on the maturation through the first two installments to craft the coming-of-age story of HCXAI, which embodies a deeper discourse around operationalizing human-centered perspectives in XAI. We aim towards actionable interventions that recognize both affordances and potential pitfalls of XAI. The goal of the third installment is to go beyond the black box and examine how human-centered perspectives in XAI can be operationalized at the conceptual, methodological, and technical levels. Encouraging holistic (historical, sociological, and technical) approaches, we emphasize ""operationalizing.""Within our research agenda for XAI, we seek actionable analysis frameworks, concrete design guidelines, transferable evaluation methods, and principles for accountability. © 2023 Owner/Author.",,Affordances; AI systems; Algorithmics; Analysis frameworks; Black boxes; Conceptual levels; Concrete design; Human rights; Research agenda; Technical levels,Conference paper,Final,,Scopus,2-s2.0-85158123516
Estivill-Castro V.; Gilmore E.; Hexel R.,"Estivill-Castro, Vladimir (55915978700); Gilmore, Eugene (57220040308); Hexel, René (23396845500)",55915978700; 57220040308; 23396845500,Constructing Explainable Classifiers from the Start—Enabling Human-in-the Loop Machine Learning,2022,Information (Switzerland),13,10,464,,,,4,10.3390/info13100464,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140467816&doi=10.3390%2finfo13100464&partnerID=40&md5=22c05ee2e98766db109295f11c54f70a,"Interactive machine learning (IML) enables the incorporation of human expertise because the human participates in the construction of the learned model. Moreover, with human-in-the-loop machine learning (HITL-ML), the human experts drive the learning, and they can steer the learning objective not only for accuracy but perhaps for characterisation and discrimination rules, where separating one class from others is the primary objective. Moreover, this interaction enables humans to explore and gain insights into the dataset as well as validate the learned models. Validation requires transparency and interpretable classifiers. The huge relevance of understandable classification has been recently emphasised for many applications under the banner of explainable artificial intelligence (XAI). We use parallel coordinates to deploy an IML system that enables the visualisation of decision tree classifiers but also the generation of interpretable splits beyond parallel axis splits. Moreover, we show that characterisation and discrimination rules are also well communicated using parallel coordinates. In particular, we report results from the largest usability study of a IML system, confirming the merits of our approach. © 2022 by the authors.",decision tree classifiers; interactive machine learning; parallel coordinates; transparent-by-design,Decision trees; Decision tree classifiers; Human expert; Human expertise; Human-in-the-loop; Interactive machine learning; Learning objectives; Machine learning systems; Machine-learning; Parallel coordinates; Transparent-by-design; Machine learning,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85140467816
Friedrich M.; Richards D.; Huttner J.-P.,"Friedrich, Max (55957065200); Richards, Dale (56872394100); Huttner, Jan-Paul (59432255000)",55957065200; 56872394100; 59432255000,Evaluation of Icons to Support Safety Risk Monitoring of Autonomous Small Unmanned Aircraft Systems,2024,"2024 IEEE 4th International Conference on Human-Machine Systems, ICHMS 2024",,,,,,,0,10.1109/ICHMS59971.2024.10555752,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197435232&doi=10.1109%2fICHMS59971.2024.10555752&partnerID=40&md5=6fd49eeee7a42ff06d5dc132bc073410,"The integration of small Unmanned Aircraft Systems (sUAS) into low-altitude urban airspace is gaining momentum. Safety challenges arise, necessitating automated (and in some cases autonomous) technical solutions. In order for the operator to monitor and engage with the sUAS an effective human-machine interface (HMI) is a vital component. Equally, the manner in how information is presented on this HMI requires careful consideration - more so when the operator may very well have more than one aerial platform under their control and/or supervision. This study explores the role of icon design for autonomous sUAS supervisory control to contribute to the growing body of research on explainable artificial intelligence. Altogether, 14 icons are proposed as representations of safety-critical functions related to autonomous sUAS operation in low-altitude urban airspace. In an online questionnaire study, 46 participants with experience in operating sUAS rated the icons on established icon-function fit metrics. The analysis of agreement scores indicates that the icons related to battery health, geofence conformance, and meteorological constraints were well-recognized, while those representing casualty risk, positional accuracy, airspace conformance, and sensor health performed poorly. These findings emphasize the importance of concreteness, familiarity, and semantic distance in icon design, where higher values positively influence icon recognition and thus icon-function fit. The integration of empirically derived icon design principles is proposed to enhance transparency in safety-critical autonomous systems. This study underscores the significance of targeted usage of unambiguous icons to facilitate deeper user understanding through making system-side decision-making processes transparent to the user, enabling more effective interaction between humans and autonomous systems.  © 2024 IEEE.",autonomous systems; human machine interface; icon design; unmanned aircraft systems,Decision making; Health risks; Integration; Man machine systems; Risk assessment; Safety engineering; Semantics; Unmanned aerial vehicles (UAV); Autonomous system; Gaining momentum; Human Machine Interface; Icon designs; Low altitudes; Risk monitoring; Safety risks; Small unmanned aircrafts; Technical solutions; Unmanned aircraft system; Antennas,Conference paper,Final,,Scopus,2-s2.0-85197435232
Prabhudesai S.; Yang L.; Asthana S.; Huan X.; Vera Liao Q.; Banovic N.,"Prabhudesai, Snehal (57302158500); Yang, Leyao (58177387700); Asthana, Sumit (57209061010); Huan, Xun (55362263900); Vera Liao, Q. (36095944800); Banovic, Nikola (54794750000)",57302158500; 58177387700; 57209061010; 55362263900; 36095944800; 54794750000,Understanding Uncertainty: How Lay Decision-makers Perceive and Interpret Uncertainty in Human-AI Decision Making,2023,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,379,396,17,35,10.1145/3581641.3584033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152132042&doi=10.1145%2f3581641.3584033&partnerID=40&md5=9574365dc41056e90595baab33e766f9,"Decision Support Systems (DSS) based on Machine Learning (ML) often aim to assist lay decision-makers, who are not math-savvy, in making high-stakes decisions. However, existing ML-based DSS are not always transparent about the probabilistic nature of ML predictions and how uncertain each prediction is. This lack of transparency could give lay decision-makers a false sense of reliability. Growing calls for AI transparency have led to increasing efforts to quantify and communicate model uncertainty. However, there are still gaps in knowledge regarding how and why the decision-makers utilize ML uncertainty information in their decision process. Here, we conducted a qualitative, think-aloud user study with 17 lay decision-makers who interacted with three different DSS: 1) interactive visualization, 2) DSS based on an ML model that provides predictions without uncertainty information, and 3) the same DSS with uncertainty information. Our qualitative analysis found that communicating uncertainty about ML predictions forced participants to slow down and think analytically about their decisions. This in turn made participants more vigilant, resulting in reduction in over-reliance on ML-based DSS. Our work contributes empirical knowledge on how lay decision-makers perceive, interpret, and make use of uncertainty information when interacting with DSS. Such foundational knowledge informs the design of future ML-based DSS that embrace transparent uncertainty communication.  © 2023 ACM.",Decision-making; Machine Learning; Uncertainty,Decision support systems; Forecasting; Machine learning; Transparency; Uncertainty analysis; Visualization; Decision makers; Decision process; Decisions makings; Machine-learning; Modeling uncertainties; On-machines; Probabilistics; Think aloud; Uncertainty; Uncertainty informations; Decision making,Conference paper,Final,,Scopus,2-s2.0-85152132042
Nauta M.; Hegeman J.H.; Geerdink J.; Schlötterer J.; Keulen M.; Seifert C.,"Nauta, Meike (57194974711); Hegeman, Johannes H. (33367775400); Geerdink, Jeroen (57200256008); Schlötterer, Jörg (56568331100); Keulen, Maurice van (6603274087); Seifert, Christin (8850014900)",57194974711; 33367775400; 57200256008; 56568331100; 6603274087; 8850014900,Interpreting and Correcting Medical Image Classification with PIP-Net,2024,Communications in Computer and Information Science,1947,,,198,215,17,9,10.1007/978-3-031-50396-2_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184120156&doi=10.1007%2f978-3-031-50396-2_11&partnerID=40&md5=79fd00affdfcec892203436d6ac5a122,"Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net’s decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net’s unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Explainable AI; hybrid intelligence; interpretable machine learning; medical imaging; prototypes,Computer aided diagnosis; Decision making; Image classification; Medical applications; Medical imaging; Black boxes; Explainable AI; Hybrid intelligence; Image Classifiers; Interpretability; Interpretable machine learning; Machine-learning; Medical image classification; Prototype; Prototype models; Machine learning,Conference paper,Final,,Scopus,2-s2.0-85184120156
Plass M.; Kargl M.; Evans T.; Brcic L.; Regitnig P.; Geißler C.; Carvalho R.; Jansen C.; Zerbe N.; Holzinger A.; Müller H.,"Plass, Markus (57190968111); Kargl, Michaela (57214681139); Evans, Theodore (57552757100); Brcic, Luka (7801511896); Regitnig, Peter (55894295100); Geißler, Christian (57219343065); Carvalho, Rita (57353992700); Jansen, Christoph (56286202100); Zerbe, Norman (37058017200); Holzinger, Andreas (23396282000); Müller, Heimo (7404944998)",57190968111; 57214681139; 57552757100; 7801511896; 55894295100; 57219343065; 57353992700; 56286202100; 37058017200; 23396282000; 7404944998,Human-AI Interfaces are a Central Component of Trustworthy AI,2023,Intelligent Systems Reference Library,232,,,225,256,31,4,10.1007/978-3-031-12807-3_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140745775&doi=10.1007%2f978-3-031-12807-3_11&partnerID=40&md5=f3f15e3bd1c9c24e20420cd2658abb37,"This chapter demonstrates the crucial role that human-AI interfaces play in conveying the trustworthiness of AI solutions to their users. Explainability is a central component of such interfaces, particularly in high-stake domains where human oversight is essential: justice, finance, security, and medicine. To successfully build and communicate trustworthiness, a user-centered approach to the design and development of AI solutions and their human interfaces is essential. In this chapter, we explain how proven methods for stakeholder analysis and user testing from human-computer interaction (HCI) research can be adapted to human-AI interaction (HAII) in support of this goal. The practical implementation of a user-centric approach is described within the context of AI applications in computational pathology. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,,Book chapter,Final,,Scopus,2-s2.0-85140745775
Anderegg A.H.A.; Balakrishnan K.; Mulcare S.P.,"Anderegg, Alfred H. Andy (26648068800); Balakrishnan, Krishnan (56767656100); Mulcare, Shweta P. (57411381600)",26648068800; 56767656100; 57411381600,Predicting New Risks: Crew Resource Management in a Human-Machine Team,2022,AIAA AVIATION 2022 Forum,,,AIAA 2022-3833,,,,0,10.2514/6.2022-3833,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135087011&doi=10.2514%2f6.2022-3833&partnerID=40&md5=4ae26ed5013642eda786ef7cd4c1deb1,"Increased operational complexity often adds new hazards with procedural controls that increase the crew task loads. The duties shared among crew members must constantly be coordinated and rebalanced. Crew resource management (CRM) reduces the likelihood of errors by building and maintaining flight crew cohesion. CRM clarifies the authorities, communications process, decision making, and coordination to manage distractions and uncertainties that might impede sound aeronautical decision making. Simplified vehicle operations are redefining the nature of a flight crew, bringing humans and machines into peer relationships with complementary cognitive roles. As operations continue to evolve toward remotely piloted and uncrewed vehicles, more of the functions will move to automated crew members. In prior work we developed cognitive decision models to reflect different peer and supervisory roles in decision-making. This paper examines the information sharing, shared accountability, and other CRM principles as they would apply to a crew comprised of people and machines in various roles. Automated crew members are allocated roles that can be renegotiated between the parties with a human pilot ultimately having supervisory controls. In CRM, control procedures assume the crew can communicate in real-time to create situational awareness, shared understanding and reasoning for informed decisions and actions. In developing the application of CRM to a human-machine team, this paper extends the artificial intelligence concept of explainability to a bidirectional exchange between human and automated crew members. The paper shows how interfaces must enable crew member engagement to verify situations and perceptions, confirm mental models, and develop a coordinated course of action. © 2022, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.",AI; AI Explainability; Autonomous Systems; Crew Resource Management; Distributed Decision Making; Human-Machine Teaming; Predictive analytics; Run-time Assurance,Automation; Aviation; Behavioral research; Human resource management; Natural resources management; Predictive analytics; Resource allocation; AI explainability; Autonomous system; Crew members; Crew resource managements; Distributed decision making; Distributed-decision makings; Human-machine; Human-machine teaming; Run-time assurance; Runtimes; Decision making,Conference paper,Final,,Scopus,2-s2.0-85135087011
Panigutti C.; Perotti A.; Panisson A.; Bajardi P.; Pedreschi D.,"Panigutti, Cecilia (57194345147); Perotti, Alan (58321306400); Panisson, André (9133831400); Bajardi, Paolo (35114469000); Pedreschi, Dino (6603935985)",57194345147; 58321306400; 9133831400; 35114469000; 6603935985,FairLens: Auditing black-box clinical decision support systems,2021,Information Processing and Management,58,5,102657,,,,55,10.1016/j.ipm.2021.102657,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108332873&doi=10.1016%2fj.ipm.2021.102657&partnerID=40&md5=5baef6cbf23a8b6c54abb95fffcbc14d,"The pervasive application of algorithmic decision-making is raising concerns on the risk of unintended bias in AI systems deployed in critical settings such as healthcare. The detection and mitigation of model bias is a very delicate task that should be tackled with care and involving domain experts in the loop. In this paper we introduce FairLens, a methodology for discovering and explaining biases. We show how this tool can audit a fictional commercial black-box model acting as a clinical decision support system (DSS). In this scenario, the healthcare facility experts can use FairLens on their historical data to discover the biases of the model before incorporating it into the clinical decision flow. FairLens first stratifies the available patient data according to demographic attributes such as age, ethnicity, gender and healthcare insurance; it then assesses the model performance on such groups highlighting the most common misclassifications. Finally, FairLens allows the expert to examine one misclassification of interest by explaining which elements of the affected patients’ clinical history drive the model error in the problematic group. We validate FairLens’ ability to highlight bias in multilabel clinical DSSs introducing a multilabel-appropriate metric of disparity and proving its efficacy against other standard metrics. © 2021",Clinical decision support systems; eXplainable artificial intelligence; Fairness and bias in machine learning systems,Decision making; Digital storage; Health care; Hospital data processing; Machine learning; Algorithmics; Black boxes; Clinical decision support systems; Decisions makings; Explainable artificial intelligence; Fairness and bias in machine learning system; Misclassifications; Multi-labels; Multilabel; Pervasive applications; Decision support systems,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85108332873
He Y.; Gu B.; Li C.; Yan L.,"He, Yuanqi (59007486100); Gu, Bin (7201864428); Li, Chunxiao (58086476700); Yan, Lingling (59124545200)",59007486100; 7201864428; 58086476700; 59124545200,Black-box Models’ Explainability: A Theoretical and Practical Perspective,2023,"29th Annual Americas Conference on Information Systems, AMCIS 2023",,,,,,,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192879269&partnerID=40&md5=f13ae0e3a9e97b6db517fa7aa2eabe0f,"The lack of explainability remains a critical challenge to the widespread adoption of artificial intelligence (AI) in many fields. “Understanding brings in trust”, while machine-learning models offer superior prediction accuracy, understanding the underlying logic is equally important to foster trust in these models. In this paper, we present eXplainable AI (XAI) as a solution to this challenge. Our research focuses on three key aspects of XAI: mathematics, humanities and social sciences, and practical applications. We demonstrated the feasibility of XAI through the use of artificially-constructed and model-derived ground truth, and verified performances of different XAIs. We also explored three dimensions of explainable consistency and emphasized the significance of human-machine consistency. Finally, we applied our research to a real-world scenario by cooperating with a national bank in China. Our findings highlight that XAI is both mathematically and practically meaningful, but more efforts need to be dedicated to this human-machine communication field. © 2023 29th Annual Americas Conference on Information Systems, AMCIS 2023. All rights reserved.",explainability-accuracy frontiers; eXplainable Artificial Intelligence; human-machine consistency,Artificial intelligence; Information use; Black box modelling; Critical challenges; Explainability-accuracy frontier; Explainable artificial intelligence; Human-machine; Human-machine consistency; Humanities and social science; Machine learning models; Prediction accuracy; Research focus; Information systems,Conference paper,Final,,Scopus,2-s2.0-85192879269
de Boer M.H.T.; Vethman S.; Bakker R.M.; Adhikari A.; Marcus M.; de Greeff J.; van der Waa J.; Schoonderwoerd T.A.J.; Tolios I.; van Zoelen E.M.; Hillerström F.; Kamphorst B.,"de Boer, Maaike H.T. (56021703700); Vethman, Steven (57223035728); Bakker, Roos M. (57611467200); Adhikari, Ajaya (57209912511); Marcus, Michiel (57201020815); de Greeff, Joachim (35193645900); van der Waa, Jasper (57193858656); Schoonderwoerd, Tjeerd A.J. (57210144415); Tolios, Ioannis (57219974059); van Zoelen, Emma M. (57211354974); Hillerström, Fieke (56453263100); Kamphorst, Bart (36782239600)",56021703700; 57223035728; 57611467200; 57209912511; 57201020815; 35193645900; 57193858656; 57210144415; 57219974059; 57211354974; 56453263100; 36782239600,"The FATE System Iterated: Fair, Transparent and Explainable Decision Making in a Juridical Case",2022,CEUR Workshop Proceedings,3121,,,,,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128741731&partnerID=40&md5=6f80ed6f95e368a246fd8487f49bd14a,"The goal of the FATE system is decision support with use of state-of-the-art human-AI co-learning, explainable AI and fair, secure and privacy-preserving usage of data. This AI-based support system is a general system, in which the modules can be tuned to specific use cases. The FATE system is designed to address different user roles, such as a researcher, domain expert/consultant and subject/patient, each with their own requirements. Having examined a Diabetes Type 2 use case before, in this paper we slightly iterate the FATE system and focus on a juridical use case. For a given new juridical case the relevant older court cases are suggested by the system. The relevant older cases can be explained using the eXplainable AI (XAI) module, and the system can be improved based on feedback about the relevant cases using the Co-learning module through interaction with a user. In the Bias module, the use of the system is investigated for potential bias by inspecting the properties of suggested cases. Secure Learning offers privacy-by-design alternatives for functionality found in the aforementioned modules. These results show how the generic FATE system can be implemented in a number of real-world use cases. In future work we plan to explore more use cases within this system. © 2022 Copyright for this paper by its authors",Bias; Co-Learning; Explainable AI; FAIR AI; Hybrid AI; Knowledge Engineering; Secure Learning,Decision support systems; Knowledge engineering; Learning systems; Bias; Co-learning; Decision supports; Decisions makings; Explainable AI; FAIR AI; Hybrid AI; Privacy preserving; Secure learning; State of the art; Decision making,Conference paper,Final,,Scopus,2-s2.0-85128741731
Sheu R.-K.; Pardeshi M.S.; Pai K.-C.; Chen L.-C.; Wu C.-L.; Chen W.-C.,"Sheu, Ruey-Kai (7003750859); Pardeshi, Mayuresh Sunil (57210279276); Pai, Kai-Chih (36987258300); Chen, Lun-Chi (57210279023); Wu, Chieh-Liang (7501672595); Chen, Wei-Cheng (57876812500)",7003750859; 57210279276; 36987258300; 57210279023; 7501672595; 57876812500,Interpretable Classification of Pneumonia Infection Using eXplainable AI (XAI-ICP),2023,IEEE Access,11,,,28896,28919,23,24,10.1109/ACCESS.2023.3255403,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149878571&doi=10.1109%2fACCESS.2023.3255403&partnerID=40&md5=11b5fba4d101e70dc49d11f9f4c4b151,"Open-box models in the medical domain have high acceptance and demand by many medical examiners. Even though the accuracy predicted by most of convolutional neural network (CNN) models is high, it is still not convincing as the detailed discussion regarding the outcome is semi-transparent in the functioning process. Pneumonia is known as one of the top contagious infections that makes most of the population affected due to low immunity. Therefore, the goal of this paper is to implement an interpretable classification of pneumonia infection using eXplainable AI (XAI-ICP). Thus, XAI-ICP is the highly efficient system designed to solve this challenge by adapting to the recent population health conditions. The aim is to design an interpretable deep classification and transfer learning based evaluation for pneumonia infection classification. The model is primarily pre-trained using the open Chest X-Ray (CXR) dataset from National Institutes of Health (NIH). Whereas, the training input and testing given to this system is Taichung Veterans General Hospital (TCVGH) for independent learning, Taiwan + VinDr open dataset for transfer learning of pneumonia affected patients with labeled CXR images possessing three features of infiltrate, cardiomegaly and effusion. The data labeling is performed by the medical examiners with the XAI human-in-the-loop approach. XAI-ICP demonstrates the XAI based reconfigurable DCNN with human-in-the-loop as a novel approach. The interpretable deep classification provides detailed transparency analysis and transfer learning for competitive accuracy. The purpose of this work, to design a re-configurable model that can continuously improve itself by using a feedback system and provide feasibility for the model deployment across multiple countries to provide an efficient system for the pneumonia infection classification. The designed model then provides detailed decisions taken at each step as transparency and features used within the algorithm for the pneumonia classification during the hospitalization. Thus, the scope can be given as explainable AI usage for the diagnosis classification using data preprocessing and interpretable deep convolutional neural network by the CXR evaluation. The accuracy achieved by using independent learning classification is 92.14% and is further improved based on successive transfer learning based evaluation is 93.29%. The XAI-ICP model adapts to the different populations by using transfer learning, while providing competitive results to the affected conditions. © 2013 IEEE.",Medical XAI; pneumonia infection; transfer learning; XAI pnuemonia,Classification (of information); Computer aided diagnosis; Deep neural networks; Hospitals; Medical computing; Medical imaging; Pulmonary diseases; Statistical tests; Transparency; Convolutional neural network; Deep classifications; Independent learning; Medical diagnostic imaging; Medical XAI; Pneumonia infection; Solid modelling; Transfer learning; XAI pnuemonia; Convolution,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85149878571
Gowtham Reddy A.,"Gowtham Reddy, Abbavaram (57225089809)",57225089809,Causality in Neural Networks - An Extended Abstract,2021,"AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,,271,272,1,1,10.1145/3461702.3462467,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112440996&doi=10.1145%2f3461702.3462467&partnerID=40&md5=58a202fcc178996266d4303788e64d67,"Causal reasoning is the main learning and explanation tool used by humans. AI systems should possess causal reasoning capabilities to be deployed in the real world with trust and reliability. Introducing the ideas of causality to machine learning helps in providing better learning and explainable models. Explainability, causal disentanglement are some important aspects of any machine learning model. Causal explanations are required to believe in a model's decision and causal disentanglement learning is important for transfer learning applications. We exploit the ideas of causality to be used in deep learning models to achieve better and causally explainable models that are useful in fairness, disentangled representation, etc. © 2021 Owner/Author.",causality; counterfactuals; disentanglement; explainability; machine learning; neural networks,Deep learning; Neural networks; Philosophical aspects; Transfer learning; AI systems; Causal explanations; Causal reasoning; Extended abstracts; Learning models; Machine learning models; Real-world; Learning systems,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85112440996
Chen X.; Tang T.; Ren J.; Lee I.; Chen H.; Xia F.,"Chen, Xiangtai (57220742205); Tang, Tao (58134528100); Ren, Jing (57211853303); Lee, Ivan (7404442312); Chen, Honglong (24174425300); Xia, Feng (35239267100)",57220742205; 58134528100; 57211853303; 7404442312; 24174425300; 35239267100,Heterogeneous Graph Learning for Explainable Recommendation over Academic Networks,2021,ACM International Conference Proceeding Series,,,,29,36,7,6,10.1145/3498851.3498926,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128549969&doi=10.1145%2f3498851.3498926&partnerID=40&md5=7c5ce130773150cab4ac9d4127bd9104,"With the explosive growth of new graduates with research degrees every year, unprecedented challenges arise for early-career researchers to find a job at a suitable institution. This study aims to understand the behavior of academic job transition and hence recommend suitable institutions for PhD graduates. Specifically, we design a deep learning model to predict the career move of early-career researchers and provide suggestions. The design is built on top of scholarly/academic networks, which contains abundant information about scientific collaboration among scholars and institutions. We construct a heterogeneous scholarly network to facilitate the exploring of the behavior of career moves and the recommendation of institutions for scholars. We devise an unsupervised learning model called HAI (Heterogeneous graph Attention InfoMax) which aggregates attention mechanism and mutual information for institution recommendation. Moreover, we propose scholar attention and meta-path attention to discover the hidden relationships between several meta-paths. With these mechanisms, HAI provides ordered recommendations with explainability. We evaluate HAI upon a real-world dataset against baseline methods. Experimental results verify the effectiveness and efficiency of our approach.  © 2021 ACM.",academic social networks; explainability; graph learning; heterogeneous networks; recommender systems,Deep learning; Academic jobs; Academic social network; Attention mechanisms; Explainability; Explosive growth; Graph learning; Heterogeneous graph; Infomax; Learning models; Scientific collaboration; Heterogeneous networks,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85128549969
Schloer N.; Boos S.; Harst F.; Lanquillon C.; Ohrnberger M.; Schoch F.; Stache N.C.; Wittenberg C.,"Schloer, Nicholas (57270422600); Boos, Sabine (58629458200); Harst, Felix (57282328300); Lanquillon, Carsten (24178557600); Ohrnberger, Morris (57221819114); Schoch, Fabian (58629251600); Stache, Nicolaj C. (15924610900); Wittenberg, Carsten (7006561833)",57270422600; 58629458200; 57282328300; 24178557600; 57221819114; 58629251600; 15924610900; 7006561833,Optimizing the Industrial Human-Machine Interface: Holographic Surfaces and Their Role in Anomaly Detection,2024,Communications in Computer and Information Science,2120 CCIS,,,430,435,5,1,10.1007/978-3-031-62110-9_47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196738230&doi=10.1007%2f978-3-031-62110-9_47&partnerID=40&md5=7ede322fe909bb16618cce3bef574cdb,"This abstract explores the integration of holographic surfaces into industrial human-machine interfaces (HMIs) to optimize anomaly detection processes. By utilizing this cutting-edge technology, industrial environments can achieve more efficient and effective anomaly detection, ultimately leading to increased safety, productivity, and operational integrity. Holographic user interfaces provide a novel approach by visualizing complex data in an immersive, intuitive, and interactive way. By projecting three-dimensional representations of equipment, processes, and real-time data onto user interfaces, operators can quickly recognize anomalies and deviations from normal operating conditions. The holographic interfaces provide contextual awareness, improving operators’ understanding of the industrial environment and enabling them to make informed decisions with confidence. Holographic interfaces have the potential to revolutionize anomaly detection in industrial HMIs, resulting in improved efficiency and safety in industrial operations. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Anomaly detection; Artificial Intelligence; Explainable AI; Human-Computer Interaction; Mixed Reality; Predictive Maintenance; Predictive Quality; process parameter optimization,Accident prevention; Anomaly detection; Holography; Mixed reality; User interfaces; Anomaly detection; Cutting edge technology; Detection process; Explainable AI; Human Machine Interface; Industrial environments; Mixed reality; Predictive maintenance; Predictive quality; Process parameters optimizations; Human computer interaction,Conference paper,Final,,Scopus,2-s2.0-85196738230
Ehsan U.; Saha K.; De Choudhury M.; Riedl M.O.,"Ehsan, Upol (57195223484); Saha, Koustuv (57194395042); De Choudhury, Munmun (18433530100); Riedl, Mark O. (7004421643)",57195223484; 57194395042; 18433530100; 7004421643,Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI,2023,Proceedings of the ACM on Human-Computer Interaction,7,1 CSCW,34,,,,40,10.1145/3579467,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153945382&doi=10.1145%2f3579467&partnerID=40&md5=ff3b1929d8f8abc61e96c5327d931c2b,"Explainable AI (XAI) systems are sociotechnical in nature; thus, they are subject to the sociotechnical gap-divide between the technical affordances and the social needs. However, charting this gap is challenging. In the context of XAI, we argue that charting the gap improves our problem understanding, which can reflexively provide actionable insights to improve explainability. Utilizing two case studies in distinct domains, we empirically derive a framework that facilitates systematic charting of the sociotechnical gap by connecting AI guidelines in the context of XAI and elucidating how to use them to address the gap. We apply the framework to a third case in a new domain, showcasing its affordances. Finally, we discuss conceptual implications of the framework, share practical considerations in its operationalization, and offer guidance on transferring it to new contexts. By making conceptual and practical contributions to understanding the sociotechnical gap in XAI, the framework expands the XAI design space.  © 2023 Owner/Author.",AI ethics; AI governance; explainable ai; fate; framework; human-AI interaction; human-centered explainable ai; organizational dynamics; participatory design; responsible ai; sociotechnical gap; user study,AI ethic; AI governance; Explainable ai; Fate; Framework; Human-AI interaction; Human-centered explainable ai; Organizational dynamics; Participatory design; Responsible ai; Sociotechnical; Sociotechnical gap; User study,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85153945382
Wang X.; Liang C.; Yin M.,"Wang, Xinru (57223025368); Liang, Chen (57194243606); Yin, Ming (57220860849)",57223025368; 57194243606; 57220860849,The Effects of AI Biases and Explanations on Human Decision Fairness: A Case Study of Bidding in Rental Housing Markets,2023,IJCAI International Joint Conference on Artificial Intelligence,2023-August,,,3076,3084,8,8,10.24963/ijcai.2023/343,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170401323&doi=10.24963%2fijcai.2023%2f343&partnerID=40&md5=bef8466f9afaa8212fcf894ec08858f9,"The use of AI-based decision aids in diverse domains has inspired many empirical investigations into how AI models' decision recommendations impact humans' decision accuracy in AI-assisted decision making, while explorations on the impacts on humans' decision fairness are largely lacking despite their clear importance. In this paper, using a real-world business decision making scenario-bidding in rental housing markets-as our testbed, we present an experimental study on understanding how the bias level of the AI-based decision aid as well as the provision of AI explanations affect the fairness level of humans' decisions, both during and after their usage of the decision aid. Our results suggest that when people are assisted by an AI-based decision aid, both the higher level of racial biases the decision aid exhibits and surprisingly, the presence of AI explanations, result in more unfair human decisions across racial groups. Moreover, these impacts are partly made through triggering humans' “disparate interactions” with AI. However, regardless of the AI bias level and the presence of AI explanations, when people return to make independent decisions after their usage of the AI-based decision aid, their decisions no longer exhibit significant unfairness across racial groups. © 2023 International Joint Conferences on Artificial Intelligence. All rights reserved.",,Apartment houses; Artificial intelligence; Behavioral research; Commerce; Decision support systems; Bias levels; Case-studies; Decision accuracies; Decision aids; Decisions makings; Diverse domains; Empirical investigation; Housing markets; Human decisions; Modeling decisions; Decision making,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85170401323
Engelmann D.C.; Ferrando A.; Panisson A.R.; Ancona D.; Bordini R.H.; Mascardi V.,"Engelmann, Debora C. (57201774580); Ferrando, Angelo (55669011800); Panisson, Alison R. (56422142500); Ancona, Davide (7003625858); Bordini, Rafael H. (57203102955); Mascardi, Viviana (6506722954)",57201774580; 55669011800; 56422142500; 7003625858; 57203102955; 6506722954,RV4JaCa—Towards Runtime Verification of Multi-Agent Systems and Robotic Applications,2023,Robotics,12,2,49,,,,8,10.3390/robotics12020049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153763040&doi=10.3390%2frobotics12020049&partnerID=40&md5=ee7e9c42cd6eadde971edfbbdf08012b,"This paper presents a Runtime Verification (RV) approach for Multi-Agent Systems (MAS) using the JaCaMo framework. Our objective is to bring a layer of security to the MAS. This is achieved keeping in mind possible safety-critical uses of the MAS, such as robotic applications. This layer is capable of controlling events during the execution of the system without needing a specific implementation in the behaviour of each agent to recognise the events. In this paper, we mainly focus on MAS when used in the context of hybrid intelligence. This use requires communication between software agents and human beings. In some cases, communication takes place via natural language dialogues. However, this kind of communication brings us to a concern related to controlling the flow of dialogue so that agents can prevent any change in the topic of discussion that could impair their reasoning. The latter may be a problem and undermine the development of the software agents. In this paper, we tackle this problem by proposing and demonstrating the implementation of a framework that aims to control the dialogue flow in a MAS; especially when the MAS communicates with the user through natural language to aid decision-making in a hospital bed allocation scenario. © 2023 by the authors.",dialogue systems; explainable artificial intelligence; JaCaMo framework; multi-agent systems; robotic applications; runtime verification,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85153763040
Lyu Y.; Liang P.P.; Deng Z.; Salakhutdinov R.; Morency L.-P.,"Lyu, Yiwei (57211207258); Liang, Paul Pu (57203051940); Deng, Zihao (57350186500); Salakhutdinov, Ruslan (57203057355); Morency, Louis-Philippe (6603047400)",57211207258; 57203051940; 57350186500; 57203057355; 6603047400,DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations,2022,"AIES 2022 - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,,455,467,12,13,10.1145/3514094.3534148,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137165724&doi=10.1145%2f3514094.3534148&partnerID=40&md5=6a30f3d27f98a2740ac7cbf72130d948,"The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-The-Art in interpreting multimodal models-a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment.  © 2022 Owner/Author.",explainability; interpretability; multimodal machine learning; visualization,Decision making; Human computer interaction; Human robot interaction; Interactive computer systems; Learning systems; Explainability; Fine grained; Intelligence models; Interpretability; Machine-learning; Modeling behaviour; Multi-modal; Multimodal machine learning; Multimodal models; Real-world; Machine learning,Conference paper,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85137165724
Aloisi C.,"Aloisi, Cesare (57201637297)",57201637297,The future of standardised assessment: Validity and trust in algorithms for assessment and scoring,2023,European Journal of Education,58,1,,98,110,12,19,10.1111/ejed.12542,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145032494&doi=10.1111%2fejed.12542&partnerID=40&md5=ab05a508a302c6d69dea7181ede319f7,"This article considers the challenges of using artificial intelligence (AI) and machine learning (ML) to assist high-stakes standardised assessment. It focuses on the detrimental effect that even state-of-the-art AI and ML systems could have on the validity of national exams of secondary education, and how lower validity would negatively affect trust in the system. To reach this conclusion, three unresolved issues in AI (unreliability, low explainability and bias) are addressed, to show how each of them would compromise the interpretations and uses of exam results (i.e., exam validity). Furthermore, the article relates validity to trust, and specifically to the ABI+ model of trust. Evidence gathered as part of exam validation supports each of the four trust-enabling components of the ABI+ model (ability, benevolence, integrity and predictability). It is argued, therefore, that the three AI barriers to exam validity limit the extent to which an AI-assisted exam system could be trusted. The article suggests that addressing the issues of AI unreliability, low explainability and bias should be sufficient to put AI-assisted exams on par with traditional ones, but might not go as far as fully reassure the public. To achieve this, it is argued that changes to the quality assurance mechanisms of the exam system will be required. This may involve, for example, integrating principled AI frameworks in assessment policy and regulation. © 2023 John Wiley & Sons Ltd.",artificial intelligence; automated assessment; bias; education; trust; United Kingdom,,Article,Final,,Scopus,2-s2.0-85145032494
Englisch J.; Schuh M.,"Englisch, Joachim (14621391900); Schuh, Mathias (58037647600)",14621391900; 58037647600,"Algorithm-supported administrative procedures - Fields of application, risks and the need for additional controls; [ALGORITHMENGESTüTZTE VERWALTUNGSVERFAHREN – EINSATZFELDER, RISIKEN UND NOTWENDIGKEIT ERGäNZENDER KONTROLLEN]",2022,Verwaltung,55,2,,155,190,35,3,10.3790/verw.55.2.155,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145276587&doi=10.3790%2fverw.55.2.155&partnerID=40&md5=12caea383e27bd9f41dd898ac2fced26,"The digitization of the German public sector is gaining momentum and it now also extends to the use of algorithms and artificial intelligence in administrative procedures. On the basis of recently published official data, a broad spectrum of fields and variants of application can be identified. However, this increasing reliance on algorithms also creates particular challenges to the rule of law. In particular, the reliance on algorithms can increase the risk of systematic unlawfulness and bias in administrative decision-making, whether fully automated or still accounted for by a human official. Recent studies suggest that also in the latter case, those risks are often not properly managed – and are sometimes even exacerbated – by the humans in the loop. The traditional instruments of judicial protection and internal administrative supervision are not well suited to address the aforementioned inherent and operational risks either; moreover, they offer only ex post remedies and have no preventive effect. Therefore, additional technical precautions and institutional arrangements are required to uphold the rule of law. They should include investments in explainable artificial intelligence (“XAI”) and adequate staff training programs, as well as the creation of public registers and independent standard-setting and auditing bodies for public sector algorithms. In particularly sensitive areas of administration, the use of technical or proprietary black box algorithms should be entirely banned until further technological progress on transparency and risk control has been made. © 2022 Duncker und Humblot GmbH. All rights reserved.",,,Article,Final,,Scopus,2-s2.0-85145276587
Dandolo D.; Masiero C.; Carletti M.; Dalle Pezze D.; Susto G.A.,"Dandolo, David (57958145500); Masiero, Chiara (37085375800); Carletti, Mattia (57212227419); Dalle Pezze, Davide (57386192800); Susto, Gian Antonio (54382424500)",57958145500; 37085375800; 57212227419; 57386192800; 54382424500,AcME—Accelerated model-agnostic explanations: Fast whitening of the machine-learning black box,2023,Expert Systems with Applications,214,,119115,,,,20,10.1016/j.eswa.2022.119115,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141498082&doi=10.1016%2fj.eswa.2022.119115&partnerID=40&md5=b00e95b5e01c4312c44f571487f7a584,"In the context of human-in-the-loop Machine Learning applications, like Decision Support Systems, interpretability approaches should provide actionable insights without making the users wait. In this paper, we propose Accelerated Model-agnostic Explanations (AcME), an interpretability approach that quickly provides feature importance scores both at the global and the local level. AcME can be applied a posteriori to each regression or classification model based on tabular data. Not only AcME computes feature ranking, but it also provides a what-if analysis tool to assess how changes in features values would affect model predictions. We evaluated the proposed approach on synthetic and real-world datasets, also in comparison with SHapley Additive exPlanations (SHAP), the approach we drew inspiration from, which is currently one of the state-of-the-art model-agnostic interpretability approaches. We achieved comparable results in terms of quality of produced explanations while reducing dramatically the computational time and providing consistent visualization for global and local interpretations. To foster research in this field, and for the sake of reproducibility, we also provide a repository with the code used for the experiments. © 2022 Elsevier Ltd",Decision Support Systems; Explainable artificial intelligence; Machine learning; Machine learning interpretability,Machine learning; Accelerated models; Black boxes; Explainable artificial intelligence; Human-in-the-loop; Interpretability; Machine learning applications; Machine learning interpretability; Machine-learning; Posteriori; Decision support systems,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85141498082
Protopapadakis G.; Apostolidis A.; Kalfas A.I.,"Protopapadakis, Georgios (57955375400); Apostolidis, Asteris (55962709000); Kalfas, Anestis I. (6602879895)",57955375400; 55962709000; 6602879895,EXPLAINABLE AND INTERPRETABLE AI-ASSISTED REMAINING USEFUL LIFE ESTIMATION FOR AEROENGINES,2022,Proceedings of the ASME Turbo Expo,2,,V002T05A002,,,,14,10.1115/GT2022-80777,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141396871&doi=10.1115%2fGT2022-80777&partnerID=40&md5=259f42bd3450931c909d7d0dc9a49a38,"Remaining Useful Life (RUL) estimation is directly related with the application of predictive maintenance. When RUL estimation is performed via data-driven methods and Artificial Intelligence algorithms, explainability and interpretability of the model are necessary for trusted predictions. This is especially important when predictive maintenance is applied to gas turbines or aeroengines, as they have high operational and maintenance costs, while their safety standards are strict and highly regulated. The objective of this work is to study the explainability of a Deep Neural Network (DNN) RUL prediction model. An open-source database is used, which is composed by computed measurements through a thermodynamic model for a given turbofan engine, considering non-linear degradation and data points for every second of a full flight cycle. First, the necessary data pre-processing is performed, and a DNN is used for the regression model. The selection of its hyper-parameters is done using random search and Bayesian optimisation. Tests considering the feature selection and the requirements of additional virtual sensors are discussed. The generalisability of the model is performed, showing that the type of faults as well as the dominant degradation has an important effect on the overall accuracy of the model. The explainability and interpretability aspects are studied, following the Local Interpretable Model-agnostic Explanations (LIME) method. The outcomes are showing that for simple data sets, the model can better understand physics, and LIME can give a good explanation. However, as the complexity of the data increases, both the accuracy of the model drops but also LIME seems to have difficulties in giving satisfactory explanations. Copyright © 2022 by ASME.",,Aircraft engines; Data handling; Lime; Maintenance; Regression analysis; Turbofan engines; Aero-engine; Artificial intelligence algorithms; Data-driven methods; Interpretability; Life estimation; Life prediction models; Maintenance cost; Predictive maintenance; Remaining useful lives; Safety standard; Deep neural networks,Conference paper,Final,,Scopus,2-s2.0-85141396871
Schraagen J.M.,"Schraagen, Jan Maarten (6505977678)",6505977678,Responsible use of AI in military systems: prospects and challenges,2023,Ergonomics,66,11,,1719,1729,10,2,10.1080/00140139.2023.2278394,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176251711&doi=10.1080%2f00140139.2023.2278394&partnerID=40&md5=5196c95b4c66a8d412fe4ec44f592e78,"Artificial Intelligence (AI) holds great potential for the military domain but is also seen as prone to data bias and lacking transparency and explainability. In order to advance the trustworthiness of AI-enabled systems, a dynamic approach to the development, deployment and use of AI systems is required. This approach, when incorporating ethical principles such as lawfulness, traceability, reliability and bias mitigation, is called ‘Responsible AI’. This article describes the challenges of using AI responsibly in the military domain from a human factors and ergonomics perspective. Many of the ironies of automation originally described by Bainbridge still apply in the field of AI, but there are also some unique challenges and requirements that need to be considered, such as a larger emphasis on ethical risk analyses and validation and verification up-front, as well as moral situation awareness during deployment and use of AI in military systems. © 2023 Informa UK Limited, trading as Taylor & Francis Group.",Artificial Intelligence; ethics; explainability; human-machine teaming; military systems; testing and evaluation; transparency; validation and verification,Artificial Intelligence; Automation; Awareness; Humans; Military Personnel; Reproducibility of Results; Artificial intelligence; Ergonomics; Ethical technology; Risk analysis; Artificial intelligence systems; Dynamic approaches; Ethical principles; Explainability; Human-machine; Human-machine teaming; Military domains; Military systems; Testing and evaluation; Validation and verification; army; article; artificial intelligence; automation; awareness; ergonomics; ethics; human; mitigation; morality; reliability; risk assessment; military personnel; reproducibility; Transparency,Article,Final,,Scopus,2-s2.0-85176251711
Kaur D.; Uslu S.; Rittichier K.J.; Durresi A.,"Kaur, Davinder (57209112293); Uslu, Suleyman (57202760194); Rittichier, Kaley J. (57232777900); Durresi, Arjan (57207529486)",57209112293; 57202760194; 57232777900; 57207529486,Trustworthy Artificial Intelligence: A Review,2023,ACM Computing Surveys,55,2,3491209,,,,375,10.1145/3491209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128190943&doi=10.1145%2f3491209&partnerID=40&md5=622d2c23d91ae624a63a320c7d89453b,"Artificial intelligence (AI) and algorithmic decision making are having a profound impact on our daily lives. These systems are vastly used in different high-stakes applications like healthcare, business, government, education, and justice, moving us toward a more algorithmic society. However, despite so many advantages of these systems, they sometimes directly or indirectly cause harm to the users and society. Therefore, it has become essential to make these systems safe, reliable, and trustworthy. Several requirements, such as fairness, explainability, accountability, reliability, and acceptance, have been proposed in this direction to make these systems trustworthy. This survey analyzes all of these different requirements through the lens of the literature. It provides an overview of different approaches that can help mitigate AI risks and increase trust and acceptance of the systems by utilizing the users and society. It also discusses existing strategies for validating and verifying these systems and the current standardization efforts for trustworthy AI. Finally, we present a holistic view of the recent advancements in trustworthy AI to help the interested researchers grasp the crucial facets of the topic efficiently and offer possible future research directions.  © 2022 Association for Computing Machinery.",acceptance; accountability; Artificial intelligence; black-box problem; explainability; explainable AI; fairness; machine learning; privacy; trustworthy AI,Data privacy; Decision making; Acceptance; Accountability; Black boxes; Black-box problem; Explainability; Explainable artificial intelligence; Fairness; Machine-learning; Privacy; Trustworthy artificial intelligence; Machine learning,Review,Final,,Scopus,2-s2.0-85128190943
Lindner F.; Reiner G.,"Lindner, Fabian (57210569946); Reiner, Gerald (59266625300)",57210569946; 59266625300,Industry 5.0 and Operations Management - the Importance of Human Factors,2023,"Proceedings of IEEE/IFIP Network Operations and Management Symposium 2023, NOMS 2023",,,,,,,3,10.1109/NOMS56928.2023.10154282,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164735233&doi=10.1109%2fNOMS56928.2023.10154282&partnerID=40&md5=6da4fede7636f538213e2c75912432f7,"In this position paper, we highlight the importance of human factors, especially cognition, for operations management during the transition from Industry 4.0 to 5.0 and within. We argue that the increasing prevalence of (digital) technology and data for manufacturing operations urges human-centered approaches and solutions, as well - to enable efficient and effective operations that benefit from both humans' and technologies' strengths. To stress our point, we give examples from behavioral operations management where technology may both foster or mitigate deviations from rational decision-making. In addition, we show prospects of human-AI interaction and explainable AI, specifically by using visualizations, to improve operational performance. © 2023 IEEE.",behavioral manufacturing operations management; cognitive biases; human factors; Industry 5.0; visualizations,Decision making; Human engineering; Industry 4.0; Behavioral manufacturing operation management; Behavioral operations; Cognitive bias; Digital datas; Digital technologies; Industry 5.0; Manufacturing operations; Operation management; Position papers; Rational decision making; Visualization,Conference paper,Final,,Scopus,2-s2.0-85164735233
Hoffman R.R.; Mueller S.T.; Klein G.; Litman J.,"Hoffman, Robert R. (7402764832); Mueller, Shane T. (24765109700); Klein, Gary (57202862987); Litman, Jordan (6602730270)",7402764832; 24765109700; 57202862987; 6602730270,"Measures for explainable AI: Explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance",2023,Frontiers in Computer Science,5,,1096257,,,,88,10.3389/fcomp.2023.1096257,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148513751&doi=10.3389%2ffcomp.2023.1096257&partnerID=40&md5=4f90e0925d5a793fcb658604a55708c4,"If a user is presented an AI system that portends to explain how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? This question entails some key concepts of measurement such as explanation goodness and trust. We present methods for enabling developers and researchers to: (1) Assess the a priori goodness of explanations, (2) Assess users' satisfaction with explanations, (3) Reveal user's mental model of an AI system, (4) Assess user's curiosity or need for explanations, (5) Assess whether the user's trust and reliance on the AI are appropriate, and finally, (6) Assess how the human-XAI work system performs. The methods we present derive from our integration of extensive research literatures and our own psychometric evaluations. We point to the previous research that led to the measurement scales which we aggregated and tailored specifically for the XAI context. Scales are presented in sufficient detail to enable their use by XAI researchers. For Mental Model assessment and Work System Performance, XAI researchers have choices. We point to a number of methods, expressed in terms of methods' strengths and weaknesses, and pertinent measurement issues. Copyright © 2023 Hoffman, Mueller, Klein and Litman.",explanation goodness; explanatory reasoning; machine-generated explanations; measurement; mental models; performance; trust,,Review,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85148513751
Oza S.; Zou Y.,"Oza, Suchi (59090998500); Zou, Yi (57701618900)",59090998500; 57701618900,Transparency in Algorithmic Management: A Psychological Ownership Perspective,2022,"28th Americas Conference on Information Systems, AMCIS 2022",,,,,,,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192551807&partnerID=40&md5=15fb0960ab41573f7207731300c7643c,"Decision-making and forecasting capabilities of algorithmic systems have helped organizations improve work productivity and business performance. Specifically, AI-enabled information systems (IS) are increasingly being used to track employee's work hours and automate their work shifts in retail and service industries including hospitality, leisure, and health services. For example, companies such Kronos, Zoho and Deputy specialize in workforce management software programs that utilize AI technologies to match employer's staffing needs for labor to at-the-moment customer demand. Software programs do not only fine-tune and optimize scheduling decisions but also send automatic updates to employees about their shift changes (Loggins, 2020). According to a report from the Reportlinker.com (2022), the global market of cloud-based work scheduling software is estimated to grow by over 4 billion dollars during the forecast period of 2022 to 2026. With the increasing relevance of algorithmic systems in workforce scheduling and management, it is critical to understand their impact on employees' work experiences and effectiveness. Specifically, past research has indicated that the use of algorithmic systems in the workplace can lead to several ramifications including discrimination, surveillance, manipulation, disempowerment of employees, precarity, and stress (e.g., Kellogg et al., 2020). Nevertheless, there remains an equivocal understanding of why employees would have those negative experiences with the deployment of algorithmic systems and what organizations could do to mitigate those negative experiences effectively. In this research, we center on investigating the effects of employees' perceptions of transparency about work scheduling AI software on their job satisfaction and affective organizational commitment. According to a theory of psychological ownership in organizations (Pierce et al., 2001), individuals have an innate motive to be in control and to be efficient and effectant (Pierce et al., 2003). Based on this core premise, the present study suggests that when the inner workings of work management AI software are unclear to employees, the compliance to automated work schedules can negatively affect employees' perceptions of job autonomy and job-based psychological ownership, that could further decrease employees' job satisfaction and affective organizational commitment. In contrast, when employees are provided with an explanation about why and how work management AI software programs are deployed to manage their work shifts, they are likely to perceive such programs as more transparent and less opaque. As a result, employees are likely to experience freedom and flexibility in controlling their own work schedules with the use of those programs, and such work experiences can enhance job satisfaction and organizational commitment. The present research is intended to extend prior research on AI-related work design. A close examination of algorithmic transparency from a psychological ownership lens can help to shed light onto both positive and negative effects of AI-related work management on employees' work outcomes and psychological experiences. Study results from this research can also help to inform HR managers, supervisors, and stakeholders at organizations of the importance of building and using work management AI software in ways that can facilitate transparency and ensure worker well-being and a committed workforce. © 2022 28th Americas Conference on Information Systems, AMCIS 2022. All Rights Reserved.",,Computer software; Human resource management; Information systems; Information use; International trade; Job satisfaction; Sales; Transparency; Algorithmics; Negative experiences; Organizational Commitment; Psychological ownership; Software project; Work experience; Work management; Work scheduling; Work shifts; Workforce management; Decision making,Conference paper,Final,,Scopus,2-s2.0-85192551807
Bell A.; Solano-Kamaiko I.; Nov O.; Stoyanovich J.,"Bell, Andrew (57212107017); Solano-Kamaiko, Ian (57767090800); Nov, Oded (22981114200); Stoyanovich, Julia (14029242600)",57212107017; 57767090800; 22981114200; 14029242600,It's Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy,2022,ACM International Conference Proceeding Series,,,,248,266,18,61,10.1145/3531146.3533090,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133005363&doi=10.1145%2f3531146.3533090&partnerID=40&md5=3d2768d325d769b5a5461f63cf21fc7d,"To achieve high accuracy in machine learning (ML) systems, practitioners often use complex ""black-box""models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature - and even the existence - of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque. Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human's ability to anticipate a model's output or identify the most important feature of a model, and subjective measures, such as a human's perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It's just not that simple! © 2022 ACM.",explainability; machine learning; public policy; responsible AI,Decision trees; Economic and social effects; Machine learning; Random forests; Black box modelling; Empirical studies; Explainability; High-accuracy; Human-in-the-loop; Machine-learning; Modeling accuracy; Responsible AI; Simple++; Trade off; Public policy,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85133005363
Cohen M.C.; Mancenido M.V.; Chiou E.K.; Cooke N.J.,"Cohen, Myke C. (57331589100); Mancenido, Michelle V. (47962083700); Chiou, Erin K. (55552000100); Cooke, Nancy J. (7102096954)",57331589100; 47962083700; 55552000100; 7102096954,Teamness and Trust in AI-Enabled Decision Support Systems: Current Challenges and Future Directions,2023,CEUR Workshop Proceedings,3456,,,175,187,12,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171150711&partnerID=40&md5=7a1f3e6aeeac544ddfe4d7e398b0bb1d,"Artificial intelligence-enabled decision support systems (AI-DSSs) can process highly complex information to recommend or execute decisions autonomously, but often at the cost of lacking transparency and explainability. The existence of inherent human limitations in understanding increasingly inexplicable AI-DSSs, however, raise the question of people’s roles in the high-stakes, rapid decision-making domains for which AI-DSSs are being developed. In this paper, we summarize the current state of human-AI teaming research in light of how emergent cognitive properties arise from human interactions with AI-DSSs. We also identify important open research questions in accounting for the teamness of AI-DSSs in light of current directions in trust research. Finally, we outline some anticipated challenges in methodological approaches and generalizability when attempting to design studies to answer these questions. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",AI-DSS; Decision support systems; Human-AI teaming; Teamness; Trust,Artificial intelligence; Behavioral research; Decision making; 'current; Artificial intelligence-enabled decision support system; Cognitive properties; Complex information; Decisions makings; Human limitations; Human-AI teaming; Humaninteraction; Teamness; Trust; Decision support systems,Conference paper,Final,,Scopus,2-s2.0-85171150711
Jin X.; Lv S.; Kong Z.; Yang H.; Zhang Y.; Guo Y.; Xu Z.,"Jin, Xiaohang (55163143100); Lv, Shengye (59173475300); Kong, Ziqian (57786376300); Yang, Hongchun (57203814828); Zhang, Yuanming (55739911300); Guo, Yuanjing (36236843800); Xu, Zhengguo (57211427021)",55163143100; 59173475300; 57786376300; 57203814828; 55739911300; 36236843800; 57211427021,Graph Spatio-Temporal Networks for Condition Monitoring of Wind Turbine,2024,IEEE Transactions on Sustainable Energy,15,4,,2276,2286,10,2,10.1109/TSTE.2024.3411884,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196084737&doi=10.1109%2fTSTE.2024.3411884&partnerID=40&md5=26cb9d0ddf4d3d6cc840b81246d1e831,"Condition monitoring of wind turbines (WTs) is essential for advancing wind energy. Existing data-driven methods heavily rely on deep learning and big data, leading to challenges in distinguishing true faults from false alarms, impacting operational decisions negatively. Thus, this paper proposes a spatio-temporal graph neural network framework that incorporates prior knowledge. Prior WT knowledge is utilized by establishing a spatially structured directed graph embedded in a graph attention network (GAT). The features in WTs' supervisory control and data acquisition system are indicated by the nodes in GAT. Then, the global and local attention embedding layers as well as long short-term memory layers are employed to combine spatio-temporal information from each node. Finally, the condition monitoring in WTs' graph and node-level are established, and a fault propagation chain at node-level is constructed for explaining condition monitoring results. To demonstrate the explainability, robustness and sensitivity of the proposed approach, a comparative analysis between a true fault case and a false alarm case are given, and anomaly detection results are also reported.  © 2010-2012 IEEE.",condition monitoring; graph spatio-temporal neural networks; prior knowledge; Wind turbine (WT),Anomaly detection; Brain; Condition monitoring; Directed graphs; Errors; SCADA systems; Wind power; Wind turbines; Data-driven methods; Falsealarms; Features extraction; Graph spatio-temporal neural network; Operational decisions; Prior-knowledge; Spatio-temporal; Temporal networks; Temporal neural networks; Wind turbine; Long short-term memory,Article,Final,,Scopus,2-s2.0-85196084737
Graham T.; Thangavel K.,"Graham, Thomas (58896233000); Thangavel, Kathiravan (57222007577)",58896233000; 57222007577,Artificial Intelligence in Space: An Analysis of Responsible AI Principles for the Space Domain,2023,"Proceedings of the International Astronautical Congress, IAC",2023-October,,,,,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188016281&partnerID=40&md5=1cca0a1acc839aefde90bec4b730419c,"Advances in Artificial Intelligence (AI) technologies are enabling a plethora of new applications across many industries. There are already a multitude of applications for AI systems in the space industry, but as the AI and space industries continue to grow in size and value rapidly, further use-cases will become apparent and proliferate to all corners of space operations and data analytics. Such space-based AI systems will bring many economic, scientific, and environmental benefits; however, they could also enable harm to individuals, organisations, and the environment if they are not developed and managed properly. Potential breaches of privacy through AI-assisted analysis of Earth observation imagery and collisions between objects in orbit due to malfunctioning automated maneuvering systems are examples of the harms that could eventuate if poorly designed AI systems are deployed in the space-sector. Responsible AI practices are needed to ensure such risks do not eventuate. 'Responsible' (or 'ethical') AI has emerged as a discipline designed to guide responsible AI development wherein the goal is to maximise the benefits of AI systems for individuals and society while mitigating against any potential harm that they may cause. Commonly accepted Responsible AI principles include accountability, contestability, fairness, security, privacy, transparency, explainability, and reliability. At times notions of 'do-no-harm' and generating 'net benefits' for society and the environment are also included. These principles of Responsible AI are generalizable and industry agnostic; however, they should be carefully considered in the context of the unique physical, economic, political, and technological characteristics of the space domain before being adopted wholesale by the space industry. While concepts such as security and reliability can be readily applied to applications of AI systems in the space domain, other ideals such as contestability, fairness, and explainability may not be as relevant to the use cases found within the space industry. This paper introduces the concept of Responsible AI and Responsible AI principles and then examines the applicability and appropriateness of widely accepted Responsible AI principles in the context of existing and emerging regulatory instruments relevant to the space industry. This serves as a first step towards creating a standardized regulatory framework for the responsible development of space-based AI systems and preventing harms associated with such systems occurring. Copyright © 2023 by SmartSat CRC. Published by the IAF, with permission and released to the IAF to publish in all forms.",Artificial Intelligence; Guidelines and Technical Standards; International Law; Liability; National Law and Regulation; Responsible AI; Space Law,Artificial intelligence; Data Analytics; Orbits; Space flight; Artificial intelligence systems; Guideline and technical standard; Laws and regulations; Liability; National law and regulation; National laws; Responsible artificial intelligence; Space industry; Space laws; Technical standards; Earth (planet),Conference paper,Final,,Scopus,2-s2.0-85188016281
Handelman D.A.; Rivera C.G.; St. Amant R.; Holmes E.A.; Badger A.R.; Yeh B.Y.,"Handelman, David A. (6701612996); Rivera, Corban G. (57225403096); St. Amant, Robert (10241649700); Holmes, Emma A. (57218440637); Badger, Andrew R. (57191317893); Yeh, Bryanna Y. (57201338210)",6701612996; 57225403096; 10241649700; 57218440637; 57191317893; 57201338210,Adaptive human-robot teaming through integrated symbolic and subsymbolic artificial intelligence: preliminary results,2022,Proceedings of SPIE - The International Society for Optical Engineering,12113,,121130I,,,,4,10.1117/12.2618686,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134162538&doi=10.1117%2f12.2618686&partnerID=40&md5=bd62567b771d0348e26092dff29a67a1,"As the autonomy of intelligent systems continues to increase, the ability of humans to maintain control over machine behavior, work effectively in concert with them, and trust them, becomes paramount. Ideally, a machine’s plan of action would be accessible to and understandable by human team members, and machine behavior would be modifiable in real time, in the field, to accommodate unanticipated situations. The ability of machines to adapt to new situations quickly and reliably based on both human input and autonomous learning has the potential to enhance numerous human-machine teaming scenarios. Our research focuses on the question, “Can robots become competent and adaptive teammates by emulating human skill acquisition strategies?” In this paper we describe the Robotic Skill Acquisition (RSA) cognitive architecture and show preliminary results of teaming experiments involving a human wearing an augmented reality headset and a quadruped robot performing tasks related to reconnaissance. The goal is to combine instruction and discovery by integrating declarative symbolic AI and reflexive neural network learning to produce robust, explainable and trusted robot behavior, adjustable autonomy, and adaptive human-robot teaming. Humans and robots start with a playbook of modifiable hierarchical task descriptions that encode explicit task knowledge. Neural network based feedback error learning enables human-directed behavior shaping, and reinforcement learning enables discovery of novel subtask control strategies. It is anticipated that modifications to and transitions between symbolic and subsymbolic processing will enable highly adaptive behavior in support of enhanced situational awareness and operational effectiveness of human-robot teams. © 2022 SPIE.",adaptive teaming; adjustable autonomy; cognitive architecture; explainable artificial intelligence; Human-machine teaming; human-robot collaboration; machine learning; shared mental model,Augmented reality; Behavioral research; Intelligent robots; Learning systems; Network architecture; Reinforcement learning; Adaptive teaming; Adjustable autonomy; Cognitive architectures; Explainable artificial intelligence; Human robots; Human-machine; Human-machine teaming; Human-robot collaboration; Machine-learning; Shared mental model; Intelligent systems,Conference paper,Final,,Scopus,2-s2.0-85134162538
Lee M.H.; Chew C.J.,"Lee, Min Hun (57200519147); Chew, Chong Jun (58546811500)",57200519147; 58546811500,Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making,2023,Proceedings of the ACM on Human-Computer Interaction,7,CSCW2,369,,,,23,10.1145/3610218,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174692019&doi=10.1145%2f3610218&partnerID=40&md5=782cb00cf7b3bcc9cbf0df6fa15968cc,"Artificial intelligence (AI) is increasingly being considered to assist human decision-making in high-stake domains (e.g. health). However, researchers have discussed an issue that humans can over-rely on wrong suggestions of the AI model instead of achieving human AI complementary performance. In this work, we utilized salient feature explanations along with what-if, counterfactual explanations to make humans review AI suggestions more analytically to reduce overreliance on AI and explored the effect of these explanations on trust and reliance on AI during clinical decision-making. We conducted an experiment with seven therapists and ten laypersons on the task of assessing post-stroke survivors' quality of motion, and analyzed their performance, agreement level on the task, and reliance on AI without and with two types of AI explanations. Our results showed that the AI model with both salient features and counterfactual explanations assisted therapists and laypersons to improve their performance and agreement level on the task when 'right' AI outputs are presented. While both therapists and laypersons over-relied on 'wrong' AI outputs, counterfactual explanations assisted both therapists and laypersons to reduce their over-reliance on 'wrong' AI outputs by 21% compared to salient feature explanations. Specifically, laypersons had higher performance degrades by 18.0 f1-score with salient feature explanations and 14.0 f1-score with counterfactual explanations than therapists with performance degrades of 8.6 and 2.8 f1-scores respectively. Our work discusses the potential of counterfactual explanations to better estimate the accuracy of an AI model and reduce over-reliance on 'wrong' AI outputs and implications for improving human-AI collaborative decision-making. © 2023 Owner/Author.",clinical decision support systems; explainable AI; human centered AI; human-AI collaboration; physical stroke rehabilitation assessment; reliance; trust,Decision making; Decision support systems; Patient rehabilitation; Clinical decision support systems; Counterfactuals; Explainable artificial intelligence; Human centered artificial intelligence; Human-artificial intelligence collaboration; Performance; Physical stroke rehabilitation assessment; Reliance; Stroke rehabilitation; Trust; Artificial intelligence,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85174692019
Verhagen R.S.; Neerincx M.A.; Tielman M.L.,"Verhagen, Ruben S. (57219437427); Neerincx, Mark A. (9133405200); Tielman, Myrthe L. (56085264900)",57219437427; 9133405200; 56085264900,The influence of interdependence and a transparent or explainable communication style on human-robot teamwork,2022,Frontiers in Robotics and AI,9,,993997,,,,15,10.3389/frobt.2022.993997,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138570664&doi=10.3389%2ffrobt.2022.993997&partnerID=40&md5=6dd464459338c56fa260930dff1220c4,"Humans and robots are increasingly working together in human-robot teams. Teamwork requires communication, especially when interdependence between team members is high. In previous work, we identified a conceptual difference between sharing what you are doing (i.e., being transparent) and why you are doing it (i.e., being explainable). Although the second might sound better, it is important to avoid information overload. Therefore, an online experiment (n = 72) was conducted to study the effect of communication style of a robot (silent, transparent, explainable, or adaptive based on time pressure and relevancy) on human-robot teamwork. We examined the effects of these communication styles on trust in the robot, workload during the task, situation awareness, reliance on the robot, human contribution during the task, human communication frequency, and team performance. Moreover, we included two levels of interdependence between human and robot (high vs. low), since mutual dependency might influence which communication style is best. Participants collaborated with a virtual robot during two simulated search and rescue tasks varying in their level of interdependence. Results confirm that in general robot communication results in more trust in and understanding of the robot, while showing no evidence of a higher workload when the robot communicates or adds explanations to being transparent. Providing explanations, however, did result in more reliance on RescueBot. Furthermore, compared to being silent, only being explainable results a higher situation awareness when interdependence is high. Results further show that being highly interdependent decreases trust, reliance, and team performance while increasing workload and situation awareness. High interdependence also increases human communication if the robot is not silent, human rescue contribution if the robot does not provide explanations, and the strength of the positive association between situation awareness and team performance. From these results, we can conclude that robot communication is crucial for human-robot teamwork, and that important differences exist between being transparent, explainable, or adaptive. Our findings also highlight the fundamental importance of interdependence in studies on explainability in robots. Copyright © 2022 Verhagen, Neerincx and Tielman.",communication; explainability; explainable AI; human-agent teaming; human-robot teamwork; interdependence; transparency; user study,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85138570664
Barnhoorn J.S.; Rakhorst-Oudendijk M.; Veltman K.; Holleman B.; Haije T.; Wolbers J.; Janssen J.,"Barnhoorn, Jonathan Sebastiaan (56422150900); Rakhorst-Oudendijk, Marleen (56285707700); Veltman, Kim (57212256287); Holleman, Bas (56285235300); Haije, Tjalling (57224478717); Wolbers, Jelle (58525519900); Janssen, Johan (57212908318)",56422150900; 56285707700; 57212256287; 56285235300; 57224478717; 58525519900; 57212908318,IMMENS: Integrated Multi-Manager Environment for Naval Ships,2022,Proceedings of the International Ship Control Systems Symposium,,,,,,12,1,10.24868/10732,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187463474&doi=10.24868%2f10732&partnerID=40&md5=5c515c263bd4bd19411b0a5650853a43,"The Royal Netherlands Navy is moving towards more autonomous ships to cope with the increasingly complex naval operations while sailing with a reduced manning. The strong expansion of automation and autonomous systems on board requires new solutions for optimal collaboration between the crew and the extensive set of ship systems. We are developing a concept for an Integrated Multi-Manager Environment for Naval Ships: IMMENS. IMMENS aims to be an intelligent ship system that supports the crews of future ship classes, while enabling these crews to exercise and maintain meaningful human control. The ship’s subsystems are integrated in a multi-agent system that formulates and proposes plans to the command team based on an internal representation of the ship’s goals and the current situation. These plans consist of actions that are to be executed in order to achieve the mission goals. The system’s computational design, combined with specific human-machine teaming functionality facilitates the crew and IMMENS to work as a joint cognitive system. This paper shortly introduces IMMENS but reports mainly on the development and application of humanmachine teaming concepts that support cooperation between the crew and IMMENS. The first concept offers interaction based on a human-machine language using Goals, Tasks, Constraints and Resources as shared knowledge elements. Since both human and machine understand these elements, human comprehension of the machine's inner workings is supported. The second concept provides the explication of relationships between goals, tasks, constraints, resources and/or external world as a means of intuitively explaining the machine's reasoning by providing transparency regarding, for instance, to which goals certain tasks contribute. Thirdly, we applied and extended on the concept of play-based delegation. A play is a plan-template that allows a user to quickly delegate a complex task to the systems, without the need to drill down, while leaving room for system intelligence to fill in the details. These concepts have been applied in an integrated interactive demonstrator of IMMENS. An evaluation with subject matter experts from the Royal Netherlands Navy and senior human factors researchers was conducted based on a scenario of a frigate escorting a high-value asset. During the design, implementation and evaluation of the demonstrator, we learned that the first concept provided a workable shared knowledge model and indeed fitted with the IMMENS architecture on the one hand, and was intuitive to the users on the other hand. Explicating relationships provided the users with relevant insights, allowing them to understand, for instance, by means of which tasks and resources IMMENS expects to achieve the mission goals. Play-based delegation was found this a clear and valuable concept. Many opportunities for future research were identified, the most important of which concerns the feasibility of constructing a valid, complete and generic goal representation for IMMENS that takes into account the complexity of future missions and operational contexts, and includes the necessary world model. © 2022, Institute of Marine Engineering Science & Technology. All rights reserved.",Autonomous Systems; Explainable AI; Human-machine Teaming; Multi-Agent System; Transparency; Utility-based Reasoning,,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85187463474
Field J.R.,"Field, J.R. (58638615700)",58638615700,Exploring Trust and Explainability of Unmanned Systems,2023,"OCEANS 2023 - Limerick, OCEANS Limerick 2023",,,,,,,0,10.1109/OCEANSLimerick52467.2023.10244349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173671622&doi=10.1109%2fOCEANSLimerick52467.2023.10244349&partnerID=40&md5=3f2dfff8b663be96db2cd2de055e6a6a,"Explainability (often referred to as interpretability) refers to the concept of providing context to an AI/ML model and its output, thereby assisting a human user in understanding the system's decision-making process. The concept of explainability is especially helpful when we consider the high cognitive load and intense data management strategies that are required for current human-in-the-loop operations in use today. The work presented here aims to provide an explainability framework for autonomous systems, to provide system transparency, and enhance operator awareness. This work served to develop a novel method of sorting and evaluating data streams taken from an operational system, to filter and transmit data packages based on mission conditions. Post mission analysis yielded apparent trends in messaging hierarchy, indicating that certain health and status data streams were consistently prioritized, regardless of the pre-defined metrics. Additional data analysis was performed to evaluate sensor outputs with respect to health and status messaging. This process included conducting data correlation and data characterization, to evaluate relationships between data streams, identify data associated with nominal behavior, and perform anomaly detection. Key functional categories were developed, in which the system's behavior is mapped to a corresponding component (and its respective data stream). Monitoring subsystem performance assists with cross-referencing sensor outputs, to confirm data projections and/or aid in identifying faulty readings. Furthermore, the application of anomaly detection algorithms is coupled with data correlation and/or pattern recognition to extract the most important and salient information. © 2023 IEEE.",Autonomy; Explainability; UUV,Anomaly detection; Information management; Pattern recognition; Autonomy; Data correlations; Data stream; Decision-making process; Explainability; Human users; Interpretability; Sensor output; Unmanned system; UUV; Decision making,Conference paper,Final,,Scopus,2-s2.0-85173671622
Gregory J.M.; Sanchez F.; Lancaster E.; Agha-Mohammadi A.-A.; Gupta S.K.,"Gregory, Jason M. (55813455200); Sanchez, Felix (57216112560); Lancaster, Eli (57852770900); Agha-Mohammadi, Ali-Akbar (54792649700); Gupta, Satyandra K. (7407274179)",55813455200; 57216112560; 57852770900; 54792649700; 7407274179,Using Decision Support in Human-in-the-Loop Experimental Design Toward Building Trustworthy Autonomous Systems,2023,"IEEE International Workshop on Robot and Human Communication, RO-MAN",,,,205,212,7,0,10.1109/RO-MAN57019.2023.10309571,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187021369&doi=10.1109%2fRO-MAN57019.2023.10309571&partnerID=40&md5=5b1d46dc3a4139956e0792c179218878,"Experimental design of autonomous systems involves defining experimental inputs to maximize the experimenter's information gained, minimize costs, and balance risk. This effectively leads to improved understanding and trustworthiness, which are necessary for deployment in realworld settings. Since experimental design is inherently a human-in-the-loop, sequential decision making problem, and decisions are being made about complex systems, an investigation into decision-making quality and decision-supporting methods is warranted. In this work, we investigate a decision support system (DSS) to augment the human's experimental design decision making abilities, and conduct an exploratory user study to investigate the potential for decision support. Our findings show that experimenters, including experienced field roboticists, make suboptimal decisions and mistakes during the experimental design process, which suggests robotics research could benefit from DSSs. Our proposed DSS shows promise in some select aspects of experimental design, including helping to reduce suboptimal decisions, and participants in the user study reported favorable opinions of using such a system, including a sense of usefulness and lack of burden. The broader implication of this work is the identification of decision support in experimental design as one way to help bridge the gap between academia and industry by way of accelerated, informative experimentation and increased system explainability. © 2023 IEEE.",,Artificial intelligence; Bridges; Decision making; Design; Statistics; Decision supporting; Decision supports; Decision-making problem; Decisions makings; Design decision-making; Design-process; Human-in-the-loop; Real-world; Sequential decision making; User study; Decision support systems,Conference paper,Final,,Scopus,2-s2.0-85187021369
Würfel J.; Papenfuß A.; Wies M.,"Würfel, Jakob (59170862400); Papenfuß, Anne (36599173100); Wies, Matthias (42561959500)",59170862400; 36599173100; 42561959500,Operationalizing AI Explainability Using Interpretability Cues in the Cockpit: Insights from User-Centered Development of the Intelligent Pilot Advisory System (IPAS),2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),14734 LNAI,,,297,315,18,0,10.1007/978-3-031-60606-9_17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196158745&doi=10.1007%2f978-3-031-60606-9_17&partnerID=40&md5=7f8ef7e5b6070033b713562ae405ff88,"This paper presents a concept for operationalizing Artificial Intelligence (AI) explainability for the Intelligent Pilot Advisory System (IPAS) as requested in the European Aviation Safety Agency’s AI Roadmap 2.0 in order to meet the requirement of Trustworthy AI. The IPAS is currently being developed to provide AI-based decision support in commercial aircraft to assist the flight crew, especially in emergency situations. The development of the IPAS is following a user-centred and exploratory design approach, with the active involvement of airline pilots in the early stages of development to iteratively tailor the system to their requirements. The concept presented in this paper aims to provide interpretability cues to achieve “operational explainability of AI”, which should enable commercial aircraft pilots to understand and adequately trust the recommendations generated by AI when making decisions in emergencies. Focus of the research was to identify initial interpretability requirements and to answer the question of what interpretation cues pilots need from the AI-based system. Based on a user study with airline pilots, four requirements for interpretation cues were formulated. These results will form the basis for the next iteration of the IPAS, where the requirements will be implemented. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Ethical and Trustworthy AI; Explainable AI; Human-AI Teaming; Human-Centered AI; Interpretable AI,Cockpits (aircraft); User centered design; Advisory systems; Airline pilots; Commercial aircraft; Ethical and trustworthy artificial intelligence; Explainable artificial intelligence; Human-artificial intelligence teaming; Human-centered artificial intelligence; Interpretability; Interpretable artificial intelligence; User-centered development; Decision support systems,Conference paper,Final,,Scopus,2-s2.0-85196158745
Yamin P.A.R.; Park J.; Kim H.K.,"Yamin, Putra A.R. (57201500860); Park, Jaehyun (56898183400); Kim, Hyun K. (57094273400)",57201500860; 56898183400; 57094273400,Towards a human-machine interface guidance for in-vehicle augmented reality head-up displays,2021,ICIC Express Letters,15,12,,1313,1318,5,3,10.24507/icicel.15.12.1313,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118996430&doi=10.24507%2ficicel.15.12.1313&partnerID=40&md5=76bd281ea0710ca16b6b2e1d15a03a1f,"Recently Augmented-Reality Head-Up Displays (AR-HUDs) has emerged as a next evolution of in-vehicle display technologies. Augmented image should be overlayed onto real-world objects providing alerts that can be viewed in the driver’s line of sight. However, there are currently no guidelines that apply specifically to the Human-Machine Interface (HMI) for AR-HUDs. A review on existing literature on AR-HUD and applicable human factors and human-computer interaction guidelines was conducted, using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) approach. Based on the literature, an initial set of guidelines for developing AR-HUD was derived. The guidelines are grouped into virtual image distance and field of view, brightness, projected information, and legibility. Research gaps are also discussed for future experiments. Taken together, this study is a starting point for developing the interface of AR-HUDs. © 2021 ICIC International. All rights reserved.",AR-HUD; Guideline; HMI; In-vehicle,,Article,Final,,Scopus,2-s2.0-85118996430
Alzubaidi L.; Al-Sabaawi A.; Bai J.; Dukhan A.; Alkenani A.H.; Al-Asadi A.; Alwzwazy H.A.; Manoufali M.; Fadhel M.A.; Albahri A.S.; Moreira C.; Ouyang C.; Zhang J.; Santamaría J.; Salhi A.; Hollman F.; Gupta A.; Duan Y.; Rabczuk T.; Abbosh A.; Gu Y.,"Alzubaidi, Laith (57195380379); Al-Sabaawi, Aiman (57219112523); Bai, Jinshuai (57217198195); Dukhan, Ammar (57201977647); Alkenani, Ahmed H. (57219898452); Al-Asadi, Ahmed (57206243498); Alwzwazy, Haider A. (57866305500); Manoufali, Mohamed (54894519300); Fadhel, Mohammed A. (57192639808); Albahri, A.S. (57201009814); Moreira, Catarina (51663717600); Ouyang, Chun (14008574600); Zhang, Jinglan (17344775600); Santamaría, Jose (56211885400); Salhi, Asma (57196190467); Hollman, Freek (56682782300); Gupta, Ashish (57198676774); Duan, Ye (7202190080); Rabczuk, Timon (56502462200); Abbosh, Amin (13404674100); Gu, Yuantong (7403046386)",57195380379; 57219112523; 57217198195; 57201977647; 57219898452; 57206243498; 57866305500; 54894519300; 57192639808; 57201009814; 51663717600; 14008574600; 17344775600; 56211885400; 57196190467; 56682782300; 57198676774; 7202190080; 56502462200; 13404674100; 7403046386,Towards Risk-Free Trustworthy Artificial Intelligence: Significance and Requirements,2023,International Journal of Intelligent Systems,2023,,4459198,,,,43,10.1155/2023/4459198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176242263&doi=10.1155%2f2023%2f4459198&partnerID=40&md5=47549faf5f791af77e087a395a997f8b,"Given the tremendous potential and influence of artificial intelligence (AI) and algorithmic decision-making (DM), these systems have found wide-ranging applications across diverse fields, including education, business, healthcare industries, government, and justice sectors. While AI and DM offer significant benefits, they also carry the risk of unfavourable outcomes for users and society. As a result, ensuring the safety, reliability, and trustworthiness of these systems becomes crucial. This article aims to provide a comprehensive review of the synergy between AI and DM, focussing on the importance of trustworthiness. The review addresses the following four key questions, guiding readers towards a deeper understanding of this topic: (i) why do we need trustworthy AI? (ii) what are the requirements for trustworthy AI? In line with this second question, the key requirements that establish the trustworthiness of these systems have been explained, including explainability, accountability, robustness, fairness, acceptance of AI, privacy, accuracy, reproducibility, and human agency, and oversight. (iii) how can we have trustworthy data? and (iv) what are the priorities in terms of trustworthy requirements for challenging applications? Regarding this last question, six different applications have been discussed, including trustworthy AI in education, environmental science, 5G-based IoT networks, robotics for architecture, engineering and construction, financial technology, and healthcare. The review emphasises the need to address trustworthiness in AI systems before their deployment in order to achieve the AI goal for good. An example is provided that demonstrates how trustworthy AI can be employed to eliminate bias in human resources management systems. The insights and recommendations presented in this paper will serve as a valuable guide for AI researchers seeking to achieve trustworthiness in their applications.  © 2023 Laith Alzubaidi et al.",,5G mobile communication systems; Decision making; Engineering education; Environmental technology; Health care; Algorithmics; Decisions makings; Diverse fields; Healthcare industry; Human oversight; Industry government; Intelligence agencies; Reproducibilities; Risk free; Wide-ranging applications; Artificial intelligence,Review,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85176242263
Ibrahim L.; Ghassemi M.M.; Alhanai T.,"Ibrahim, Lujain (59802012100); Ghassemi, Mohammad M. (57221151207); Alhanai, Tuka (36162805500)",59802012100; 57221151207; 36162805500,Do Explanations Improve the Quality of AI-assisted Human Decisions? An Algorithm-in-the-Loop Analysis of Factual & Counterfactual Explanations,2023,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",2023-May,,,326,334,8,3,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171266753&partnerID=40&md5=f890b122821d48e52559780201d1ab3e,"The increased use of AI algorithmic aids in high-stakes decision making has prompted interest in explainable AI (xAI), and the role of counterfactual explanations to increase trust in human-algorithm collaborations and to mitigate unfair outcomes. However, research is limited in understanding how explainable AI improves human decision-making. We conduct an online experiment with 559 participants, utilizing an “algorithm-in-the-loop"" framework and real-world pre-trial data to investigate how explanations of algorithmic pretrial risk assessments generated from state-of-the-art machine learning explanation methods (counterfactual explanations via DiCE & factual explanations via SHAP) influences the quality of decision-makers' assessment of recidivism. Our results show that counterfactual and factual explanations achieve different desirable goals (separately improve human assessment of model accuracy, fairness, and calibration), yet still fall short of improving the combined accuracy, fairness, and reliability of human predictions - reinstating the need for sociotechnical, empirical evaluations in xAI. We conclude with user feedback on DiCE counterfactual explanations, as well as a discussion of the broader implications of our results to AI-assisted decision-making and xAI. © 2023 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",counterfactuals; explanations; fairness; risk assessments; sociotechnical systems; trust; user studies,Autonomous agents; Decision making; Learning algorithms; Machine learning; Multi agent systems; Algorithmics; Counterfactuals; Decisions makings; Explanation; Fairness; Human decisions; Risks assessments; Sociotechnical systems; Trust; User study; Risk assessment,Conference paper,Final,,Scopus,2-s2.0-85171266753
Gerogiannis D.; Arsenos A.; Kollias D.; Nikitopoulos D.; Kollias S.,"Gerogiannis, Demetris (6508181175); Arsenos, Anastasios (57224966095); Kollias, Dimitrios (57188870291); Nikitopoulos, Dimitris (35565986000); Kollias, Stefanos (57193712526)",6508181175; 57224966095; 57188870291; 35565986000; 57193712526,Covid-19 Computer-Aided Diagnosis through AI-Assisted CT Imaging Analysis: Deploying a Medical AI System,2024,Proceedings - International Symposium on Biomedical Imaging,,,,,,,4,10.1109/ISBI56570.2024.10635484,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197560143&doi=10.1109%2fISBI56570.2024.10635484&partnerID=40&md5=40f239ee71d30ab580c5b01f1d0cf50c,"Computer-aided diagnosis (CAD) systems stand out as potent aids for physicians in identifying the novel Coronavirus Disease 2019 (COVID-19) through medical imaging modalities. In this paper, we showcase the integration and reliable and fast deployment of a state-of-the-art AI system designed to automatically analyze CT images, offering infection probability for the swift detection of COVID-19. The suggested system, comprising both classification and segmentation components, is anticipated to reduce physicians' detection time and enhance the overall efficiency of COVID-19 detection. We successfully surmounted various challenges, such as data discrepancy and anonymisation, testing the time-effectiveness of the model, and data security, enabling reliable and scalable deployment of the system on both cloud and edge environments. Additionally, our AI system assigns a probability of infection to each 3D CT scan and enhances explainability through anchor set similarity, facilitating timely confirmation and segregation of infected patients by physicians. © 2024 IEEE.",Cloud; COVID-19 Diagnosis; Deep Learning; Edge; Medical Imaging; Microservices; Sandbox,Computerized tomography; Coronavirus; Image segmentation; AI systems; Computer-aided; Coronavirus disease 2019 diagnose; Coronaviruses; CT imaging; Deep learning; Edge; Imaging analysis; Microservice; Sandbox; COVID-19,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85197560143
Angerschmid A.; Zhou J.; Theuermann K.; Chen F.; Holzinger A.,"Angerschmid, Alessa (57575470700); Zhou, Jianlong (35304393400); Theuermann, Kevin (56566592000); Chen, Fang (55714370100); Holzinger, Andreas (23396282000)",57575470700; 35304393400; 56566592000; 55714370100; 23396282000,Fairness and Explanation in AI-Informed Decision Making,2022,Machine Learning and Knowledge Extraction,4,2,,556,579,23,95,10.3390/make4020026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137274188&doi=10.3390%2fmake4020026&partnerID=40&md5=57f74bc2e09b676fabeace5fd46299ce,"AI-assisted decision-making that impacts individuals raises critical questions about transparency and fairness in artificial intelligence (AI). Much research has highlighted the reciprocal relationships between the transparency/explanation and fairness in AI-assisted decision-making. Thus, considering their impact on user trust or perceived fairness simultaneously benefits responsible use of socio-technical AI systems, but currently receives little attention. In this paper, we investigate the effects of AI explanations and fairness on human-AI trust and perceived fairness, respectively, in specific AI-based decision-making scenarios. A user study simulating AI-assisted decision-making in two health insurance and medical treatment decision-making scenarios provided important insights. Due to the global pandemic and restrictions thereof, the user studies were conducted as online surveys. From the participant’s trust perspective, fairness was found to affect user trust only under the condition of a low fairness level, with the low fairness level reducing user trust. However, adding explanations helped users increase their trust in AI-assisted decision-making. From the perspective of perceived fairness, our work found that low levels of introduced fairness decreased users’ perceptions of fairness, while high levels of introduced fairness increased users’ perceptions of fairness. The addition of explanations definitely increased the perception of fairness. Furthermore, we found that application scenarios influenced trust and perceptions of fairness. The results show that the use of AI explanations and fairness statements in AI applications is complex: we need to consider not only the type of explanations and the degree of fairness introduced, but also the scenarios in which AI-assisted decision-making is used. © 2022 by the authors.",AI ethics; AI explanation; AI fairness; perception of fairness; trust,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85137274188
Cameron S.; Franks P.; Hamidzadeh B.,"Cameron, Scott (58838672000); Franks, Pat (55339537100); Hamidzadeh, Babak (7004818707)",58838672000; 55339537100; 7004818707,Positioning Paradata: A Conceptual Frame for AI Processual Documentation in Archives and Recordkeeping Contexts,2023,Journal on Computing and Cultural Heritage,16,4,75,,,,12,10.1145/3594728,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183042379&doi=10.1145%2f3594728&partnerID=40&md5=a6bcd8795bf5d18eed1f04d6e9ed72b2,"The emergence of sophisticated Artificial Intelligence (AI) and machine learning tools poses a challenge to archives and records professionals, who are accustomed to understanding and documenting the activities of human agents rather than the often-opaque processes of sophisticated AI functioning. Preliminary work has proposed the term paradata to describe the unique documentation needs that emerge for archivists using AI tools to process records in their collections. For the purposes of archivists working with AI, paradata is conceptualized here as information recorded and preserved about records’ processing with AI tools; it is a category of data that is defined both by its relationship with other datasets and by the documentary purpose it serves. This article surveys relevant literature across three contexts to scope the relevant scholarship that archivists may draw upon to develop appropriate AI documentation practices. From the statistical social sciences and the visual heritage fields, the article discusses existing definitions of paradata and its ambiguous, often contextually dependent relationship with existing metadata categories. Approaching the problem from a sociotechnical perspective, literature on Explainable Artificial Intelligence (XAI) insists pointedly that explainability be attuned to specific users’ stated needs—needs that archivists may better articulate using the framework of paradata. Most importantly, the article situates AI as a challenge to accountability, transparency, and impartiality in archives by introducing an unfamiliar non-human agency, one that pushes the limits of existing archival practice and demands the development of new concepts and vocabularies to shape future technological and methodological developments in archives. © 2023 Copyright held by the owner/author(s).",accountability; archives; Explainable Artificial Intelligence; metadata; Paradata; processual documentation; records; records management; XAI,Artificial intelligence; Records management; Accountability; Archive; Artificial intelligence tools; Explainable artificial intelligence; Paradata; Processual documentation; Record; Record keeping; Record management; XAI; Metadata,Article,Final,,Scopus,2-s2.0-85183042379
Graefe J.; Rittger L.; Carollo G.; Engelhardt D.; Bengler K.,"Graefe, Julia (57207878859); Rittger, Lena (56162480800); Carollo, Gabriele (58743102400); Engelhardt, Doreen (57280253800); Bengler, Klaus (6507322632)",57207878859; 56162480800; 58743102400; 57280253800; 6507322632,Evaluating the Potential of Interactivity in Explanations for User-Adaptive In-Vehicle Systems – Insights from a Real-World Driving Study,2023,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),14057 LNCS,,,294,312,18,0,10.1007/978-3-031-48047-8_19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178505297&doi=10.1007%2f978-3-031-48047-8_19&partnerID=40&md5=eec3473667ae22a3fe70b132f24dd90e,"Due to advances in artificial intelligence (AI), humans are increasingly facing algorithm-generated content in everyday applications. To avoid threads to the system’s transparency and trustworthiness, the approach of explainable AI (XAI) will play an important role when designing these systems tied to the needs and characteristics of their end-users. Our work investigates explanation strategies for AI-based adaptive in-vehicle systems from a human-centered point of view. We present two explanation concepts: one interactive and one text-based approach. The concepts were evaluated and compared in a real-world driving study with 36 participants. The aim is to assess whether interactive engagement with explanations fosters the system’s understandability and the user’s mental model. Our results did not show significant differences between the concepts. Both groups performed well when assessing their mental model after experiencing the explanation concept. However, we found significant decreases in the mental model when measuring it again after participants experienced the prototypical adaptations of the system during the test drive. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Automotive User Interfaces; Human-AI Interaction; Human-Centered Explainable AI; Real-World Driving Study; User-Adaptive Systems,Adaptive systems; Cognitive systems; Automotive user interface; Automotives; Human-artificial intelligence interaction; Human-centered explainable artificial intelligence; In-vehicle systems; Interactivity; Mental model; Real-world driving study; Real-world drivings; User-adaptive systems; User interfaces,Conference paper,Final,,Scopus,2-s2.0-85178505297
Razavi S.,"Razavi, Saman (39062138000)",39062138000,"Deep learning, explained: Fundamentals, explainability, and bridgeability to process-based modelling",2021,Environmental Modelling and Software,144,,105159,,,,116,10.1016/j.envsoft.2021.105159,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114319353&doi=10.1016%2fj.envsoft.2021.105159&partnerID=40&md5=3d45ea302786d3802f3dfbc27e8e22b8,"Recent breakthroughs in artificial intelligence (AI), and particularly in deep learning (DL), have created tremendous excitement and opportunities in the earth and environmental sciences communities. To leverage these new ‘data-driven’ technologies, however, one needs to understand the fundamental concepts that give rise to DL and how they differ from ‘process-based’, mechanistic modelling. This paper revisits those fundamentals and addresses 10 questions that might be posed by earth and environmental scientists, and with the aid of a real-world modelling experiment, it explains some critical, but often ignored, issues DL may face in practice. The overarching objective is to contribute to a future of AI-assisted earth and environmental sciences where AI models can (1) embrace the typically ignored knowledge base available, (2) function credibly in ‘true’ out-of-sample prediction, and (3) handle non-stationarity in earth and environmental systems. Comparing and contrasting earth and environmental problems with prominent AI applications, such as playing chess and trading in stock markets, provides critical insights for better directing future research in this field. © 2021 The Author(s)",Artificial intelligence; Artificial neural networks; Deep learning; Earth systems; Hydrology; Machine learning; Process-based modelling,Commerce; Deep neural networks; Data driven; Deep learning; Earth systems; Environmental science; Environmental scientists; Fundamental concepts; Mechanistic models; Process-based; Process-based modeling; Science community; artificial intelligence; environmental issue; numerical model; stock market; Knowledge based systems,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85114319353
Le Guillou M.; Prévot L.; Berberian B.,"Le Guillou, Marin (57925495200); Prévot, Laurent (57202367740); Berberian, Bruno (24068318200)",57925495200; 57202367740; 24068318200,Bringing Together Ergonomic Concepts and Cognitive Mechanisms for Human—AI Agents Cooperation,2023,International Journal of Human-Computer Interaction,39,9,,1827,1840,13,10,10.1080/10447318.2022.2129741,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139789673&doi=10.1080%2f10447318.2022.2129741&partnerID=40&md5=cdc3aeac5db43885c96ebc8adc9ad32d,"The deployment of artificial intelligence from experimental settings to concrete applications implies to consider the social aspects of the environment and consequently to conceive the interaction between humans and computers endowed with the aim of being partners in action. This article proposes a review of the research initiatives regarding human-artificial agents interaction, including eXplainable Artificial Intelligence (XAI) and HRI/HCI. We argue that even if vocabulary and approaches are different, the concepts converge on the necessity for the artificial agents to provide an accurate mental model of their behavior to the humans they are interacting with. This has different implications depending on whether we consider a tool/user interaction or a cooperation interaction—which is far less documented despite being at the heart of the future concepts of autonomous vehicles. From this observation, the article uses the cognitive science corpus on joint-action to raise finer cognitive mechanisms proved to be essential for human joint-action which could be considered as cognitive requirements for future artificial agents, including shared task representation and mentalization. Finally, interactions content hypotheses are arisen to satisfy the identified mechanisms, including the ability for the artificial agent to elicit its intentions and to trigger mentalization toward them from the human cooperators. © 2022 Taylor & Francis Group, LLC.",,Artificial intelligence; Agent cooperation; Agent interaction; Artificial agents; Autonomous Vehicles; Cognitive mechanisms; Concrete applications; Joint actions; Mental model; Research initiatives; User interaction; Social aspects,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85139789673
Omeiza D.; Webb H.; Jirotka M.; Kunze L.,"Omeiza, Daniel (57219482072); Webb, Helena (57189389446); Jirotka, Marina (8850301900); Kunze, Lars (36625530100)",57219482072; 57189389446; 8850301900; 36625530100,Explanations in Autonomous Driving: A Survey,2022,IEEE Transactions on Intelligent Transportation Systems,23,8,,10142,10162,20,176,10.1109/TITS.2021.3122865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136123594&doi=10.1109%2fTITS.2021.3122865&partnerID=40&md5=67567b47dbe26b44bd98b02c7dd55c1d,"The automotive industry has witnessed an increasing level of development in the past decades; from manufacturing manually operated vehicles to manufacturing vehicles with a high level of automation. With the recent developments in Artificial Intelligence (AI), automotive companies now employ blackbox AI models to enable vehicles to perceive their environment and make driving decisions with little or no input from a human. With the hope to deploy autonomous vehicles (AV) on a commercial scale, the acceptance of AV by society becomes paramount and may largely depend on their degree of transparency, trustworthiness, and compliance with regulations. The assessment of the compliance of AVs to these acceptance requirements can be facilitated through the provision of explanations for AVs' behaviour. Explainability is therefore seen as an important requirement for AVs. AVs should be able to explain what they have 'seen', done, and might do in environments in which they operate. In this paper, we provide a comprehensive survey of the existing work in explainable autonomous driving. First, we open by providing a motivation for explanations by highlighting the importance of transparency, accountability, and trust in AVs; and examining existing regulations and standards related to AVs. Second, we identify and categorise the different stakeholders involved in the development, use, and regulation of AVs and elicit their AV explanation requirements. Third, we provide a rigorous review of previous work on explanations for the different AV operations (i.e., perception, localisation, planning, vehicle control, and system management). Finally, we discuss pertinent challenges and provide recommendations including a conceptual framework for AV explainability. This survey aims to provide the fundamental knowledge required of researchers who are interested in explanation provisions in autonomous driving.  © 2000-2011 IEEE.",accountability; autonomous vehicles; explainable AI; Explanations; human-machine interaction; intelligent vehicles; regulations; robotics; standards; trust,Automotive industry; Commercial vehicles; Control system synthesis; Human computer interaction; Human robot interaction; Manufacture; Surveys; Transparency; Accountability; Automotive companies; Autonomous driving; Autonomous Vehicles; Explainable artificial intelligence; Explanation; Human machine interaction; Levels of automation; Regulation; Trust; Autonomous vehicles,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85136123594
Chen C.-L.; Golubchik L.; Pal R.,"Chen, Chien-Lun (57192898017); Golubchik, Leana (7004357561); Pal, Ranjan (21743825500)",57192898017; 7004357561; 21743825500,Achieving Transparency Report Privacy in Linear Time,2022,Journal of Data and Information Quality,14,2,8,,,,1,10.1145/3460001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129254825&doi=10.1145%2f3460001&partnerID=40&md5=51ef40acd74172e80f64f4af11290d4c,"An accountable algorithmic transparency report (ATR) should ideally investigate (a) transparency of the underlying algorithm, and (b) fairness of the algorithmic decisions, and at the same time preserve data subjects' privacy. However, a provably formal study of the impact to data subjects' privacy caused by the utility of releasing an ATR (that investigates transparency and fairness), has yet to be addressed in the literature. The far-fetched benefit of such a study lies in the methodical characterization of privacy-utility trade-offs for release of ATRs in public, and their consequential application-specific impact on the dimensions of society, politics, and economics. In this paper, we first investigate and demonstrate potential privacy hazards brought on by the deployment of transparency and fairness measures in released ATRs. To preserve data subjects' privacy, we then propose a linear-time optimal-privacy scheme, built upon standard linear fractional programming (LFP) theory, for announcing ATRs, subject to constraints controlling the tolerance of privacy perturbation on the utility of transparency schemes. Subsequently, we quantify the privacy-utility trade-offs induced by our scheme, and analyze the impact of privacy perturbation on fairness measures in ATRs. To the best of our knowledge, this is the first analytical work that simultaneously addresses trade-offs between the triad of privacy, utility, and fairness, applicable to algorithmic transparency reports.  © 2022 Association for Computing Machinery.",algorithmic transparency; fairness; linear fractional programming; Privacy,Commerce; Data privacy; Economic and social effects; Linear programming; Algorithmic transparency; Algorithmics; Data subjects; Fairness; Fairness measures; Formal studies; Linear fractional programming; Linear time; Privacy; Trade off; Transparency,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85129254825
Jarrahi M.H.; Davoudi V.; Haeri M.,"Jarrahi, Mohammad Hossein (36018165100); Davoudi, Vahid (36809209900); Haeri, Mohammad (57225782548)",36018165100; 36809209900; 57225782548,The key to an effective AI-powered digital pathology: Establishing a symbiotic workflow between pathologists and machine,2022,Journal of Pathology Informatics,13,,100156,,,,8,10.1016/j.jpi.2022.100156,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142832102&doi=10.1016%2fj.jpi.2022.100156&partnerID=40&md5=9e155f5b9efec4f80a300725da4a4920,"Pathology is a fundamental element of modern medicine that determines the final diagnosis of medical conditions, leads medical decisions, and portrays the prognosis. Due to continuous improvements in AI capabilities (e.g., object recognition and image processing), intelligent systems are bound to play a key role in augmenting pathology research and clinical practices. Despite the pervasive deployment of computational approaches in similar fields such as radiology, there has been less success in integrating AI in clinical practices and histopathological diagnosis. This is partly due to the opacity of end-to-end AI systems, which raises issues of interoperability and accountability of medical practices. In this article, we draw on interactive machine learning to take advantage of AI in digital pathology to open the black box of AI and generate a more effective partnership between pathologists and AI systems based on the metaphors of parameterization and implicitization. © 2022 The Authors",Artificial intelligence; Computational pathology; Digital pathology; Explainable AI; Human-in-the-loop; Image analysis,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85142832102
Tinguely P.N.; Lee J.; He V.F.,"Tinguely, Patrick Nicolas (58501776300); Lee, Junghyun (55520846300); He, Vivianna Fang (57204556685)",58501776300; 55520846300; 57204556685,Designing human resource management systems in the age of AI,2023,Journal of Organization Design,12,4,,263,269,6,6,10.1007/s41469-023-00153-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172083461&doi=10.1007%2fs41469-023-00153-x&partnerID=40&md5=ebce8ffe6b6bcb432ef71089218bf273,"The increasing adoption of artificial intelligence (AI) is reshaping the practices of human resource management (HRM). We propose a typology of HR–AI collaboration systems across the dimensions of task characteristics (routine vs. non-routine; low vs. high cognitive complexity) and social acceptability of such systems among organizational members. We discuss how organizations should design HR–AI collaboration systems in light of issues of AI explainability, high stakes contexts, and threat to employees’ professional identities. We point out important design considerations that may affect employees' perceptions of organizational fairness and emphasize HR professionals' role in the design process. We conclude by discussing how our Point of View article contributes to literatures on organization design and human–AI collaboration and suggesting potential avenues for future research. © 2023, The Author(s).",Artificial intelligence; Human resource management; Organization design; Organizational fairness; Social acceptability,,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85172083461
Lai V.; Carton S.; Bhatnagar R.; Liao Q.V.; Zhang Y.; Tan C.,"Lai, Vivian (57206664198); Carton, Samuel (57172288000); Bhatnagar, Rajat (57225066361); Liao, Q. Vera (36095944800); Zhang, Yunfeng (57201641915); Tan, Chenhao (36483489900)",57206664198; 57172288000; 57225066361; 36095944800; 57201641915; 36483489900,Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation,2022,Conference on Human Factors in Computing Systems - Proceedings,,,54,,,,95,10.1145/3491102.3501999,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130569803&doi=10.1145%2f3491102.3501999&partnerID=40&md5=3b52304ff699f31c806124cc5df96f7b,"Despite impressive performance in many benchmark datasets, AI models can still make mistakes, especially among out-of-distribution examples. It remains an open question how such imperfect models can be used effectively in collaboration with humans. Prior work has focused on AI assistance that helps people make individual high-stakes decisions, which is not scalable for a large amount of relatively low-stakes decisions, e.g., moderating social media comments. Instead, we propose conditional delegation as an alternative paradigm for human-AI collaboration where humans create rules to indicate trustworthy regions of a model. Using content moderation as a testbed, we develop novel interfaces to assist humans in creating conditional delegation rules and conduct a randomized experiment with two datasets to simulate in-distribution and out-of-distribution scenarios. Our study demonstrates the promise of conditional delegation in improving model performance and provides insights into design for this novel paradigm, including the effect of AI explanations. © 2022 Owner/Author.",,Artificial intelligence; Benchmark datasets; Case-studies; Imperfect modeling; Large amounts; Modeling performance; Performance; Randomized experiments; Social media; Benchmarking,Conference paper,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85130569803
Beltrán S.; Castro A.; Irizar I.; Naveran G.; Yeregui I.,"Beltrán, Sergio (44361818500); Castro, Alain (56111798500); Irizar, Ion (22985275300); Naveran, Gorka (57224316972); Yeregui, Imanol (57327488200)",44361818500; 56111798500; 22985275300; 57224316972; 57327488200,Framework for collaborative intelligence in forecasting day-ahead electricity price,2022,Applied Energy,306,,118049,,,,28,10.1016/j.apenergy.2021.118049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118701086&doi=10.1016%2fj.apenergy.2021.118049&partnerID=40&md5=e30a237410f3d5de91592efe2cb9ba96,"Electricity price forecasting in wholesale markets is an essential asset for deciding bidding strategies and operational schedules. The decision making process is limited if no understanding is given on how and why such electricity price points have been forecast. The present article proposes a novel framework that promotes human–machine collaboration in forecasting day-ahead electricity price in wholesale markets. The framework is based on a new model architecture that uses a plethora of statistical and machine learning models, a wide range of exogenous features, a combination of several time series decomposition methods and a collection of time series characteristics based on signal processing and time series analysis methods. The model architecture is supported by open-source automated machine learning platforms that provide a baseline reference used for comparison purposes. The objective of the framework is not only to provide forecasts, but to promote a human-in-the-loop approach by providing a data story based on a collection of model-agnostic methods aimed at interpreting the mechanisms and behavior of the new model architecture and its predictions. The framework has been applied to the Spanish wholesale market. The forecasting results show good accuracy on mean absolute error (1.859, 95% HDI [0.575, 3.924] EUR(MWh)−1) and mean absolute scaled error (0.378, 95% HDI [0.091, 0.934]). Moreover, the framework demonstrates its human-centric capabilities by providing graphical and numeric explanations that augments understanding on the model and its electricity price point forecasts. © 2021 The Authors",Augmented analytics; Automated machine learning; Ensemble models; Explainable artificial intelligence; Time series decomposition; Time series hybrid models,Decision making; Machine learning; Power markets; Signal processing; Time series analysis; Augmented analytic; Automated machine learning; Electricity prices; Ensemble models; Explainable artificial intelligence; Hybrid model; Series hybrids; Time series decomposition; Time series hybrid model; Times series; artificial intelligence; decision making; electricity; forecasting method; machine learning; price dynamics; signal processing; strategic approach; Forecasting,Article,Final,,Scopus,2-s2.0-85118701086
Jha S.; Velasquez A.; Ewetz R.; Pullum L.; Jha S.,"Jha, Sumit (57218716753); Velasquez, Alvaro (56404337600); Ewetz, Rickard (55641700700); Pullum, Laura (7004505147); Jha, Susmit (23476883200)",57218716753; 56404337600; 55641700700; 7004505147; 23476883200,ExplainIt! A Tool for Computing Robust Attributions of DNNs,2022,IJCAI International Joint Conference on Artificial Intelligence,,,,5916,5919,3,1,10.24963/ijcai.2022/853,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137926703&doi=10.24963%2fijcai.2022%2f853&partnerID=40&md5=eec5ef0855aa714734a1f92ab60322ba,"Responsible integration of deep neural networks into the design of trustworthy systems requires the ability to explain decisions made by these models. Explainability and transparency are critical for system analysis, certification, and human-machine teaming. We have recently demonstrated that neural stochastic differential equations (SDEs) present an explanation-friendly DNN architecture. In this paper, we present ExplainIt, an online tool for explaining AI decisions that uses neural SDEs to create visually sharper and more robust attributions than traditional residual neural networks. Our tool shows that the injection of noise in every layer of a residual network often leads to less noisy and less fragile integrated gradient attributions. The discrete neural stochastic differential equation model is trained on the ImageNet data set with a million images, and the demonstration produces robust attributions on images in the ImageNet validation library and on a variety of images in the wild. Our online tool is hosted publicly for educational purposes. © 2022 International Joint Conferences on Artificial Intelligence. All rights reserved.",,Deep neural networks; Differential equations; Stochastic models; Data set; Human-machine; Less noisy; Neural-networks; On-line tools; Stochastic differential equation models; Stochastic differential equations; Trustworthy systems; Stochastic systems,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85137926703
Naiseh M.; Soorati M.D.; Ramchurn S.,"Naiseh, Mohammad (57217108046); Soorati, Mohammad D. (56119083700); Ramchurn, Sarvapali (6603260372)",57217108046; 56119083700; 6603260372,Outlining the Design Space of eXplainable Swarm (xSwarm): Experts’ Perspective,2024,Springer Proceedings in Advanced Robotics,28,,,28,41,13,2,10.1007/978-3-031-51497-5_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190140280&doi=10.1007%2f978-3-031-51497-5_3&partnerID=40&md5=b0af7b8aad8c616c0a4279a289e3cae1,"In swarm robotics, agents interact through local roles to solve complex tasks beyond an individual’s ability. Even though swarms are capable of carrying out some operations without the need for human intervention, many safety-critical applications still call for human operators to control and monitor the swarm. There are novel challenges to effective Human-Swarm Interaction (HSI) that are only beginning to be addressed. Explainability is one factor that can facilitate effective and trustworthy HSI and improves the overall performance of Human-Swarm team. Explainability was studied across various Human-AI domains, such as Human-Robot Interaction and Human-Centered ML. However, it is still ambiguous whether explanations studied in Human-AI literature would be beneficial in Human-Swarm research and development. Furthermore, the literature lacks foundational research on the prerequisites for explainability requirements in swarm robotics, i.e., what kind of questions an explainable swarm is expected to answer, and what types of explanations a swarm is expected to generate. By surveying 26 swarm experts, we seek to answer these questions and identify challenges experts faced to generate explanations in Human-Swarm environments. Our work contributes insights into defining a new area of research of eXplainable Swarm (xSwarm) which looks at how explainability can be implemented and developed in swarm systems. This paper opens discussion on xSwarm and paves the way for more research in the field. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Explainable AI; Human-Swarm Interaction; Swarm Robotics,Man machine systems; Safety engineering; Swarm intelligence; Complex task; Control and monitor; Design spaces; Explainable AI; Human intervention; Human operator; Human-swarm interaction; Robotic agents; Safety critical applications; Swarm robotics; Human robot interaction,Conference paper,Final,,Scopus,2-s2.0-85190140280
Zhang N.; Bahsoon R.; Tziritas N.; Theodoropoulos G.,"Zhang, Nan (57669857100); Bahsoon, Rami (6508251119); Tziritas, Nikos (21744114100); Theodoropoulos, Georgios (7006084934)",57669857100; 6508251119; 21744114100; 7006084934,Explainable Human-in-the-Loop Dynamic Data-Driven Digital Twins,2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13984 LNCS,,,233,243,10,4,10.1007/978-3-031-52670-1_23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187794106&doi=10.1007%2f978-3-031-52670-1_23&partnerID=40&md5=27dd299be060d3b1ce083e1681d2022f,"Digital Twins (DT) are essentially dynamic data-driven models that serve as real-time symbiotic “virtual replicas” of real-world systems. DT can leverage fundamentals of Dynamic Data-Driven Applications Systems (DDDAS) bidirectional symbiotic sensing feedback loops for its continuous updates. Sensing loops can consequently steer measurement, analysis and reconfiguration aimed at more accurate modelling and analysis in DT. The reconfiguration decisions can be autonomous or interactive, keeping human-in-the-loop. The trustworthiness of these decisions can be hindered by inadequate explainability of the rationale, and utility gained in implementing the decision for the given situation among alternatives. Additionally, different decision-making algorithms and models have varying complexity, quality and can result in different utility gained for the model. The inadequacy of explainability can limit the extent to which humans can evaluate the decisions, often leading to updates which are unfit for the given situation, erroneous, compromising the overall accuracy of the model. The novel contribution of this paper is an approach to harnessing explainability in human-in-the-loop DDDAS and DT systems, leveraging bidirectional symbiotic sensing feedback. The approach utilises interpretable machine learning and goal modelling to explainability, and considers trade-off analysis of utility gained. We use examples from smart warehousing to demonstrate the approach. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",DDDAS; Digital Twins; Explainability; Human-in-the-loop,Continuous time systems; Economic and social effects; Feedback; Real time systems; Data driven; Data-driven model; Dynamic data; Dynamic Data Driven Application Systems; Explainability; Human-in-the-loop; Loop dynamics; Real- time; Real-world system; Symbiotics; Decision making,Conference paper,Final,,Scopus,2-s2.0-85187794106
Vouros G.A.,"Vouros, George A. (6603683157)",6603683157,Explainable Deep Reinforcement Learning: State of the Art and Challenges,2023,ACM Computing Surveys,55,5,92,,,,66,10.1145/3527448,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146307412&doi=10.1145%2f3527448&partnerID=40&md5=ffb5edf983cae1174db3f221f90141ef,"Interpretability, explainability, and transparency are key issues to introducing artificial intelligence methods in many critical domains. This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability, and fairness, and has important consequences toward keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. Although the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article, we aim to provide a review of state-of-the-art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators - that is, of those who make the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state-of-the-art methods, categorizing them into classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes by identifying open questions and important challenges.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; deep reinforcement learning; explainability; interpretability; transparency,Autonomous agents; Decision making; Deep learning; Ethical technology; Learning systems; Reinforcement learning; Artificial intelligence methods; Deep learning; Deep reinforcement learning; Explainability; Interpretability; Key Issues; Reinforcement learning method; Reinforcement learnings; State of the art; State-of-the-art methods; Transparency,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85146307412
Yablonsky S.,"Yablonsky, Sergey (55865353300)",55865353300,AI-driven platform enterprise maturity: from human led to machine governed,2021,Kybernetes,50,10,,2753,2789,36,21,10.1108/K-06-2020-0384,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107867806&doi=10.1108%2fK-06-2020-0384&partnerID=40&md5=89d0cff440ed979f7a8e1078184c4c9a,"Purpose: To be more effective, artificial intelligence (AI) requires a broad overall view of the design and transformation of enterprise architecture and capabilities. Maturity models (MMs) are the recognized tools to identify strengths and weaknesses of certain domains of an organization. They consist of multiple, archetypal levels of maturity of a certain domain and can be used for organizational assessment and development. In the case of AI, quite a few numbers of MMs have been proposed. Generally, the links between AI technology, AI usage and organizational performance stay unclear. To address these gaps, this paper aims to introduce the complete details of the AI maturity model (AIMM) for AI-driven platform companies. The associated AI-Driven Platform Enterprise Maturity framework proposed here can help to achieve most of the AI-driven platform companies' objectives. Design/methodology/approach: Qualitative research is performed in two stages. In the first stage, a review of the existing literature is performed to identify the types, barriers, drivers, challenges and opportunities of MMs in AI, Advanced Analytics and Big Data domains. In the second stage, a research framework is proposed to align company value chain with AI technologies and levels of the platform enterprise maturity. Findings: The paper proposes a new five level AI-Driven Platform Enterprise Maturity framework by constructing a formal organizational value chain taxonomy model that explains a vast group of MM phenomena related with the AI-Driven Platform Enterprises. In addition, this study proposes a clear and precise description and structuring of the information in the multidimensional Platform, AI, Advanced Analytics and Big Data domains. The AI-Driven Platform Enterprise Maturity framework assists in identification, creation, assessment and disclosure research of AI-driven platform business organizations. Research limitations/implications: This research is focused on the basic dimensions of AI value chain. The full reference model of AI consists of much more concepts. In the last few years, AI has achieved a notable drive that, if connected appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in machine learning, especially in deep neural networks, the entire community stands in front of the barrier of explainability. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models in industry. Our prospects lead toward the concept of a methodology for the large-scale implementation of AI methods in platform organizations with fairness, model explainability and accountability at its core. Practical implications: AI-driven platform enterprise maturity framework can be used for better communicate to clients the value of AI capabilities through the lens of changing human-machine interactions and in the context of legal, ethical and societal norms. Social implications: The authors discuss AI in the enterprise platform stack including talent platform, human capital management and recruiting. Originality/value: The AI value chain and AI-Driven Platform Enterprise Maturity framework are original and represent an effective tools for assessing AI-driven platform enterprises. © 2021, Emerald Publishing Limited.",Advance analytics; AI maturity models; AI-Driven value chains; Artificial intelligence; Big data; Business platform; Machine learning; Platform stack; Technological platform,Advanced Analytics; Big data; Deep neural networks; Digital storage; Design/methodology/approach; Enterprise Architecture; Human machine interaction; Multidimensional platforms; Organizational assessment; Organizational performance; Organizational value chain; Qualitative research; Artificial intelligence,Article,Final,,Scopus,2-s2.0-85107867806
Kisten M.; Ezugwu A.E.-S.; Olusanya M.O.,"Kisten, Melvin (58896939500); Ezugwu, Absalom El-Shamir (56458836000); Olusanya, Micheal O. (55370091600)",58896939500; 56458836000; 55370091600,Explainable Artificial Intelligence Model for Predictive Maintenance in Smart Agricultural Facilities,2024,IEEE Access,12,,,24348,24367,19,15,10.1109/ACCESS.2024.3365586,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185546988&doi=10.1109%2fACCESS.2024.3365586&partnerID=40&md5=7fed6e1bb57f87dd9c0d019c60b917fe,"Artificial Intelligence (AI) in Smart Agricultural Facilities (SAF) often lacks explainability, hindering farmers from taking full advantage of their capabilities. This study tackles this gap by introducing a model that combines eXplainable Artificial Intelligence (XAI), with Predictive Maintenance (PdM). The model aims to provide both predictive insights and explanations across four key dimensions, namely data, model, outcome, and end-user. This approach marks a shift in agricultural AI, reshaping how these technologies are understood and applied. The model outperforms related studies, showing quantifiable improvements. Specifically, the Long-Short-Term Memory (LSTM) classifier shows a 5.81% rise in accuracy. The eXtreme Gradient Boosting (XGBoost) classifier exhibits a 7.09% higher F1 score, 10.66% increased accuracy, and a 4.29% increase in Receiver Operating Characteristic-Area Under the Curve (ROC-AUC). These results could lead to more precise maintenance predictions in real-world settings. This study also provides insights into data purity, global and local explanations, and counterfactual scenarios for PdM in SAF. It advances AI by emphasising the importance of explainability beyond traditional accuracy metrics. The results confirm the superiority of the proposed model, marking a significant contribution to PdM in SAF. Moreover, this study promotes the understanding of AI in agriculture, emphasising explainability dimensions. Future research directions are advocated, including multi-modal data integration and implementing Human-in-the-Loop (HITL) systems aimed at improving the effectiveness of AI and addressing ethical concerns such as Fairness, Accountability, and Transparency (FAT) in agricultural AI applications.  © 2013 IEEE.",Agriculture; deep learning; explainable artificial intelligence; machine learning; predictive maintenance; smart agricultural facilities,Data integration; Learning systems; Long short-term memory; Maintenance; Modal analysis; Boosting; Deep learning; Explainable artificial intelligence; Machine-learning; Predictive maintenance; Predictive models; Smart agricultural facility; Smart agricultures; Agriculture,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85185546988
Cavagnetto N.; Venditti R.; Cocchioni M.; Bonelli S.,"Cavagnetto, Nicola (58061944800); Venditti, Roberto (58638839500); Cocchioni, Matteo (58561156800); Bonelli, Stefano (55751227200)",58061944800; 58638839500; 58561156800; 55751227200,"Demo: SectorX, an en-route ATC simulator for AI-based decision support to Air Traffic Controllers: A case study in the MAHALO project",2023,ACM International Conference Proceeding Series,,,56,,,,0,10.1145/3605390.3610822,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173705907&doi=10.1145%2f3605390.3610822&partnerID=40&md5=d966dc41a2f19933e95d006b1df03ba7,"The EU funded MAHALO Project investigated the effects of transparent and conformal Machine Learning models in support of the safety-critical time-pressured task of Conflict Detection and Resolution for en-route Air Traffic Controllers. The experimental phase was conducted in Italy and Sweden, involving 36 Air Traffic Controllers playing simulated en-route Air Traffic Control scenarios. These scenarios were presented through Sector X, an en-route Air Traffic Control simulator with an ecological User Interface based on the Maastricht Upper Area Control's Controller Working Position. The researchers investigated transparency by introducing three different explainability levels through three visual explanations, and conformance by training with three different conflict resolution styles in the Machine Learning model. The researchers will present the UI designed to achieve transparency to the participants of the demo sessions, who will be able to play Air Traffic Control scenarios.  © 2023 Owner/Author.",Conformance; Explainable AI; Human-AI Teaming; Human-Computer Interaction; Transparency,Air navigation; Air traffic control; Controllers; Decision support systems; Human computer interaction; Machine learning; Safety engineering; User interfaces; Air traffic controller; Case-studies; Conflict detection and resolution; Conformance; Critical time; Decision supports; En-route; Explainable AI; Human-AI teaming; Machine learning models; Transparency,Conference paper,Final,,Scopus,2-s2.0-85173705907
Saranti A.; Hudec M.; Mináriková E.; Takáč Z.; Großschedl U.; Koch C.; Pfeifer B.; Angerschmid A.; Holzinger A.,"Saranti, Anna (36761552200); Hudec, Miroslav (56236748300); Mináriková, Erika (57222276687); Takáč, Zdenko (55546506200); Großschedl, Udo (58029458100); Koch, Christoph (58029458200); Pfeifer, Bastian (56244908600); Angerschmid, Alessa (57575470700); Holzinger, Andreas (23396282000)",36761552200; 56236748300; 57222276687; 55546506200; 58029458100; 58029458200; 56244908600; 57575470700; 23396282000,Actionable Explainable AI (AxAI): A Practical Example with Aggregation Functions for Adaptive Classification and Textual Explanations for Interpretable Machine Learning,2022,Machine Learning and Knowledge Extraction,4,4,,924,953,29,21,10.3390/make4040047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144729975&doi=10.3390%2fmake4040047&partnerID=40&md5=fa969a4713666675ab9e8c756de97854,"In many domains of our daily life (e.g., agriculture, forestry, health, etc.), both laymen and experts need to classify entities into two binary classes (yes/no, good/bad, sufficient/insufficient, benign/malign, etc.). For many entities, this decision is difficult and we need another class called “maybe”, which contains a corresponding quantifiable tendency toward one of these two opposites. Human domain experts are often able to mark any entity, place it in a different class and adjust the position of the slope in the class. Moreover, they can often explain the classification space linguistically—depending on their individual domain experience and previous knowledge. We consider this human-in-the-loop extremely important and call our approach actionable explainable AI. Consequently, the parameters of the functions are adapted to these requirements and the solution is explained to the domain experts accordingly. Specifically, this paper contains three novelties going beyond the state-of-the-art: (1) A novel method for detecting the appropriate parameter range for the averaging function to treat the slope in the “maybe” class, along with a proposal for a better generalisation than the existing solution. (2) the insight that for a given problem, the family of t-norms and t-conorms covering the whole range of nilpotency is suitable because we need a clear “no” or “yes” not only for the borderline cases. Consequently, we adopted the Schweizer–Sklar family of t-norms or t-conorms in ordinal sums. (3) A new fuzzy quasi-dissimilarity function for classification into three classes: Main difference, irrelevant difference and partial difference. We conducted all of our experiments with real-world datasets. © 2022 by the authors.",actionable explainable AI; aggregation functions; classification; continuous XOR-problem; interpretable machine learning; ordinal sums,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85144729975
Kiani M.; Andreu-Perez J.; Hagras H.,"Kiani, Mehrin (57195927735); Andreu-Perez, Javier (55653526300); Hagras, Hani (6701586037)",57195927735; 55653526300; 6701586037,A Temporal Type-2 Fuzzy System for Time-Dependent Explainable Artificial Intelligence,2023,IEEE Transactions on Artificial Intelligence,4,3,,573,586,13,8,10.1109/TAI.2022.3210895,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139437865&doi=10.1109%2fTAI.2022.3210895&partnerID=40&md5=a39be0001b94e5b84ac03f18ecf179bb,"Explainable artificial intelligence (XAI) focuses on transparent AI models and decisions, which are easy to understand, analyze, and augment by a nontechnical audience. Fuzzy logic systems (FLS)-based XAI provides an explainable framework while also modeling uncertainties in real-world environments. However, most real-life processes are not characterized by high uncertainty alone; they are also inherently time dependent, i.e., the processes are time variant. In this work, we present a novel temporal type-2 FLS-based approach for time-dependent XAI (TXAI) systems, which can account for the likelihood of a sample occurrence in the time domain by its the frequency. In the proposed temporal type-2 fuzzy sets (TT2FSs), a 4-D time-dependent membership function integrates the universe of discourse, its membership, and its frequency of occurrence across time. The TXAI system manifested better classification prowess in cross-validation tests, with a mean recall of 95.40% than a standard XAI system (based on nontemporal general type-2 fuzzy sets) that had a mean recall of 87.04%. TXAI also performed significantly better than most nonexplainable AI systems, with between 3.95% and 19.04% improvement gain in mean recall. In addition, TXAI can also outline the most likely time-dependent trajectories using the frequency and time dimensions embedded in the TXAI model; viz. given a rule at a determined time interval, what will be the next most likely rule at a subsequent time interval. In this regard, the proposed TXAI system can have profound implications for delineating the evolution of real-life time-dependent processes, such as behavioral or biological processes.  © 2020 IEEE.",Artificial intelligence; explainable artificial intelligence; fuzzy systems; human computer interaction; human in the loop; time-varying; trusted computing,Artificial intelligence; Classification (of information); Computer circuits; Fuzzy sets; Membership functions; Temperature measurement; Time domain analysis; Uncertainty analysis; Computational modelling; Fuzzy logic system; Fuzzy-Logic; Modeling uncertainties; Most likely; Time dependent; Time interval; Type-2 fuzzy set; Type-2 fuzzy systems; Uncertainty; Fuzzy logic,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85139437865
Henin C.; Le Métayer D.,"Henin, Clément (57221977502); Le Métayer, Daniel (7003713333)",57221977502; 7003713333,Beyond explainability: justifiability and contestability of algorithmic decision systems,2022,AI and Society,37,4,,1397,1410,13,27,10.1007/s00146-021-01251-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111488372&doi=10.1007%2fs00146-021-01251-8&partnerID=40&md5=2b2ed463315c011db6d04c0cc91ae48b,"In this paper, we point out that explainability is useful but not sufficient to ensure the legitimacy of algorithmic decision systems. We argue that the key requirements for high-stakes decision systems should be justifiability and contestability. We highlight the conceptual differences between explanations and justifications, provide dual definitions of justifications and contestations, and suggest different ways to operationalize justifiability and contestability. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Algorithmic decision system; Challenge; Contestation; Evidence; Explanation; Justification; Machine learning,Algorithmic decision system; Algorithmics; Challenge; Contestation; Decision systems; Evidence; Explanation; Justification; Machine-learning,Article,Final,,Scopus,2-s2.0-85111488372
Qian P.; Unhelkar V.,"Qian, Peizhu (57806716500); Unhelkar, Vaibhav (56086482900)",57806716500; 56086482900,Evaluating the Role of Interactivity on Improving Transparency in Autonomous Agents,2022,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",2,,,1083,1091,8,12,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134321454&partnerID=40&md5=0663b43de829093145a69430de431914,"Autonomous agents are increasingly being deployed amongst human end-users. Yet, human users often have little knowledge of how these agents work or what they will do next. This lack of transparency has already resulted in unintended consequences during AI use: a concerning trend which is projected to increase with the proliferation of autonomous agents. To curb this trend and ensure safe use of AI, assisting users in establishing an accurate understanding of agents that they work with is essential. In this work, we present AI Teacher, a user-centered Explainable AI framework to address this need for autonomous agents that follow a Markovian policy. Our framework first computes salient instructions of agent behavior by estimating a user's mental model and utilizing algorithms for sequential decision-making. Next, in contrast to existing solutions, these instructions are presented interactively to the end-users, thereby enabling a personalized approach to improving AI transparency. We evaluate our framework, with emphasis on its interactive features, through experiments with human participants. The experiment results suggest that, relative to non-interactive approaches, interactive teaching can both reduce the amount of time it takes for humans to create accurate mental models of these agents and is subjectively preferred by human users. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved",Explainable AI; Human-AI Collaboration; Machine Teaching; Monte-Carlo Tree Search; Shared Mental Models,Autonomous agents; Behavioral research; Cognitive systems; Decision making; Multi agent systems; End-users; Explainable AI; Human users; Human-AI collaboration; Interactivity; Mental model; Monte-carlo tree search; Shared mental model; Tree-search; Unintended consequences; Transparency,Conference paper,Final,,Scopus,2-s2.0-85134321454
Ball B.; Koliousis A.,"Ball, Brian (55536509900); Koliousis, Alexandros (22034841700)",55536509900; 22034841700,Training philosopher engineers for better AI,2023,AI and Society,38,2,,861,868,7,1,10.1007/s00146-022-01535-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134644710&doi=10.1007%2fs00146-022-01535-7&partnerID=40&md5=39cf732c772a57756254d976686b509d,"There is a deluge of AI-assisted decision-making systems, where our data serve as proxy to our actions, suggested by AI. The closer we investigate our data (raw input, or their learned representations, or the suggested actions), we begin to discover “bugs”. Outside of their test, controlled environments, AI systems may encounter situations investigated primarily by those in other disciplines, but experts in those fields are typically excluded from the design process and are only invited to attest to the ethical features of the resulting system or to comment on demonstrations of intelligence and aspects of craftmanship after the fact. This communicative impasse must be overcome. Our idea is that philosophical and engineering considerations interact and can be fruitfully combined in the AI design process from the very beginning. We embody this idea in the role of a philosopher engineer. We discuss the role of philosopher engineers in the three main design stages of an AI system: deployment management (what is the system’s intended use, in what environment?); objective setting (what should the system be trained to do, and how?); and training (what model should be used, and why?). We then exemplify the need for philosopher engineers with an illustrative example, investigating how the future decisions of an AI-based hiring system can be fairer than those contained in the biased input data on which it is trained; and we briefly sketch the kind of interdisciplinary education that we envision will help to bring about better AI. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Explainability; Fairness; Inductive bias; Interdisciplinary training; Societal bias,Decision making; Personnel training; Philosophical aspects; AI systems; Controlled environment; Craftmanship; Decision-making systems; Design-process; Explainability; Fairness; Inductive bias; Interdisciplinary training; Societal bias; Engineers,Article,Final,,Scopus,2-s2.0-85134644710
Ramesh D.; Kameswaran V.; Wang D.; Sambasivan N.,"Ramesh, Divya (57219108034); Kameswaran, Vaishnav (57190061358); Wang, Ding (57324460400); Sambasivan, Nithya (22836402400)",57219108034; 57190061358; 57324460400; 22836402400,How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India,2022,ACM International Conference Proceeding Series,,,,1917,1928,11,26,10.1145/3531146.3533237,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132969228&doi=10.1145%2f3531146.3533237&partnerID=40&md5=fba658cd91ef2c69eec8e03761ebc93e,"Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a g high-risk' AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the g boon' of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond. © 2022 Owner/Author.",algorithmic accountability; algorithmic fairness; human-ai interaction; instant loans; socio-technical systems,Transparency; Algorithmic accountability; Algorithmic fairness; Algorithmics; Human-ai interaction; Instant loan; Policy makers; Power relations; Shape algorithmics; Sociotechnical systems; Sensitive data,Conference paper,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85132969228
Bacula A.; Mercer J.; Berger J.; Adams J.; Knight H.,"Bacula, Alexandra (57191976691); Mercer, Jason (57216343953); Berger, Jaden (57215292375); Adams, Julie (35546747200); Knight, Heather (26321722700)",57191976691; 57216343953; 57215292375; 35546747200; 26321722700,Integrating Robot Manufacturer Perspectives into Legible Factory Robot Light Communications,2023,ACM Transactions on Human-Robot Interaction,12,1,13,,,,3,10.1145/3570732,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149017305&doi=10.1145%2f3570732&partnerID=40&md5=3566cc2090ce5c6e3617511585a5c254,"In a world with increasing numbers of robots operating in everyday human spaces, the employees at this robotics company are pioneers, with intelligent point-to-point path planning and autonomous transport operations in 150+ factory and warehouse locations in North America. At the time of research, this robotics company consisted of 250 employees. Unlike other industry models, their robots are designed to operate with people in mixed human-machine spaces, yet no HRI style evaluations had previously been run with their robots. As early observers of how factory workers and transport robot interact, across varied job roles ranging from technology design to customer relations, this work sought to leverage employee knowledge and experiences to identify opportunities for improving the communication capabilities of the robots, resulting in the addition of several robot state communications to their initial software set leveraging both employee- and social robotics literature- sourced ideas for communicating with lights. To achieve this a social robotics researcher spent a summer onsite at the robotics company, getting to know their software stack and culture. Her research activities included: (1) a company-wide survey relative to the robot's light, sound, and motion communications was sent out and analyzed, (2) the development of three new light sets (car-like, sweeping, heartbeat) and five overall states (blocked, at goal, turning, idle), and (3) a user study evaluating the developed light sets relative to the current robot default light patterns, all significantly improving the overall legibility of the targeted robot state communications: at goal, blocked, turning, and idle. Our initial findings advance knowledge in which style of light patterns is best for different communication states, showing that eye-catching lights are best for high urgency states, such as blocked, and subtle lights are best for low urgency states, such as idle. Finally, the latest software release for this robot has deployed a subset of these light patterns to all of their currently operating client sites, i.e., anyone who updates their robots to the latest release will benefit from these research results. This deployment sets the ground for future researchers exploring how end-users at different sites have responded to the new, more communicative light patterns.  © 2023 Association for Computing Machinery.",expressive lights; Factory robotics; social robotics,Human robot interaction; Intelligent robots; Knowledge management; Machine design; Personnel; Public relations; Robot programming; Expressive light; Factory robotic; Human-machine; Light patterns; Robot manufacturers; Social robotics; Technology designs; Transport operations; Warehouse location; Workers'; Motion planning,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85149017305
Majumdar S.,"Majumdar, Subhabrata (55972063800)",55972063800,"Fairness, explainability, privacy, and robustness for trustworthy algorithmic decision-making",2022,"Big Data Analytics in Chemoinformatics and Bioinformatics: with Applications to Computer-Aided Drug Design, Cancer Biology, Emerging Pathogens and Computational Toxicology",,,,61,95,34,8,10.1016/B978-0-323-85713-0.00017-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144531723&doi=10.1016%2fB978-0-323-85713-0.00017-7&partnerID=40&md5=e8914a7b48833c4bb88f82bd1bda15ca,"With the rapid increase in the use and deployment of machine learning (ML) systems in the world, concomitant concerns on the ethical implications of their downstream effect have surfaced in recent years. Responding to this challenge, the field of trustworthy ML has grown rapidly and resulted in a large body of methods and algorithms that embody desirable qualities such as fairness, transparency, privacy, and robustness. In this chapter, we survey the current landscape of trustworthy ML methods, introduce fundamental concepts, and summarize research directions. To bridge the gap between theory and practice, we provide implementation details of each category of methods that are currently available publicly. © 2023 Elsevier Inc. All rights reserved.",adversarial robustness; algorithmic fairness; differential privacy; explainable AI; ML bias; Trustworthy machine learning,,Book chapter,Final,,Scopus,2-s2.0-85144531723
Dargaud L.; Ibsen M.; Tapia J.; Busch C.,"Dargaud, Laurine (58107441400); Ibsen, Mathias (57222871782); Tapia, Juan (7005419930); Busch, Christoph (7101767185)",58107441400; 57222871782; 7005419930; 7101767185,A Principal Component Analysis-Based Approach for Single Morphing Attack Detection,2023,"Proceedings - 2023 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops, WACVW 2023",,,,683,692,9,7,10.1109/WACVW58289.2023.00075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148323034&doi=10.1109%2fWACVW58289.2023.00075&partnerID=40&md5=43f4b01dcc900bfa9686aa8cd94f1913,"This paper proposes an explicit method for single face image morphing attack detection, using an RGB decomposition based on Principal Component Analysis from texture patterns. Handcrafted detection algorithms can be advantageous over deep learning-based methods as they constitute increased explainability, showcased in this work by visualizing relevant face areas for morphing attack detection. Such information can be relevant for deployed systems in real-world scenarios with humans in the loop. The morphing detection capability of the proposed method is evaluated extensively across three datasets and six morphing algorithms in single, cross-dataset and cross-morphed scenarios and compared to a fine-tuned MobileNetV2 architecture. The results show how single image morphing attack detection remains challenging, especially in cross-domain scenarios involving realistic diversity of morphing algorithms, including StyleGAN-based approaches. In such conditions, the proposed method can be as good or even better than the evaluated MobileNetV2 approach.  © 2023 IEEE.",,Computer vision; Deep learning; Textures; Analysis-based approaches; Attack detection; Detection algorithm; Explicit method; Face images; Image morphing; Morphing; Morphing algorithms; Principal-component analysis; Texture patterns; Principal component analysis,Conference paper,Final,,Scopus,2-s2.0-85148323034
Vasu B.; Hu B.; Dong B.; Collins R.; Hoogs A.,"Vasu, Bhavan (57203991018); Hu, Brian (56640331800); Dong, Bo (57226606667); Collins, Roddy (7403347573); Hoogs, Anthony (6603254299)",57203991018; 56640331800; 57226606667; 7403347573; 6603254299,"Explainable, interactive content-based image retrieval",2021,Applied AI Letters,2,4,e41,,,,4,10.1002/ail2.41,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159929554&doi=10.1002%2fail2.41&partnerID=40&md5=44f221818a7127d4a54e434497c8d0b4,"Quantifying the value of explanations in a human-in-the-loop (HITL) system is difficult. Previous methods either measure explanation-specific values that do not correspond to user tasks and needs or poll users on how useful they find the explanations to be. In this work, we quantify how much explanations help the user through a utility-based paradigm that measures change in task performance when using explanations vs not. Our chosen task is content-based image retrieval (CBIR), which has well-established baselines and performance metrics independent of explainability. We extend an existing HITL image retrieval system that incorporates user feedback with similarity-based saliency maps (SBSM) that indicate to the user which parts of the retrieved images are most similar to the query image. The system helps the user understand what it is paying attention to through saliency maps, and the user helps the system understand their goal through saliency-guided relevance feedback. Using the MS-COCO dataset, a standard object detection and segmentation dataset, we conducted extensive, crowd-sourced experiments validating that SBSM improves interactive image retrieval. Although the performance increase is modest in the general case, in more difficult cases such as cluttered scenes, using explanations yields an 6.5% increase in accuracy. To the best of our knowledge, this is the first large-scale user study showing that visual saliency map explanations improve performance on a real-world, interactive task. Our utility-based evaluation paradigm is general and potentially applicable to any task for which explainability can be incorporated. © 2021 Kitware, Inc. Applied AI Letters published by John Wiley & Sons Ltd.",explainable AI; image retrieval; saliency; user study,,Letter,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85159929554
Ott T.; Dabrock P.,"Ott, Tabea (57887370400); Dabrock, Peter (22955337200)",57887370400; 22955337200,Transparent human – (non-) transparent technology? The Janus-faced call for transparency in AI-based health care technologies,2022,Frontiers in Genetics,13,,902960,,,,2,10.3389/fgene.2022.902960,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137847247&doi=10.3389%2ffgene.2022.902960&partnerID=40&md5=234618c3906f48b6ba7e9687ffc82e51,"The use of Artificial Intelligence and Big Data in health care opens up new opportunities for the measurement of the human. Their application aims not only at gathering more and better data points but also at doing it less invasive. With this change in health care towards its extension to almost all areas of life and its increasing invisibility and opacity, new questions of transparency arise. While the complex human-machine interactions involved in deploying and using AI tend to become non-transparent, the use of these technologies makes the patient seemingly transparent. Papers on the ethical implementation of AI plead for transparency but neglect the factor of the “transparent patient” as intertwined with AI. Transparency in this regard appears to be Janus-faced: The precondition for receiving help - e.g., treatment advice regarding the own health - is to become transparent for the digitized health care system. That is, for instance, to donate data and become visible to the AI and its operators. The paper reflects on this entanglement of transparent patients and (non-) transparent technology. It argues that transparency regarding both AI and humans is not an ethical principle per se but an infraethical concept. Further, it is no sufficient basis for avoiding harm and human dignity violations. Rather, transparency must be enriched by intelligibility following Judith Butler’s use of the term. Intelligibility is understood as an epistemological presupposition for recognition and the ensuing humane treatment. Finally, the paper highlights ways to testify intelligibility in dealing with AI in health care ex ante, ex post, and continuously. Copyright © 2022 Ott and Dabrock.",AI; Data; Ethics; Health Care; Infraethics; Intelligibility; Learning Systems; Transparency,adult; article; artificial intelligence; big data; ethics; health care system; human; human dignity; learning; neglect; speech intelligibility,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85137847247
Keppel J.; Liebers J.; Auda J.; Gruenefeld U.; Schneegass S.,"Keppel, Jonas (57913904500); Liebers, Jonathan (57214449262); Auda, Jonas (56104286300); Gruenefeld, Uwe (56426574900); Schneegass, Stefan (55317907700)",57913904500; 57214449262; 56104286300; 56426574900; 55317907700,ExplAInable Pixels: Investigating One-Pixel Attacks on Deep Learning Models with Explainable Visualizations,2022,ACM International Conference Proceeding Series,,,,231,242,11,2,10.1145/3568444.3568469,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145875220&doi=10.1145%2f3568444.3568469&partnerID=40&md5=99a50f6b53efc10555c615af7e7f2a1e,"Nowadays, deep learning models enable numerous safety-critical applications, such as biometric authentication, medical diagnosis support, and self-driving cars. However, previous studies have frequently demonstrated that these models are attackable through slight modifications of their inputs, so-called adversarial attacks. Hence, researchers proposed investigating examples of these attacks with explainable artificial intelligence to understand them better. In this line, we developed an expert tool to explore adversarial attacks and defenses against them. To demonstrate the capabilities of our visualization tool, we worked with the publicly available CIFAR-10 dataset and generated one-pixel attacks. After that, we conducted an online evaluation with 16 experts. We found that our tool is usable and practical, providing evidence that it can support understanding, explaining, and preventing adversarial examples.  © 2022 ACM.",adversarial examples; explainability; human-in-The-loop; one-pixel attacks,Deep learning; Diagnosis; Learning systems; Safety engineering; Visualization; Adversarial example; Biometric authentication; Explainability; Human-in-the-loop; Learning models; Medical diagnosis support; On-line evaluation; One-pixel attack; Safety critical applications; Visualization tools; Pixels,Conference paper,Final,,Scopus,2-s2.0-85145875220
Robertson S.; Díaz M.,"Robertson, Samantha (57219758688); Díaz, Mark (36678677500)",57219758688; 36678677500,Understanding and Being Understood: User Strategies for Identifying and Recovering From Mistranslations in Machine Translation-Mediated Chat,2022,ACM International Conference Proceeding Series,,,,2223,2238,15,10,10.1145/3531146.3534638,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130777908&doi=10.1145%2f3531146.3534638&partnerID=40&md5=d830539398d328a34550a97006814500,"Machine translation (MT) is now widely and freely available, and has the potential to greatly improve cross-lingual communication. In order to use MT reliably and safely, end users must be able to assess the quality of system outputs and determine how much they can rely on them to guide their decisions and actions. However, it can be difficult for users to detect and recover from mistranslations due to limited language skills. In this work we collected 19 MT-mediated role-play conversations in housing and employment scenarios, and conducted in-depth interviews to understand how users identify and recover from translation errors. Participants communicated using four language pairs: English, and one of Spanish, Farsi, Igbo, or Tagalog. We conducted qualitative analysis to understand user challenges in light of limited system transparency, strategies for recovery, and the kinds of translation errors that proved more or less difficult for users to overcome. We found that users broadly lacked relevant and helpful information to guide their assessments of translation quality. Instances where a user erroneously thought they had understood a translation correctly were rare but held the potential for serious consequences in the real world. Finally, inaccurate and disfluent translations had social consequences for participants, because it was difficult to discern when a disfluent message was reflective of the other person's intentions, or an artifact of imperfect MT. We draw on theories of grounding and repair in communication to contextualize these findings, and propose design implications for explainable AI (XAI) researchers, MT researchers, as well as collaboration among them to support transparency and explainability in MT. These directions include handling typos and non-standard grammar common in interpersonal communication, making MT in interfaces more visible to help users evaluate errors, supporting collaborative repair of conversation breakdowns, and communicating model strengths and weaknesses to users. © 2022 Owner/Author.",computer-mediated communication; explainable machine learning; human-AI interaction; machine translation,Computational linguistics; Computer aided language translation; Errors; Human computer interaction; Machine learning; Machine translation; Transparency; User interfaces; Computer-mediated communication; Cross-lingual communication; End-users; Explainable machine learning; Human-AI interaction; Machine translations; Machine-learning; Role-plays; System output; User strategies; Recovery,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85130777908
Nelekar S.; Abdulrahman A.; Gupta M.; Richards D.,"Nelekar, Shreeya (57377532900); Abdulrahman, Amal (57205076336); Gupta, Manik (57756887900); Richards, Deborah (57193711592)",57377532900; 57205076336; 57756887900; 57193711592,Effectiveness of embodied conversational agents for managing academic stress at an Indian University (ARU) during COVID-19,2022,British Journal of Educational Technology,53,3,,491,511,20,19,10.1111/bjet.13174,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121470125&doi=10.1111%2fbjet.13174&partnerID=40&md5=5ae6f5ea07f2647a05e6454e7808e598,"Stress has become one of the major reasons for many mental health related issues among students of all age groups, which has resulted in devastating personal losses including suicide. Societal and familial pressure to succeed is high, particularly in developing countries where education is highly valued as a key enabler. As part of stress management during the COVID-19 pandemic, demand for online intelligent virtual advisors has risen and, consequently, the need for personalised explanation that is culturally sensitive to the user's context is essential to improve the user's understanding of and trust in the recommendations provided by the virtual advisor. This paper presents the mAnaging stRess at University embodied conversational agent (ECA) that has been adapted for Indian university students from an explainable agent that was found to help Western students reduce their stress by providing study tips with explanations based on the student's beliefs and/or goals. We conducted a research study with sixty students which measured the impact of providing three different patterns of tailored explanations (belief-based, goal-based, and belief and goal-based explanation) on the students' intentions to change the recommended behaviours and the relationship built with the ECA. The experimental results indicate that there was stress reduction across all student groups provided with different types of explanations. Further, the students showed trust and a good working alliance with the conversational agent, along with an intention to change behaviour across all types of explanations. However, it was observed that the user context played an important role in behaviour change intention and hence explanations could be tailored further, making them culturally more relevant to Indian students. Practitioner notes What is already known about this topic Embodied conversational agents (ECAs) have been mostly developed, applied and shown to be effective in developed countries. Hence, their design and development are mostly guided by the intended user's needs and preferences. In a Western context, ECAs have been found to be beneficial for reducing study stress in university students. There is a pertinent need for use of low cost, effective technology that can aid academic stress reduction in higher educational institutions in developing countries owing to their high youth populations, lack of adequate mental healthcare facilities and associated social stigma. What this paper adds The adaptation and use of ECAs to reduce study stress in higher education students in a developing country is evaluated. The ECA technology is adapted for an Indian context in terms of its physical appearance, colour, speech dialect and dialog content so that it is culturally more aligned to the target population. The ECA engages in an empathic conversation tailored for the Indian students and their COVID-19 context providing them with explanation-backed behaviour recommendations that take their beliefs and goals into account. The ECA provides three types of explanation: belief-only; goal-only; and both belief and goal. Results of a study carried out in an Indian university with 61 students, randomly assigned to one of the explanation types, to capture their demographics, study stress statistics, behaviour change intentions and trust/working alliance with the conversational agent. The major findings include stress reduction across all explanation groups, development of a positive relationship between the ECA and the students regardless of its explanation pattern, and changes in behaviour intentions across all types of explanations for all recommended behaviours. However, differences in change intentions for certain behaviours indicate further tailoring of explanations is required based on the user context. Implications for practice and/or policy The ECA technology has shown promise in terms of stress reduction amongst Indian students. Higher Education Institutions in developing countries could utilise low-cost and widely accessible ECAs to overcome lack of access to human-based support and reluctance to use available services due to stigmatized attitudes to mental health issues. This technology can be further improved and deployed into a larger number of Indian educational institutions leading to a widespread impact on overall student health and wellbeing. Digital technologies to support mental health have become more prominent during the COVID-19 pandemic, at least in Western countries. The ECA technology evaluated in our study demonstrates its viability and potential value for use in developing countries, with appropriate tailoring. © 2021 British Educational Research Association.",artificial intelligence; behavior change; embodied conversational agents; human-machine interface; learner attitudes/perceptions; stress management; undergraduate education; virtual assistant,Artificial intelligence; Developing countries; E-learning; Human computer interaction; Man machine systems; Risk management; Virtual reality; Behaviour changes; Embodied conversational agent; Human Machine Interface; Learner attitude/perception; Learner's attitudes; Mental health; Stress management; Stress reduction; Undergraduate education; Virtual assistants; Students,Article,Final,,Scopus,2-s2.0-85121470125
Lebovitz S.; Lifshitz-Assaf H.; Levina N.,"Lebovitz, Sarah (57191161594); Lifshitz-Assaf, Hila (56711400300); Levina, Natalia (8779298100)",57191161594; 56711400300; 8779298100,To Engage or Not to Engage with AI for Critical Judgments: How Professionals Deal with Opacity When Using AI for Medical Diagnosis,2022,Organization Science,33,1,,126,148,22,238,10.1287/ORSC.2021.1549,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125569707&doi=10.1287%2fORSC.2021.1549&partnerID=40&md5=23c2dfe4c60b500c31012b4000cf5177,"Artificial intelligence (AI) technologies promise to transform how professionals conduct knowledge work by augmenting their capabilities for making professional judgments. We know little, however, about how human-AI augmentation takes place in practice. Yet, gaining this understanding is particularly important when professionals use AI tools to form judgments on critical decisions. We conducted an in-depth field study in a major U.S. hospital where AI tools were used in three departments by diagnostic radiologists making breast cancer, lung cancer, and bone age determinations. The study illustrates the hindering effects of opacity that professionals experienced when using AI tools and explores how these professionals grappled with it in practice. In all three departments, this opacity resulted in professionals experiencing increased uncertainty because AI tool results often diverged from their initial judgment without providing underlying reasoning. Only in one department (of the three) did professionals consistently incorporate AI results into their final judgments, achieving what we call engaged augmentation. These professionals invested in AI interrogation practices-practices enacted by human experts to relate their own knowledge claims to AI knowledge claims. Professionals in the other two departments did not enact such practices and did not incorporate AI inputs into their final decisions, which we call unengaged “augmentation.” Our study unpacks the challenges involved in augmenting professional judgment with powerful, yet opaque, technologies and contributes to literature on AI adoption in knowledge work. © 2022 INFORMS",Artificial intelligence; Augmentation; Decision making; Expertise; Explainability; Innovation; Medical diagnosis; Opacity; Professional judgment; Technology adoption and use; Transparency; Uncertainty,,Review,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85125569707
Recki L.; Esau-Held M.; Lawo D.; Stevens G.,"Recki, Lena (58574883100); Esau-Held, Margarita (57212454329); Lawo, Dennis (57193532321); Stevens, Gunnar (8908623500)",58574883100; 57212454329; 57193532321; 8908623500,"AI said, She said - How Users Perceive Consumer Scoring in Practice",2023,ACM International Conference Proceeding Series,,,,149,160,11,0,10.1145/3603555.3603562,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171163858&doi=10.1145%2f3603555.3603562&partnerID=40&md5=7d55de9e887f24804f31f389ef2e11a2,"As digitization continues, consumers are increasingly exposed to AI scoring decisions. However, currently lacking is a thorough understanding of how users' misjudgments of an AI-supported system lead to it being rejected. Therefore, investigations are needed into the appropriation of such socio-technical systems in practice and how users describe their experience with algorithm-based scoring. To address this issue, we evaluated 1,003 user reviews of an app on car insurance that calculates premiums based on the consumers' individual driving behavior. We find evidence that users develop their own folk theories to explain the algorithms with the help of situation-related experiences and that insufficient explanations lead to power asymmetries between consumers, the system, and the company. In particular, as a result of the different needs of the stakeholders, we uncover a fundamental conflict between computational risk assessment and the perceived agency to influence the score.  © 2023 ACM.",Algorithmic Decision Making; Empirical study; Explainable AI; Fairness; Perception,Computation theory; Consumer behavior; Risk assessment; Algorithmic decision making; Algorithmics; Decisions makings; Digitisation; Empirical studies; Explainable AI; Exposed to; Fairness; Sociotechnical systems; User reviews; Decision making,Conference paper,Final,,Scopus,2-s2.0-85171163858
Kim S.S.Y.; Watkins E.A.; Russakovsky O.; Fong R.; Monroy-Hernández A.,"Kim, Sunnie S. Y. (57221605355); Watkins, Elizabeth Anne (57193833255); Russakovsky, Olga (36352471800); Fong, Ruth (57200621318); Monroy-Hernández, Andrés (23973552900)",57221605355; 57193833255; 36352471800; 57200621318; 23973552900,"""Help Me Help the AI"": Understanding How Explainability Can Support Human-AI Interaction",2023,Conference on Human Factors in Computing Systems - Proceedings,,,250,,,,104,10.1145/3544548.3581001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152299586&doi=10.1145%2f3544548.3581001&partnerID=40&md5=357a3d0006ebef8761dbd422c3f9dee9,"Despite the proliferation of explainable AI (XAI) methods, little is understood about end-users' explainability needs and behaviors around XAI explanations. To address this gap and contribute to understanding how explainability can support human-AI interaction, we conducted a mixed-methods study with 20 end-users of a real-world AI application, the Merlin bird identification app, and inquired about their XAI needs, uses, and perceptions. We found that participants desire practically useful information that can improve their collaboration with the AI, more so than technical system details. Relatedly, participants intended to use XAI explanations for various purposes beyond understanding the AI's outputs: calibrating trust, improving their task skills, changing their behavior to supply better inputs to the AI, and giving constructive feedback to developers. Finally, among existing XAI approaches, participants preferred part-based explanations that resemble human reasoning and explanations. We discuss the implications of our findings and provide recommendations for future XAI design. © 2023 Owner/Author.",Explainable AI (XAI); Human-AI Collaboration; Human-AI Interaction; Human-Centered XAI; Interpretability; Local Explanations; XAI for Computer Vision,Human computer interaction; User interfaces; End-users; Explainable AI (XAI); Human-AI collaboration; Human-AI interaction; Human-centered XAI; Interpretability; Local explanation; Mixed method; Real-world; XAI for computer vision; Computer vision,Conference paper,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85152299586
Hassan A.; Abdulhak M.A.A.; Sulaiman R.B.; Kahtan H.,"Hassan, Ali (56770579200); Abdulhak, Mansoor Abdullateef Abdulgabber (58159704300); Sulaiman, Riza Bin (6602275590); Kahtan, Hasan (55600272800)",56770579200; 58159704300; 6602275590; 55600272800,User centric explanations: A breakthrough for explainable models,2021,"2021 International Conference on Information Technology, ICIT 2021 - Proceedings",,,9491641,702,707,5,4,10.1109/ICIT52682.2021.9491641,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112177882&doi=10.1109%2fICIT52682.2021.9491641&partnerID=40&md5=7b9459571ec0379673b4765a55181b63,"Thanks to recent developments in explainable Deep Learning models, researchers have shown that these models can be incredibly successful and provide encouraging results. However, a lack of model interpretability can hinder the efficient implementation of Deep Learning models in real-world applications. This has encouraged researchers to develop and design a large number of algorithms to support transparency. Although studies have raised awareness of the importance of explainable artificial intelligence, the question of how to solve the needs of real users to understand artificial intelligence remains unanswered. In this paper, we provide an overview of the current state of the research field at Human-Centered Machine Learning and new methods for user-centric explanations for deep learning models. Furthermore, we outline future directions for interpretable machine learning and discuss the challenges facing this research field, as well as the importance and motivation behind developing user-centric explanations for Deep Learning models. © 2021 IEEE.",explainable artificial intelligence; human-AI interaction; machine learning,Deep learning; Efficient implementation; Interpretability; Learning models; Real-world; Research fields; User-centric; Learning systems,Conference paper,Final,,Scopus,2-s2.0-85112177882
Alm C.O.; Alvarez A.; Font J.; Liapis A.; Pederson T.; Salo J.,"Alm, Cecilia Ovesdotter (13608169500); Alvarez, Alberto (57204416263); Font, José (56344008800); Liapis, Antonios (53264261900); Pederson, Thomas (23390386800); Salo, Johan (53868183800)",13608169500; 57204416263; 56344008800; 53264261900; 23390386800; 53868183800,"Invisible AI-driven HCI Systems - When, Why and How",2020,ACM International Conference Proceeding Series,,,,,,,2,10.1145/3419249.3420099,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123040047&doi=10.1145%2f3419249.3420099&partnerID=40&md5=cc5ca128c605ef629c8da5fc6c7b3939,"The InvisibleAI (InvAI'20) workshop aims to systematically discuss a growing class of interactive systems that invisibly remove some decision-making tasks away from humans to machines, based on recent advances in artificial intelligence (AI), data science, and sensor or actuation technology. While the interest in the affordances as well as the risks of hidden pervasive AI are high on the agenda in public debate, discussion on the topic is needed within the human-computer interaction (HCI) community. In particular, we want to gather insights, ideas, and models for approaching the use of barely noticeable AI decision-making in systems design from a human-centered perspective, so as to make the most out of the automated systems and algorithms that support human activity both as designers and users. Concurrently, these systems should safeguard that humans remain in charge when it counts (high stakes decisions, privacy, monitoring lack of explainability and fairness, etc.). What to automate and what not to automate is often a system designer's choice [8]. By taking the established concept of explicit interaction between a system and its user as a point of departure, and inviting authors to provide examples from their own research, we aim to stimulate dynamic discussion while keeping the workshop concrete and system design-focused. The workshop especially directs itself to participants from the interaction design, AI, and HCI communities. The targeted scientific outcome of the workshop is an up-to-date ontology of invisible AI-HCI systems and hybrid human-AI collaboration mechanisms, and approaches. Additionally, we expect that the workgroups and the roundtables will provide starting points shaping continued discussions, new collaborations, and innovative scientific contributions that springboard from the workgroups' findings. The focus of the proposed workshop involves the bridging of two spaces of computational research that impact user experiences and societal domains (HCI and AI). Thus, the proposed workshop topic aligns well with the theme of this year's NordiCHI conference which is Shaping Experiences, Shaping Society. © 2020 Owner/Author.",artificial intelligence; Human-computer interaction; human-machine collaboration,Automation; Data Science; Decision making; Human computer interaction; Privacy by design; Public risks; Systems analysis; User experience; User interfaces; Actuation technologies; Collaboration mechanisms; Computational researches; Human computer interaction (HCI); Interaction design; Interactive system; Point of departures; Scientific contributions; Artificial intelligence,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85123040047
Gee A.H.; Garcia-Olano D.; Ghosh J.; Paydarfar D.,"Gee, Alan H. (57038174600); Garcia-Olano, Diego (57173021200); Ghosh, Joydeep (35556611300); Paydarfar, David (6701618628)",57038174600; 57173021200; 35556611300; 6701618628,Explaining deep classification of time-series data with learned prototypes,2019,CEUR Workshop Proceedings,2429,,,15,22,7,32,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071664014&partnerID=40&md5=7ac5451acedcfebb59b81e2e19358207,"The emergence of deep learning networks raises a need for explainable AI so that users and domain experts can be confident applying them to high-risk decisions. In this paper, we leverage data from the latent space induced by deep learning models to learn stereotypical representations or ""prototypes"" during training to elucidate the algorithmic decision-making process. We study how leveraging prototypes effect classification decisions of two dimensional time-series data in a few different settings: (1) electrocardiogram (ECG) waveforms to detect clinical bradycardia, a slowing of heart rate, in preterm infants, (2) respiration waveforms to detect apnea of prematurity, and (3) audio waveforms to classify spoken digits. We improve upon existing models by optimizing for increased prototype diversity and robustness, visualize how these prototypes in the latent space are used by the model to distinguish classes, and show that prototypes are capable of learning features on two dimensional time-series data to produce explainable insights during classification tasks. We show that the prototypes are capable of learning real-world features - bradycardia in ECG, apnea in respiration, and articulation in speech - as well as features within sub-classes. Our novel work leverages learned prototypical framework on two dimensional time-series data to produce explainable insights during classification tasks. © 2019 for this paper by its authors.",,Decision making; Deep learning; Electrocardiography; Health care; Time series; Classification decision; Classification tasks; Decision making process; Deep classifications; Learning models; Learning network; Respiration waveform; Time-series data; Classification (of information),Conference paper,Final,,Scopus,2-s2.0-85071664014
Neves I.; Folgado D.; Santos S.; Barandas M.; Campagner A.; Ronzio L.; Cabitza F.; Gamboa H.,"Neves, Inês (57223353618); Folgado, Duarte (57201637199); Santos, Sara (57215774162); Barandas, Marília (56436894500); Campagner, Andrea (57195064969); Ronzio, Luca (57222340475); Cabitza, Federico (16199544700); Gamboa, Hugo (57200265948)",57223353618; 57201637199; 57215774162; 56436894500; 57195064969; 57222340475; 16199544700; 57200265948,Interpretable heartbeat classification using local model-agnostic explanations on ECGs,2021,Computers in Biology and Medicine,133,,104393,,,,70,10.1016/j.compbiomed.2021.104393,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105692908&doi=10.1016%2fj.compbiomed.2021.104393&partnerID=40&md5=af03380d665e7ac5f319cbacd40fa127,"Treatment and prevention of cardiovascular diseases often rely on Electrocardiogram (ECG) interpretation. Dependent on the physician's variability, ECG interpretation is subjective and prone to errors. Machine learning models are often developed and used to support doctors; however, their lack of interpretability stands as one of the main drawbacks of their widespread operation. This paper focuses on an Explainable Artificial Intelligence (XAI) solution to make heartbeat classification more explainable using several state-of-the-art model-agnostic methods. We introduce a high-level conceptual framework for explainable time series and propose an original method that adds temporal dependency between time samples using the time series' derivative. The results were validated in the MIT-BIH arrhythmia dataset: we performed a performance's analysis to evaluate whether the explanations fit the model's behaviour; and employed the 1-D Jaccard's index to compare the subsequences extracted from an interpretable model and the XAI methods used. Our results show that the use of the raw signal and its derivative includes temporal dependency between samples to promote classification explanation. A small but informative user study concludes this study to evaluate the potential of the visual explanations produced by our original method for being adopted in real-world clinical settings, either as diagnostic aids or training resource. © 2021 Elsevier Ltd",Electrocardiogram; Explainable artificial intelligence; Heartbeat classification; Human–AI interfaces; Machine learning; Model-agnostic method; Time series; Usability; Visual explanations,"Arrhythmias, Cardiac; Artificial Intelligence; Electrocardiography; Heart Rate; Humans; Machine Learning; Diseases; Electrocardiography; Machine learning; Cardiovascular disease; Explainable artificial intelligence; Heartbeat classifications; Human–AI interface; Local model; Machine learning models; Machine-learning; Model-agnostic method; Times series; Visual explanation; Article; artificial intelligence; binary classification; clinical article; conceptual framework; controlled study; data classification; detection algorithm; electrocardiogram; evaluation study; explainable artificial intelligence; heart arrhythmia; heart beat; heart ventricle extrasystole; human; image segmentation; Jaccard index; machine learning; model agnostic method; multiclass classification; priority journal; receiver operating characteristic; statistical model; taxonomy; time series analysis; usability; electrocardiography; heart arrhythmia; heart rate; Time series",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85105692908
Lin J.; Pan S.; Lee C.S.; Oviatt S.,"Lin, Jionghao (57211753281); Pan, Shirui (55522732400); Lee, Cheng Siong (57211951305); Oviatt, Sharon (7003592124)",57211753281; 55522732400; 57211951305; 7003592124,An explainable deep fusion network for affect recognition using physiological signals,2019,"International Conference on Information and Knowledge Management, Proceedings",,,,2069,2072,3,73,10.1145/3357384.3358160,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075478890&doi=10.1145%2f3357384.3358160&partnerID=40&md5=6132d67a7408138e61bf339ab82d2746,"Affective computing is an emerging research area which provides insights on human's mental state through human-machine interaction. During the interaction process, bio-signal analysis is essential to detect human affective changes. Currently, machine learning methods to analyse bio-signals are the state of the art to detect the affective states, but most empirical works mainly deploy traditional machine learning methods rather than deep learning models due to the need for explainability. In this paper, we propose a deep learning model to process multimodal-multisensory bio-signals for affect recognition. It supports batch training for different sampling rate signals at the same time, and our results show significant improvement compared to the state of the art. Furthermore, the results are interpreted at the sensor- and signal- level to improve the explainaibility of our deep learning model. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Affect recognition; Deep learning; Explainability; Multimodal fusion,Biomedical signal processing; Knowledge management; Machine learning; Affect recognition; Affective Computing; Explainability; Human machine interaction; Interaction process; Machine learning methods; Multi-modal fusion; Physiological signals; Deep learning,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85075478890
Subramonyam H.; Seifert C.; Adar E.,"Subramonyam, Hariharan (57056308000); Seifert, Colleen (35606631400); Adar, Eytan (8395017700)",57056308000; 35606631400; 8395017700,ProtoAI: Model-Informed Prototyping for AI-Powered Interfaces,2021,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,48,58,10,34,10.1145/3397481.3450640,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104556035&doi=10.1145%2f3397481.3450640&partnerID=40&md5=8fba24c315ca423713d673744baa3b9c,"When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. However, AI poses challenges to design due to its dynamic behavior in response to training data, end-user data, and feedback. Designers must consider AI's uncertainties and offer adaptations such as explainability, error recovery, and automation vs. human task control. Unfortunately, current prototyping tools assume a black-box view of AI, forcing designers to work with separate tools to explore machine learning models, understand model performance, and align interface choices with model behavior. This introduces friction to rapid and iterative prototyping. We propose Model-Informed Prototyping (MIP), a workflow for AIX design that combines model exploration with UI prototyping tasks. Our system, ProtoAI, allows designers to directly incorporate model outputs into interface designs, evaluate design choices across different inputs, and iteratively revise designs by analyzing model breakdowns. We demonstrate how ProtoAI can readily operationalize human-AI design guidelines. Our user study finds that designers can effectively engage in MIP to create and evaluate AI-powered interfaces during AIX design.  © 2021 ACM.",AI-Powered Interfaces; Design-by-Instance; Human-Centered AI,Artificial intelligence; Dynamic behaviors; Interface designers; Interface designs; Iterative prototyping; Machine learning models; Model performance; Modeling behavior; Prototyping tools; User interfaces,Conference paper,Final,,Scopus,2-s2.0-85104556035
Rožanec J.M.; Zajec P.; Kenda K.; Novalija I.; Fortuna B.; Mladenić D.; Veliou E.; Papamartzivanos D.; Giannetsos T.; Menesidou S.A.; Alonso R.; Cauli N.; Recupero D.R.; Kyriazis D.; Sofianidis G.; Theodoropoulos S.; Soldatos J.,"Rožanec, Jože M. (57217150010); Zajec, Patrik (57223729060); Kenda, Klemen (24724773000); Novalija, Inna (36551531500); Fortuna, Blaž (55925609700); Mladenić, Dunja (6602880697); Veliou, Entso (57219174848); Papamartzivanos, Dimitrios (57144675900); Giannetsos, Thanassis (24605106000); Menesidou, Sofia Anna (36337929900); Alonso, Rubén (58184893400); Cauli, Nino (54792741300); Recupero, Diego Reforgiato (57206674454); Kyriazis, Dimosthenis (16301182100); Sofianidis, Georgios (59810392200); Theodoropoulos, Spyros (57223759269); Soldatos, John (8662280800)",57217150010; 57223729060; 24724773000; 36551531500; 55925609700; 6602880697; 57219174848; 57144675900; 24605106000; 36337929900; 58184893400; 54792741300; 57206674454; 16301182100; 59810392200; 57223759269; 8662280800,STARdom: An Architecture for Trusted and Secure Human-Centered Manufacturing Systems,2021,IFIP Advances in Information and Communication Technology,633 IFIP,,,199,207,8,11,10.1007/978-3-030-85910-7_21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115251296&doi=10.1007%2f978-3-030-85910-7_21&partnerID=40&md5=c71fecc8144710df8de74258e20292a5,"There is a lack of a single architecture specification that addresses the needs of trusted and secure Artificial Intelligence systems with humans in the loop, such as human-centered manufacturing systems at the core of the evolution towards Industry 5.0. To realize this, we propose an architecture that integrates forecasts, Explainable Artificial Intelligence, supports collecting users’ feedback and uses Active Learning and Simulated Reality to enhance forecasts and provide decision-making recommendations. The architecture security is addressed at all levels. We align the proposed architecture with the Big Data Value Association Reference Architecture Model. We tailor it for the domain of demand forecasting and validate it on a real-world case study. © 2021, IFIP International Federation for Information Processing.",Active learning; Demand forecasting; Explainable Artificial Intelligence (XAI); Industry 4.0; Smart manufacturing,Architecture; Decision making; Forecasting; Industrial management; Manufacture; Architecture specification; Artificial intelligence systems; Decision making recommendations; Demand forecasting; Human-centered manufacturing; Proposed architectures; Reference architecture; Simulated reality; Artificial intelligence,Conference paper,Final,,Scopus,2-s2.0-85115251296
van de Poel I.,"van de Poel, Ibo (6602685731)",6602685731,Embedding Values in Artificial Intelligence (AI) Systems,2020,Minds and Machines,30,3,,385,409,24,149,10.1007/s11023-020-09537-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090214633&doi=10.1007%2fs11023-020-09537-4&partnerID=40&md5=e6dfeba44f1e6bf7a4d43b13321bc393,"Organizations such as the EU High-Level Expert Group on AI and the IEEE have recently formulated ethical principles and (moral) values that should be adhered to in the design and deployment of artificial intelligence (AI). These include respect for autonomy, non-maleficence, fairness, transparency, explainability, and accountability. But how can we ensure and verify that an AI system actually respects these values? To help answer this question, I propose an account for determining when an AI system can be said to embody certain values. This account understands embodied values as the result of design activities intended to embed those values in such systems. AI systems are here understood as a special kind of sociotechnical system that, like traditional sociotechnical systems, are composed of technical artifacts, human agents, and institutions but—in addition—contain artificial agents and certain technical norms that regulate interactions between artificial agents and other elements of the system. The specific challenges and opportunities of embedding values in AI systems are discussed, and some lessons for better embedding values in AI systems are drawn. © 2020, The Author(s).",Artificial agent; Artificial intelligence; Ethics; Institution; Multi-agent system; Norms; Sociotechnical system; Value embedding; Values,Embeddings; AI systems; Artificial agents; Design activity; Ethical principles; Human agent; Sociotechnical systems; Technical artifacts; Technical norms; Artificial intelligence,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85090214633
,,,"20th IFIP WG 6.11 Conference on e-Business, e-Services and e-Society, I3E 2021",2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12896 LNCS,,,,,786,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115191822&partnerID=40&md5=dd33c6f1dc80f55f3f9baa7b5865bb76,"The proceedings contain 65 papers. The special focus in this conference is on e-Business, e-Services and e-Society. The topics include: Data-Driven Collaborative Human-AI Decision Making; Always Trust the Advice of AI in Difficulties? Perceptions Around AI in Decision Making; big Data Analytics Affordances for Social Innovation: A Theoretical Framework; COVID-19 Discrepancies Rising from Population Density Political Polarization Exacerbates Policy Gap; Ethics and AI Issues: Old Container with New Wine?; governing Artificial Intelligence and Algorithmic Decision Making: Human Rights and Beyond; Analysing AI via Husserl and Kuhn How a Phenomenological Approach to Artificial Intelligence Imposes a Paradigm Shift; the Ethical Implications of Lawtech; Deploying AI Governance Practices: A Revelatory Case Study; AI in the Workplace: Exploring Chatbot Use and Users’ Emotions; Towards Ecosystems for Responsible AI: Expectations on Sociotechnical Systems, Agendas, and Networks in EU Documents; Ethics in AI: A Software Developmental and Philosophical Perspective; stop Ordering Machine Learning Algorithms by Their Explainability! An Empirical Investigation of the Tradeoff Between Performance and Explainability; Gender Bias in AI: Implications for Managerial Practices; a Systematic Review of Fairness in Artificial Intelligence Algorithms; is Downloading This App Consistent with My Values?: Conceptualizing a Value-Centered Privacy Assistant; operationalization of a Glass Box Through Visualization: Applied to a Data Driven Profiling Approach; artificial Intelligence and the Evolution of Managerial Skills: An Exploratory Study; the Diffusion of Innovation Experience: Leveraging the Human Factor to Improve Technological Adoption Within an Organisation; exploring the Link Between Digitalization and Sustainable Development: Research Agendas; industry 4.0 and Organisations: Key Organisational Capabilities.",,,Conference review,Final,,Scopus,2-s2.0-85115191822
van der Waa J.; Verdult S.; van den Bosch K.; van Diggelen J.; Haije T.; van der Stigchel B.; Cocu I.,"van der Waa, Jasper (57193858656); Verdult, Sabine (57208165034); van den Bosch, Karel (35103709000); van Diggelen, Jurriaan (8833914200); Haije, Tjalling (57224478717); van der Stigchel, Birgit (57224475306); Cocu, Ioana (57224486755)",57193858656; 57208165034; 35103709000; 8833914200; 57224478717; 57224475306; 57224486755,Moral Decision Making in Human-Agent Teams: Human Control and the Role of Explanations,2021,Frontiers in Robotics and AI,8,,640647,,,,19,10.3389/frobt.2021.640647,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107645105&doi=10.3389%2ffrobt.2021.640647&partnerID=40&md5=9e403f9aa57a6d99eef76a6c7b85f5f0,"With the progress of Artificial Intelligence, intelligent agents are increasingly being deployed in tasks for which ethical guidelines and moral values apply. As artificial agents do not have a legal position, humans should be held accountable if actions do not comply, implying humans need to exercise control. This is often labeled as Meaningful Human Control (MHC). In this paper, achieving MHC is addressed as a design problem, defining the collaboration between humans and agents. We propose three possible team designs (Team Design Patterns), varying in the level of autonomy on the agent’s part. The team designs include explanations given by the agent to clarify its reasoning and decision-making. The designs were implemented in a simulation of a medical triage task, to be executed by a domain expert and an artificial agent. The triage task simulates making decisions under time pressure, with too few resources available to comply with all medical guidelines all the time, hence involving moral choices. Domain experts (i.e., health care professionals) participated in the present study. One goal was to assess the ecological relevance of the simulation. Secondly, to explore the control that the human has over the agent to warrant moral compliant behavior in each proposed team design. Thirdly, to evaluate the role of agent explanations on the human’s understanding in the agent’s reasoning. Results showed that the experts overall found the task a believable simulation of what might occur in reality. Domain experts experienced control over the team’s moral compliance when consequences were quickly noticeable. When instead the consequences emerged much later, the experts experienced less control and felt less responsible. Possibly due to the experienced time pressure implemented in the task or over trust in the agent, the experts did not use explanations much during the task; when asked afterwards they however considered these to be useful. It is concluded that a team design should emphasize and support the human to develop a sense of responsibility for the agent’s behavior and for the team’s decisions. The design should include explanations that fit with the assigned team roles as well as the human cognitive state. © Copyright © 2021 van der Waa, Verdult, van den Bosch, van Diggelen, Haije, van der Stigchel and Cocu.",artificial intelligence; ethical AI; explainable AI; human study; human-agent teaming; meaningful human control; moral AI; team design patterns,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85107645105
Qian K.; Popa L.; Sen P.,"Qian, Kun (56727570300); Popa, Lucian (7006538930); Sen, Prithviraj (57201526762)",56727570300; 7006538930; 57201526762,SystemER: A humanintheloop system for explainable entity resolution,2018,Proceedings of the VLDB Endowment,12,12,,1794,1797,3,16,10.14778/3352063.3352068,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074532449&doi=10.14778%2f3352063.3352068&partnerID=40&md5=a88ca915f6f348a4b095ff38f5a9fc97,"Entity Resolution (ER) is the task of identifying different representations of the same real-world object. To achieve scalability and the desired level of quality, the typical ER pipeline includes multiple steps that may involve low-level coding and extensive human labor. We present SystemER, a tool for learning explainable ER models that reduces the human labor all throughout the stages of the ER pipeline. SystemER achieves explainability by learning rules that not only perform a given ER task but are human-comprehensible; this provides transparency into the learning process, and further enables verification and customization of the learned model by the domain experts. By leveraging a human in the loop and active learning, SystemER also ensures that a small number of labeled examples is sufficient to learn high-quality ER models. SystemER is a fulledged tool that includes an easy to use interface, support for both flat files and semi-structured data, and scale-out capabilities by distributing computation via Apache Spark. © 2019 VLDB Endowment.",,Pipelines; Active Learning; Domain experts; Entity resolutions; Human-in-the-loop; Learning process; Learning rules; Real-world objects; Semi structured data; Learning systems,Conference paper,Final,,Scopus,2-s2.0-85074532449
Zhang Z.T.; HuÃ mann H.,"Zhang, Zelun Tony (57226128028); HuÃ mann, Heinrich (57226098627)",57226128028; 57226098627,How to Manage Output Uncertainty: Targeting the Actual End User Problem in Interactions with AI,2021,CEUR Workshop Proceedings,2903,,,,,,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110534161&partnerID=40&md5=45ef43fd6796cf3a8c88c4f6295d9464,"Given the opaqueness and complexity of modern AI algorithms, there is currently a strong focus on developing transparent and explainable AI, especially in high-stakes domains. We claim that opaqueness and complexity are not the core issues for end users when interacting with AI. Instead, we propose that the output uncertainty inherent to AI systems is the actual problem, with opaqueness and complexity as contributing factors. Transparency and explainability should therefore not be the end goals, as such a focus tends to place the human into a passive supervisory role in what is in reality an algorithm-centered system design. To enable effective management of output uncertainty, we believe it is necessary to focus on truly human-centered AI designs that keep the human in an active role of control. We discuss the conceptual implications of such a shift in focus and give examples from literature to illustrate the more holistic, interactive designs that we envision. © 2021 Copyright for this paper by its authors.",Explainability; Human-ai interaction; Intelligent systems; Output uncertainty; Transparency; User control,User interfaces; AI algorithms; AI systems; Contributing factor; Effective management; End users; Interactive design; Artificial intelligence,Conference paper,Final,,Scopus,2-s2.0-85110534161
Lage I.; Doshi-Velez F.; Lifschitz D.; Amir O.,"Lage, Isaac (57208445031); Doshi-Velez, Finale (34874672900); Lifschitz, Daphna (57211744093); Amir, Ofra (55023457200)",57208445031; 34874672900; 57211744093; 55023457200,Toward robust policy summarization,2019,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",4,,,2081,2083,2,11,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069868681&partnerID=40&md5=aa27f20c7d8b7522f484beff1ce8af29,"AI agents are being developed to help people with high stakes decision-making processes from driving cars to prescribing drugs. It is therefore becoming increasingly important to develop ""explainable AI"" methods that help people understand the behavior of such agents. Summaries of agent policies can help human users anticipate agent behavior and facilitate more effective collaboration. Prior work has framed agent summarization as a machine teaching problem where examples of agent behavior are chosen to maximize reconstruction quality under the assumption that people do inverse reinforcement learning to infer an agent's policy from demonstrations. We compare summaries generated under this assumption to summaries generated under the assumption that people use imitation learning. We show through simulations that in some domains, there exist summaries that produce high-quality reconstructions under different models, but in other domains, only matching the summary extraction model to the reconstruction model produces high-quality reconstructions. These results highlight the importance of assuming correct computational models for how humans extrapolate from a summary, suggesting human-in-the-loop approaches to summary extraction. © 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",Explainable AI; Policy summarization,Behavioral research; Decision making; Extraction; Inverse problems; Multi agent systems; Reinforcement learning; Repair; Computational model; Decision making process; High quality reconstruction; Human-in-the-loop; Imitation learning; Inverse reinforcement learning; Reconstruction quality; Teaching problems; Autonomous agents,Conference paper,Final,,Scopus,2-s2.0-85069868681
Hoffman R.R.; Klein G.; Mueller S.T.,"Hoffman, Robert R. (7402764832); Klein, Gary (57202862987); Mueller, Shane T. (24765109700)",7402764832; 57202862987; 24765109700,"Explaining explanation for ""explainable AI",2018,Proceedings of the Human Factors and Ergonomics Society,1,,,197,201,4,74,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072724450&partnerID=40&md5=ddd1ae7deed5ea1f065ddf2818765134,"What makes for an explanation of ""black box"" AI systems such as Deep Nets? We reviewed the pertinent literatures on explanation and derived key ideas. This set the stage for our empirical inquiries, which include conceptual cognitive modeling, the analysis of a corpus of cases of ""naturalistic explanation"" of computational systems, computational cognitive modeling, and the development of measures for performance evaluation. The purpose of our work is to contribute to the program of research on ""Explainable AI."" In this report we focus on our initial synthetic modeling activities and the development of measures for the evaluation of explainability in human-machine work systems. © 2018 Human Factors an Ergonomics Society Inc.. All rights reserved.",,Ergonomics; AI systems; Black boxes; Cognitive model; Computational cognitive modeling; Computational system; Human-machine; Synthetic models; Work system; Cognitive systems,Conference paper,Final,,Scopus,2-s2.0-85072724450
Shen H.; Liao K.; Liao Z.; Doornberg J.; Qiao M.; Van Den Hengel A.; Verjans J.W.,"Shen, Haifeng (9336109400); Liao, Kewen (36170545500); Liao, Zhibin (57204064101); Doornberg, Job (57201000145); Qiao, Maoying (57150222700); Van Den Hengel, Anton (35587677100); Verjans, Johan W. (6602444569)",9336109400; 36170545500; 57204064101; 57201000145; 57150222700; 35587677100; 6602444569,Human-AI Interactive and Continuous Sensemaking: A Case Study of Image Classification using Scribble Attention Maps,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,10,10.1145/3411763.3451798,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105775455&doi=10.1145%2f3411763.3451798&partnerID=40&md5=90e11ee29f0c40ac8e3a7da8fc2bff24,"Advances in Artificial Intelligence (AI), especially the stunning achievements of Deep Learning (DL) in recent years, have shown AI/DL models possess remarkable understanding towards the logic reasoning behind the solved tasks. However, human understanding towards what knowledge is captured by deep neural networks is still elementary and this has a detrimental effect on human's trust in the decisions made by AI systems. Explainable AI (XAI) is a hot topic in both AI and HCI communities in order to open up the blackbox to elucidate the reasoning processes of AI algorithms in such a way that makes sense to humans. However, XAI is only half of human-AI interaction and research on the other half - human's feedback on AI explanations together with AI making sense of the feedback - is generally lacking. Human cognition is also a blackbox to AI and effective human-AI interaction requires unveiling both blackboxes to each other for mutual sensemaking. The main contribution of this paper is a conceptual framework for supporting effective human-AI interaction, referred to as interactive and continuous sensemaking (HAICS). We further implement this framework in an image classification application using deep Convolutional Neural Network (CNN) classifiers as a browser-based tool that displays network attention maps to the human for explainability and collects human's feedback in the form of scribble annotations overlaid onto the maps. Experimental results using a real-world dataset has shown significant improvement of classification accuracy (the AI performance) with the HAICS framework. © 2021 ACM.",attention map; explainable AI; image classification; interactive sensemaking; scribble interaction,Classification (of information); Computation theory; Convolutional neural networks; Deep learning; Deep neural networks; Human engineering; AI algorithms; Classification accuracy; Conceptual frameworks; Human cognition; Human understanding; Logic reasoning; Reasoning process; Sensemaking; Image classification,Conference paper,Final,,Scopus,2-s2.0-85105775455
Krishnan J.; Coronado P.; Reed T.,"Krishnan, Jitin (57208507509); Coronado, Patrick (7004866149); Reed, Trevor (57208511466)",57208507509; 7004866149; 57208511466,Seva: A systems engineer’s virtual assistant,2019,CEUR Workshop Proceedings,2350,,,,,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064968994&partnerID=40&md5=d6af8f4e57864861809ba87ceb2d2063,"A Systems Engineer’s Virtual Assistant (SEVA) is a novel attempt to bridge the gap between Natural Language Processing (NLP), Knowledge Base (KB) Construction research, and NASA’s Systems Engineering domain. In this work, we propose the design of an explainable, human-in-the-loop, and interactive personal assistant system. The assistant will help a Systems Engineer in their daily work environment through complex information management and high-level question-answering to augment their problem-solving abilities. We describe the fundamental characteristics of the assistant by understanding operational, functional, and system requirements from Systems Engineers and NASA’s Systems Engineering Handbook. The assistant is designed to act as a workbench to manage dynamic information about projects and analyze hypothetical scenarios. It is also designed to make logical inferences and perform temporal reasoning by handling domain information and information related to schedule and resources. In addition, the system learns new information over time by interacting with its user and can perform case-based reasoning from previous experiences. The knowledge base design describes a novel hybrid approach to build a domain-independent common-sense framework with which domain-specific engineers can attune it and build their projects. Using these specific objectives and constraints, the architecture of a personal assistant is proposed. Main contributions of this design paper are Systems Engineering (SE) domain analysis, a survey of existing research, preliminary experiments using the state-of-the-art systems to explore the feasibility, a proposal of a complete architecture with component level detail, and identification of areas that require further research and development. Copyright held by the author(s).",Explainable AI; Intelligent Agents; Knowledge Base Construction; Natural Language Processing; Ontology; Question-Answering; Systems Engineering,Bridges; Case based reasoning; Engineering education; Engineers; Information management; Intelligent agents; Knowledge based systems; Knowledge engineering; Learning algorithms; Machine learning; NASA; Ontology; Springs (components); Systems engineering; Fundamental characteristics; Knowledge base designs; Knowledge-base construction; NAtural language processing; Problem-solving abilities; Question Answering; Research and development; State-of-the-art system; Natural language processing systems,Conference paper,Final,,Scopus,2-s2.0-85064968994
Alexandrov N.M.,"Alexandrov, Natalia M. (7004299616)",7004299616,Explainable AI decisions for human-autonomy interactions,2017,"17th AIAA Aviation Technology, Integration, and Operations Conference, 2017",,,,,,7,10,10.2514/6.2017-3991,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085850591&doi=10.2514%2f6.2017-3991&partnerID=40&md5=cb81b95dbe5686a86b83c2f0c3a3b714,"Autonomous systems governed by a variety of adaptive algorithms are making an appearance in safety-critical and time-critical domains, such as automobile traffic and aviation. Until autonomous systems are proven and perceived to be as or more adaptable than humans, and resilient in the face of unanticipated faults and variable conditions, humans will have to remain in ultimate control of decision-making, while supported by machine-based information and advice. Human-machine interaction in many domains has numerous well-known difficulties, including lack or excess of trust, both of which can lead to serious problems, especially when human decision makers are overwhelmed with information. Interactions between humans and autonomous systems-a subset of general human-machine interactions--will be more problematic still. One complication is that sophisticated machine learning systems produce outputs that may be difficult for a human user to interpret. The development of trust has been a major motivation for the area of explainable AI, or XAI. We offer initial observations about the relation between trust and explicability and propose candidate methods for formalizing these notions in technical domains. © 2017, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.",,Adaptive algorithms; Decision making; Man machine systems; Safety engineering; Automobile traffic; Autonomous systems; Human decisions; Human machine interaction; Human users; Sophisticated machines; Time-critical domains; Variable conditions; Learning systems,Conference paper,Final,,Scopus,2-s2.0-85085850591
Pan M.; Huang W.; Li Y.; Zhou X.; Luo J.,"Pan, Menghai (57200450171); Huang, Weixiao (57211978130); Li, Yanhua (56193387600); Zhou, Xun (57211748095); Luo, Jun (56463258200)",57200450171; 57211978130; 56193387600; 57211748095; 56463258200,XGAIL: Explainable Generative Adversarial Imitation Learning for Explainable Human Decision Analysis,2020,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,,,,1334,1343,9,31,10.1145/3394486.3403186,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090418654&doi=10.1145%2f3394486.3403186&partnerID=40&md5=52047ca2260182747f032111d168524f,"To make daily decisions, human agents devise their own ""strategies"" governing their mobility dynamics (e.g., taxi drivers have preferred working regions and times, and urban commuters have preferred routes and transit modes). Recent research such as generative adversarial imitation learning (GAIL) demonstrates successes in learning human decision-making strategies from their behavior data using deep neural networks (DNNs), which can accurately mimic how humans behave in various scenarios, e.g., playing video games, etc. However, such DNN-based models are ""black box"" models in nature, making it hard to explain what knowledge the models have learned from human, and how the models make such decisions, which was not addressed in the literature of imitation learning. This paper addresses this research gap by proposing xGAIL, the first explainable generative adversarial imitation learning framework. The proposed xGAIL framework consists of two novel components, including Spatial Activation Maximization (SpatialAM) and Spatial Randomized Input Sampling Explanation (SpatialRISE), to extract both global and local knowledge from a well-trained GAIL model that explains how a human agent makes decisions. Especially, we take taxi drivers' passenger-seeking strategy as an example to validate the effectiveness of the proposed xGAIL framework. Our analysis on a large-scale real-world taxi trajectory data shows promising results from two aspects: i) global explainable knowledge of what nearby traffic condition impels a taxi driver to choose a particular direction to find the next passenger, and ii) local explainable knowledge of what key (sometimes hidden) factors a taxi driver considers when making a particular decision. © 2020 ACM.",explainable artificial intelligence; generative adversarial imitation learning; human behavior analysis,Behavioral research; Decision making; Deep learning; Deep neural networks; Taxicabs; Human decision making; Imitation learning; Local knowledge; Novel component; Preferred routes; Recent researches; Traffic conditions; Trajectory data; Data mining,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85090418654
Knight H.; Simmons R.,"Knight, Heather (26321722700); Simmons, Reid (7402334420)",26321722700; 7402334420,Laban head-motions convey robot state: A call for robot body language,2016,Proceedings - IEEE International Conference on Robotics and Automation,2016-June,,7487451,2881,2888,7,51,10.1109/ICRA.2016.7487451,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977520910&doi=10.1109%2fICRA.2016.7487451&partnerID=40&md5=8e19180cfffcbe08d34f4cdf4ba6d7c6,"Functional robots are an increasing presence in shared human-machine environments. Humans efficiently parse motion expressions, gaining an immediate impression of an agent's current action and state. Past work has shown that motion can effectively reveal a robot's current task objective to bystanders and collaborators, however, the layering of expression on pre-existing robot task motions has yet to be explored. Rather than showing us what the robot is doing, these layered motion characteristics leverage the how of the task motions to convey additional robot attitudes, e.g., confidence, adherence to deadline or flexibility of attention. To lay the foundations for this objective, we adapt the Laban Efforts, a system from dance and acting training in use for over 50 years. We operationalize features representing the four Laban Efforts (Time, Space, Weight, and Flow) to the movements of a 2-DOF Nao head and a 4-DOF Keepon robot during simple dance and look-for-someone behaviors. Using online survey, we collect 1028 motion ratings for 72 robot motion videos depicting contrasting Effort motion examples. We achieve statistically significant legibility results for all four Effort implementations. Even without human degrees of freedom, we find that robot motion patterns can convey complex expressions to people. © 2016 IEEE.",,Degrees of freedom (mechanics); Robotics; Head motion; Human-machine; Motion characteristics; Motion expression; Online surveys; Robot motion; Robot tasks; Task motion; Robots,Conference paper,Final,,Scopus,2-s2.0-84977520910
Das H.P.; Konstantakopoulos I.C.; Manasawala A.B.; Veeravalli T.; Liu H.; Spanos C.J.,"Das, Hari Prasanna (57208125988); Konstantakopoulos, Ioannis C. (56440284300); Manasawala, Aummul Baneen (57215425788); Veeravalli, Tanya (57205386850); Liu, Huihan (59816772100); Spanos, Costas J. (7004447999)",57208125988; 56440284300; 57215425788; 57205386850; 59816772100; 7004447999,A novel graphical lasso based approach towards segmentation analysis in energy game-theoretic frameworks,2019,"Proceedings - 18th IEEE International Conference on Machine Learning and Applications, ICMLA 2019",,,8999336,1702,1709,7,14,10.1109/ICMLA.2019.00277,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080916211&doi=10.1109%2fICMLA.2019.00277&partnerID=40&md5=242d72bdc3e2cd7c3557b5e1f7ff9101,"Energy game-theoretic frameworks have emerged to be a successful strategy to encourage energy efficient behavior in large scale by leveraging human-in-the-loop strategy. A number of such frameworks have been introduced over the years which formulate the energy saving process as a competitive game with appropriate incentives for energy efficient players. However, prior works involve an incentive design mechanism which is dependent on knowledge of utility functions for all the players in the game, which is hard to compute especially when the number of players is high, common in energy game-theoretic frameworks. Our research proposes that the utilities of players in such a framework can be grouped together to a relatively small number of clusters, and the clusters can then be targeted with tailored incentives. The key to above segmentation analysis is to learn the features leading to human decision making towards energy usage in competitive environments. We propose a novel graphical lasso based approach to perform such segmentation, by studying the feature correlations in a real-world energy social game dataset. To further improve the explainability of the model, we perform causality study using grangers causality. Proposed segmentation analysis results in characteristic clusters demonstrating different energy usage behaviors. We also present avenues to implement intelligent incentive design using proposed segmentation method. © 2019 IEEE.",Energy Game-Theoretic Frameworks; Graphical Lasso; Segmentation Analysis; Smart Building,Decision making; Energy efficiency; Intelligent buildings; Machine learning; Competitive environment; Feature correlation; Game-theoretic; Graphical lassos; Human decision making; Number of clusters; Segmentation analysis; Segmentation methods; Game theory,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85080916211
Wang D.; Churchill E.; Maes P.; Fan X.; Shneiderman B.; Shi Y.; Wang Q.,"Wang, Dakuo (57013823000); Churchill, Elizabeth (7003488324); Maes, Pattie (56268463300); Fan, Xiangmin (57202047305); Shneiderman, Ben (7006313615); Shi, Yuanchun (7404964393); Wang, Qianying (57209304229)",57013823000; 7003488324; 56268463300; 57202047305; 7006313615; 7404964393; 57209304229,From human-human collaboration to Human-AI collaboration: Designing AI systems that can work together with people,2020,Conference on Human Factors in Computing Systems - Proceedings,,,3381069,,,,161,10.1145/3334480.3381069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090243725&doi=10.1145%2f3334480.3381069&partnerID=40&md5=b9586a9f9296988f0ff835a02e65a82c,"Artificial Intelligent (AI) and Machine Learning (ML) algorithms are coming out of research labs into the real-world applications, and recent research has focused a lot on Human-AI Interaction (HAI) and Explainable AI (XAI). However, Interaction is not the same as Collaboration. Collaboration involves mutual goal understanding, preemptive task co-management and shared progress tracking. Most of human activities today are done collaboratively, thus, to integrate AI into the already-complicated human workflow, it is critical to bring the Computer-Supported Cooperative Work (CSCW) perspective into the root of the algorithmic research and plan for a Human-AI Collaboration future of work. In this panel we ask: Can this future for trusted human-AI collaboration be realized? If so, what will it take? This panel will bring together HCI experts who work on human collaboration and AI applications in various application contexts, from industry and academia and from both the U.S. and China. Panelists will engage the audience through discussion of their shared and diverging visions, and through suggestions for opportunities and challenges for the future of human-AI collaboration. © 2020 Owner/Author.",Ai partner; Ai-powered healthcare; Computer-supported corporative work; Explainable ai; Group collaboration; Human-ai collaboration; Trusted ai,Behavioral research; Human engineering; AI applications; Algorithmic research; Application contexts; Artificial intelligent; Future of works; Human activities; Recent researches; Research labs; Machine learning,Conference paper,Final,,Scopus,2-s2.0-85090243725
Cech F.,"Cech, Florian (57195685366)",57195685366,Beyond transparency: Exploring algorithmic accountability,2020,Proceedings of the International ACM SIGGROUP Conference on Supporting Group Work,,,,11,14,3,4,10.1145/3323994.3371015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078363671&doi=10.1145%2f3323994.3371015&partnerID=40&md5=3717fe464ef055c4f1cca70125f0b0e6,"Many of the ubiquitous algorithmic systems permeating society have come under scrutiny due to their lack of accountability. As algorithmic decision making increasingly affects our lives, calls to improve the transparency of these systems are met with social, legal and technical limitations that challenge whether transparency alone is the solution to algorithmic accountability. In my dissertation, I explore the role of algorithmic tranparency, algorithmic literacy and related issues as approaches towards holding algorithmic systems more accountable. Bridging HCI and STS communities, my work is grounded in a critical ethnography of algorithmic systems and their impact on its stakeholders. Through this approach, I aim to provide both theoretical insights and material solutions to the problem of accountability. By unpacking the complex socio-technical assemblage that make up these systems and employing both participatory and user-centred design principles, my goal is to co-design measures that support sense-making of algorithmic processes and allow holding these systems accountable. © is held by the owner/author(s).",Algorithmic Accountability; Algorithmic Literacy; Algorithmic Transparency; Critical Algorithm Studies; Participatory Design,Decision making; Enterprise resource planning; User centered design; Algorithm study; Algorithmic Accountability; Algorithmic Literacy; Algorithmic process; Critical ethnography; Participatory design; Sociotechnical; Technical limitations; Transparency,Conference paper,Final,,Scopus,2-s2.0-85078363671
Smith-Renner A.; Rua R.; Colony M.,"Smith-Renner, Alison (57204791821); Rua, Rob (57207914038); Colony, Mike (24334242700)",57204791821; 57207914038; 24334242700,Towards an explainable threat detection tool,2019,CEUR Workshop Proceedings,2327,,,,,,5,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063230002&partnerID=40&md5=18f71c59a1a8ce3e82b3f60b1805fced,"In general, threats can be loosely divided into two categories – known threats and unknown threats. Traditional threat detection systems are limited to the identification of known threats that have been previously encountered and labeled by a security expert. These supervised learning systems are able to learn to detect and identify known threats but are unable to react to unknown threats. To this end, we have developed an unsupervised learning anomaly detection system to identify anomalous behavior without training data. Our system’s interactive interface supports human-machine teaming to classify these identified anomalies as threats or benign events; however, system transparency is required to enhance operator trust and improve their feedback into the system. Transparency in this case is particularly challenging as our anomaly detection framework is based on algorithms which are inherently hard to explain (neural networks). In this paper, we introduce a real-world task and system that requires transparency, and we propose explanation methods for increasing the transparency of our threat detection tool alongside a user study for evaluating these explanations. © 2019 for the individual papers by the papers’ authors. Copying permitted for private and academic purposes. This volume is published and copyrighted by its editors.",Anomaly detection; Explanations; Human-machine teaming; Transparency,Inspection equipment; Machine learning; Transparency; User interfaces; Anomalous behavior; Anomaly detection frameworks; Anomaly detection systems; Explanations; Human-machine; Interactive interfaces; Security experts; Threat detection system; Anomaly detection,Conference paper,Final,,Scopus,2-s2.0-85063230002
Cirqueira D.; Helfert M.; Bezbradica M.,"Cirqueira, Douglas (55920387200); Helfert, Markus (24723906500); Bezbradica, Marija (55372494000)",55920387200; 24723906500; 55372494000,Towards Design Principles for User-Centric Explainable AI in Fraud Detection,2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12797 LNAI,,,21,40,19,23,10.1007/978-3-030-77772-2_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112178703&doi=10.1007%2f978-3-030-77772-2_2&partnerID=40&md5=60f3b781993f600dd9942b7bb89535ac,"Experts rely on fraud detection and decision support systems to analyze fraud cases, a growing problem in digital retailing and banking. With the advent of Artificial Intelligence (AI) for decision support, those experts face the black-box problem and lack trust in AI predictions for fraud. Such an issue has been tackled by employing Explainable AI (XAI) to provide experts with explained AI predictions through various explanation methods. However, fraud detection studies supported by XAI lack a user-centric perspective and discussion on how principles are deployed, both important requirements for experts to choose an appropriate explanation method. On the other hand, recent research in Information Systems (IS) and Human-Computer Interaction highlights the need for understanding user requirements to develop tailored design principles for decision support systems. In this research, we adopt a design science research methodology and IS theoretical lens to develop and evaluate design principles, which align fraud expert’s tasks with explanation methods for Explainable AI decision support. We evaluate the utility of these principles using an information quality framework to interview experts in banking fraud, plus a simulation. The results show that the principles are an useful tool for designing decision support systems for fraud detection with embedded user-centric Explainable AI. © 2021, Springer Nature Switzerland AG.",Artificial intelligence; Decision support systems; Design principles; Explainable AI; Fraud detection; HCI; Human-AI interaction; Human-centered AI,Banking; Crime; Decision support systems; Embedded systems; Human computer interaction; Quality control; User centered design; Decision supports; Design Principles; Design-science researches; Fraud detection; Information quality framework; Recent researches; User requirements; User-centric; Artificial intelligence,Conference paper,Final,,Scopus,2-s2.0-85112178703
Kökciyan N.; Parsons S.; Sassoon I.; Sklar E.; Modgil S.,"Kökciyan, Nadin (55587819200); Parsons, Simon (57203087931); Sassoon, Isabel (56595046600); Sklar, Elizabeth (7003818615); Modgil, Sanjay (55886567200)",55587819200; 57203087931; 56595046600; 7003818615; 55886567200,An Argumentation-Based Approach to Generate Domain-Specific Explanations,2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12520 LNAI,,,319,337,18,7,10.1007/978-3-030-66412-1_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101334983&doi=10.1007%2f978-3-030-66412-1_20&partnerID=40&md5=b576704b91c9b99a17f71e4cbb60fa8f,"In argumentation theory, argument schemes are constructs to generalise common patterns of reasoning; whereas critical questions (CQs) capture the reasons why argument schemes might not generate arguments. Argument schemes together with CQs are widely used to instantiate arguments; however when it comes to making decisions, much less attention has been paid to the attacks among arguments. This paper provides a high-level description of the key elements necessary for the formalisation of argumentation frameworks such as argument schemes and CQs. Attack schemes are then introduced to represent attacks among arguments, which enable the definition of domain-specific attacks. One algorithm is articulated to operationalise the use of schemes to generate an argumentation framework, and another algorithm to support decision making by generating domain-specific explanations. Such algorithms can then be used by agents to make recommendations and to provide explanations for humans. The applicability of this approach is demonstrated within the context of a medical case study. © 2020, Springer Nature Switzerland AG.",Computational argumentation; Explainability; Human-agent systems,Decision making; Argumentation frameworks; Argumentation theory; Critical questions; Domain specific; Formalisation; High level description; Key elements; Making decision; Multi agent systems,Conference paper,Final,,Scopus,2-s2.0-85101334983
Ehsan U.; Liao Q.V.; Muller M.; Riedl M.O.; Weisz J.D.,"Ehsan, Upol (57195223484); Liao, Q. Vera (36095944800); Muller, Michael (7404688942); Riedl, Mark O. (7004421643); Weisz, Justin D. (14036587800)",57195223484; 36095944800; 7404688942; 7004421643; 14036587800,Expanding explainability: Towards social transparency in ai systems,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,289,10.1145/3411764.3445188,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106073891&doi=10.1145%2f3411764.3445188&partnerID=40&md5=4f8e76cd0c93dff9717e3cc3c2072b17,"As AI-powered systems increasingly mediate consequential decision-making, their explainability is critical for end-users to take informed and accountable actions. Explanations in human-human interactions are socially-situated. AI systems are often socio-organizationally embedded. However, Explainable AI (XAI) approaches have been predominantly algorithm-centered. We take a developmental step towards socially-situated XAI by introducing and exploring Social Transparency (ST), a sociotechnically informed perspective that incorporates the socio-organizational context into explaining AI-mediated decision-making. To explore ST conceptually, we conducted interviews with 29 AI users and practitioners grounded in a speculative design scenario. We suggested constitutive design elements of ST and developed a conceptual framework to unpack ST's efect and implications at the technical, decision-making, and organizational level. The framework showcases how ST can potentially calibrate trust in AI, improve decision-making, facilitate organizational collective actions, and cultivate holistic explainability. Our work contributes to the discourse of Human-Centered XAI by expanding the design space of XAI. © 2021 ACM.",Artifcial intelligence; Explainable ai; Explanations; Human-ai interaction; Social transparency; socio-organizational context; Sociotechnical,Behavioral research; Embedded systems; Human engineering; Transparency; Collective action; Conceptual frameworks; Design elements; Design spaces; Human-human interactions; Organizational context; Organizational levels; Social transparencies; Decision making,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85106073891
Wang J.; Liu C.; Zhu M.; Guo P.; Hu Y.,"Wang, Jianwu (57201399639); Liu, Chen (55680756900); Zhu, Meiling (57001849100); Guo, Pei (57202293494); Hu, Yapeng (57204899160)",57201399639; 55680756900; 57001849100; 57202293494; 57204899160,Sensor Data Based System-Level Anomaly Prediction for Smart Manufacturing,2018,"Proceedings - 2018 IEEE International Congress on Big Data, BigData Congress 2018 - Part of the 2018 IEEE World Congress on Services",,,8457744,158,165,7,28,10.1109/BigDataCongress.2018.00028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057745185&doi=10.1109%2fBigDataCongress.2018.00028&partnerID=40&md5=3025a2f39d5dfa2936165fed56f844d2,"With the popularity of Supervisory Information System (SIS), Supervisory Control and Data Acquisition (SCADA) system and Internet of Things (IoT) sensors, we can easily obtain abundant sensor data in manufacturing. We could save manufacturing maintenance costs and prevent further damages if we can accurately predict system anomalies from the sensor data. Yet learning from individual sensors often cannot directly determine whether the system will have anomaly because each sensor only measures a partial state of a big system. By detecting events across sensors collectively and their temporal dependencies, this paper proposes a new system-level anomaly prediction framework by mining anomaly dependency graph from sensor data. The advantages of the approach include explainability, collective prediction and temporal sensitivity. We applied our approach with a real-world power plant dataset to evaluate its feasibility. © 2018 IEEE.",Anomaly Prediction; Data Stream Mining; Predictive Maintenance; Sensor Data Driven; Smart Manufacturing,Data acquisition; Flow control; Forecasting; Internet of things; Manufacture; Anomaly predictions; Data stream mining; Predictive maintenance; Sensor data; Smart manufacturing; Big data,Conference paper,Final,,Scopus,2-s2.0-85057745185
Jarrahi M.H.; Haeri M.,"Jarrahi, Mohammad Hossein (36018165100); Haeri, Mohammad (57225782548)",36018165100; 57225782548,Developing a Symbiotic Workflow between Pathologists and AI through Parameterization and Implicitization,2021,CEUR Workshop Proceedings,2903,,,,,,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110557354&partnerID=40&md5=536b446c69e79fe6ec2a4640ed41e349,"Pathology is a fundamental element of modern medicine that determines the final diagnosis and portrays the prognosis in most medical conditions. Due to continuous improvements in AI capabilities (e.g., object recognition and image processing), intelligent systems are bound to play a key role in augmenting pathology research and clinical practices. Despite the pervasive deployment of computational approaches in similar fields such as radiology, there has been less success in integrating AI in clinical practices and histopathologic diagnosis. This partly has to do with the opacity of end-to-end AI systems, which raises issues of interoperability and accountability of medical practices. In this article, we draw on interactive machine learning to take advantage of AI in digital pathology in an attempt to open the Blackbox of AI and generate a more effective partnership between pathologists and AI systems based on the metaphors of parameterization and implicitization. © 2020 Copyright for this paper by its authors.",Accountability; Artificial intelligence; End-to-end ai; Explainable ai; Histopathologic diagnosis; Human-ai partnership; Interactive machine learning; Interpretability; Medical diagnosis; Pathology,Clinical research; Diagnosis; Image enhancement; Intelligent systems; Interoperability; Object recognition; Pathology; Clinical practices; Computational approach; Continuous improvements; Digital pathologies; Histopathologic diagnosis; Interactive machine learning; Medical conditions; Medical practice; User interfaces,Conference paper,Final,,Scopus,2-s2.0-85110557354
Lepri B.; Oliver N.; Letouzé E.; Pentland A.; Vinck P.,"Lepri, Bruno (14008023300); Oliver, Nuria (26534738900); Letouzé, Emmanuel (57191592415); Pentland, Alex (7102755925); Vinck, Patrick (18234401200)",14008023300; 26534738900; 57191592415; 7102755925; 18234401200,"Fair, Transparent, and Accountable Algorithmic Decision-making Processes: The Premise, the Proposed Solutions, and the Open Challenges",2018,Philosophy and Technology,31,4,,611,627,16,455,10.1007/s13347-017-0279-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050889982&doi=10.1007%2fs13347-017-0279-x&partnerID=40&md5=abcacb3682c48cee3be18f3007121318,"The combination of increased availability of large amounts of fine-grained human behavioral data and advances in machine learning is presiding over a growing reliance on algorithms to address complex societal problems. Algorithmic decision-making processes might lead to more objective and thus potentially fairer decisions than those made by humans who may be influenced by greed, prejudice, fatigue, or hunger. However, algorithmic decision-making has been criticized for its potential to enhance discrimination, information and power asymmetry, and opacity. In this paper, we provide an overview of available technical solutions to enhance fairness, accountability, and transparency in algorithmic decision-making. We also highlight the criticality and urgency to engage multi-disciplinary teams of researchers, practitioners, policy-makers, and citizens to co-develop, deploy, and evaluate in the real-world algorithmic decision-making processes designed to maximize fairness and transparency. In doing so, we describe the Open Algortihms (OPAL) project as a step towards realizing the vision of a world where data and algorithms are used as lenses and levers in support of democracy and development. © 2017, Springer Science+Business Media B.V.",Accountability; Algorithmic decision-making; Algorithmic transparency; Fairness; Social good,,Article,Final,,Scopus,2-s2.0-85050889982
Ogunbiyi N.; Basukoski A.; Chaussalet T.,"Ogunbiyi, Niyi (57210898182); Basukoski, Artie (6505660647); Chaussalet, Thierry (6603082623)",57210898182; 6505660647; 6603082623,An exploration of ethical decision making with intelligence augmentation,2021,Social Sciences,10,2,57,1,14,13,4,10.3390/socsci10020057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100794660&doi=10.3390%2fsocsci10020057&partnerID=40&md5=1011d5640733175596455a755af18060,"In recent years, the use of Artificial Intelligence agents to augment and enhance the operational decision making of human agents has increased. This has delivered real benefits in terms of improved service quality, delivery of more personalised services, reduction in processing time, and more efficient allocation of resources, amongst others. However, it has also raised issues which have real-world ethical implications such as recommending different credit outcomes for individuals who have an identical financial profile but different characteristics (e.g., gender, race). The popular press has highlighted several high-profile cases of algorithmic discrimination and the issue has gained traction. While both the fields of ethical decision making and Explainable AI (XAI) have been extensively researched, as yet we are not aware of any studies which have examined the process of ethical decision making with Intelligence augmentation (IA). We aim to address that gap with this study. We amalgamate the literature in both fields of research and propose, but not attempt to validate empirically, propositions and belief statements based on the synthesis of the existing literature, observation, logic, and empirical analogy. We aim to test these propositions in future studies. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Ethical decision making; Explainable AI; Intelligence augmentation; Values in Design,,Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85100794660
Wickramasinghe C.S.; Marino D.L.; Grandio J.; Manic M.,"Wickramasinghe, Chathurika S. (57201792033); Marino, Daniel L. (57192210929); Grandio, Javier (57219132675); Manic, Milos (6603474732)",57201792033; 57192210929; 57219132675; 6603474732,Trustworthy AI development guidelines for human system interaction,2020,"International Conference on Human System Interaction, HSI",2020-June,,9142644,130,136,6,46,10.1109/HSI49210.2020.9142644,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091398123&doi=10.1109%2fHSI49210.2020.9142644&partnerID=40&md5=731def9409838f43d9fe3fb1024ed2c4,"Artificial Intelligence (AI) is influencing almost all areas of human life. Even though these AI-based systems frequently provide state-of-the-art performance, humans still hesitate to develop, deploy, and use AI systems. The main reason for this is the lack of trust in AI systems caused by the deficiency of transparency of existing AI systems. As a solution, 'Trustworthy AI' research area merged with the goal of defining guidelines and frameworks for improving user trust in AI systems, allowing humans to use them without fear. While trust in AI is an active area of research, very little work exists where the focus is to build human trust to improve the interactions between human and AI systems. In this paper, we provide a concise survey on concepts of trustworthy AI. Further, we present trustworthy AI development guidelines for improving the user trust to enhance the interactions between AI systems and humans, that happen during the AI system life cycle.  © 2020 IEEE.",AI Life Cycle; Explainable AI; Human Machine Interactions; Human System Interactions; Transparency; Trustworthy AI,Life cycle; Active area; AI systems; Human lives; Human-system interaction; State-of-the-art performance; Artificial intelligence,Conference paper,Final,,Scopus,2-s2.0-85091398123
Devitt S.K.,"Devitt, S. Kate (57208901211)",57208901211,Normative epistemology for lethal autonomous weapons systems,2021,Lethal Autonomous Weapons: Re-Examining the Law and Ethics of Robotic Warfare,,,,237,258,21,4,10.1093/oso/9780197546048.003.0016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108299232&doi=10.1093%2foso%2f9780197546048.003.0016&partnerID=40&md5=dc23e00780272abb152bce64ffe38dba,"The rise of human-information systems, cybernetic systems, and increasingly autonomous systems requires the application of epistemic frameworks to machines and human-machine teams. This chapter discusses higher-order design principles to guide the design, evaluation, deployment, and iteration of Lethal Autonomous Weapons Systems (LAWS) based on epistemic models. Epistemology is the study of knowledge. Epistemic models consider the role of accuracy, likelihoods, beliefs, competencies, capabilities, context, and luck in the justification of actions and the attribution of knowledge. The aim is not to provide ethical justification for or against LAWS, but to illustrate how epistemological frameworks can be used in conjunction with moral apparatus to guide the design and deployment of future systems. The models discussed in this chapter aim to make Article 36 reviews of LAWS systematic, expedient, and evaluable. A Bayesian virtue epistemology is proposed to enable justified actions under uncertainty that meet the requirements of the Laws of Armed Conflict and International Humanitarian Law. Epistemic concepts can provide some of the apparatus to meet explainability and transparency requirements in the development, evaluation, deployment, and review of ethical AI. © Oxford University Press 2021.",Bayesian epistemology; Epistemology; Virtue epistemology,,Book chapter,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85108299232
Treiss A.; Walk J.; Kühl N.,"Treiss, Alexander (57219816444); Walk, Jannis (57194042777); Kühl, Niklas (57196220660)",57219816444; 57194042777; 57196220660,An Uncertainty-Based Human-in-the-Loop System for Industrial Tool Wear Analysis,2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12461 LNAI,,,85,100,15,3,10.1007/978-3-030-67670-4_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103247450&doi=10.1007%2f978-3-030-67670-4_6&partnerID=40&md5=c80f72252a28b7c1da219dc9dcab1227,"Convolutional neural networks have shown to achieve superior performance on image segmentation tasks. However, convolutional neural networks, operating as black-box systems, generally do not provide a reliable measure about the confidence of their decisions. This leads to various problems in industrial settings, amongst others, inadequate levels of trust from users in the model’s outputs as well as a non-compliance with current policy guidelines (e.g., EU AI Strategy). To address these issues, we use uncertainty measures based on Monte-Carlo dropout in the context of a human-in-the-loop system to increase the system’s transparency and performance. In particular, we demonstrate the benefits described above on a real-world multi-class image segmentation task of wear analysis in the machining industry. Following previous work, we show that the quality of a prediction correlates with the model’s uncertainty. Additionally, we demonstrate that a multiple linear regression using the model’s uncertainties as independent variables significantly explains the quality of a prediction (R2= 0.718 ). Within the uncertainty-based human-in-the-loop system, the multiple regression aims at identifying failed predictions on an image-level. The system utilizes a human expert to label these failed predictions manually. A simulation study demonstrates that the uncertainty-based human-in-the-loop system increases performance for different levels of human involvement in comparison to a random-based human-in-the-loop system. To ensure generalizability, we show that the presented approach achieves similar results on the publicly available Cityscapes dataset. © 2021, Springer Nature Switzerland AG.",Deep learning; Human-in-the-loop; Image segmentation; Uncertainty,Convolution; Convolutional neural networks; Cutting tools; Data mining; Forecasting; Image segmentation; Linear regression; Machine learning; Wear of materials; Human-in-the-loop; Independent variables; Industrial settings; Industrial tools; Multiple linear regressions; Multiple regressions; Simulation studies; Uncertainty measures; Uncertainty analysis,Conference paper,Final,,Scopus,2-s2.0-85103247450
Sokol K.; Flach P.,"Sokol, Kacper (57195074647); Flach, Peter (7004057691)",57195074647; 7004057691,One Explanation Does Not Fit All: The Promise of Interactive Explanations for Machine Learning Transparency,2020,KI - Kunstliche Intelligenz,34,2,,235,250,15,113,10.1007/s13218-020-00637-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088051328&doi=10.1007%2fs13218-020-00637-y&partnerID=40&md5=cc5636525c456a3c3de2b235cdb415d9,"The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations—a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human–machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms. © 2020, The Author(s).",Counterfactuals; Explanations; Interactive; Personalised,Learning algorithms; Machine learning; User interfaces; Black boxes; Counterfactuals; Explanation; Interactive; Machine learning algorithms; Machine-learning; On-machines; Personalized; Predictive systems; Property; Transparency,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85088051328
How M.-L.,"How, Meng-Leong (57204283174)",57204283174,Future-ready strategic oversight of multiple artificial superintelligence-enabled adaptive learning systems via human-centric explainable ai-empowered predictive optimizations of educational outcomes,2019,Big Data and Cognitive Computing,3,3,46,1,43,42,15,10.3390/bdcc3030046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079043753&doi=10.3390%2fbdcc3030046&partnerID=40&md5=2c3e00c1e3a9bb5e385e2e14e5203b54,"Artificial intelligence-enabled adaptive learning systems (AI-ALS) have been increasingly utilized in education. Schools are usually afforded the freedom to deploy the AI-ALS that they prefer. However, even before artificial intelligence autonomously develops into artificial superintelligence in the future, it would be remiss to entirely leave the students to the AI-ALS without any independent oversight of the potential issues. For example, if the students score well in formative assessments within the AI-ALS but subsequently perform badly in paper-based post-tests, or if the relentless algorithm of a particular AI-ALS is suspected of causing undue stress for the students, they should be addressed by educational stakeholders. Policy makers and educational stakeholders should collaborate to analyze the data from multiple AI-ALS deployed in different schools to achieve strategic oversight. The current paper provides exemplars to illustrate how this future-ready strategic oversight could be implemented using an artificial intelligence-based Bayesian network software to analyze the data from five dissimilar AI-ALS, each deployed in a different school. Besides using descriptive analytics to reveal potential issues experienced by students within each AI-ALS, this human-centric AI-empowered approach also enables explainable predictive analytics of the students’ learning outcomes in paper-based summative assessments after training is completed in each AI-ALS. © 2019 by the author. Licensee MDPI, Basel, Switzerland.",Adaptive learning systems; AI Thinking; Artificial intelligence; Artificial superintelligence; Bayesian networks; Explainable AI; Forecasting AI behavior; Future-ready; Human-centric reasoning; Human-in-the-loop; Pedagogical motif; Policy making on AI; Predictive optimization; Simulations; Strategic oversight,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85079043753
Almagor S.; Lahijanian M.,"Almagor, Shaull (36627051400); Lahijanian, Morteza (35105134400)",36627051400; 35105134400,Explainable multi agent path finding,2020,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",2020-May,,,34,42,8,21,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091994842&partnerID=40&md5=139b0b1aff4f314dc082caf025b2642e,"Multi Agent Path Finding (MAPF) is the problem of planning paths for agents to reach their targets from their start locations, such that the agents do not collide while executing the plan. In safety-critical systems, the plan is typically checked by a human supervisor, who decides on whether to allow its execution. In such cases, we wish to convince the human that the plan is indeed collision free. To this end, we propose an explanation scheme for MAPF, which bases explanations on simplicity of visual verification by human's cognitive process. The scheme decomposes a plan into segments such that within each segment, the paths of the agents are disjoint. Then, we can convince the supervisor that the plan is collision free using a small number of images (dubbed an explanation). In addition, we can measure the simplicity of a plan by the number of segments required for the decomposition. We study the complexity of algorithmic problems that arise by the explanation scheme, as well as the tradeoff between the length (makespan) of a plan and its minimal decomposition. We also provide experimental results of our scheme both in a continuous and in a discrete setting. © 2020 International Foundation for Autonomous.",Explainability; MAPF; Motion planning; Multi-agent systems; Path finding; Path planning,Computational complexity; Multi agent systems; Parallel processing systems; Safety engineering; Supervisory personnel; Algorithmic problems; Cognitive process; Collision-free; Discrete settings; Human supervisors; Planning paths; Safety critical systems; Visual verification; Autonomous agents,Conference paper,Final,,Scopus,2-s2.0-85091994842
Nunes T.M.M.; Borst C.; van Kampen E.-J.; Westin C.; Hilburn B.,"Nunes, Tiago Miguel Monteiro (58297000100); Borst, Clark (10640758100); van Kampen, Erik-Jan (15752332400); Westin, Carl (57213612637); Hilburn, Brian (23090695500)",58297000100; 10640758100; 15752332400; 57213612637; 23090695500,Human-interpretable Input for Machine Learning in Tactical Air Traffic Control,2021,SESAR Innovation Days,,,,,,,2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160711753&partnerID=40&md5=c27a758ce4b9b1f50dc839e421ebf797,"Increasing airspace demand requires an increase in effectiveness and efficiency of the ATC system. Automation, and specifically Machine Learning (ML), may present good prospects for increasing system performance and decreasing workload of ATCOs. AI, however, is typically a “black box” making it hard to include in a socio-technical environment. This exploratory research aims to increase operator trust and acceptance and move towards a more “cooperative” approach to automation in ATC. It focuses on building upon previous efforts by using two different approaches: Strategically Conformal AI and Explainable AI methods to AI-Human interactions. Strategic Conformance aims to increase acceptance by producing individual-sensitive advisories. Explainable AI focuses on producing more optimal solutions and providing a clear explanation for these solutions. In this article, we propose the use of a single visual representation for tactical conflict detection and resolution, called the Solution Space Diagram (SSD), to serve as a common ground for both explainable and conformal AI. Through this research, it has become clear that there needs to be a careful definition given both to optimality and conformance. Likewise, the training of the AI agents comes with requirements for a large amount of data to be available and displaying these solutions in a human-interpretable way, while maintaining optimality, has its own unique challenges to overcome. © 2021 IADIS. All rights reserved.",Decision Support Systems; Human Machine Interaction; Machine Learning,Air traffic control; Aviation; Information systems; Information use; Machine learning; Black boxes; Box making; Effectiveness and efficiencies; Human machine interaction; Machine-learning; Optimality; Sociotechnical; Systems performance; Tacticals; Technical environments; Decision support systems,Conference paper,Final,,Scopus,2-s2.0-85160711753
Ding A.W.,"Ding, Amy Wenxuan (57215636776)",57215636776,A model of fair and explainable artificial intelligence,2021,Artificial Intelligence for Sustainable Value Creation,,,,122,150,28,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130176797&partnerID=40&md5=db9605877a3da000dcb521c5bb95ff3d,"Artificial intelligence (AI) is making more and more algorithmic decisions for humans. However, the “intelligence” function in AI relies heavily on learning technologies, which suffer two major flaws leading to legal and technical challenges: potentially discriminative/biased decisions, and unable to explain why and how a machine makes such decisions. Therefore, building a fair and explainable AI model is important and urgent. This article presents a novel theory-based individual-level dynamic learning method that performs learning using data of an individual subject without employing others’ information, and identifies causal mechanism from unobserved data generating process that each subject exhibits. Thus, data selection bias is avoided and a fair and interpretable decision is achieved. We empirically test our method using a real-world dataset on risk assessment for lending decisions. Our results show that the proposed method outperforms conventional learning methods in terms of fairness in treating data subjects, decision accuracy and interpretability. © Margherita Pagani and Renaud Champion 2021.",,,Book chapter,Final,,Scopus,2-s2.0-85130176797
Korteling J.E.; van de Boer-Visschedijk G.C.; Blankendaal R.A.M.; Boonekamp R.C.; Eikelboom A.R.,"Korteling, J. E. (Hans). (6603173936); van de Boer-Visschedijk, G.C. (57198346024); Blankendaal, R.A.M. (56912371600); Boonekamp, R.C. (42061010500); Eikelboom, A.R. (23109970800)",6603173936; 57198346024; 56912371600; 42061010500; 23109970800,Human- versus Artificial Intelligence,2021,Frontiers in Artificial Intelligence,4,,622364,,,,253,10.3389/frai.2021.622364,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108600226&doi=10.3389%2ffrai.2021.622364&partnerID=40&md5=581e7b935f20339b75092cc001b708a9,"AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions and, for instance, the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. In order to provide more agreement and to substantiate possible future research objectives, this paper presents three notions on the similarities and differences between human- and artificial intelligence: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple (integrated) forms of narrow-hybrid AI applications. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and “collaborate” with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgment required? How can we capitalize on the specific strengths of human- and artificial intelligence? How to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition (and vice versa)? Should we pursue the development of AI “partners” with human (-level) intelligence or should we focus more at supplementing human limitations? In order to answer these questions, humans working with AI systems in the workplace or in policy making have to develop an adequate mental model of the underlying ‘psychological’ mechanisms of AI. So, in order to obtain well-functioning human-AI systems, Intelligence Awareness in humans should be addressed more vigorously. For this purpose a first framework for educational content is proposed. © Copyright © 2021 Korteling, van de Boer-Visschedijk, Blankendaal, Boonekamp and Eikelboom.",artificial general intelligence; artificial intelligence; cognitive bias; cognitive complexity; human intelligence; human-AI collaboration; human-level artificial intelligence; narrow artificial intelligence,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85108600226
Park S.Y.; Kuo P.-Y.; Barbarin A.; Kaziunas E.; Chow A.; Singh K.; Wilcox L.; Lasecki W.S.,"Park, Sun Young (52864509600); Kuo, Pei-Yi (55800600600); Barbarin, Andrea (57189308738); Kaziunas, Elizabeth (37023365200); Chow, Astrid (58253871800); Singh, Karandeep (56531139000); Wilcox, Lauren (23040470800); Lasecki, Walter S. (54383728900)",52864509600; 55800600600; 57189308738; 37023365200; 58253871800; 56531139000; 23040470800; 54383728900,Identifying challenges and opportunities in human-AI collaboration in healthcare,2019,"Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW",,,,506,510,4,57,10.1145/3311957.3359433,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076091592&doi=10.1145%2f3311957.3359433&partnerID=40&md5=33cd170342790cfd26067b51e6327e53,"The proposed workshop will identify research questions that will enable the field to uncover the types of work, labor relations, and social impacts that should be considered when designing AI-based healthcare technology. The workshop aims to outline key challenges, guidelines, and future agendas for the field, and provide collaboration opportunities for CSCW researchers, social scientists, AI researchers, clinicians, and relevant stakeholders in healthcare, to share their perspectives and co-create sociotechnical approaches to tackle timely issues related to AI and automation in healthcare work. © 2019 Copyright is held by the author/owner(s).",AI fairness; AI transparency; Algorithms; Artificial intelligence; Automation; Explainable AI; Healthcare; Machine learning; Sociotechnical systems,Algorithms; Artificial intelligence; Automation; Groupware; Health care; Interactive computer systems; Learning systems; Machine learning; Healthcare technology; Labor relations; Research questions; Social impact; Social scientists; Socio-technical approach; Sociotechnical systems; Economic and social effects,Conference paper,Final,,Scopus,2-s2.0-85076091592
de Regt A.; Gagnon E.,"de Regt, Anouk (55520009700); Gagnon, Elisa (57220835966)",55520009700; 57220835966,Rethinking how humans and machines make sense together,2020,"26th Americas Conference on Information Systems, AMCIS 2020",,,,,,,2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097714493&partnerID=40&md5=46cdf4e93fd21003ab722afa907036ca,"There is a growing realization of Artificial Intelligence (AI)'s importance, including its ability to provide competitive advantage and change work for the better. Indeed, organizations are investing in various AI applications in the hope to automate or augment human judgment. Despite the promise of AI, many organizations' efforts with it are falling short. Therefore, adopting sensemaking theory as a theoretical lens, this study is to investigate under which conditions and how human and machines collaborations should be structured, to enhance each other's capabilities and facilitate optimal strategical decision-making and operational effectiveness. Four types of human-machines interaction are proposed based on the level of complexity of the context and the severity of wrong decisions. Besides providing a new instrument for the analysis and assessment of human-AI interactions, this research aids the development of guidelines and facilitates the move towards explainable AI (XAI) design, development and practices. © 2020 26th Americas Conference on Information Systems, AMCIS 2020. All rights reserved.",Artificial Intelligence; Human-computer interactions; Sensemaking; XAI,Competition; Decision making; Decision theory; Information systems; Information use; AI applications; Competitive advantage; Human judgments; Human-machine; Operational effectiveness; Sense-making theory; Artificial intelligence,Conference paper,Final,,Scopus,2-s2.0-85097714493
Duin A.H.; Pedersen I.,"Duin, Ann Hill (6603386845); Pedersen, Isabel (55914987500)",6603386845; 55914987500,Working Alongside Non-Human Agents,2021,IEEE International Professional Communication Conference,2021-October,,,1,5,4,1,10.1109/ProComm52174.2021.00005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133010179&doi=10.1109%2fProComm52174.2021.00005&partnerID=40&md5=804d885d0763b75d2edcce8f422626ea,"We coexist with non-human AI agents, and we now must plan for human and non-human-agent teaming, for cooperation and collaboration, as a means to expand collaborative intelligence in our ongoing quest for user advocacy. For practice and experimentation, we provide links to current non-human agents. We then distinguish automation and autonomy, and discuss humanness design, teaming. A deeper understanding of usability and ethical considerations for working alongside these systems, deploying robots and building bonds and trust with nonhuman agents, begins with differentiation of automation and autonomy, human-autonomy teaming, and a humanness design approach as a means to prevent undesirable autonomy. While TPC scholarship attends to privacy, accountability, safety and security, and transparency and explainability, we need additional vigilance regarding fairness and non-discrimination, human control of technology, TPC professional responsibility, and continued promotion of human values as we work alongside non-human agents.  © 2021 IEEE.",artificial intelligence; autonomous agents; collaboration; non-human agents,Autonomous agents; Ethical technology; 'current; Collaboration; Design approaches; Ethical considerations; Human agent; Human control; Human-agent teaming; Non-human agent; Professional responsibilities; Safety and securities; Machine design,Conference paper,Final,,Scopus,2-s2.0-85133010179
Zhang Y.; Vera Liao Q.; Bellamy R.K.E.,"Zhang, Yunfeng (57201641915); Vera Liao, Q. (36095944800); Bellamy, Rachel K.E. (7006073277)",57201641915; 36095944800; 7006073277,Efect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making,2020,"FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",,,,295,305,10,530,10.1145/3351095.3372852,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079608746&doi=10.1145%2f3351095.3372852&partnerID=40&md5=b8faad3cd8a5ff4da0c33e9a645fb466,"Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the signiicance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-speciic model information can calibrate trust and improve the joint performance of the human and AI. Speciically, we study the efect of showing conidence score and local explanation for a particular prediction. Through two human experiments, we show that conidence score can help calibrate people's trust in an AI model, but trust calibration alone is not suicient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI. © 2020 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery.",Conidence; Decision support; Explainable AI; Trust,Behavioral research; Calibration; Decision support systems; Transparency; Conidence; Decision outcome; Decision supports; Individual strength; Joint performance; Model informations; Research communities; Trust; Decision making,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85079608746
Zhang Z.T.; Liu Y.; Hussmann H.,"Zhang, Zelun Tony (57226128028); Liu, Yuanting (57207882809); Hussmann, Heinrich (23389275800)",57226128028; 57207882809; 23389275800,Forward reasoning decision support: Toward a more complete view of the human-ai interaction design space,2021,ACM International Conference Proceeding Series,,,,,,,4,10.1145/3464385.3464696,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112717719&doi=10.1145%2f3464385.3464696&partnerID=40&md5=d4472eb8e08f16cbb1e37240146f2b55,"Decision support systems based on AI are usually designed to generate complete outputs entirely automatically and to explain those to users. However, explanations, no matter how well designed, might not adequately address the output uncertainty of such systems in many applications. This is especially the case when the human-out-of-the-loop problem persists, which is a fundamental human limitation. There is no reason to limit decision support systems to such backward reasoning designs, though. We argue how more interactive forward reasoning designs where users are actively involved in the task can be effective in managing output uncertainty. We therefore call for a more complete view of the design space for decision support systems that includes both backward and forward reasoning designs. We argue that such a more complete view is necessary to overcome the barriers that hinder AI deployment especially in high-stakes applications.  © 2021 ACM.",Backward reasoning; Decision support; Explainability; Forward reasoning; Human-AI interaction; Intelligent systems; Output uncertainty; Transparency; User control,Computer applications; Computer programming; Backward reasoning; Decision supports; Design spaces; Forward reasoning; Human limitations; Interaction design; Decision support systems,Conference paper,Final,,Scopus,2-s2.0-85112717719
,,,"21st International Conference on Human-Computer Interaction, HCI International 2019",2019,Communications in Computer and Information Science,1033,,,,,1560,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069729608&partnerID=40&md5=e1a9861928d9585877e250cea8db233b,The proceedings contain 207 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: Research on Evaluation Model of Social Game Advertising Effect Based on Eye Movement Experiment; towards a Narrative Driven Understanding of Games User Experience; Application of Archery to VR Interface; what Drives Female Players’ Continuance Intention to Play Mobile Games? The Role of Aesthetic and Narrative Design Factors; simultaneous Dialog Robot System; a Robot System Using Mixed Reality to Encourage Driving Review; self-learning Guide for Bioloid Humanoid Robot Assembly with Elements of Augmented Reality to Support Experiential Learning in Sauro Research Seeding; developing a Behavior Converter to Make a Robot Child-Like for Enhancing Human Utterances; can We Recognize Atmosphere as an Agent?: Pilot Study; design Strategies of Corporate Gamification Systems that Evokes Employee Motivation – Creative Process of Gathering Game Design Elements into Working System; GEC-HR: Gamification Exercise Companion for Home Robot with IoT; discussion on the Feasibility of Soft Actuator as an Assistive Tool for Seniors in Minimally Invasive Surgery; Recognition of Listener’s Nodding by LSTM Based on Movement of Facial Keypoints and Speech Intonation; AI-Based Technical Approach for Designing Mobile Decision Aids; human Learning in Data Science; How to Achieve Explainability and Transparency in Human AI Interaction; Data on RAILs: On Interactive Generation of Artificial Linear Correlated Data; adaptation of Machine Learning Frameworks for Use in a Management Environment: Development of a Generic Workflow; software to Support Layout and Data Collection for Machine-Learning-Based Real-World Sensors; phenomenology of Experience in Ambient Intelligence.,,,Conference review,Final,,Scopus,2-s2.0-85069729608
Chakraborti T.; Fadnis K.P.; Talamadupula K.; Dholakia M.; Srivastava B.; Kephart J.O.; Bellamy R.K.E.,"Chakraborti, Tathagata (41261182500); Fadnis, Kshitij P. (57202457861); Talamadupula, Kartik (25960433100); Dholakia, Mishal (57194716633); Srivastava, Biplav (55650207400); Kephart, Jeffrey O. (57192124884); Bellamy, Rachel K. E. (7006073277)",41261182500; 57202457861; 25960433100; 57194716633; 55650207400; 57192124884; 7006073277,Planning and visualization for a smart meeting room assistant,2019,AI Communications,32,1,,91,99,8,5,10.3233/AIC-180609,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063102217&doi=10.3233%2fAIC-180609&partnerID=40&md5=1cc6e6a82284b2bed6ed9fc9e918e9a1,"In this paper, we report on the planning and visualization capabilities of Mr.Jones-a proactive orchestrator and decision-support agent for a collaborative decision making setting embodied by a smart room. The duties of such an agent may range across interactive problem solving with other agents in the environment, developing automated summaries of meetings, visualization of the internal decision-making process, proactive data and resource management, and so on. Specifically, we focus on how the visualization of the planning and plan recognition processes forms a key component of the smart assistant, and establishes transparency in the decision-making process. We also highlight how these processes contribute to the proactive nature of the agent. We demonstrate some of these functionalities in a successful deployment of the system in the CEL-the Cognitive Environments Laboratory at IBM's T.J. Watson Research Center (Yorktown, USA), and report on emerging deployments of the system that have turned into success stories. © 2019-IOS Press and the authors. All rights reserved.",Explainable AI; human-in-the-loop; planning and decision-making; smart room; visualization,Data visualization; Decision support systems; Flow visualization; Information management; Problem solving; Visualization; Automated summaries; Collaborative decision making; Decision making process; Human-in-the-loop; Interactive problem solving; Planning and plan recognition; Resource management; Smart rooms; Decision making,Article,Final,,Scopus,2-s2.0-85063102217
Liao Q.V.; Gruen D.; Miller S.,"Liao, Q. Vera (36095944800); Gruen, Daniel (8521973800); Miller, Sarah (57219109515)",36095944800; 8521973800; 57219109515,Questioning the AI: Informing Design Practices for Explainable AI User Experiences,2020,Conference on Human Factors in Computing Systems - Proceedings,,,3376590,,,,571,10.1145/3313831.3376590,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082464591&doi=10.1145%2f3313831.3376590&partnerID=40&md5=9a22757e53129d30ce79c7a43eba8e61,"A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI. © 2020 ACM.",explainable AI; human-AI interaction; user experience,Human engineering; Product design; AI systems; Design practice; Design practitioners; Design spaces; Question banks; Real-world; Support design; User-centered; User experience,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85082464591
Cai C.J.; Winter S.; Steiner D.; Wilcox L.; Terry M.,"Cai, Carrie J (57219528527); Winter, Samantha (57255647200); Steiner, David (57204607770); Wilcox, Lauren (23040470800); Terry, Michael (18937795900)",57219528527; 57255647200; 57204607770; 23040470800; 18937795900,Onboarding Materials as Cross-functional Boundary Objects for Developing AI Assistants,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,34,10.1145/3411763.3443435,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105798197&doi=10.1145%2f3411763.3443435&partnerID=40&md5=be027bbbd6b2989896dbb34901106ac1,"Deep neural networks (DNNs) routinely achieve state-of-the-art performance in a wide range of tasks, but it can often be challenging for them to meet end-user needs in practice. This case study reports on the development of human-AI onboarding materials (i.e., training materials for users prior to using an AI) for a DNN-based medical AI Assistant to aid in the grading of prostate cancer. Specifically, we describe how the process of developing these materials changed the team's understanding of end-user requirements, contributing to modifications in the development and assessment of the underlying machine learning model. Importantly, we discovered that onboarding materials served as a useful boundary object for cross-functional teams, uncovering a new way to assess the ML model and specify its end-user requirements. We also present evidence of the utility of the onboarding materials by describing how it affected user strategies and decision-making with AI in a study deployment to pathologists. © 2021 Owner/Author.",AI transparency; boundary objects; explainability; machine learning,Decision making; Diseases; Grading; Human engineering; Boundary objects; Cross-functional; Cross-functional teams; End user requirements; Machine learning models; Prostate cancers; State-of-the-art performance; Training material; Deep neural networks,Conference paper,Final,,Scopus,2-s2.0-85105798197
Lima G.; Grgic-Hlaca N.; Cha M.,"Lima, Gabriel (57218770114); Grgic-Hlaca, Nina (57203397997); Cha, Meeyoung (24069708400)",57218770114; 57203397997; 24069708400,Human perceptions on moral responsibility of ai: A case study in ai-assisted bail decision-making,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,55,10.1145/3411764.3445260,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002586046&doi=10.1145%2f3411764.3445260&partnerID=40&md5=35de86b8c3291a43f3f850128144c2a3,"How to attribute responsibility for autonomous artifcial intelligence (AI) systems' actions has been widely debated across the humanities and social science disciplines. This work presents two experiments (N=200 each) that measure people's perceptions of eight diferent notions of moral responsibility concerning AI and human agents in the context of bail decision-making. Using real-life adapted vignettes, our experiments show that AI agents are held causally responsible and blamed similarly to human agents for an identical task. However, there was a meaningful diference in how people perceived these agents' moral responsibility; human agents were ascribed to a higher degree of present-looking and forward-looking notions of responsibility than AI agents. We also found that people expect both AI and human decision-makers and advisors to justify their decisions regardless of their nature. We discuss policy and HCI implications of these fndings, such as the need for explainable AI in high-stakes scenarios. © 2021 ACM.",Ai; Bail decision-making; Blame; Compas; Liability; Moral judgment; Moral responsibility; Responsibility,Behavioral research; Ai; Bail decision-making; Blame; Compa; Decisions makings; Intelligence agents; Liability; Moral judgment; Moral responsibility; Responsibility; Decision making,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-105002586046
Watkins E.A.,"Watkins, Elizabeth Anne (57193833255)",57193833255,The Tension between Information Justice and Security: Perceptions of Facial Recognition Targeting,2021,CEUR Workshop Proceedings,2903,,,,,,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110527290&partnerID=40&md5=4210af23d3b20cc0288d589a61bd2cc0,"In the discourse on human perceptions of algorithmic fairness, researchers have begun to analyze how these perceptions are shaped by sociotechnical context. In thinking through contexts of work, a half-century of research on organizational decision-making tells us that perceptions and interpretations made within these spaces are highly bounded by surrounding contextual constraints. In this paper I report early findings from a survey I conducted to bridge these two conversations, and scrutinize real-world perceptions of algorithmic decision-making in situ in a space of work. I analyze these perceptions through the case of facial recognition (or more accurately, facial verification) as account verification in gig work. In this survey I asked 100 Uber drivers, who all had been actually subjected to Uber's facial verification process known as Real Time Check ID, their fairness perceptions of this process. I designed the survey to elicit their perceptions across five disparate dimensions of justice: Informational, distributive, procedural, reciprocal, and interactional. I also asked them about their strategies for integrating Real Time Check ID into their work flow, including efforts at repair when the system breaks down and their potential preferences for subversive practices. Of those workers who report engaging in subversive tactics to avoid facial recognition, such as taking a picture of their car seat, their hand, or their passenger instead of their own face, one dimension of fairness elicited worse perceptions than any other: Informational justice, a.k.a. transparency, of facial recognition targeting (the process for deciding which workers trigger this extra layer of verification). This research reveals tensions between transparency, security, and workers' perceptions of the ""fairness""of an algorithmic system: While ""too much""transparency into how workers are targeted for verification may permit bad actors to defraud the system, ""too little""explanation, this research shows, is no solution either. Results have crucial implications for the allocation of transparency and the design of explanations in user-facing algorithmic fraud detection, which must address tensions between information justice and security. © 2020 Copyright for this paper by its authors.",Algorithmic fairness; Biometric; Explainability; Facial recognition; Facial verification; Security; Sociotechnical; Transparency,Decision making; Surveys; Transparency; User interfaces; Contextual constraints; Facial recognition; Fraud detection; Human perception; One dimension; Organizational decision making; Sociotechnical; Verification process; Face recognition,Conference paper,Final,,Scopus,2-s2.0-85110527290
Wang L.; Wang D.; Tian F.; Peng Z.; Fan X.; Zhang Z.; Ma S.; Yu M.; Ma X.; Wang H.,"Wang, Liuping (57203515967); Wang, Dakuo (57013823000); Tian, Feng (57202049684); Peng, Zhenhui (57202799508); Fan, Xiangmin (57202047305); Zhang, Zhan (55619663000); Ma, Shuai (57203514033); Yu, Mo (35174012000); Ma, Xiaojuan (56242668800); Wang, Hongan (57202272600)",57203515967; 57013823000; 57202049684; 57202799508; 57202047305; 55619663000; 57203514033; 35174012000; 56242668800; 57202272600,CASS: Towards Building a Social-Support Chatbot for Online Health Community,2021,Proceedings of the ACM on Human-Computer Interaction,5,CSCW1,3449083,,,,61,10.1145/3449083,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104946162&doi=10.1145%2f3449083&partnerID=40&md5=c4fbd3194d600290ea4a2a55a177e6e9,"Chatbots systems, despite their popularity in today's HCI and CSCW research, fall short for one of the two reasons: 1) many of the systems use a rule-based dialog flow, thus they can only respond to a limited number of pre-defined inputs with pre-scripted responses; or 2) they are designed with a focus on single-user scenarios, thus it is unclear how these systems may affect other users or the community. In this paper, we develop a generalizable chatbot architecture (CASS) to provide social support for community members in an online health community. The CASS architecture is based on advanced neural network algorithms, thus it can handle new inputs from users and generate a variety of responses to them. CASS is also generalizable as it can be easily migrate to other online communities. With a follow-up field experiment, CASS is proven useful in supporting individual members who seek emotional support. Our work also contributes to fill the research gap on how a chatbot may influence the whole community's engagement.  © 2021 ACM.",ai deployment; bot; chatbot; conversational agent; emotional support; explainable ai; healthcare; human ai collaboration; human ai interaction; machine learning; neural network; online community; peer support; pregnancy; social support; system building; trustworthy ai,Human computer interaction; User interfaces; Emotional supports; Field experiment; Neural network algorithm; On-line communities; Online health communities; Rule based; Single users; Social support; Network architecture,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85104946162
Lu Z.; Huang P.; Dai P.; Liu Z.; Meng Z.,"Lu, Zhenyu (56411276300); Huang, Panfeng (8608333800); Dai, Pei (57188580314); Liu, Zhengxiong (24450548800); Meng, Zhongjie (24832186200)",56411276300; 8608333800; 57188580314; 24450548800; 24832186200,Enhanced transparency dual-user shared control teleoperation architecture with multiple adaptive dominance factors,2017,"International Journal of Control, Automation and Systems",15,5,,2301,2312,11,17,10.1007/s12555-016-0467-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028854913&doi=10.1007%2fs12555-016-0467-y&partnerID=40&md5=38461922fbb41ea49082dcef8b261624,"In traditional dual-user shared control teleoperation, the users’ operational transparency is influenced by the dominance factorα. Especially when α = 1 and α = 0 the trainer or trainee cannot receive any information form the slave side. For enlarging the control flexibility and enhancing the system transparency, a novel method with multiple adaptive dominance factors is proposed in this paper. An ideal application of this method is on-line operation and supervision. The supervisor and operator can adjust the factors and switch the operation roles and states freely. Once the slave robot handled by the operator diverges from the planned path, the dominance factors will change adaptive to limit the operator’s movement. The adaptive principles are concluded from the dynamic performance measured by the measuring functions. The conclusions suggest that the varying range of the system dynamic performance is wider than the traditional method. In addition, considering the time delays between the slave and master sides, we proved the system stability conditions covering all the range of dominance factors. Finally, we make a discussion of the applying area of the novel shared control architecture. © 2017, Institute of Control, Robotics and Systems and The Korean Institute of Electrical Engineers and Springer-Verlag GmbH Germany.",Multiple dominance factors; stability conditions; teleoperation; time delay; transparency,Delay control systems; Memory architecture; Remote control; System stability; Time delay; Transparency; Adaptive principles; Dynamic performance; Multiple dominance factors; Online operations; Planned paths; Shared control; Stability condition; System dynamic performance; Adaptive control systems,Article,Final,,Scopus,2-s2.0-85028854913
Arous I.; Dolamic L.; Yang J.; Bhardwaj A.; Cuccu G.; Cudré-Mauroux P.,"Arous, Ines (57209513113); Dolamic, Ljiljana (27067553800); Yang, Jie (56370016500); Bhardwaj, Akansha (57197825219); Cuccu, Giuseppe (35239509200); Cudré-Mauroux, Philippe (22333444100)",57209513113; 27067553800; 56370016500; 57197825219; 35239509200; 22333444100,MARTA: Leveraging Human Rationales for Explainable Text Classification,2021,"35th AAAI Conference on Artificial Intelligence, AAAI 2021",7,,,5868,5876,8,39,10.1609/aaai.v35i7.16734,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129363314&doi=10.1609%2faaai.v35i7.16734&partnerID=40&md5=6178b212289c9d190ac2957aa14aef00,"Explainability is a key requirement for text classification in many application domains ranging from sentiment analysis to medical diagnosis or legal reviews. Existing methods often rely on “attention” mechanisms for explaining classification results by estimating the relative importance of input units. However, recent studies have shown that such mechanisms tend to mis-identify irrelevant input units in their explanation. In this work, we propose a hybrid human-AI approach that incorporates human rationales into attention-based text classification models to improve the explainability of classification results. Specifically, we ask workers to provide rationales for their annotation by selecting relevant pieces of text. We introduce MARTA, a Bayesian framework that jointly learns an attention-based model and the reliability of workers while injecting human rationales into model training. We derive a principled optimization algorithm based on variational inference with efficient updating rules for learning MARTA parameters. Extensive validation on real-world datasets shows that our framework significantly improves the state of the art both in terms of classification explainability and accuracy. Copyright © 2021,",,Classification (of information); Computer aided diagnosis; Inference engines; Applications domains; Attention mechanisms; Bayesian frameworks; Classification results; Learn+; Model training; Sentiment analysis; Text classification; Text classification models; Workers'; Sentiment analysis,Conference paper,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85129363314
Waefler T.; Schmid U.,"Waefler, Toni (6505641943); Schmid, Ute (56032455700)",6505641943; 56032455700,Explainability is not enough: Requirements for human-AI-partnership in complex socio-technical systems,2020,"Proceedings of the European Conference on the Impact of Artificial Intelligence and Robotics, ECIAIR 2020",,,,185,193,8,10,10.34190/EAIR.20.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097816886&doi=10.34190%2fEAIR.20.007&partnerID=40&md5=aa4d34561f5bd6ae88037960fcafd4cc,"Explainability has been recognized as an important requirement of artificial intelligence (AI) systems. Transparent decision policies and explanations regarding why an AI system comes about a certain decision is a pre-requisite if AI is supposed to support human decision-making or if human-AI collaborative decision-making is envisioned. Human-AI interaction and joint decision-making is required in many real-world domains, where risky decisions have to be made (e.g. medical diagnosis) or complex situations have to be assessed (e.g. states of machines or production processes). However, in this paper we theorize that explainability is necessary but not sufficient. Coming from the point of view of work psychology we argue that for the human part of the human-AI system much more is required than intelligibility. In joint human-AI decision-making a certain role is assigned to the human, which normally encompasses tasks such as (i) verifying AI based decision suggestions, (ii) improving AI systems, (iii) learning from AI systems, and (iv) taking responsibility for the final decision as well as for compliance with legislation and ethical standards. Empowering the human to take this demanding role requires not only human expertise but e.g. also human motivation, which is triggered by a suitable task design. Furthermore, at work humans normally do not take decisions as lonely wolves but in formal and informal cooperation with other humans. Hence, to design effective explainability and to empower for true human-AI collaborative decision-making, embedding human-AI dyads into a socio-technical context is necessary. Coming from theory, this paper presents system design criteria on different levels substantiated by work psychology. The criteria are described and confronted with a use case scenario of AI-supported medical decision making in the context of digital pathology. On this basis, the need for further research is outlined. © ECIAIR 2020.All right reserved.",Companion technology; Explainable AI; Human factors; Interactive learning; Motivation; Socio-technical systems,Agricultural robots; Behavioral research; Diagnosis; Regulatory compliance; Robotics; Collaborative decision making; Digital pathologies; Ethical standards; Human decision making; Joint decision making; Medical decision making; Production process; Sociotechnical systems; Decision making,Conference paper,Final,,Scopus,2-s2.0-85097816886
Streitz N.; Charitos D.; Kaptein M.; Böhlen M.,"Streitz, Norbert (6603608191); Charitos, Dimitris (6603310093); Kaptein, Maurits (23392867700); Böhlen, Marc (55884351400)",6603608191; 6603310093; 23392867700; 55884351400,Grand challenges for ambient intelligence and implications for design contexts and smart societies,2019,Journal of Ambient Intelligence and Smart Environments,11,1,,87,107,20,59,10.3233/AIS-180507,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060941496&doi=10.3233%2fAIS-180507&partnerID=40&md5=16fa5cb78497d5eb9aad6b3e73c47743,"This paper highlights selected grand challenges that concern especially the social and the design dimensions of research and development in Ambient Intelligence (AmI) and Smart Environments (SmE). Due to the increasing deployment and usage of 'smart' technologies determining a wide range of everyday life activities, there is an urgent need to reconsider their societal implications and how to address these implications with appropriate design methods. The paper presents four perspectives on the subject grounded in different approaches. First, introducing and reflecting on the implications of the 'smart-everything' paradigm, the resulting design trade-offs and their application to smart cities. Second, discussing the potential of non-verbal communication for informing the design of spatial interfaces for AmI design practices. Third, reflecting on the role of new data categories such as 'future data' and the role of uncertainty and their implications for the next generation of AmI environments. Finally, debating the merits and shortfalls of the world's largest professional engineering community effort to craft a global standards body on ethically aligned design for autonomous and intelligent systems. The paper benefits from taking different perspectives on common issues, provides commonalities and relationships between them and provides anchor points for important challenges in the field of ambient intelligence. © 2019 - IOS Press and the authors. All rights reserved.",Algorithmic transparency; Ambient intelligence; Artificial intelligence; Autonomous intelligent systems; Citizen-centered design; Data science; Design trade-offs; Ethically aligned design; Future data; GDPR; General artificial intelligence; General data protection regulations; Governance of technology; Human control; Human in the loop; Humane and sociable AmI; Hybrid city; Machine learning; Multi-armed bandit problem; Non-verbal communication; Opaque AI; Privacy by design; Self-aware city; Smart city; Smart environments; Smart-everything; Spatial communication interfaces; Traceability of algorithms; Uncertainty,Ambient intelligence; Artificial intelligence; Commerce; Data Science; Design; Economic and social effects; Intelligent systems; Learning algorithms; Learning systems; Machine learning; Samarium compounds; Smart city; Autonomous intelligent systems; Communication interface; Design tradeoff; Future data; GDPR; General data protection regulations; Human control; Human-in-the-loop; Humane and sociable AmI; Hybrid city; Multi-armed bandit problem; Non-verbal communications; Self-aware; Smart environment; Smart-everything; Uncertainty; Iodine compounds,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85060941496
Scalas M.; Giacinto G.,"Scalas, Michele (57209399188); Giacinto, Giorgio (6701832237)",57209399188; 6701832237,On the role of explainable machine learning for secure smart vehicles,2020,"2020 AEIT International Conference of Electrical and Electronic Technologies for Automotive, AEIT AUTOMOTIVE 2020",,,9307431,,,,5,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099683035&partnerID=40&md5=6551a29c839354268ccc0d53d42344c2,"The concept of mobility is experiencing a serious transformation due to the Mobility-as-a-Service paradigm. Accordingly, vehicles, usually referred to as smart, are seeing their architecture revamped to integrate connection to the outside environment (V2X) and autonomous driving. A significant part of these innovations is enabled by machine learning. However, deploying such systems raises some concerns. First, the complexity of the algorithms often prevents understanding what these models learn, which is relevant in the safety-critical context of mobility. Second, several studies have demonstrated the vulnerability of machine learning-based algorithms to adversarial attacks. For these reasons, research on the explainability of machine learning is raising. In this paper, we then explore the role of interpretable machine learning in the ecosystem of smart vehicles, with the goal of figuring out if and in what terms explanations help to design secure vehicles. We provide an overview of the potential uses of explainable machine learning, along with recent work in the literature that has started to investigate the topic, including from the perspectives of human-agent systems and cyber-physical systems. Our analysis highlights both benefits and criticalities in employing explanations.  © 2020 AEIT.",Automotive; Autonomous Driving; Connected Cars; Cybersecurity; Explainability; Machine learning; Mobility; Smart Vehicles,Computational complexity; Embedded systems; Learning algorithms; Safety engineering; Vehicles; Autonomous driving; Human agent; Is-enabled; Service paradigm; Smart vehicles; Machine learning,Conference paper,Final,,Scopus,2-s2.0-85099683035
Wrobel A.; Placzek M.,"Wrobel, A. (57214987076); Placzek, M. (35366753400)",57214987076; 35366753400,Visualization systems for industrial automation systems,2018,IOP Conference Series: Materials Science and Engineering,400,6,62032,,,,1,10.1088/1757-899X/400/6/062032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055780700&doi=10.1088%2f1757-899X%2f400%2f6%2f062032&partnerID=40&md5=731cbf9196c0cacf4e32b318cd355c26,"In currently designed industrial automation systems, a very important task is fulfilled by appropriate legible and properly designed system visualization. You cannot imagine, for example, a control room in a power station where there are lights, recorders and switches located on the walls. Currently, all power plants or, alternatively designated areas are controlled by automation systems with SCADA systems. These systems allow you to observe variables, change their status, record in real time as well as archiving variables, creating transparent charts, etc. operations [1-4,5,6]. The subject of the article is to present the real object of a high bay warehouse with a SCADA environment as a supervisory control. In the Proficy iFix environment, a visualization was made thanks to which the operator has the possibility to change the process of using the virtual buttons and manually entering setpoints. In addition, the amount of process data obtained is significantly increased, which facilitates and speeds up the operator's decision making. The machine status data is presented in the form of virtual signaling lamps, displayed values of process variables, graphical representation of process variables, and animation of the high bay warehouse execution member reflecting the actual traffic. © Published under licence by IOP Publishing Ltd.",,Automation; Decision making; SCADA systems; Visualization; Warehouses; Automation systems; Graphical representations; Industrial automation system; Power station; Process Variables; Supervisory control; System visualization; Visualization system; Real time systems,Conference paper,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85055780700
,,,"17th International Conference on Engineering Psychology and Cognitive Ergonomics, EPCE 2020, held as part of the 22nd International Conference on Human-Computer Interaction, HCII 2020",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12187 LNAI,,,,,774,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088515428&partnerID=40&md5=bdaac8c609abc3a6bf86d14d5c3654b0,"The proceedings contain 60 papers. The special focus in this conference is on Engineering Psychology and Cognitive Ergonomics. The topics include: Promoting operational readiness through procedures in nuclear domain; research on bim and mobile equipment in substation construction schedule management; modeling distributed situational awareness to improve handling emergency calls in operation centres; multidimensional risk dynamics modeling on operator errors of nuclear power plant; using idheas to analyze incident reports in nuclear power plant commissioning: a case study; cognitive-based severe accident information system development in a human factors project; foreword; measuring situation awareness in control room teams; mixed-initiative human-automated agents teaming: towards a flexible cooperation framework; a framework for human-autonomy team research; safety challenges of ai in autonomous systems design – solutions from human factors perspective emphasizing ai awareness; human-autonomy teaming and explainable ai capabilities in rts games; rationality, cognitive bias, and artificial intelligence: a structural perspective on quantum cognitive science; a concept on the shared use of unmanned assets by multiple users in a manned-unmanned-teaming application; allocation of moral decision-making in human-agent teams: a pattern approach; the cueing effect in retrieval of expertise: designing for future intelligent knowledge management system; the effect of group membership, system reliability and anthropomorphic appearance on user’s trust in intelligent decision support system; assessing professional cultural differences between airline pilots and air traffic controllers; exploring the effects of large screen overview displays in a nuclear control room setting; comparison of pedestrians’ gap acceptance behavior towards automated and human-driven vehicles.",,,Conference review,Final,,Scopus,2-s2.0-85088515428
Sawyer B.D.; Dobres J.; Chahine N.; Reimer B.,"Sawyer, Ben D. (36025972800); Dobres, Jonathan (36628055900); Chahine, Nadine (55511610300); Reimer, Bryan (7003475727)",36025972800; 36628055900; 55511610300; 7003475727,The cost of cool: Typographic style legibility in reading at a Glance,2017,Proceedings of the Human Factors and Ergonomics Society,2017-October,,,833,837,4,16,10.1177/1541931213601698,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042491818&doi=10.1177%2f1541931213601698&partnerID=40&md5=bd5dd034d917e613ef65709ba31c00c6,"When designers typographically tweak fonts to make an interface look 'cool,' they do so amid a rich design tradition, albeit one that is little-studied in regards to the rapid 'at a glance' reading afforded by many modern electronic displays. Such glanceable reading is routinely performed during human-machine interactions where accessing text competes with attention to crucial operational environments. There, adverse events of significant consequence can materialize in milliseconds. As such, the present study set out to test the lower threshold of time needed to read and process text modified with three common typographic manipulations: letter height, width, and case. Results showed significant penalties for the smaller size. Lowercase and condensed width text also decreased performance, especially when presented at a smaller size. These results have important implications for the types of design decisions commonly faced by interface professionals, and underscore the importance of typographic research into the human performance impact of seemingly ""aesthetic"" design decisions. The cost of ""cool"" design may be quite steep in high-risk contexts. Copyright 2017 by Human Factors and Ergonomics Society.",,Design; Human engineering; Adverse events; Design decisions; Electronic display; Human machine interaction; Human performance; Operational environments; Ergonomics,Conference paper,Final,,Scopus,2-s2.0-85042491818
Cai C.J.; Jongejan J.; Holbrook J.,"Cai, Carrie J. (57219528527); Jongejan, Jonas (57208703292); Holbrook, Jess (57205326616)",57219528527; 57208703292; 57205326616,The effects of example-based explanations in a machine learning interface,2019,"International Conference on Intelligent User Interfaces, Proceedings IUI",Part F147615,,,258,262,4,179,10.1145/3301275.3302289,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065577583&doi=10.1145%2f3301275.3302289&partnerID=40&md5=d598433d5a952339ba1fb903dd733220,"The black-box nature of machine learning algorithms can make their predictions difficult to understand and explain to end-users. In this paper, we propose and evaluate two kinds of example-based explanations in the visual domain, normative explanations and comparative explanations (Figure 1), which automatically surface examples from the training set of a deep neural net sketch-recognition algorithm. To investigate their effects, we deployed these explanations to 1150 users on QuickDraw, an online platform where users draw images and see whether a recognizer has correctly guessed the intended drawing. When the algorithm failed to recognize the drawing, those who received normative explanations felt they had a better understanding of the system, and perceived the system to have higher capability. However, comparative explanations did not always improve perceptions of the algorithm, possibly because they sometimes exposed limitations of the algorithm and may have led to surprise. These findings suggest that examples can serve as a vehicle for explaining algorithmic behavior, but point to relative advantages and disadvantages of using different kinds of examples, depending on the goal. © 2019 Copyright held by the owner/author(s).",Example-based explanations; Explainable AI; Human-AI interaction; Machine learning,Deep neural networks; Learning algorithms; Learning systems; User interfaces; Black boxes; End users; Example based; Human-AI interaction; Online platforms; Sketch recognition; Training sets; Machine learning,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85065577583
Viana J.; Cohen K.,"Viana, Javier (57216319684); Cohen, Kelly (7102615249)",57216319684; 7102615249,Fault Tolerance Tool for Human and Machine Interaction & Application to Civilian Aircraft,2019,"2019 IEEE Latin American Conference on Computational Intelligence, LA-CCI 2019",,,9037045,,,,0,10.1109/LA-CCI47412.2019.9037045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083110988&doi=10.1109%2fLA-CCI47412.2019.9037045&partnerID=40&md5=f18638fbc4c24f545cf2455fd452f62e,"Enhancing human-machine interaction is critical to aerospace applications. An essential requirement in safety critical systems is the clear need to guarantee trustworthiness of a system as well as V&V (Verification and Validation). However, the current state of the art concerning decision support systems lacks effective tools in this area. The Coherence Function Package, introduced in this research, is a tool towards providing assurance that the action needed has the approval of both the human and the machine in terms of SAFETY. These algorithms shed light on the future of an Explainable Artificial Intelligence (XAI, [1]), that fosters a synergy between these two factors. This vital requirement that has been further underscored after the tragic events of the Boeing 737 Max 8 crashes [2]. Preliminary results show that the proposed approach is not only able to detect any errors in the system, it also assists in circumventing conflicts leading to incoherence and suggests a preferred solution in real-time. © 2019 IEEE.",Aircraft Safety; Equivalences; Flight Critical Systems; Human-Machine Interaction; Implications; Recursive Functions; Tautology,Aerospace applications; Aircraft; Aircraft accidents; Decision support systems; Intelligent computing; Civilian aircrafts; Coherence function; Effective tool; Human machine interaction; Preferred solutions; Safety critical systems; State of the art; Verification-and-validation; Fault tolerance,Conference paper,Final,,Scopus,2-s2.0-85083110988
Omeiza D.; Anjomshoae S.; Kollnig K.; Camburu O.-M.; Främling K.; Kunze L.,"Omeiza, Daniel (57219482072); Anjomshoae, Sule (56674620000); Kollnig, Konrad (57221875382); Camburu, Oana-Maria (57063266900); Främling, Kary (6506103412); Kunze, Lars (36625530100)",57219482072; 56674620000; 57221875382; 57063266900; 6506103412; 36625530100,Towards Explainable and Trustworthy Autonomous Physical Systems,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,3,10.1145/3411763.3441338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105807078&doi=10.1145%2f3411763.3441338&partnerID=40&md5=aa7f2158265e53a05e358be07999abf3,"The safe deployment of autonomous physical systems in real-world scenarios requires them to be explainable and trustworthy, especially in critical domains. In contrast with g€black-box' systems, explainable and trustworthy autonomous physical systems will lend themselves to easy assessments by system designers and regulators. This promises to pave ways for easy improvements that can lead to enhanced performance, and as well, increased public trust. In this one-day virtual workshop, we aim to gather a globally distributed group of researchers and practitioners to discuss the opportunities and social challenges in the design, implementation, and deployment of explainable and trustworthy autonomous physical systems, especially in a post-pandemic era. Interactions will be fostered through panel discussions and a series of spotlight talks. To ensure lasting impact of the workshop, we will conduct a pre-workshop survey which will examine the public perception of the trustworthiness of autonomous physical systems. Further, we will publish a summary report providing details about the survey as well as the identified challenges resulting from the workshop's panel discussions. © 2021 Owner/Author.",collaboration; Explainability; human-machine interaction; trust,Human engineering; Distributed groups; Panel discussions; Physical systems; Public perception; Real-world scenario; Social challenges; System designers; Virtual workshops; Surveys,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85105807078