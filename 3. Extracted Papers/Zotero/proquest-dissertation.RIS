TY  - Dissertations & Theses
T1  - The Technology Adoption Model for Cloud Computing, Storytelling Artificial Intelligence, and the Federal Risk and Authorization Management Program
AU  - Jackson, Freeman Augustus
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798381947045
AB  - This dissertation examines challenges and opportunities in federal government Conversational AI and Machine Learning (CAIML) integration. It emphasizes Conversational AI (CAI)'s impact on decision-making across sectors and its improvement of human-technology interactions, particularly in IoT and cloud computing. The study reviews AI storytelling, including NLP, character generation, VR/AR, deep learning, leadership, and FedRAMP compliance.Qualitative research follows PRISMA and uses NVivo for data analysis. A comprehensive literature review, qualitative analysis, and expert interviews reveal CAIML adoption challenges and opportunities. Leadership, economic and legal factors, privacy, data protection, and national security are studied. The dissertation concludes with a summary of its findings and research questions on federal agency CAI and ML adoption barriers, operational improvements, legal/regulatory issues, privacy, data protection, security threats, and effective leadership strategies. CAI and ML integration into federal infrastructure and national security and intelligence implications are also discussed. The dissertation recommends federal agencies prioritize personnel training, migration planning, and solution sustainability to advance CAIML adoption. It helps federal policymakers and practitioners adopt CAIML technologies by highlighting their transformative potential and challenges.
UR  - https://www.proquest.com/docview/2955907218?accountid=15181&bdid=109696&_bd=7s%2FwE9GpG%2BHj%2FB%2F54oMZrGXI0HE%3D
ER  - 

TY  - Dissertations & Theses
T1  - Context-Aware Machine Learning for Low-Burden Brain-Computer Interfaces
AU  - de Wit, T. Warren
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798383570999
AB  - We are interested in the utility that artificially intelligent mobile systems such as drones offer to personnel in fast-paced, high-stakes situations such as disaster relief that demand real-time situational awareness. Ideally, these assistive systems place no additional cognitive or physical burden on their user; rather, they should respond to the user’s intent with minimal physical or cognitive impact. Artificial intelligence (AI) and machine learning (ML) are already widely applied in both brain-computer interfaces (BCI) and drone navigation. We propose leveraging the robust computer-vision based AI that exists on modern drones to use objects as waypoints and fly a reconnaissance drone mostly autonomously, with electroencephalography (EEG) in an object recognition paradigm for selecting the drone’s waypoint. In this work, our goal is to provide a proof-of-concept for the intent recognition portion of this design through a context fusion approach that allows selecting a waypoint without using existing techniques that require environmental modification or techniques such as Rapid Serial Visual Presentation (RSVP) that do not translate to kinetic situations. We outline a framework we call Human Intent-Guided Autonomous Systems (HIGAS) as a general paradigm for this type of system-of-systems that facilitate human-AI teaming by using decision fusion between biosignal-based intent recognition and sensor-borne context awareness. We introduce the Context-Signal Decision Fusion (CSDF) model to merge EEG with imagery and conduct a 42-subject experiment to explore its viability, requirements, performance, and dynamics. In the end, we show that CSDF shows potential for implementation in the wider HI-GAS framework even with relatively low-cost, portable hardware. We evaluate the model under a variety of dynamics, identify results regarding subject-independence and architectural variation, and present mechanisms to explore the model from an explainability perspective.
UR  - https://www.proquest.com/docview/3084670643?accountid=15181&bdid=109696&_bd=bhvC66CiJlQPFM05VrVPpoMtNpU%3D
ER  - 

TY  - Dissertations & Theses
T1  - More Applicable Text Classification with Human-in-the-Loop: Patterns, Frameworks, and Tools
AU  - Andersen, Jakob Smedegaard
JF  - PQDT - Global
Y1  - 2024-01-01
DA  - 2024
SN  - 9798346770039
AB  - Machine Learning (ML)-based text classification offers a promising and scalable approach to automate the classification of text, such as app reviews or social media posts. However, the applicability of text classifiers in real-world settings is limited by inherent uncertainties and potential performance gaps.This thesis explores the potential of the emerging field of Human-in-the-Loop (HiL) to address the applicability challenges for ML-based text classification. Our goal is to increase applicability by efficiently incorporating humans into the text classification pipeline. To this end, we review the current literature on HiL and develop a pattern catalog to provide software developers with best practices for designing HiL systems. Our catalog aims to facilitate the development and deployment of applicable HiL systems that integrate humans into the ML loop.Next, this thesis proposes and evaluates novel frameworks that implement three HiL patterns. Addressing common applicability challenges such as high computational resource requirements, limited classification performance, and high model latency. In particular, the proposed frameworks utilize the con- cept of prediction uncertainty to coordinate human efforts. We evaluated the effectiveness of our frameworks using datasets from the domains of software engi- neering, online journalism, and social media analysis. Our contributions enable more applicable solutions for the cost-optimized training and deployment of text classifiers.Furthermore, we propose a novel framework for explaining the prediction uncertainty of text classifiers in order to improve user understanding of clas- sification decisions. While existing explanation techniques mainly explain the provided class label, we build on the concept of prediction uncertainty and make it explicit to users.Finally, based on our HiL patterns, this thesis develops REM, a visual tool. for the real-time moderation of online forums. REM enables a semi-automated moderation of continuous streams of user comments by integrating various HiL patterns and HiL collaboration mechanisms. Experiments with REM show promising results, achieving a significant increase in classification performance (from 78.48% to 96.08%) while requiring manual moderation of only 25% of the data. These contributions empower domain experts to be efficiently in- volved in the text classification process, ultimately improving the applicability and efficiency of forum moderation in real-world settings. REM was developed specifically for online journalism, but can be easily adapted to other domains.
UR  - https://www.proquest.com/docview/3143984917?accountid=15181&bdid=109696&_bd=TRKuuWl4Ux7ca4i0lceO%2BLSvBiw%3D
ER  - 

TY  - Dissertations & Theses
T1  - Embedded in Informality: Political Economy of Platform Work in India
AU  - Chintala, Swati Reddy
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798383166000
AB  - Through this dissertation, I systematically explore the platform economy in India, studying how it is shaped by its embeddedness in the informal economy, the predominant form in which economic activity is organized in India. Drawing on 155 interviews, document analysis, and hours of observational data, I study three closely inter-linked sociological questions – How do platform companies organize work and what is the labour process experienced by platform workers? What are the consequences of this labour process on workers’ actions – collective and individual? What, if any, is the role of the state in the functioning of the platform economy? I find that technology allows platforms to complement and intensify control over parts of the labour process, rather than replace conventional means of control. Technology does not determine the extent of control which instead depends on the extent of standardization of an occupation and the predictability of demand for related services. Next I find that platform work organization, especially algorithmic work allocation and control, enable the emergence of a collective identity amongst platform workers who then address their demands to platform companies - visible entities unlike informal enterprises which operate in the shadows - and to the state, which continues to be looked upon as a provider of social security services, however meager. Finally, evaluating the role of the state in the platform economy, I find that the state is neither unwilling nor unable to regulate platform companies. Instead, the state's interest lies in sustaining the platform economy as a critical source of employment against a backdrop of historically high unemployment. As such, the state has to undertake a balancing act - regulating platform companies on behalf of consumers, creating social security legislation for platform workers, and supporting platform companies by protecting them from the costs of providing social security to workers.
UR  - https://www.proquest.com/docview/3072191523?accountid=15181&bdid=109696&_bd=tE6R1Iw7yLdl6u1%2F14pf7FNgA%2BE%3D
ER  - 

TY  - Dissertations & Theses
T1  - Gene Set Anomaly Score: A Genomic Data and Knowledge Driven Approach for Analysing Anomalous Gene Expression in Cancer Patients
AU  - Kamal, Md Sarwar
JF  - PQDT - Global
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381034271
AB  - Genomics research often uses Gene Set Enrichment Analysis (GSEA) to rank genes that correlate with the presence of phenotypical traits and to interpret how variations in gene expression infuence those traits. GSEA provides an explanation of found genes through their associations with gene sets. As gene sets represent different biological mechanisms, they can indicate overall shifts in expression values in relation to their biology.This thesis investigated the relationships between patients and diseases by using gene sets, integrating gene expression data and gene set ontologies to develop a new analytics method called gene expression anomaly scores. These scores measure the deviation of expression values from expected values.This thesis investigated the representation of patient biology as two-dimensional graphs derived from anomaly scores. There are thousands of patient gene sets relating to a given disease, such as cancer. To identify strongly associated gene sets, this thesis apply principal component analysis (PCA) and maximum relevance and minimum redundancy (MRMR), selecting the two most prominent dimensions. Thus, PCA and MRMR were each used to embed patients into a 2-dimensional anomaly score space. Embedding patients using anomaly scores revealed relationships between patients and patient biology through clustering and feature selection in this space. Moreover, this thesis applied explainable AI (XAI) to understand patients’ biology (gene sets) responsible for prediction by predictive models or AI algorithms. This thesis applied Local Interpretation-Driven Abstract Bayesian Network (LINDA-BN) which extracts patients biology and shows the relationships between biologies responsible for a prediction.The proposed method was used to analyse gene expression data of cancer patients from four different data sets. More specifcally, anomaly scores followed by PCA or MRMR showed groups of cancer patients in scatter plots. These groups appeared to be related to treatment outcomes. In addition, MRMR was able to identify potential gene sets with meaningful biological implications. Comparatively, when raw and state-of-the-art gene expression scores were analyzed, only genes patterns were apparent. The outcomes of the distributions showed that the distribution of anomaly scores varied signifcantly between patients who relapsed and those who did not. In addition, the k-means algorithm revealed that the anomaly score performs better clustering than state-of-the-art methodologies.Furthermore, anomaly scores uncovered novel cancer biology in contrast to gene set enrichment analysis (GSEA) and state-of-the-art approaches. Finally, the outcomes of instance-based LINDA-BN showed an interpretable and explicable method for predicting medical condition a cancer patient.
UR  - https://www.proquest.com/docview/2901815266?accountid=15181&bdid=109696&_bd=XAtBW2D6N%2FbAV9AWx7J1hA1cKnA%3D
ER  - 

TY  - Dissertations & Theses
T1  - The Integration of AI and Involved Humans into Decision-Making Structures: A Multiple-Case Study
AU  - Wahler, Fabian Rüdiger
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798308193104
AB  - The growing significance of artificial intelligence in global industries is evident and accelerates each year. This progression is visible in the doubling of private investments to $93.5 billion in 2021 from the previous year. The specific research problem of this study is that there is a lack of understanding of the perceptions of top- and middle-management leaders about the integration of AI and involved humans into their decision-making structures. The purpose of this qualitative multiple case study is to explore the perceptions of top- and middle-management leaders about the integration of AI and involved humans into their decision-making structures in the automotive industry in 2024 in Germany. By conducting a qualitative multiple-case study grounded on a comprehensive literature review and the actor-network theory as a conceptual framework, the study has derived a process to successfully integrate humans and AI in decision-making processes supported by best practices to further increase the acceptance level of human users. The process consists of 5 major fields, which are connected: Current challenges in decision-making processes, potential of AI to improve decision-making processes, concerns and risks on AI integration in decision-making processes, influence on human and human involvement, and initial process recommendation and areas for successful AI integration. Practical implications include the changing role of stakeholders and the evolving decision-making process by the introduction of AI. The results further provide theoretical expansions of knowledge of AI and humans in decision-making processes. Further research should focus on quantitative studies based on my results to validate the emerged theories further.
UR  - https://www.proquest.com/docview/3175900951?accountid=15181&bdid=109696&_bd=m4j303y1MEDCf7AkeMh0jJfYlUI%3D
ER  - 

TY  - Dissertations & Theses
T1  - Safety Engineering of Computational Cognitive Architectures within Safety-Critical Systems
AU  - Dreany, Harry Hayes
JF  - ProQuest Dissertations and Theses
Y1  - 2018-01-01
DA  - 2018
SN  - 978-0-355-63162-3
AB  - This paper presents the integration of an intelligent decision support model (IDSM) with a cognitive architecture that controls an autonomous non-deterministic safety-critical system. The IDSM will integrate multi-criteria, decision-making tools via intelligent technologies such as expert systems, fuzzy logic, machine learning, and genetic algorithms. Cognitive technology is currently simulated within safety-critical systems to highlight variables of interest, interface with intelligent technologies, and provide an environment that improves the system’s cognitive performance. In this study, the IDSM is being applied to an actual safety-critical system, an unmanned surface vehicle (USV) with embedded artificial intelligence (AI) software. The USV’s safety performance is being researched in a simulated and a real-world, maritime based environment. The objective is to build a dynamically changing model to evaluate a cognitive architecture’s ability to ensure safe performance of an intelligent safety-critical system. The IDSM does this by finding a set of key safety performance parameters that can be critiqued via safety measurements, mechanisms, and methodologies. The uniqueness of this research lies in bounding the decision-making associated with the cognitive architecture’s key safety parameters (KSPs). Other real-time applications (RTAs) that would benefit from advancing cognitive science associated with safety are unmanned platforms, transportation technologies, and service robotics. Results will provide cognitive science researchers with a reference for the safety engineering of artificially intelligent safety-critical systems.
UR  - https://www.proquest.com/docview/2013312506?accountid=15181&bdid=109696&_bd=G%2Fmwyg0YI9Gcy1JqPRTtvjqJCAg%3D
ER  - 

TY  - Dissertations & Theses
T1  - The Evolving Role of Programmers in an AI-Chatbot Dominated World: Challenges, Adaptation Strategies, and Future Prospects
AU  - Le, Hanh
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798384499374
AB  - In recent years, chat and conversational AI technologies have seen growing integration across various industries, improving customer service, streamlining operations, and enhancing user experiences. This qualitative study examined the transformative impact of AI chatbots on the programming field and their broader effects. The study examined how AI chatbots have changed programming practices, boosted productivity, and introduced new opportunities and challenges. The study used hermeneutic phenomenology to interpret and understand these changes and their implications. The findings underscored the importance of addressing ethical issues, such as data privacy and unintended biases in AI systems, while highlighting the need for cross-disciplinary collaboration and a focus on user experience to maximize the benefits of AI chatbots. The study revealed that the role of programmers must evolve to integrate AI and chatbots effectively, emphasizing the importance of adapting to new responsibilities, including ethical considerations and continuous learning. Recommendations included upskilling in AI and machine learning and updating educational programs to prepare professionals for an AI-driven world. The findings highlighted the necessity for additional research to examine the broader effects of AI on related fields beyond programming. Future studies can explore the relationship between job characteristics, deskilling, and adaptation strategies across various industries. Such research would offer valuable insights into the broader effects of AI on different roles and career outcomes across multiple sectors.
UR  - https://www.proquest.com/docview/3117977902?accountid=15181&bdid=109696&_bd=XrRbEmib8HuxCwcjl8vP0EzkeQM%3D
ER  - 

TY  - Dissertations & Theses
T1  - Artificial Neural Networks in Public Policy: Towards an Analytical Framework
AU  - Lee, Joshua A.
JF  - ProQuest Dissertations and Theses
Y1  - 2020-01-01
DA  - 2020
SN  - 9798662408616
AB  - This dissertation assesses how artificial neural networks (ANNs) and other machine learning systems should be devised, built, and implemented in US governmental organizations (i.e. public agencies). While it primarily focuses on ANNs given their current prevalence and accuracy, many of its conclusions are broadly applicable to other kinds of machine learning as well.It develops an analytical framework, drawn from diverse fields including law, behavioral psychology, public policy, and computer science, that public agency managers and analysts can utilize. The framework yields a series of principles based on my research methodology that I argue are the most relevant to public agencies. The qualitative methodology consists of an iterative approach based on archival research, peer review, expert interviews, and comparative analysis.Critically, this dissertation’s intent is not to provide the specific answers to all questions related to machine learning in public agencies. Given the speed at which this field changes, attempting to provide universally applicable answers would be difficult and short term at best. Rather, this framework focuses on principles which can help guide the user to the proper questions they need to ask for their particular use case. In that same vein, the normative principles it provides are procedurally focused in scope rather than focused on policy outcomes. In other words, this framework is meant to be equally applicable regardless of what one’s specific policy goals are.
UR  - https://www.proquest.com/docview/2427238273?accountid=15181&bdid=109696&_bd=SdKx%2BWrqtuygdWW2NIF9F%2FKrykM%3D
ER  - 

TY  - Dissertations & Theses
T1  - Exploring Algorithmic Literacy for College Students: An Educator’s Roadmap
AU  - Archambault, Susan Gardner
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798841719168
AB  - Research shows that college students are largely unaware of the impact of algorithms on their everyday lives. Also, most university students are not being taught about algorithms as part of the regular curriculum. This exploratory, qualitative study aimed to explore subject-matter experts’ insights and perceptions of the knowledge components, coping behaviors, and pedagogical considerations to aid faculty in teaching algorithmic literacy to college students. Eleven individual, semi-structured interviews and one focus group were conducted with scholars and teachers of critical algorithm studies and related fields. Findings suggested three sets of knowledge components that would contribute to students’ algorithmic literacy: general characteristics and distinguishing traits of algorithms, key domains in everyday life using algorithms (including the potential benefits and risks), and ethical considerations for the use and application of algorithms. Findings also suggested five behaviors that students could use to help them better cope with algorithmic systems and nine teaching strategies to help improve students’ algorithmic literacy. Suggestions also surfaced for alternative forms of assessment, potential placement in the curriculum, and how to distinguish between basic algorithmic awareness compared to algorithmic literacy. Recommendations for expanding on the current Association of College and Research Libraries’ Framework for Information Literacy for Higher Education (2016) to more explicitly include algorithmic literacy were presented.
UR  - https://www.proquest.com/docview/2706673106?accountid=15181&bdid=109696&_bd=%2FldReV8v8KTOjKUN3%2FgUl7zg8qc%3D
ER  - 

TY  - Dissertations & Theses
T1  - Where the Wild Things Are: Computer Vision for Global-Scale Biodiversity Monitoring
AU  - Beery, Sara Meghan
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380870627
AB  - We require a real-time, modular earth observation system that unites efforts across research groups in order to provide the necessary information necessary for global-scale impact in sustainability and conservation in the face of climate change. The development of such systems requires collaborative, interdisciplinary approaches that translate diverse sources of raw information into accessible scientific insight. For example, we need to monitor species in real time and in greater detail to quickly understand which conservation efforts are most effective and take corrective action. Current ecological monitoring systems generate data far faster than researchers can analyze it, making scaling up impossible without automated data processing. However, ecological data collected in the field presents a number of challenges that current methods, like deep learning, are not designed to tackle. These include strong spatiotemporal correlations, imperfect data quality, fine-grained categories, and long-tailed distributions. Our work seeks to overcome these challenges, and this thesis includes methods which can learn from imperfect data, systematic frameworks and benchmarks for measuring and overcoming performance drops due to domain shift, and the development and deployment of efficient human-AI systems that have real-world conservation impact.
UR  - https://www.proquest.com/docview/2898870216?accountid=15181&bdid=109696&_bd=VglIJRMoVPakGePqnRM6aNYg4i8%3D
DO  - https://doi.org/10.7907/m4mt-2q51
ER  - 

TY  - Dissertations & Theses
T1  - Essays on Online Platforms and Human-Algorithm Interaction
AU  - Moehring, Alex
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798304955072
AB  - This dissertation contains three chapters that analyze how algorithms on social media platforms influence the content that users engage with and how individuals incorporate algorithmic predictions in their decision-making. In Chapter 1, I study how engagement maximizing news feed algorithms on social media affect the credibility of news content with which users engage. This allows me to estimate the extent to which engagement-maximizing algorithms promote and incentivize low-quality content. In addition, I evaluate how the ranking algorithm itself can be designed to promote and encourage engagement with high quality content. In Chapter 2, I analyze how the introduction of a new non-personalized news feed impacts user engagement quantity, quality, and diversity on the Reddit platform. I find that this auxiliary feed increases the share of users that engage with news-related content and the diversity of engagement within news categories and within articles from publishers across the political spectrum increases as a result of the feed. In Chapter 3, in collaboration with Nikhil Agarwal, Tobias Salz, and Pranav Rajpurkar, we study human-AI collaboration using an information experiment with professional radiologists. Results show that providing (i) AI predictions does not always improve performance, whereas (ii) contextual information does. Radiologists do not realize the gains from AI assistance because of errors in belief updating – they underweight AI predictions and treat their own information and AI predictions as statistically independent.
UR  - https://www.proquest.com/docview/3171962953?accountid=15181&bdid=109696&_bd=SqNThj1isYg21O2%2BQUR5bg9OLi8%3D
ER  - 

TY  - Dissertations & Theses
T1  - Designing Human-Centered Algorithms for the Public Sector: A Case Study of the U.S. Child Welfare System
AU  - Saxena, Devansh
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798379899646
AB  - Public sector agencies in the United States are increasingly seeking to emulate business models of the private sector centered in efficiency, cost reduction, and innovation through the adoption of algorithmic systems. These data-driven systems purportedly improve decision-making; however, the public sector poses its own unique challenges where policies, practices, and organizational constraints mediate all decisions. Algorithms that do not account for these pertinent aspects of professional practice frustrate practitioners, diminish the quality of human discretionary work, and amplify biases in decision-making. A human-centered research agenda can help us develop algorithms centered in social-ecological theories that support the decision-making processes of practitioners, incorporate novel sources of data, and offer a means to evaluate algorithms in their real-world contexts.This dissertation draws upon a case study of the child-welfare system and outlines responsible pathways forward for the design of human-centered algorithms in the public sector and contributes a holistic understanding of a complex sociotechnical system through deep ethnographic work, the design of a theoretical framework for algorithmic decision-making in the public sector, and computational narrative analysis of a critical data source that can help contextualize critical factors and improve decision-making. It showcases the practical tradeoffs that need to be balanced for algorithm design - 1) at the human discretion level, I highlight different insertion points and goals of algorithms to augment practitioners’ decision-making processes, 2) at the bureaucratic level, I highlight the constraints within which all decisions (human or algorithmic) must be made and how organizational resources can be leveraged to ensure the proper integration and adoption of an algorithmic system, 3) at the algorithmic level, I showcase how algorithm design can account for the uncertainties inherent within cases and support decision-making processes instead of providing predicted outcomes. This dissertation work has provided actionable steps for human-centered algorithm design to child-welfare leadership and public interest technologists that will further help ensure that decisions are centered in evidence-based practice and lead to positive outcomes for families.
UR  - https://www.proquest.com/docview/2835781583?accountid=15181&bdid=109696&_bd=I6%2B%2FhyfREUF8Ni94zfR%2BV5dJU6o%3D
ER  - 

TY  - Dissertations & Theses
T1  - Revealing and Mitigating Harmful Assumptions and Behaviors in Human-Autonomy Teaming
AU  - Chang, Christine T.
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798384048893
AB  - Human-autonomy teaming in complex environments continues to evolve with technological innovations like mixed reality and rapidly improving large language models. With this evolution comes a need for increased safety measures and better ways for humans to learn and understand these systems. The work presented in this dissertation aims to address questions about safety, appropriate trust, and appropriate use of autonomy by and for humans. I begin with an overview of how mixed reality, and mainly augmented reality, is used for human-robot collaboration. I then explore how we might use augmented reality to promote safety and compliance in a shared space environment with humans and robots. This leads to the question of how we can actively warn humans about failures of autonomous chatbots. And finally I investigate the use of iteratively adding latent human knowledge to an autonomous robot's trajectory optimization as a way of improving both learning and mission outcomes. Ultimately I show that humans have a propensity to dangerously overtrust robots and other forms of autonomy, however we can mitigate this bias with certain design considerations including iteration and transparency.
UR  - https://www.proquest.com/docview/3100430138?accountid=15181&bdid=109696&_bd=5SLfpeTFQk2GcCdwBO8L9XSpuUc%3D
ER  - 

TY  - Dissertations & Theses
T1  - AI Enhanced Reasoning: Augmenting Human Critical Thinking With AI Systems
AU  - Danry, Valdemar M.
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381957044
AB  - The pursuit of knowledge and understanding has been a driving force for humanity since the beginning of time. This relentless quest for reason has shaped the world as we know it, enabling us to unlock secrets of the cosmos, develop innovative technologies, and address complex global challenges. How- ever, despite our cognitive leaps, we still grapple with the limitations of our rationality, biases, and emotions, especially in today's increasingly complex and information-saturated world. As Al systems become more entwined with our daily lives and institutions, there is a growing need to design and deploy AI systems that augment human reasoning, foster critical thinking, and promote well-informed decision-making.This thesis investigates the potential for Al-enhanced reasoning systems and their impact on human decision-making. Specifically, it explores three distinct aspects of critical thinking with Al systems: (1) the development of AI logic-checking systems designed to help identify reasoning flaws, (2) examining the susceptibility of individuals to deceptive Al-generated explanations, and (3) assessing the potential of a novel Al-framed questioning interaction method to provoke critical thinking through a series of human subjects experiments.These investigations aim to shed light on the implications of Al systems on human reasoning and provide insights into designing AI interventions that meaningfully enhance our cognitive abilities. The findings demonstrate the potential for intelligently designed Al systems to support human reasoning, while also highlighting the potential risks associated with overreliance on these tools. By addressing these challenges, this thesis contributes to the ongoing conversation around the development of AI systems that advance our reasoning, and steps towards cultivating a discerning and rational citizenry capable of navigating the complexities of the modern world.
UR  - https://www.proquest.com/docview/3031029446?accountid=15181&bdid=109696&_bd=DSJUBPt%2BuQMf5kKAxnqWhiTu1BU%3D
ER  - 

TY  - Dissertations & Theses
T1  - Beyond Labels and Captions: Contextualizing Grounded Semantics for Explainable Visual Interpretation
AU  - Aakur, Sathyanarayanan Narasimhan
JF  - ProQuest Dissertations and Theses
Y1  - 2019-01-01
DA  - 2019
SN  - 9781085736855
AB  - One of the long-standing problems in artificial intelligence is the development of intelligent agents with complete visual understanding. Understanding entails recognition of scene attributes such as actors, objects and actions as well as reasoning about the common semantic structure that combines these attributes into a coherent description. While significant milestones have been achieved in the field of computer vision, majority of the work has been concentrated on supervised visual recognition where complex visual representations are learned and a few discrete categories or labels are assigned to these representations. This implies a closed world where the underlying assumption is that all environments contain the same objects and events, which are in one-to-one correspondence with the ground evidence in the image. Hence, the learned knowledge is limited to the annotated training set. An open world, on the other hand, does not assume the distribution of semantics and requires generalization beyond the training annotations. Increasingly complex models require massive amounts of training data and offer little to no explainability due to the lack of transparency in the decision-making process. The strength of artificial intelligence systems to offer explanations for their decisions is central to building user confidence and structuring smart human-machine interactions.In this dissertation, we develop an inherently explainable approach for generating rich interpretations of visual scenes. We move towards an open world open-domain visual understanding by decoupling the ideas of recognition and reasoning. We integrate common sense knowledge from large knowledge bases such as ConceptNet and the representation learning capabilities of deep learning approaches in a pattern theory formalism to interpret a complex visual scene. To be specific, we first define and develop the idea of contextualization to model and establish complex semantic relationships among concepts grounded in visual data. The resulting semantic structures, called interpretations allow us to represent the visual scene in an intermediate representation that can then be used as the source of knowledge for various modes of expression such as labels, captions and even question answering. Second, we explore the inherent explainability of such visual interpretations and define key components for extending the notion of explainability to intelligent agents for visual recognition. Finally, we describe a self-supervised model for segmenting untrimmed videos into its constituent events. We show that this approach can segment videos without the need for supervision - neither implicit nor explicit.Combined, we argue that these approaches offer an elegant path to inherently explainable, open domain visual understanding while negating the need for human supervision in the form of labels and/or captions. We show that the proposed approach can advance the state-of-the-art results in complex benchmarks to handle data imbalance, complex semantics, and complex visual scenes without the need for vast amounts of domain-specific training data. Extensive experiments on several publicly available datasets show the efficacy of the proposed approaches. We show that the proposed approaches outperform weakly-supervised and unsupervised baselines by up to 24% and achieves competitive segmentation results compared to fully supervised baselines. The self-supervised approach for video segmentation complements this top-down inference with efficient bottom-up processing, resulting in an elegant formalism for open-domain visual understanding.
UR  - https://www.proquest.com/docview/2295446231?accountid=15181&bdid=109696&_bd=XKJtV1TzEk4VmSTrRFUHuC8%2BOVU%3D
ER  - 

TY  - Dissertations & Theses
T1  - A Critical Study of Geospatial Algorithm Use in Crime Analysis and Predictive Policing
AU  - Weathington, Katy
JF  - ProQuest Dissertations and Theses
Y1  - 2020-01-01
DA  - 2020
SN  - 9798617024731
AB  - We examine in detail two geospatial analysis algorithms commonly used in predictive policing. The k-means clustering algorithm is used to partition input data into k clusters, while Kernel Density Estimation algorithms convert geospatial data into a 2-dimensional probability distribution function. Both algorithms serve unique roles in predictive policing, helping to inform the allocation of limited police resources. Through critical analysis of the k-means algorithm, we found that parameter choice can greatly impact how crime in a city is clustered, which therefor impacts how mental models of crime in the city are developed. Interviews with crime analysts who regularly used k-means revealed that parameters are overwhelmingly chosen arbitrarily. Similarly, KDE parameters greatly influence the resulting PDF, which are visualized in difficult to interpret heatmaps. A mixed method user study with participants of varying backgrounds revealed that those with backgrounds in law enforcement and/or criminal justice rarely actively chose the parameters used, in part due to not fully comprehending the meaning of less obvious parameters. It was also found that individuals with different backgrounds tended to interpret heatmaps and make resource distribution decisions differently.There are several implications from these findings. Primarily, this implies that most would-be users lack the training and expertise to reliably implement and interpret geospatial crime analysis algorithms. Both within and without crime labs, critical thought is rarely given to parameter choice, especially for parameters without a clear, easily understandable explanation. These factors illuminate predictive policing being an inexact science, despite being taken as reliable and objective. These shortcomings and misconceptions, due to their pivotal role at the earliest part of the policing and criminal justice system, have long term consequences for denizens of any place being policed at behest of an algorithm.
UR  - https://www.proquest.com/docview/2395725853?accountid=15181&bdid=109696&_bd=xFuaKE9qlPW4Iuq%2Fyur%2B2X8IODc%3D
ER  - 

TY  - Dissertations & Theses
T1  - Quantitative Neuroimaging with Handcrafted and Deep Radiomics in Neurological Diseases
AU  - Lavrova, Elizaveta
JF  - PQDT - Global
Y1  - 2024-01-01
DA  - 2024
SN  - 9798384171836
AB  - The motivation behind this thesis is to explore the potential of "radiomics" in the field of neurology, where early diagnosis and accurate treatment selection are crucial for improving patient outcomes. Neurological diseases are a major cause of disability and death globally, and there is a pressing need for reliable imaging biomarkers to aid in disease detection and monitoring. While radiomics has shown promising results in oncology, its application in neurology remains relatively unexplored. Therefore, this work aims to investigate the feasibility and challenges of implementing radiomics in the neurological context, addressing various limitations and proposing potential solutions.The thesis begins with a demonstration of the predictive power of radiomics for identifying important diagnostic biomarkers in neuro-oncology. Building on this foundation, the research then delves into radiomics in non-oncological neurology, providing an overview of the pipeline steps, potential clinical applications, and existing challenges. Despite promising results in proof-of-concept studies, the field faces limitations, mostly data-related, such as small sample sizes, retrospective nature, and lack of external validation.To explore the predictive power of radiomics in non-oncological tasks, a radiomics approach was implemented to distinguish between multiple sclerosis patients and normal controls. Notably, radiomic features extracted from normal-appearing white matter were found to contain distinctive information for multiple sclerosis detection, confirming the hypothesis of the thesis.To overcome the data harmonization challenge, in this work quantitative mapping of the brain was used. Unlike traditional imaging methods, quantitative mapping involves measuring the physical properties of brain tissues, providing a more standardized and consistent data representation. By reconstructing the physical properties of each voxel based on multi-echo MRI acquisition, quantitative mapping produces data that is less susceptible to domain-specific biases and scanner variability. Additionally, the insights gained from quantitative mapping are building the bridge toward the physical and biological properties of brain tissues, providing a deeper understanding of the underlying pathology. Another crucial challenge in radiomics is robust and fast data labeling, particularly segmentation. A deep learning method was proposed to perform automated carotid artery segmentation in stroke at-risk patients, surpassing current state-of-the-art approaches. This novel method showcases the potential of automated segmentation to enhance radiomics pipeline implementation.In addition to addressing specific challenges, the thesis also proposes a community-driven open-source toolbox for radiomics, aimed at enhancing pipeline standardization and transparency. This software package would facilitate data curation and exploratory analysis, fostering collaboration and reproducibility in radiomics research.Through an in-depth exploration of radiomics in neuroimaging, this thesis demonstrates its potential to enhance neurological disease diagnosis and monitoring. By uncovering valuable information from seemingly normal brain tissues, radiomics holds promise for early disease detection. Furthermore, the development of innovative tools and methods, including deep learning and quantitative mapping, has the potential to address data labeling and harmonization challenges. Looking to the future, embracing larger, diverse datasets and longitudinal studies will further enhance the generalizability and predictive power of radiomics in neurology. By addressing the challenges identified in this thesis and fostering collaboration within the research community, radiomics can advance toward clinical implementation, revolutionizing precision medicine in neurology.
UR  - https://www.proquest.com/docview/3110364682?accountid=15181&bdid=109696&_bd=BmN9bAjVUmh%2BBmG5dzCn%2FaumSbY%3D
ER  - 

TY  - Dissertations & Theses
T1  - SenseMate: An AI-Based Platform to Support Qualitative Coding
AU  - Overney, Cassandra
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381958287
AB  - Unstructured data can be analyzed numerically or qualitatively through methods like sensemaking. One of the key stages of sensemaking is qualitative coding, where the data is divided into units, and each unit is assigned a category or code. Unfortunately, coding is tedious and time-consuming when carried out manually. Finding a balance between manual and fully-automated coding can help increase efficiency while allowing human judgment and preventing systematic machine errors. In this thesis, I propose an accessible semi-automated approach to qualitative coding. First, I apply a novel machine learning method, rationale extraction models, to qualitative coding. These models recommend themes for each unit of analysis in qualitative data and tend to perform better with less ambiguous themes. Through an online experiment, I find that assistance from rationale extraction models increases coding performance and reliability. Next, I execute an iterative, human-centered design process to create SenseMate, an AI-based platform for qualitative coding. After 13 user testing sessions and 3 design iterations, I observe that model overreliance can be minimized through cognitive forcing functions and easy-to-understand model explanations. I also design several ways for users to efficiently provide feedback on machine-generated rationales. To connect my model and design evaluations, I implement a prototype of SenseMate and conduct a summative user evaluation through an online experiment. The evaluation reveals that participants with access to AI assistance have higher coding performances but spend more time on the platform. The effectiveness of various design decisions within SenseMate is also explored. Finally, I discuss a myriad of future work possibilities. Overall, this thesis offers a practical and accessible solution to analyzing unstructured data, which has broad applications for researchers and organizations across various fields.
UR  - https://www.proquest.com/docview/3031032282?accountid=15181&bdid=109696&_bd=5m8iSxqAwGgAuR3LuPmnKsHEsE4%3D
ER  - 

TY  - Dissertations & Theses
T1  - Explainable AI for High-Stakes Decision-Making
AU  - Carmichael, Zachariah
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798382736853
AB  - Canonical AI algorithms are black boxes. When it comes to high-stakes applications, this is highly undesirable—we need algorithms that we can understand and, in turn, trust. Doctors will not, and should not, trust a machine that cannot reason about its decisions. The push for this explainability has led to the emergence of the explainable AI (XAI) sub-field. While XAI has made progress over the years, there still exists a wealth of issues. In this dissertation, we demonstrate a path toward the glass box ideal: highly performant algorithms that are fully human-comprehensible. We first demonstrate that post hoc explainers, while incredibly popular, should not be trusted. In certain cases in which they must be used, we provide a solution that helps the explainers become more reliable. We characterize the open problems of building an AI system feasible for high-stakes applications through the design of a real-world emergency drone response system. This motivates us to challenge the fundamentals of deep learning architectural design. We bridge XAI and automated machine learning (AutoML) to discover intrinsically debuggable neural networks. Next, we propose novel, intrinsically interpretable approaches to computer vision based on prototypical networks, a type of concept-based neural network. Critically, we address the human-machine semantic similarity gap associated with learned prototypes. First, we enable the automatic learning of prototypical parts with weak supervision using receptive-field constrained networks. Second, we enrich the interpretability of learned concepts by learning prototypical distributions in the invertible latent space of a normalizing flow. We demonstrate a measurable improvement in the comprehensibility of decisions made in both predictive and generative computer vision tasks. The contributions of this dissertation open up the AI black box by substituting correlative explainers with faithful, actionable, intuitive, and debuggable algorithms.
UR  - https://www.proquest.com/docview/3064867539?accountid=15181&bdid=109696&_bd=MuQDorNIB2CNw1UCLudYKIIQVDE%3D
ER  - 

TY  - Dissertations & Theses
T1  - Transportation Mode Choice Behavior in the Era of Autonomous Vehicles: The Application of Discrete Choice Modeling and Machine Learning
AU  - Lee, Sangwan
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798834027041
AB  - New mobility technologies, such as shared mobility services (e.g., car-sharing) and, more importantly, autonomous vehicles (AVs), continue to evolve. The supply-side advancement will likely disrupt and transform transportation mode choice behaviors, and create a new paradigm since they are emerging and becoming increasingly feasible alternatives to the existing modes of transportation. Accordingly, this dissertation employs discrete choice modeling (DCM) and machine learning (ML) using a U.S. nationwide stated choice experiment to understand how travelers adopt new transportation modes or continue to use conventional modes of transportation. This dissertation consists of three papers. The first examines future market shares of each available mode of transportation in the era of AVs, factors influencing mode choice behaviors, and their marginal effects using a mixed logit model. The second uses interpretable ML to investigate the optimal algorithm (i.e., stochastic gradient boosting decision tree model) in greater depth, including feature importance and non-linear marginal effects. Focusing on methodology, the final paper assesses the limitations of ML when applied to transportation mode choice modeling and suggests future research directions for methodological improvements by comparing ML to DCM. The dissertation contributes to three major elements of the current understanding of transportation mode choice behavior in the era of AVs and choice modeling as follows: First, consumers in the AV era could choose from a variety of transportation modes likely to coexist, including private AVs, shared mobility services, and conventional transportation modes. This dissertation thus makes a significant contribution by examining more comprehensive transportation mode choice behaviors and expanding demand-side discussions. Second, since current transportation planning efforts have relied on estimates and expectations, this dissertation contributes to the decision-making process by offering crucial underlying knowledge not currently available. Third, this dissertation assesses the limitations of ML for transportation mode choice modeling and suggests potential future avenues for methodological improvement.
UR  - https://www.proquest.com/docview/2687764542?accountid=15181&bdid=109696&_bd=jTQyNcl2uol0LqJXt5UhIxuH44g%3D
ER  - 

TY  - Dissertations & Theses
T1  - Cyborg Psychology: The Art & Science of Designing Human-AI Systems That Support Human Flourishing
AU  - Pataranutaporn, Pat
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798304946087
AB  - As Artificial Intelligence (AI) becomes increasingly integrated into our daily lives, understanding the psychological implications of human-AI interaction is crucial for developing systems that truly support human capabilities. This dissertation introduces “Cyborg Psychology,” an interdisciplinary, human-centered approach to understanding how AI systems influence human psychological processes. Cyborg Psychology also emphasizes applying these insights to design and develop AI systems that support human flourishing. Cyborg Psychology recognizes the complex, non-linear interactions between humans and AI, acknowledging that both can influence and shape each other in dynamic and often unpredictable ways. Informed by human-computer interaction, psychology, and behavioral sciences, this dissertation focuses on understanding AI’s impact on crucial cognitive and behavioral processes, including motivation, critical thinking, self-reflection, confidence, beliefs, biases, and more. In addition, the work presents several AI systems that apply psychological insights to support human cognition and behavior. For example, the “Wearable Reasoner” seeks to enhance human rationality, “Personalized Virtual Characters” aims to support learning motivation, and “Future You” is designed to encourage long-term oriented thinking and behavior. Employing a diverse array of research methodologies, this work proposes a framework for investigating the implications of interaction design choices. The ultimate goal is to empower the development of AI systems that foster human flourishing by nurturing intellectual growth, cultivating motivation, stimulating critical thinking, and preserving individual autonomy in decision-making.
UR  - https://www.proquest.com/docview/3171136435?accountid=15181&bdid=109696&_bd=DbLkZldrMHvJZCHj8aMfg2TwESE%3D
ER  - 

TY  - Dissertations & Theses
T1  - Artificial Intelligence in Practice
AU  - Lebovitz, Sarah
JF  - ProQuest Dissertations and Theses
Y1  - 2020-01-01
DA  - 2020
SN  - 9798643178774
AB  - Technological advances in artificial intelligence (AI) are promising continuous improvements in problem-solving, perception, and reasoning that are edging closer to human capabilities. AI technologies are raising important questions regarding fundamental issues of organizing, especially regarding the impact of AI on professionals and their work practices. I examine the work practices of professionals facing growing availability of AI tools in their field: medical diagnosis. I draw on ethnographic field data collected across four radiology units within at a major hospital in the United States at the cutting-edge of adopting and evaluating diagnostic AI tools. My data collection and analysis spans processes of developing, evaluating, adopting, and using numerous AI tools over time and in multiple diagnostic contexts. In one chapter, I compare four theoretical approaches to studying technology in organizational processes, answering the question, how does each perspective account for material agency? In the following chapter, I investigate how are professionals using AI tools in their judgment forming processes? I uncover how physicians were experiencing ambiguity throughout their process, and even more so after viewing AI results. I unpack the ways in which they manage this surge in ambiguity and its impact on how AI results are incorporated (or not) into their final diagnosis. These findings contribute to literatures of augmentation, ambiguity, and how professionals experience opaque technologies. In the final chapter, I ask how are managers evaluating which AI tools to adopt in their organizations? I shed light on how organizational leaders are evaluating the potential opportunities and challenges of adopting AI tools and addressing new knowledge issues that emerge. This chapter contributes to literatures on ground truth, technology evaluation practices, and studies of sociomateriality. Overall, the findings of this project illuminate critical challenges to the adoption and evaluation of AI tools that must be understood and addressed if professionals, organizations, and society is to gain the full extent of the transformative promise of AI technologies.
UR  - https://www.proquest.com/docview/2404407253?accountid=15181&bdid=109696&_bd=AO%2FuYy%2Brf%2BK0ToEQM3r%2FNyK5714%3D
ER  - 

TY  - Dissertations & Theses
T1  - Unconscious Bias on the Implementation and Utilization of Emerging Technologies by Law Enforcement Agencies, and Effects on the Security and Privacy of Citizens in Florida: A Case Study of Florida
AU  - McMullen, Trei
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798310391703
AB  - This qualitative study examines the impact of unconscious bias on law enforcement's implementation and utilization of emerging technologies in the United States, focusing on the implications for citizen security and privacy. It incorporates the Technology Acceptance Model 2 (TAM2) (Venkatesh & Davis, 2000) to dissect micro-level factors influencing technological adoption. In parallel, it applies Procedural Justice Theory (Tyler, 1990) to address macro-level implications for community trust and safeguarding citizen rights. The study investigates the presence and role of unconscious biases and their effects on technology implementation, potentially leading to inequitable practices. The research aims to develop strategies to mitigate bias and establish precise regulations through literature reviews, surveys with law enforcement, security, military professionals, and private citizens.An example under scrutiny is the COMPAS system, an algorithm used to predict recidivism. It has raised concerns over procedural justice due to its opaque nature and significant role in judicial decision-making (Yong, 2018). By exploring the complexity of technology adoption and its diverse impact on policing and citizen security and privacy, this study underscores the need for transparent processes and the ethical deployment of technology. The aim is to contribute to scholarly discourse and inform policy and training, deepening our understanding of the interplay between unconscious biases and technological implementation and usage.
UR  - https://www.proquest.com/docview/3192204026?accountid=15181&bdid=109696&_bd=P%2FuEmmbkpa76nrXVeoIXv%2FmgQzQ%3D
ER  - 

TY  - Dissertations & Theses
T1  - Transparent Value Alignment: Foundations for Human-Centered Explainable AI in Alignment
AU  - Sanneman, Lindsay
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381959345
AB  - Alignment of autonomous agents’ values and objectives with those of humans can greatly enhance these agents’ ability to act flexibly to safely and reliably meet humans’ goals across diverse contexts from space exploration to robotic manufacturing. However, it is often difficult or impossible for humans, both expert and non-expert, to enumerate their objectives comprehensively, accurately, and in forms that are readily usable for agent planning. Value alignment is an open challenge in artificial intelligence that aims to address this problem by enabling agents to infer human goals and values through interaction. Providing humans with direct and explicit feedback about this value learning process through approaches for explainable AI (XAI) can enable humans to more efficiently and effectively teach robots about their goals. In this thesis, we introduce the Transparent Value Alignment (TVA) paradigm which captures this two-way communication and inference process and discuss foundations for the design and evaluation of XAI within this paradigm.First, we introduce the Situation Awareness Framework for Explainable AI (SAFEAI), which provides a rigorous approach for comprehensively determining a user’s informational needs for their given role and context, identifying which XAI techniques can be applied to meet these needs or gaps in the current state-of-the-art, and evaluating explanation quality. We also review other human factors literature related to cognitive workload and trust in automation and discuss how these constructs additionally inform the design and evaluation of XAI systems.Next, we propose four metrics for assessing the alignment of reward functions between humans and autonomous agents (i.e. “reward alignment”). These metrics can be applied to study alignment in scenarios where the human’s ground truth reward function is not necessarily directly accessible, as is the case in many real-world settings. We also validate these metrics through a human-subject experiment and a subsequent factor analysis. Findings from this factor analysis indicate the existence of two components comprising the overall reward alignment between humans and agents: feature alignment, which captures how similar a human’s reward features and weights are to an agent’s, and policy alignment, which captures how similar human and agent policies are for a given reward function.We also present a series of human-subject experiments which study the efficacy of a broad range of reward explanation techniques across multiple domains. These experiments consider variable reward complexity (defined as the number of features in the reward function), variable task complexity (defined as the number of tasks the human must perform simultaneously when the explanation is provided), and variable team complexity (defined as the number of agents performing the set of required tasks). The results from these experiments together suggest a trade-off between providing users with direct and complete information about the agent’s reward function through XAI and increasing their workload. Abstraction-based explanations were a promising approach for balancing these factors, but results also indicated the importance of selecting appropriate abstractions for the particular domain, context, and user. Scenarios with higher team complexities (a larger number of agents) were also subjectively assessed more positively than those with lower team complexities, indicating that in terms of interpretability, simpler decoupled agent plans for larger numbers of agents may be preferable to more complex agent plans for fewer agents.Finally, we discuss how the TVA problem framing could be applied to real-world domains in the future through a set of case studies. In particular, we highlight findings from a study of key players in the industrial robotics ecosystem in Europe which identified the importance of developing improved robot interfaces and easier-to-program systems for robotics in manufacturing. We also discuss the applicability of TVA to space mission planning, which is informed by observations of the tactical planning process for the Mars Curiosity rover at the NASA Jet Propulsion Laboratory (JPL). Lastly, we discuss how TVA could be applied to human-autonomy teaming scenarios such as search-and-rescue mission planning. 
UR  - https://www.proquest.com/docview/3030980771?accountid=15181&bdid=109696&_bd=ANSPBj3Y%2FOJ3sOzZKdvKO%2FXtW20%3D
ER  - 

TY  - Dissertations & Theses
T1  - Trustworthy Machine Learning: Learning Under Security, Explainability and Uncertainty Constraints
AU  - Le, Thai Quang
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798841569657
AB  - Trustworthy machine learning models are ones that not only have high accuracy but also well perform under various realistic constraints, security threats, and are transparent to users. By satisfying these constraints, machine learning models can gain trust from their users and thus make it easier for them to be adopted in practice. This thesis makes contributions on three aspects of trustworthy machine learning, namely (i) learning under uncertainty--i.e., able to learn with limited and/or noisy data, (ii) transparent to the end-users--i.e., being explainable to the end-users, and (iii) secured and resilient machine learning--i.e., adversarial attacks and defense from/against malicious actors. Particularly, this thesis proposes to overcome the lack of high-quality labeled textual data that is necessary for training effective ML classification models by directly synthesizing them in the data space using generative neural networks. Moreover, this thesis designs a novel algorithm that facilitates accurate and effective post-hoc explanations of neural networks' predictions to the end-users. Furthermore, this thesis also demonstrates the vulnerability of a wide range of fake news detection models in the literature against a carefully designed adversarial attack mechanism where the attackers can promote fake news or demote real news on social media via social discourse. This thesis also proposes a novel approach that adapts the "honeypot" concept from cybersecurity to proactively defend against a strong universal trigger attack. Last but not least, this thesis contributes to the adversarial text literature by proposing to study, extract and utilize not machine-generated but realistic human-written perturbations online. Through these technical contributions, this thesis hopes to advance the adoption of ML systems in high-stakes fields where mutual trust between humans and machines is paramount.
UR  - https://www.proquest.com/docview/2700375408?accountid=15181&bdid=109696&_bd=z9ncsv%2B%2B%2BahWPonDvJVtFolkRck%3D
ER  - 

TY  - Dissertations & Theses
T1  - Towards Actionable Data Science Systems: An End-User Approach
AU  - Jung, Ju Yeon
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798368475905
AB  - How can we make data science systems more actionable? This dissertation explores this question by placing end-users and their data practices, rather than data scientists and their technical work of building models and algorithms, at the center of data science systems. Inspired by phenomenological views of technical systems from CSCW, HCI, and STS, I use ethnographic and other qualitative methods to understand how participants from four studies worked with data across three settings: craft brewers producing beers, people with visual impairments engaging with image descriptions of their photos on their smartphones, and repair workers repairing broken artifacts. I analyze implications for making data science systems actionable by framing the participants as potential end-users of these systems.  My findings emphasize that actionability in data science systems concerns not just predictions made on mostly given datasets. Actionability in my settings arose from the ongoing work of making data relevant to artifacts and phenomena that end-users engaged with in their practices and settings. I show how this ongoing work of making data relevant was challenging. The properties of artifacts and phenomena were inherently multiple and their relevance was contingent on end-users’ situations. I describe end-users’ data practices as processes of “registering” (making intelligible) a contingent yet coherent set of properties to turn multiple, uncertain artifacts and phenomena into actionable versions.  My dissertation makes several contributions to emerging research on actionability and data science in CSCW, HCI, and STS literature. First, based on my findings, I theorize an approach to data science systems that imagines actionability as driven not so much by data scientists generating predictions, or even by putting humans in the loop, but by placing end-users at the center. Second, my end-user approach to data science systems informs the technical work of data science by proposing requirements for models and algorithms to be accountable not just in their predictions but to end-users’ practices and settings. Third, my dissertation integrates into data science research foundational phenomenological views from CSCW that focus on how technological systems can account for and support end-users in their domains of practice, rather than the other way around.
UR  - https://www.proquest.com/docview/2780687260?accountid=15181&bdid=109696&_bd=oPERp1JIZhV4APGMYoT4L7H1Nxo%3D
ER  - 

TY  - Dissertations & Theses
T1  - Designing Explainable Autonomous Driving System for Trustworthy Interaction
AU  - Tang, Chen
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798351477282
AB  - The past decade has witnessed significant breakthroughs in autonomous driving technologies. We are heading toward an intelligent and efficient transportation system where human errors are eliminated. While excited about the emergence of autonomous vehicles with increasing intelligence, the public has also raised concerns about their reliability. Modern autonomous driving systems usually adopt black-box deep-learning models for multiple function modules (e.g., perception, behavior prediction, behavior generation). The opaque nature of neural networks and their complex system architecture make it extremely difficult to understand the behavior of the overall system, which prevents humans from confidingly sharing the road and interacting with autonomous vehicles. This motivates the design of a more transparent system to build a foundation for trustworthy interaction between humans and autonomous vehicles.This dissertation is concerned with the design of an explainable autonomous driving system, leveraging the strengths of explainable artificial intelligence, control, and causality. In particular, we focus on the behavior system of an autonomous vehicle, which plays a crucial role in its interaction with human road participants. The work consists of two parts. In Part I, we explore methods to improve model interpretability. The goal is to ensure that the model is more intelligible for humans in the design stage, which is achieved by introducing hard or soft constraints formulated from domain knowledge. We demonstrate how to formulate domain knowledge of social interaction into structured reward functions (Chapter 2) and pseudo labels (Chapter 3) as well as how to utilize them to induce interpretable driving behavior models. We also introduce an interpretable and transferable hierarchical driving policy that combines deep learning with robust model-based control (Chapter 4). In Part II, we explore the usage of post hoc explanation techniques in diagnosing model behavior. We introduce two case studies, in which we utilize sparse graph attention to diagnose interaction modeling in behavior prediction (Chapter 5) and develop a Shapley-value-based method to study the inherent causality issue in conditional behavior prediction (Chapter 6).
UR  - https://www.proquest.com/docview/2724206546?accountid=15181&bdid=109696&_bd=u56Haffr1aaameweacz6dCexX38%3D
ER  - 

TY  - Dissertations & Theses
T1  - A Lightweight Deep Learning Framework for Real-Time Detection of Industrial Traumatic Brain Injuries
AU  - Cobbina, Akosua
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798315757702
AB  - Traumatic brain injury (TBI) remains a significant health concern in industrial settings, where timely and accurate diagnosis is essential. While deep learning models like U-Net offer high diagnostic accuracy, their computational demands and lack of real-time responsiveness limit their use in fast-paced or resource-constrained environments. To address this, the present study introduces a dual-framework system combining You Only Look Once, version 8 (YOLOv8) for automated hemorrhage detection and MediaPipe for gesture-based image classification and interaction.The YOLOv8 model was trained on the Radiological Society of North America (RSNA) Intracranial Hemorrhage Detection dataset, achieving strong results: on the training set, it attained a precision of 0.914, a recall of 0.89, mAP@0.5 of 0.95, and a Dice coefficient of 0.93. On the test set, it maintained generalization with a precision of 0.85, a recall of 0.87, mAP@0.5 of 0.88, AUC of 0.88, and Dice coefficient of 0.85. In parallel, MediaPipe was adapted for gesture-controlled CT navigation and on-device hemorrhage classification using a lightweight TensorFlow Lite model. It achieved a validation accuracy of 75.68%, showing strong F1 performance for non-hemorrhagic cases but reduced sensitivity for hemorrhagic ones.Together, these models form an accessible, AI-assisted diagnostic tool that supports early TBI triage, enhances clinician interaction, and shows promise for deployment in mobile, industrial, and low-resource healthcare environments.
UR  - https://www.proquest.com/docview/3212956479?accountid=15181&bdid=109696&_bd=b%2FevW3WFxoXN6HCBtcKBFcf2CM4%3D
ER  - 

TY  - Dissertations & Theses
T1  - Of Discussions, Beliefs, and Algorithms: Essays in Experimental Economics
AU  - Biermann, Jan
JF  - PQDT - Global
Y1  - 2024-01-01
DA  - 2024
SN  - 9798311911580
AB  - This dissertation contains three empirical essays. The first essay examines the effects of a debating process prior to a collective decision. We conduct a lab-in-the-field experiment with school minors in Germany. We randomly assign some of them to discuss via chat how much they want to donate to a charity supporting incoming refugee minors. In our study, the pre-vote debate leads to higher donations. It does not directly affect trust among discussants, but subjects who are perceived as refugee-friendly after the discussion benefit from increased trust from their chat partners.The second essay addresses belief formation and asks how breaking a social norm affects what people think others would do in the same situation. To study this question, I employ an online experiment with a representative sample of the UK’s general population exposing subjects to either a high or a low temptation to lie. I find that subjects in the tempting environment lie more and, importantly, hold more pessimistic beliefs about what others would do in the same situation. I also include additional treatments in order to argue that the observed effects are not driven by rational expectations. The study provides causal evidence that breaking a social norm leads to strategic belief distortion about other people’s behavior in order to maintain a positive self-image.The third essay studies a situation in which a human decision-maker is assisted by an algorithm. We conduct an online experiment with US participants who repeatedly perform an estimation task while receiving (largely biased) recommendations from an algorithm. We analyze two interventions and ask whether they can help humans to assess the quality of algorithmic advice. First, we find that explaining the functioning of the algorithm in abstract terms reduces adherence to algorithmic advice, but it does not improve decisionmaking performance. Second, disclosing the correct answer after each round reduces adherence to algorithmic advice and improves human decision-making performance. While existing literature suggests that people abandon algorithms after seeing them err, this is not confirmed in our setting. This is likely because in our setting people can comprehend the reasons why some of the algorithmic predictions are inaccurate. Jointly, the three essays provide insights into (behavioral) economic concepts of trust, generosity, belief formation, and advice taking through the lens of experimental methods.
UR  - https://www.proquest.com/docview/3195715890?accountid=15181&bdid=109696&_bd=fvwvvA%2BD3ZCVpI1ikwufLVJTMNk%3D
ER  - 

TY  - Dissertations & Theses
T1  - Lightly Supervised Machine Learning for Wireless Signals
AU  - Sanz, Joshua
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798310103436
AB  - Modern wireless communication systems face unprecedented challenges in managing finite spectrum resources while meeting growing demands for data and connectivity. This dissertation explores how machine learning techniques with reduced supervision requirements can address these challenges through three complementary approaches. First, I demonstrate that two radio agents with minimal shared assumptions can learn compatible modulation schemes through cooperative interaction, enabling communication without explicit protocol design. Careful experimentation, including simulation and implementation on software-defined radios, shows that while reduced supervision increases learning time, agents can still achieve near-optimal performance. Second, I develop techniques for automatic calibration and metadata generation in distributed spectrum sensing networks using signals of opportunity as a form of environmental supervision. These techniques enable verification of sensor characteristics like field of view and location without manual intervention, facilitating trustworthy large-scale deployments. Finally, I propose using generative models to allow the sharing of wireless datasets while preserving privacy, addressing a key barrier to advancing wireless machine learning research. The unifying theme is the development of techniques that minimize required human supervision while maintaining robust performance, enabling more autonomous and scalable wireless systems. This research is a step toward cognitive radio networks that can adaptively and cooperatively manage spectrum resources with reduced human oversight.
UR  - https://www.proquest.com/docview/3175890439?accountid=15181&bdid=109696&_bd=1dkzLMEAuAdIXT1czxDEC7jLjfE%3D
ER  - 

TY  - Dissertations & Theses
T1  - Exploring the Underutilization of AI in Courtroom Decision-Making Processes
AU  - Hilliard, Dominique L.
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798315738855
AB  - The underutilization of AI in Texas state courtrooms challenges judicial efficiency, fairness, and transparency. Despite AI’s potential to enhance evidence management and decision-making, its adoption remains limited. Legal professionals and information technology (IT) managers stress the need for strategies to address these barriers and promote equitable justice. The purpose of this qualitative multiple-case study, which was grounded in the technology acceptance model, diffusion of innovation theory, and unified theory of acceptance and use of technology, was to explore strategies IT managers use to implement AI in criminal justice information systems. Semi-structured interviews and organizational documents provided data for thematic analysis, revealing five themes: (a) mixed attitudes toward AI; (b) challenges in technology use, including resource disparities; (c) barriers like budget constraints and limited training; (d) the irreplaceable role of human judgment; and (e) opportunities for advancements such as AI-driven bias detection and standardization. The findings emphasize the importance of balancing AI’s efficiency with human oversight to ensure fairness and ethical decision-making. A key recommendation is that IT leaders implement training programs, secure funding to reduce disparities, and launch pilot initiatives to build confidence in the practical benefits of AI. The implications for positive social change include the potential to enhance procedural efficiency, ensure equitable justice, and foster public trust by offering strategies to integrate AI into courtrooms.
UR  - https://www.proquest.com/docview/3206741491?accountid=15181&bdid=109696&_bd=Ub%2FROxr3TkSlnIGRJuJFi8Z5XcY%3D
ER  - 

TY  - Dissertations & Theses
T1  - Cultural-Social Motivation Model and Stem Employee Retention: A Quantitative, Correlational Study
AU  - Rehorn, Becky
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798896077558
AB  - This study examined the relationships between employee engagement, social health, organizational trust, and exit intention among STEM (Science, Technology, Engineering and Mathematics) professionals in the U.S. using a quantitative, non-experimental correlational design. With a sample of 178 participants, regression analysis via SPSS revealed significant findings. Employee engagement negatively predicted exit intention (B = -0.139, p = .040), indicating that higher engagement correlates with a decreased likelihood of turnover. Social health, conversely, positively predicted exit intention (B = .570, p < .001), suggesting that strong external social networks may increase employees' propensity to seek new opportunities. Organizational trust showed a strong negative correlation with exit intention (B = -0.345, p < .001) highlighting its importance in reducing turnover. Key recommendations include fostering an engagement and purpose-driven work environment, enhancing social health through balanced social exchanges, and reinforcing organizational trust to mitigate exit intentions. For practitioners and leaders within STEM fields, these findings emphasized the importance of holistic, employee-centric approaches to retention strategies that align with the evolving expectations of the workforce. The research contributes to the broader understanding of employee retention in the modern workplace, offering actionable insights for improving organizational practices and reducing voluntary turnover in STEM industries.
UR  - https://www.proquest.com/docview/3119868270?accountid=15181&bdid=109696&_bd=L10wGZoDX8eCvT1Y6Vbl04WOw%2FI%3D
ER  - 

TY  - Dissertations & Theses
T1  - Gremlin: Goes Radar Estimation via Machine Learning to Inform NWP
AU  - Hilburn, Kyle Aaron
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381191059
AB  - Imagery from the Geostationary Operational Environmental Satellite (GOES) has been a key element of U.S. operational weather forecasting since 1975. The latest generation, the GOES-R Series, offers new capabilities to support the need for high-resolution rapidly refreshing imagery for situational awareness. Despite the well demonstrated value to human forecasters, usage of GOES imagery in data assimilation (DA) for initializing numerical weather prediction (NWP) has been limited, particularly in cloudy and precipitating scenes. By providing a rich and powerful library of nonlinear statistical tools, artificial intelligence (AI) / machine learning (ML) enables new approaches for connecting models and observations. The objective of this research is to develop techniques for assimilating GOES-R Series observations in precipitating scenes for the purpose of improving short-term convective-scale forecasts of high-impact weather hazards. The hypothesis of this dissertation is that by harnessing the power of ML, the new GOES-R capabilities can be used to create equivalent radar reflectivity suitable for initializing convection in high-resolution NWP models.Chapter 1 will present a proof-of-concept that ML can be used as an observation operator for GOES-R to simulate Multi-Radar Multi-Sensor (MRMS) composite reflectivity data and therby initialize convection in NOAA’s Rapid Refresh and High-Resolution Rapid Refresh (RAP/HRRR). Development of the GREMLIN (GOES Radar Estimation via Machine Learning to Inform NWP) convolutional neural network (CNN) will be described. This includes the creation of a hierarchy of open source datasets, and will emphasize the importance of the neural network loss function in focusing the attention of the network on the most important meteorological features. Explainable AI (XAI) tools are applied to GREMLIN to discover three primary strategies employed by the network in making predictions, highlighting the unique ability of CNNs to utilize spatial context in satellite imagery. The results of retrospective Rapid Refresh Forecast System (RRFS) forecasts will be described, which show that GREMLIN can produce more accurate short-term forecasts than using real radar data over areas of the U.S. with poor radar coverage.In Chapter 2, the Interpretable GREMLIN model is developed to elucidate the nature of the spatial context utilized by CNNs to make accurate predictions. This clarity is accomplished by moving the inner workings of the CNN out into a feature engineering step and replacing the neural network with a linear regression model. This exposes the effective input space of the CNN and establishes well defined relationships between inputs and outputs, which provides guarantees on how the model will respond to novel inputs. Despite a 24x reduction in the number of trainable parameters, the interpretable model has similar accuracy as the original CNN. Using the interpretable model, five additional physical strategies missed by XAI are discovered. The pros and cons of interpretable model development and implications for generalizability, consistency, and trustworthy AI will be discussed.Finally, Chapter 3 will extend this research for the development of Global GREMLIN, discussing the challenges and opportunities. GREMLIN is validated for regimes outside of the training dataset, and regime dependence is quantified in terms of temperature and moisture. The impacts of additional predictors and advanced ML architectures, and the derivation of uncertainty estimates that will be needed for new DA approaches in RRFS, will be discussed. Current efforts to implement GREMLIN on NOAA’s GeoCloud, which will make GREMLIN available to a broader base of users, will be described.
UR  - https://www.proquest.com/docview/2908252663?accountid=15181&bdid=109696&_bd=qkgYvTsLUEZRPZY3z7KRe22ns2k%3D
ER  - 

TY  - Dissertations & Theses
T1  - How Artificial Intelligence Will Power Emergency Preparedness and Response for Small Modular Reactors
AU  - Rockabrand, Ryan
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798314809563
AB  - This dissertation examines the intersection of artificial intelligence (AI) and small modular reactors (SMRs), analyzing their combined impact on emergency preparedness and response (EPR). The study suggests that while SMR technology holds significant promise for sustainable energy production, integration with AI necessitates a re-evaluation of traditional EPR frameworks. Through a modified Delphi study, insights were gleaned from a panel of subject matter experts representing diverse fields such as nuclear engineering, AI development, and emergency management. This iterative process focused on key research questions regarding the role of AI in SMR deployments, its influence on EPR during various stages of an emergency, and essential actions required for successful implementation. The findings indicate that AI possesses the potential to significantly enhance SMR safety and efficiency by enabling real-time monitoring, predictive analytics, automated responses, and improved situational awareness. However, the study also identified fundamental challenges, including the need for seamless communication between AI systems and human operators to prevent misinterpretations and ensure effective decision-making during critical situations. Furthermore, the complexity of SMR technology necessitates specialized training programs for both on-site and remote personnel to guarantee proper understanding and operation in emergency contexts. Based on these findings, the researcher recommends a multi-pronged approach: leveraging AI for proactive risk assessment and management; integrating AI into emergency plans, simulations, and communication systems; and utilizing AI-powered tools for community engagement and education regarding SMR technology and emergency procedures. This research contributes to the growing body of knowledge surrounding AI's impact on the nuclear energy sector, providing valuable insights for policymakers, industry leaders, and emergency responders.
UR  - https://www.proquest.com/docview/3196021776?accountid=15181&bdid=109696&_bd=xSYJHHaJjKqL%2FO18cYbXpZAOZ%2B0%3D
ER  - 

TY  - Dissertations & Theses
T1  - Linguistically-Inclusive Natural Language Processing
AU  - Samson, Tan Min Rong
JF  - PQDT - Global
Y1  - 2022-01-01
DA  - 2022
SN  - 9798374486254
AB  - Language is a largely social construct, shaped by each community’s lived experiences, culture, and language repertoire. However, current natural language processing (NLP) systems fail to account for sociolinguistic variation: the supervised learning paradigm and common NLP practices implicitly assume that all speakers of a language speak a single, “standard” version, with one set of linguistic rules. This is especially damaging to minority language varieties, perpetuating the perception of being “ungrammatical” and “incorrect”.Failing to address this gap predisposes NLP systems to discriminate against minority language communities. This can take the form of disproportionately poor performance or encoding harmful stereotypes (e.g., classifying colloquial varieties as ungrammatical). Hence, this thesis focuses on the issues surrounding sociolinguistic generalization, defined as an NLP system’s ability to generalize beyond the language variety it was trained on. In some situations, this can be viewed as the ability to be robust to sociolinguistic variation.We first demonstrate, using a morphological adversarial attack, that text classification, question answering, and machine translation models are not robust to a common form of sociolinguistic variation: inflectional variation in English. This is particularly worrying, given English’s status as a world language: a model’s (in)ability to reliably process “nonstandard” Englishes greatly impacts the inclusiveness of the overall NLP system. Therefore, we propose a sample-efficient modification to the subword tokenizer that significantly improves the model’s robustness. This improvement is achieved without the model ever observing any adversarial examples during training, and generalizes to out-of-domain data.Another common form of sociolinguistic variation in multilingual societies is codemixing, where multiple languages are used in a single sentence. To expose the inability of current multilingual models to handle extreme code-mixing, we construct two multilingual adversarial attacks. Despite claims of zero-shot cross-lingual transfer, we find that model performance drops substantially even in the bilingual case. We then improve worst-case performance using a variant of adversarial training that requires the same number of steps as conventional fine-tuning. Our adversarial training method not only improves robustness to adversaries constructed with unseen embedded languages, but also achieves this without sacrificing performance on monolingual examples. This contrasts with existing work arguing for the existence of a trade-off between robustness and accuracy. We further demonstrate that these improvements transfer to real code-mixed Twitter data.We conclude by generalizing the prior adversarial attacks into a framework for testing NLP system reliability in the presence of language variation. We posit that any sociolinguistic environment can be viewed as an n-dimensional subspace of variation, with each dimension corresponding to a particular type of linguistic variation. Examples can then be randomly sampled from the distributions modeling these dimensions to evaluate average-case performance, or adversarially sampled to measure worst-case performance. We further propose a reliability testing process that allows NLP practitioners, subject matter experts, and affected communities to collaboratively specify reliability requirements. Natural language technology is often hailed as an avenue of improving technological accessibility. This thesis strives for a world in which language technology not only works for the privileged, but for everyone — regardless of social, cultural, and linguistic background.
UR  - https://www.proquest.com/docview/2787194407?accountid=15181&bdid=109696&_bd=BwGOqd4%2FV2zexaoI1qhdCFwPw6E%3D
ER  - 

TY  - Dissertations & Theses
T1  - Techniques for Interpretability and Transparency of Black-Box Models
AU  - Zhou, Yilun
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380096508
AB  - The last decade witnessed immense progress in machine learning, which has been deployed in many domains such as healthcare, finance and justice. However, recent advances are largely powered by deep neural networks, whose opacity hinders people's ability to inspect these models. Furthermore, legal requirements are being proposed to require a level of model understanding as a prerequisite to the deployment and use. These factors have spurred research that increases the interpretability and transparency of these models.  This thesis makes several contributions in this direction. We start with a concise but practical overview of the current techniques for defining and evaluating explanations for model predictions. Then, we observe a novel duality between definitions and evaluations of various interpretability concepts, propose a new way to generate explanations and study the properties of these new explanations. Next, we investigate two fundamental properties of good explanations in detail: correctness -- whether the explanations are reflective of the model's internal decision making logic, and understandability -- whether humans can accurately infer the higher level and more general model behaviors from these explanations. For each aspect, we propose evaluations to assess existing model explanation methods and discuss their strengths and weaknesses. Following this, we ask the question of what instances to explain, and introduce the transparency-by-example perspective as an answer to this question. We demonstrate its benefits in revealing hidden properties of both image classifiers and robot controllers. Last, the thesis identifies directions for future research, and advocates for a tighter integration of model interpretability and transparency into the ecosystem of trustworthy machine learning research that also encompass efforts such as fairness, robustness and privacy.
UR  - https://www.proquest.com/docview/2848459141?accountid=15181&bdid=109696&_bd=hRMAWIq%2FGBZ1J1zyrQpanJiP6uY%3D
ER  - 

TY  - Dissertations & Theses
T1  - Game Theoretical Adversarial Deep Learning Algorithms for Robust Neural Network Models
AU  - Chivukula, Aneesh Srivallabh
JF  - PQDT - Global
Y1  - 2020-01-01
DA  - 2020
SN  - 9798380485081
AB  - Despite recent advances in deep learning, there is still a significant gap between the robustness of human perception and machine intelligence. Deep learning is not provably secure. In fact, deep neural networks are vulnerable to security attacks from malicious adversaries, which is an ongoing and critical challenge for deep learning researchers. Even innocuous perturbations in training data can change the way a deep network behaves in unintended ways. This means that imperceptibly and immeasurably small departures from the training data can result in a completely different label classification when using the model for supervised deep learning.In this thesis, we explore adversarial deep learning algorithms. We examine how they exploit vulnerabilities in deep networks and how to make deep networks robust to their attacks. To explore the vulnerabilities, we simulate various model training processes under a range of various attack scenarios. Each attack strategy is assumed to be formulated by an intelligent adversary that is capable of either feature manipulation, label manipulation, or both. The optimal attack policy of our adversaries is determined by the solution for optimization problems that output the adversarial data. We then apply the knowledge that we learned to improve and reinforce the learning procedure so as to better defend against attacks.As part of this research process, we developed new adversarial learning algorithms to solve for adversarial manipulations in supervised classification networks such as Convolutional Neural Networks (CNNs). The adversarial learning objective for our adversaries is to inject small changes into the data distributions, defined over positive and negative class labels, to the extent that the CNN subsequently misclassifies the data distribution. Thus, the theoretical goal of our deep learning process becomes one of determining whether a manipulation of the input data has reached a learner decision boundary, i.e., where too many positive labels have become negative labels. We began this research undertaking by first studying the performance vulnerabilities in CNNs. With these vulnerabilities identified, we were able to propose CNNs that are secure to those types of adversarial attacks.We generate adversarial data by solving for optimal attack policies in Stackelberg games where adversaries target the misclassification performance of CNNs. In a sequential game-theoretic formulation, we model the interaction between an intelligent adversary and a deep learning model (a CNN) to generate adversarial manipulations by solving a two-player sequential noncooperative Stackelberg game where each player’s payoff function increases with interactions to a local optimum. With a stochastic game-theoretic formulation, we then extend the two-player Stackelberg game into a multiplayer Stackelberg game with stochastic payoff functions for the adversaries. Both versions of the game are resolved through the Nash equilibrium, which refers to a pair of strategies in which there is no incentive for either the learner or the adversary to deviate from their optimal strategy. In this case, the strategy pair is a learner weight and an evolutionary operation. In addition, we devised each attack scenario as a blackbox attack where the adversaries have no prior knowledge of the CNN’s learning processes and its best response strategies. In each case, we show that the Nash equilibrium leads to a supervised classification network that is robust to subsequent data manipulation by a game-theoretic adversary.
UR  - https://www.proquest.com/docview/2877961635?accountid=15181&bdid=109696&_bd=URSoVwCnC%2FiWzEqx2MI6ezqKvOI%3D
ER  - 

TY  - Dissertations & Theses
T1  - Learning the Language of Biomolecular Interactions
AU  - Sledzieski, Samuel
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798304957540
AB  - Proteins are the primary functional unit of the cell, and their interactions drive cellular function. Interactions between proteins are responsible for a wide variety of functions raning from catalytic activity to cellular transport and signaling, and interactions between small molecules and proteins are the foundation of many therapeutics. However, the experimental determination of these interactions is expensive and relatively slow, limiting the ability to model interactions at genome scale. It is therefore critical to develop computational approaches for modeling these interactions. Unsupervised language models trained on amino acid sequences, namely protein language models, learn patterns in sequence evolution that encode protein structure and function. These protein language models are thus a powerful tool for extracting features of proteins, enabling the adoption of lightweight downstream models. Here, we present novel machine learning techniques for adapting protein language modeling to the prediction of protein interactions at scale, enabling de novo interaction network inference and large-scale drug compound screening. We show that these methods achieve state-ofthe-art performance, and allow us to discover new biology and therapeutic candidates. In addition, we introduce methods for efficient training and adaptation of these models, and outline several applications which take advantage of the scale enabled by lightweight models. As a whole, this thesis demonstrates how computational advances in language modeling and the massive growth of data brought about by the sequencing revolution can be leveraged to tackle the genotype-to-phenotype challenge in biology, and lays the groundwork for more widespread adoption of these techniques for proteomic modeling.
UR  - https://www.proquest.com/docview/3171666550?accountid=15181&bdid=109696&_bd=CtuF%2BaoH5IvD33JXR5cXFc9dB3Y%3D
ER  - 

TY  - Dissertations & Theses
T1  - The Power of Perception in Human-AI Interaction: Investigating Psychological Factors and Cognitive Biases That Shape User Belief and Behavior
AU  - Lee, Eunhae
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798304951487
AB  - This thesis investigates the psychological factors that influence belief in AI predictions, comparing them to belief in astrology- and personality-based predictions, and examines the "personal validation effect" in the context of AI, particularly with Large Language Models (LLMs). Through two interconnected studies involving 238 participants, the first study explores how cognitive style, paranormal beliefs, AI attitudes, and personality traits impact perceptions of the validity, reliability, usefulness, and personalization of predictions from different sources. The study finds a positive correlation between belief in AI predictions and belief in astrology- and personality-based predictions, highlighting a "rational superstition" phenomenon where belief is more influenced by mental heuristics and intuition than by critical evaluation. Interestingly, cognitive style did not significantly affect belief in predictions, while paranormal beliefs, positive AI attitudes, and conscientiousness played significant roles. The second study reveals that positive predictions are perceived as significantly more valid, personalized, reliable, and useful than negative ones, emphasizing the strong influence of prediction valence on user perceptions. This underscores the need for AI systems to manage user expectations and foster balanced trust. The thesis concludes with a proposal for future research on how belief in AI predictions influences actual user behavior, exploring it through the lens of self-fulfilling prophecy. Overall, this thesis enhances understanding of human-AI interaction and provides insights for developing AI systems across various applications.
UR  - https://www.proquest.com/docview/3171510627?accountid=15181&bdid=109696&_bd=J88ABabGH0LZb82nAuqafcm2Y7M%3D
ER  - 

TY  - Dissertations & Theses
T1  - The Individual’s Perception of AI and Its Effects on the Organization’s AI Initiatives
AU  - Petersen, Catherine
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798384102960
AB  - Investment in Artificial Intelligence (AI) does not result in returns for business despite improving technology (Ransbotham et al., 2020; Storm & Borgman, 2020). This study investigated one potential reason for the failure of AI when implemented within an organization with the intent to drive success with this technology. AI is unique amongst technologies due to its ability to inspire emotions from potential users and these emotions can affect the individual’s willingness to adopt the technology. The research questions were:Does an individual’s perception of AI have a moderating effect on their behavioral intention to use a product that includes machine learning and/or AI?Does an individual’s experience with AI indicate their perception of AI?Using an explanatory sequential research design, this study examined the relationship between how an individual perceives AI and their willingness to accept the technology in their role within an organization. It also considered the role of experience and an individual’s perception of AI. These findings can be used by organizations to assess readiness for change that includes the implementation of AI.The results found that including AI in the description of the initiative decreased support from 81.5% to 54.8%. There was also a significant relationship between an individual’s perception of AI and their support of an initiative that specifically names AI (p< .05). In interviews, nine out of ten respondents discussed job replacement and showed a tension between hopes and fears with regard to the technology. No statistically significant relationship was found between an individual’s experience with AI and their perception of AI but there was a relationship between experience and their expectations for effort and performance. Interviews showed that an individual’s perception of job replacement or challenges with the technology were not affected or increased with experience using AI. Based on these findings, managers can better determine if their organizations are ready to implement AI successfully. To improve readiness, managers must consider the cost-benefit analysis that an individual will use when considering the technology and tip the equation toward the benefits through increased training, transparency, and communication.
UR  - https://www.proquest.com/docview/3106292066?accountid=15181&bdid=109696&_bd=pLwwo0qwXF%2Bq0McgW2L1YS%2BD3qo%3D
ER  - 

TY  - Dissertations & Theses
T1  - Queering Futures with Data-driven Speculation: The Design of an Expanded Mixed Methods Research Framework Integrating Quantitative, Qualitative, and Practice-based Modes
AU  - Westbrook, Jess Parris
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798382737850
AB  - Queering’ questions, unlearns, disrupts, and transforms approaches, expectations, and realities. Futures are time and change. The approach I have designed to operationalize Queering and futures, or Queering futures, is the Queering Futures Framework (QFF). The Queering Futures Framework (QFF) is a brand-new transdisciplinary research framework intersecting values, positionality, complexity, Queerness themes, and futures praxis. This framework expands traditional mixed methods research conventions by integrating quantitative, qualitative, and practice-based research modes and mindsets.The Queering Futures Framework (QFF) prototype presented in this dissertation functions as a test case and proof of concept. The prototype quantitative mode measures attitudes towards AI, and a qualitative mode explores impressions of mental time travel, AI, and futures. Within the culminating practice-based mode, signals identified in the quantitative results and qualitative findings are integrated and inform a new practice-based method, called data-driven speculation. I created data-driven speculation method to connect data and imagination. Readers are invited to adapt and iterate.
UR  - https://www.proquest.com/docview/3058416208?accountid=15181&bdid=109696&_bd=EwI5vs%2FYu5I08REhoZ54h1FcTgs%3D
ER  - 

TY  - Dissertations & Theses
T1  - Evidence Evaluation in Biomedical Knowledge Graphs for Pharmaceutical Discovery
AU  - Yang, Jeremy Joseph
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 979-8-209-91029-9
AB  - What is the strongest biomedical evidence about a disease for discovery of novel pharmaceutical therapies? This is a fundamental challenge for biomedical scientists, but also directly translates to a parallel question for informatics and data science: Can we systematically assemble and query biomedical heterogeneous knowledge graphs in a computational discovery platform guided by rational, algorithmic measures of relevance and confidence, facilitating scientific discovery? And, how have continuing waves of scientific and technological progress informed and empowered these inquiries? The research described herein consists of several projects unified by this common theme, each from a distinct area of molecular biomedicine. The three main projects are (1) Badapple: Bioassay data associative promiscuity prediction learning engine, (2) TIGA: Target illumination GWAS analytics, and (3) KGAP: Knowledge graph analytics platform. Badapple employs empirical bioassay data from PubChem and the NIH Molecular Libraries Program to recognize patterns of promiscuity (non-selectivity), associated with molecular scaffolds. KGAP combines data from two NIH programs, LINCS (Library of integrated network-based cell signatures), i.e. genomic signatures, and IDG (Illuminating the druggable genome) to generate and evaluate hypotheses for novel drug targets from gene expression profiles. TIGA processes data from the NHGRI-EBI GWAS Catalog to aggregate experimental genome wide variant to trait associations as novel drug target hypotheses. Peer-reviewed papers, with the author as first author, have been published, for Badapple in 2016, TIGA in 2021 and KGAP in 2022. Relevant portions of other projects are also described, each reinforcing the common theme, that scientific discovery is empowered by rational, algorithmic, semantic, domain-aware assembly and querying of knowledge graphs.
UR  - https://www.proquest.com/docview/2645879860?accountid=15181&bdid=109696&_bd=T5uxbLCgsj2PUBQP9Kd1dIDNavw%3D
ER  - 

TY  - Dissertations & Theses
T1  - Artificial Intelligence (AI) and Worker Selection Criteria: A Pathway to Fair Hiring and Positive Organizational Outcomes
AU  - Ledbetter, Kwema J.
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798308182382
AB  - Fair hiring processes can reduce the risk of lawsuits, an issue which has cost organizations hundreds of millions of dollars in settlements over the last ten years. Artificial Intelligence (AI) presents a promising solution, as it has the capability of making objective decisions despite its reputation for bias. By leveraging worker selection criteria, such as skills and education, AI can predict job candidates’ performance and potential retention. The problem is that despite its capabilities, AI’s use of worker selection criteria is not always fully leveraged to enhance hiring fairness and as a result, organizations continue to face costly discrimination lawsuits. This study answers the research question: How do AI-driven hiring systems utilize worker selection criteria, and what are the implications for AI prediction accuracy, hiring fairness, and organizational outcomes? A systematic review of 51 studies was conducted, supplemented by interviews with two subject matter experts (SMEs). The systematic review identified five themes related to the research question. The most notable theme highlights that AI models are being tested for accuracy in predicting job candidate performance and retention based on worker selection criteria. AI model testing underscores the need for organizations to prepare for the imminent adoption of AI models in the workplace by analyzing the worker selection criteria used by organizations to make hiring decisions. The study concludes with the introduction of SMARTCriteria templates, a practical tool to help organizations establish and maintain transparent worker selection criteria. By implementing these templates, organizations can better prepare for the integration of AI prediction models into their hiring processes, fostering fairness, improving performance, increasing retention rates, and reducing financial risk.
UR  - https://www.proquest.com/docview/3174316853?accountid=15181&bdid=109696&_bd=q84SVnyKFQlcy3VN211Ff58Hn1c%3D
ER  - 

TY  - Dissertations & Theses
T1  - Transfer Learning in Natural Language Processing through Interactive Feedback
AU  - Yuan, Michelle
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798351471211
AB  - Machine learning models cannot easily adapt to new domains and applications. This drawback becomes detrimental for natural language processing (NLP) because language is perpetually changing. Across disciplines and languages, there are noticeable differences in content, grammar, and vocabulary. To overcome these shifts, recent NLP breakthroughs focus on transfer learning. Through clever optimization and engineering, a model can successfully adapt to a new domain or task. However, these modifications are still computationally inefficient or resource-intensive. Compared to machines, humans are more capable at generalizing knowledge across different situations, especially in low-resource ones. Therefore, the research on transfer learning should carefully consider how the user interacts with the model. The goal of this dissertation is to investigate “human-in-the-loop” approaches for transfer learning in NLP.First, we design annotation frameworks for inductive transfer learning, which is the transfer of models across tasks. We create an interactive topic modeling system for users to find topics useful for classifying documents in multiple languages. The user-constructed topic model bridges improves classification accuracy and bridges cross-lingual gaps in knowledge. Next, we look at popular language models, like BERT, that can be applied to various tasks. While these models are useful, they still require a large amount of labeled data to learn a new task. To reduce labeling, we develop an active learning strategy which samples documents that surprise the language model. Users only need to annotate a small subset of these unexpected documents to adapt the language model for text classification.Then, we transition to user interaction in transductive transfer learning, which is the transfer of models across domains. We focus our efforts on low-resource languages to develop an interactive system for word embeddings. In this approach, the feedback from bilingual speakers refines the cross-lingual embedding space for classification tasks. Subsequently, we look at domain shift for tasks beyond text classification. Coreference resolution is fundamental for NLP applications, like question-answering and dialogue, but the models are typically trained and evaluated on one dataset. We use active learning to find spans of text in the new domain for users to label. Furthermore, we provide important insights on annotating spans for domain adaptation.Finally, we summarize the contributions of each chapter. We focus on aspects like the scope of applications and model complexity. We conclude with a discussion of future directions. Researchers may extend the ideas in our thesis to topics like user-centric active learning and proactive learning.
UR  - https://www.proquest.com/docview/2719424809?accountid=15181&bdid=109696&_bd=9nOkW0OmsHlfoNzGdhNsdyzJayM%3D
ER  - 

TY  - Dissertations & Theses
T1  - Functional Components as a Paradigm for Neural Model Explainability
AU  - Fiacco, James
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381445572
AB  - Despite their ubiquity, trained neural models remain a challenging subject for explainability, with neural net researchers applying what might be considered esoteric and arcane knowledge and skills to understand what the models are learning and how the internal workings of the models change their learning outcomes. Understanding what these models are learning is a field of utmost importance as more and more production systems rely on neural models to provide more and more high-impact utilities.This work lays out an interpretability methodology, built on a design philosophy for neural models that redefines the unit of analysis for these models from individual neurons to a set of interconnected functional components which we call neural pathways. These functional components, which are a consequence of the architecture, data, and training scheme, have the capacity to cut across structural boundaries. This enables a method of functionally-grounded, human-in-the-loop model understanding through increased transparency, encouraging a dialogue between the models and the researchers.Over the course of this work for this thesis, we contribute to the literature in four ways: First, we provide the method for neural model interpretability at the subtask level, rigorously validating it against a suite of synthetic datasets. Second, we extend the method by providing a framework for aligning learned functional components to causal structures. This enables the comparison of the learned functions of a neural model with a theoretical causal structure allowing for rapid validation of our understanding of how a neural model is approaching a task. Third, we expand the method to compare and align functional components across models with differing architectures or training procedures. And lastly, we demonstrate the capabilities of the neural pathways approach in several domains of education technologies. This includes automatic essay feedback via rhetorical structure analysis, group formation via transactivity detection, and automated essay scoring.This last contribution can be further specified into three facets separated by their domains and foci. First, neural pathways are employed to scaffold a neural discourse parser to more easily generalize to student writing. Next, we demonstrate that neural pathways can be used as a method for error analysis by exploring the discrepancy in performance between models trained on detecting transactivity in different domains. And lastly, we demonstrate the capability of tracking changes in problematic pathways across fine-tuning an AI writing detector.With the broad applicability of the neural pathways approach, we are optimistic that the method can have a wide impact on the the design and development of neural models and we aim to provide a foundational work that has the capability of being extended far beyond the scope of the thesis.
UR  - https://www.proquest.com/docview/2920003879?accountid=15181&bdid=109696&_bd=RNc1pGnYHWGcAIKhYbFivC%2FtZ6k%3D
ER  - 

TY  - Dissertations & Theses
T1  - An Application of the Unified Theory of Acceptance and Use of Technology (UTAUT) Model for Exploring Business Intelligence in the Nonprofit Sector
AU  - Asante, Collins
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798311900546
AB  - This study investigated the application of the unified theory of acceptance and use of technology model in the adoption of business intelligence technologies within nonprofit organizations, focusing on managers in Northeast Washington, DC. Despite the growing significance of business intelligence for data-driven decision-making, nonprofits often face challenges such as limited resources, lack of technical expertise, and resistance to change, leading to suboptimal business intelligence utilization. Using a qualitative single-case study approach, the research explored the factors influencing the adoption and implementation of business intelligence systems, guided by the unified theory of acceptance and use of technology’s constructs: performance expectancy, effort expectancy, social influence, and facilitating conditions. Data were collected through semi-structured interviews with nonprofit managers and a review of organizational documents. The findings identified key barriers to business intelligence adoption and strategies to overcome them, including targeted training, user-friendly tools, and fostering a data-driven organizational culture. The study offers actionable insights for improving business intelligence implementation in resource-constrained environments, enhancing organizational performance, and achieving strategic goals. The research contributes to the academic discourse on technology adoption in the nonprofit sector and provides a roadmap for leveraging business intelligence to optimize decision-making processes and operational efficiency.
UR  - https://www.proquest.com/docview/3194001281?accountid=15181&bdid=109696&_bd=tVBp1tHM4GU5dpIyXA7X0j%2FP0O4%3D
ER  - 

TY  - Dissertations & Theses
T1  - Designing Feedback for Collocated Teams Using Multimodal Learning Analytics
AU  - Barzola, Vanessa Echeverria
JF  - PQDT - Global
Y1  - 2020-01-01
DA  - 2020
SN  - 9798380485074
AB  - The ability to communicate, be an effective team or group member and collaborate face-to-face are critical skills for employability in the 21st century workplace. Previous research suggests that learning to collaborate effectively requires practice, awareness of group dynamics and reflection upon past activities. However, although having a teacher closely supervising and providing detailed feedback to each group would be ideal, it may be unrealistic in practice. A promising way to approach this challenge could be to capture behavioural traces from group interactions in order to generate comprehensible and actionable feedback to support team reflection. In this sense, Multimodal Learning Analytics (MMLA) is a promising field, offering the potential to track learners’ activity across digital and collocated contexts, using emerging sensing and pervasive computing technologies. Most of the research in MMLA has been conducted in lab conditions, to help researchers validate learning theories or generate more comprehensive learner models. However, one of the most underexplored aspects of MMLA has been the generation of feedback to support teaching and learning, and moreover, in authentic locations and activities.This thesis reports progress in tackling this challenge by designing and validating computer-based feedback, by means of visual representations and narrative, to support effective, guided reflection using multimodal learning analytics evidence. To achieve this, three contributions are presented. The first contribution is a human-centred design method to translate the informal outputs of codesign sessions with teachers and students, into more meaningful group work constructs with clear MMLA design requirements. The second contribution is a modelling approach to add meaning to low-level multimodal group data based on the characteristics of the context (domain expertise, theory, and the learning design). Finally, the third contribution is an approach for augmenting visual representations with data storytelling elements to facilitate the interpretation of group dynamics insights by educators and students. This thesis is developed in the context of two distinct, collocated group work settings, in the domains of collaborative database design and healthcare simulation. Using a Design-Based Research process, a set of explanatoryinterfaces (i.e. interfaces that communicate insights) was designed and validated with teachers and students. The thesis provides timely and necessary groundwork for researchers and practitioners to design visual representations capable of communicating actionable insights, using multimodal data in complex and authentic collaboration scenarios.
UR  - https://www.proquest.com/docview/2877963126?accountid=15181&bdid=109696&_bd=CUY9t8hOaSIgzr88D%2Bn4Xa2kklM%3D
ER  - 

TY  - Dissertations & Theses
T1  - Enhancing Partnership Quality: Unlocking AI's Impact on B2B Partnerships
AU  - Schneller, Timothy M.
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798310300354
AB  - Artificial intelligence (AI) is increasingly integrated into business-to-business (B2B) partnerships, enhancing decision-making, automation, and communication. However, AI’s impact on partnership quality (PQ) remains insufficiently understood. This study investigates how AI-enabled technology systems influence PQ by examining key relational, operational, and strategic factors and identifying managerial strategies to optimize AI’s positive influence. Using a qualitative systematic review and realist review methodology, this research synthesizes evidence from the past decade, including empirical studies, conceptual analyses, and practitioner insights, to assess AI’s role in shaping trust, commitment, communication efficiency, and power dynamics in B2B relationships. Social exchange theory, marketing channel theory, and the technology acceptance model provide the theoretical foundation for evaluating AI’s relational and structural impacts. Findings reveal that AI-enabled technology systems influence PQ through relational, operational, and strategic mechanisms. AI-driven insights can strengthen trust but also introduce dependency risks, requiring careful governance. Operationally, AI enhances efficiency and decision-making but may centralize control, shifting power dynamics between partners. Strategically, AI redefines long-term alignment by altering coordination mechanisms and collaboration patterns. This study identifies managerial strategies such as shared governance models, transparent data-sharing agreements, and human-AI collaboration frameworks to ensure AI-enabled technology systems strengthen rather than undermine PQ. These strategies mitigate risks while fostering trust, commitment, and relational stability. This research contributes to both theory and practice by structuring an understanding of AI’s impact on B2B partnerships and providing actionable strategies for organizations. This study establishes a foundation for future inquiry into AI’s evolving role in inter-organizational relationships and highlights the need for ongoing collaboration between academia and industry to ensure AI adoption enhances rather than disrupts business ecosystems.
UR  - https://www.proquest.com/docview/3186230760?accountid=15181&bdid=109696&_bd=CqRxU5NZsi9%2FMy4dL5kgiuyzNxY%3D
ER  - 

TY  - Dissertations & Theses
T1  - The Accountability, Responsibility & Governance as a Unified Strategy for AI
AU  - Kurre, John
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798302874320
AB  - This research meticulously evaluates U.S. National AI strategies, assessing their alignment with the OECD AI Principles within the U.S. National Security and Intelligence Community (IC) Nexus. The study analyzes the profound implications of integrating Artificial Intelligence (AI) by crucial U.S. Government agencies responsible for these domains to navigate the complex interplay between AI capabilities and ethical imperatives. The research delves into critical concerns surrounding algorithmic bias, the "explainability gap" that can undermine trust in AI-driven insights, the ethical dimensions of AI-enabled surveillance technologies, the need for robust data governance across often-fragmented agencies, and the development of comprehensive AI Governance frameworks. The research aims to generate actionable knowledge through a robust qualitative methodology encompassing narrative research, case studies of AI implementations within the intelligence community, and systematic content analysis of U.S. National AI Strategies vis-à-vis the OECD AI Principles. The findings will provide practitioners with practical guidelines, contribute to a more nuanced scholarly rendition of AI Governance in U.S. National Security environments, and inform the development of a Unified National AI Strategy that prioritizes human-centered values, transparency, accountability, and international collaboration.
UR  - https://www.proquest.com/docview/3163016683?accountid=15181&bdid=109696&_bd=FDD0VO0MOeVXYKyCVW4TjGakLms%3D
ER  - 

TY  - Dissertations & Theses
T1  - Physiology-Inspired Deep Learning for Improved Heart Failure Management
AU  - Schlesinger, Daphne E.
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798304954846
AB  - Heart failure is an increasingly prevalent condition, which is associated with significant morbidity and mortality. While there has been profound progress in the development of pharmacotherapy and specialized devices for heart failure in recent decades, challenges remain in disease diagnosis and management. One of the key issues is that central hemodynamics and cardiac mechanics, the quantities that characterize the state of a heart failure patient, are difficult to measure. Deep learning methods have shown promise for addressing problems in clinical medicine but are fundamentally limited by their opacity to interpretation, which inhibits model trust and adoption. In this thesis, we propose physiology-inspired deep learning approaches to improve heart failure management.Central hemodynamic parameters are typically measured via pulmonary artery catheterization — an invasive procedure that involves some risk to the patient and is not routinely available in all settings. In Chapter 2, we sought to develop a noninvasive method to identify elevated mean pulmonary capillary wedge pressure (mPCWP). We leveraged data from 248,955 clinical records at the Massachusetts General Hospital (MGH) to develop a deep learning model that can infer when the mPCWP > 15 mmHg using the 12-lead electrocardiogram (ECG). Of these data, 242,216 records were used to pre-train a model that generates useful ECG representations. The remaining 6739 records contain encounters with direct measurements of the mPCWP from right heart catheterizations (RHCs), which provide gold-standard hemodynamic measurements. Eighty percent of these data were used for model development and testing (4304 in train, 546 validation, and 540 in the test set), and the remaining records comprise a holdout set (1349) that was used to evaluate the model. We developed an associated unreliability score that identifies when model predictions are likely to be untrustworthy. The model achieves area under the receiver operating characteristic curve (AUROC) scores of 0.80 ± 0.02 (test set) and 0.79 ± 0.01 (holdout set). Model performance varies as a function of the unreliability, where patients with high unreliability scores correspond to a subgroup where model performance is poor: for example, patients in the holdout set with unreliability scores in the highest decile have a reduced AUC of 0.70 ± 0.06. These results demonstrate that the mPCWP can be inferred from the ECG, and the reliability of this inference can be measured. When invasive monitoring cannot be expeditiously performed, deep learning models may provide information that can inform clinical care.We extended this work in Chapter 3, and developed a Cardiac Hemodynamic Artificial Intelligence monitoring System (CHAIS) that uses single-lead ECG data to infer when cardiac hemodynamics are abnormal. CHAIS is a deep neural network that was trained to detect abnormal cardiac hemodynamics using just lead I of the 5930 paired ECG recordings and RHCs from MGH used in Chapter 2. CHAIS was tested on the internal holdout set of 1439 paired single-lead ECGs and RHCs (858 patients) from MGH and on an external validation set of 4629 paired ECGs and RHCs (2577 patients) from another institution. We also prospectively collected single-lead ECG data using a commercially available wearable ECG monitor, from 83 patients who were scheduled for a RHC at MGH, and used CHAIS to infer if their left atrial pressures would be elevated at the time of their RHC. CHAIS achieves an AUROC of 0.80 for detecting elevated left atrial pressures on the internal test dataset and 0.76 on the external validation set. On patients who wore a wearable ECG monitor before RHC, CHAIS had an AUROC of 0.70; however, when ECG data are available within 1.25 hours before catheterization, the AUROC is 0.875. These results demonstrate the utility of ambulatory cardiac hemodynamic monitoring with a wearable ECG monitor.Finally, in Chapter 4, we described an approach to directly incorporating knowledge of cardiovascular physiology into a deep learning framework. The framework consists of a neural network encoder, to map from arterial blood pressure (ABP) waveforms to latent cardiovascular parameters, and a mechanistic model which maps from those parameters to hemodynamic waveforms, called Cardiovascular Simulator (CVSim). We trained the model on a synthetic data set, and found that the model achieved a multiclass accuracy of 47.8 percent for placing samples into classes in terms of their left ventricular end diastolic pressure (LVEDP), cardiac output (CO), and left ventricular ejection fraction (LVEF), using clinically relevant thresholds. Here, we also proposed a physiology-inspired trust score, and find that the multiclass accuracy is higher in the subset of samples in the lowest decile with respect to the score, compared to results on the rest of the data. On applying the model to a small clinical data set from patients in intensive care settings at MGH, classification performance in terms of LVEDP, CO, and LVEF was limited, given the various challenges of transfer from synthetic to real clinical data. However, we observed that cardiac mechanical parameters inferred from the clinical data set trended positively with the administration of pharmaceutical agents expected to modulate those parameters. This suggests that the model can glean meaningful information from clinical ABP waveforms and shows promise for future development.With further clinical testing, the suite of methods described in this thesis have the potential to advance heart failure care by enabling non-invasive central hemodynamic monitoring and minimally-invasive inference of cardiac mechanics.
UR  - https://www.proquest.com/docview/3171666546?accountid=15181&bdid=109696&_bd=6UIQ5vX91TR4aKO3rjYiAY2POmo%3D
ER  - 

TY  - Dissertations & Theses
T1  - Self-Healing Robust and Fair Neural Networks via Optimal Control
AU  - Chen, Zhuotong
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798342718806
AB  - This dissertation investigates the challenges of improving the robustness and fairness of deep neural networks through the lens of optimal control theory. Deep neural networks, despite their extensive application across various engineering fields, are vulnerable to imperceptible perturbations and often exhibit biased performance towards underrepresented demographic populations. The first part of this dissertation presents a novel self-healing framework designed to improve the robustness of deep neural networks against unforeseen perturbations. This framework is realized by a novel closed-loop control approach grounded in optimal control theory, which adaptively generates control signals to identify and correct potential errors in the state trajectory of perturbed input data during inference. The second part of this dissertation introduces a PID control framework that generalizes the closed-loop method with additional integral and derivative controllers. We derive an analytical solution for fast online inference, making our control framework applicable to large-scale models. The third part of this dissertation addresses the fairness issue in machine learning within dynamic environments, where undesired model biases against minority users could lead to significant user churn, thereby diminishing the training data for model tuning in subsequent time steps. This negative feedback loop can further exacerbate demographic disparity. To address this, we introduce the concept of asymptotic fairness to maintain consistent model performance across all demographic groups and propose an optimal control solution to achieve this goal.
UR  - https://www.proquest.com/docview/3123627170?accountid=15181&bdid=109696&_bd=lM%2Bo%2Bttywp0R773W2t13waQ7UjE%3D
ER  - 

TY  - Dissertations & Theses
T1  - Automating Multimodal Data Storytelling for Embodied Team Learning
AU  - Fernández Nieto, Gloria Milena
JF  - PQDT - Global
Y1  - 2022-01-01
DA  - 2022
SN  - 9798379784607
AB  - There is a growing interest in creating Learning Analytics (LA) interfaces that support students and teachers directly. Thus far, many of these solutions have been materialised as dashboards and visualisations. However, although a growing number of prototypes and commercial products aimed at supporting students/teachers exist, their limitations are coming under scrutiny. For instance, many visual LA tools are failing to provide meaningful and relevant insights that can support students reflections on their embodied teamwork activity. Moreover, there are additional challenges in visualising and communicating the wide variety of multimodal sensor data captured from physical spaces, in a way that supports educational stakeholders (e.g., teachers or students), who as casual users, have limited training in data analysis and interpretation. Thus, this thesis engages research in Information Visualisation (InfoVis) and specifically the Guidance visualisation paradigm that aims to support casual users, or those users with low analysis expertise, to narrow the gap of data visualisation interpretations. Data Storytelling is one way to provide guidance, as a compression technique to help an audience effectively understand what is important in a visualisation, communicating key messages combining data, visualisations, and narratives. 'Telling stories' with data in these ways should enable the elicitation of deeper reflections in an effective manner. This thesis tackles the above challenges specifically for professional sectors, whose educational and training scenarios can be challenging because they need to develop theory, procedural knowledge and also learn from bodily experiences. This research progresses in by investigating: "How can salient aspects of embodied team activity be automatically identified, and derived insights be communicated to support timely, productive reflection?" Four research questions were derived: (1) What modelling techniques can enable identification of salient aspects of multimodal embodied team activity according to the learning design (i.e., teachers' pedagogical intentions)? (2) How can insights be extracted from multimodal sensors and communicated to students and teachers to support teaching and reflection on embodied team activity? (3) To what extent can students and teachers reflect on embodied team activity using MMLA interfaces? and (4) To what extent can MMLA interfaces for students and teachers be automatically generated? This research makes three types of contribution: modelling, prototypes (MMLA interfaces), and implementation. Results from this research point to the potential of creating alternative ways to communicate multimodal data insights to teachers and students, by combining visualisation, narrative and storytelling, driven by teachers' pedagogical intentions and the learning design.
UR  - https://www.proquest.com/docview/2833240744?accountid=15181&bdid=109696&_bd=PQ3jleKqNHUU4T6QZG0mboQKZhc%3D
ER  - 

TY  - Dissertations & Theses
T1  - Building Equity for English Language Learners: Technology Employees in Fortune 500 Companies
AU  - Travis, Karen Anne
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798363503511
AB  - Given the labor shortage of highly skilled technical professionals in the United States and the influx of English Language Learners (ELLs) to address this, and predictions for this trend to continue and accelerate, organizations must provide equity to harness the full potential of ELLs. Equity focused on ELLs will deliver increased profitability, innovation, and organizational climate improvement opportunities. The study population comprised ELLs working in technology roles at Fortune 500 companies in January and February 2022. In total, 23 ELLs completed a 50-question survey, and seven completed a 1-hour interview. Based on survey data, participants speak 16 languages (other than English) and represent all generations in the working population from 18–65. Overall, 17% of respondents spoke three or more languages, including English, 38% were born in the United States, and 62% were born outside the United States in seven different countries. Then, 50% of respondents indicated they prefer speaking English with family and friends even when not at work. ELLs participating in this study are highly educated, with 20% holding a doctoral degree, 60% holding a master’s degree, and 20% holding a bachelor’s degree. Participants were supervisors, team leaders, and individual contributors. The study intentionally excluded senior-level executives. The research and the literature indicate that organizations must create equity focused on ELLs. In particular, surveys and interviews identified oral presentation skills as an area of interest for ELLs. The study presents five recommendations, including creating a community of practice focused on building ELLs’ presentation skills. The literature and a cost-benefit analysis support the recommendations as methodologically sound and cost-effective, offering organizations opportunities to improve bottom-line results.
UR  - https://www.proquest.com/docview/2755912006?accountid=15181&bdid=109696&_bd=ZY6Rzt6kY20YuhCOilbOHFi5Zvo%3D
ER  - 

TY  - Dissertations & Theses
T1  - The Intersection of Artificial Intelligence in Human Resource Management: An Exploration of Talent Communication
AU  - Votto, Alexis Megan
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380125741
AB  - Within this research, we sought to explore and understand where Artificial Intelligence (AI) intersects with human resource information systems. We further investigated the AI phenomenon by leveraging machine learning tools to understand the following :1) where AI exists within human resource management (HRM) via a systematic literature review* ; 2) how to propose a research framework to investigate data science competencies within private and federal sector job postings, and 3) how military veterans with data science skills communicate their experience concerning employability. The following sections discuss these papers further. Paper 1: Artificial intelligence in tactical human resource management: A systematic literature review (2021)The first essay is a systematic literature review investigating where AI exists within published HRM literature. Within this literature review, we leveraged a 2-phased methodology to navigate 315,053 articles from various multidisciplinary publication sources. The findings of this review indicate research opportunities to grow within HRM's competency management systems (pay and benefits). Furthermore, it highlighted a gap within AI research exploring qualitative components of recruitment information systems, which spurred the motivation for the rest of this dissertation.Paper 2: JC-Compass: A Framework for Conducting Competency-Based Job Posting Research and Analysis (In Progress)†The second essay proposes a research framework to conduct competency-based job description research. This paper leverages a design science approach to propose and explore a new competency-based job description research method. The results indicate that the federal sector potentially places a decisive influence on problem-solving terms rather than artificial intelligence terms. We also discovered that the private sector strongly influenced statistics and ethics terminology, whereas the federal focused on problem solving and ethics. Paper 3: Veteran Talent within Data Science: An Exploratory Resume Analysis on the Employability of Active-Duty Veterans The third and final paper explores how military veterans seeking jobs within the data science community communicate their skills and how their employability is affected by their choice of words within resumes. Understanding that drawdowns are posturing some military veterans to leave the service and pursue other endeavors, we sought to evaluate how Gulf War II ("post 9/11") veterans advertise their skills relative to how many times they have been unemployed throughout their resume. In conducting this research, we aspire to provide insight into how veterans seeking data science jobs could better posture themselves to be more marketable and provide employers insight into current trends and expectations.
UR  - https://www.proquest.com/docview/2853753859?accountid=15181&bdid=109696&_bd=Bh6t8re%2BYf4dkLbW%2B9NNvN7naus%3D
ER  - 

TY  - Dissertations & Theses
T1  - Intuitive Human-Understandable AI Models Based on Fuzzy Neural Network
AU  - Ou, Liang
JF  - PQDT - Global
Y1  - 2024-01-01
DA  - 2024
SN  - 9798346573586
AB  - This research aims to build Intuitive Human-Understandable AI Models, focusing on the application and analysis of Fuzzy Neural Networks (FNNs). FNN is employed to interpret black-boxed models or integrate explainability into deep models, with a particular emphasis on analyzing fuzzy rules. This research aims to address three major problems: 1) How to explain a trained deep neural network (DNN) locally? 2) How to design an explainable neural network? 3) How to explain multiple neural networks’ collaborative decision?In the frst part of the research, the author provides an answer to the frst question. This part explores the approximation ability of FNN under the Local Interpretable Model-agnostic Explanations (LIME) framework. A method called Fuzzy-LIME is proposed for interpreting black-boxed models, particularly for largescale pre-trained image processing models. Fuzzy-LIME utilizes FNNs to generate explanations in the form of IF-THEN rules and analyzes the black-boxed mode with both factual and counterfactual decision logic. In other words, Fuzzy-LIME brings a comprehensive explanation of the impact of the decision by including/excluding certain features of an image. This approach provides a deep understanding of the model’s decision-making process and reveals the impact of perturbations on data samples, enabling analysis of incorrect predictions. Fuzzy-LIME also outperforms in terms of both approximation ability and the comprehensiveness of explanations in compare with LIME and Anchors.The answer to the second research problem is explored in the second part of the research, in which the author combines FNN with deep learning models to enhance the transparency of decision-making in a Reinforcement Learning (RL) agent. A solution called Explainable Fuzzy Reconstruction Net (EFRN) is proposed, which is designed to interpret agent’s decisions in RL. Leveraging FNNs, EFRN generates easily understandable IF-THEN rules and employs a generative model to visually represent the learned knowledge. The author demonstrates that EFRN maintains performance levels comparable to traditional RL methods while notably enhancing the global and local explainability of RL agents.In the third part of the research, the author further investigated the problem of explainable reinforcement learning (xRL). Based on research results from EFRN, the authors proposed Fuzzy Centered Explainable Network (FCEN). It utilises FNNs together with an attention-based vision transformer to establish IF-THEN rules and a generative model for visualizing learned knowledge. FCEN employs humanunderstandable logic in IF-THEN rules connects them with a generative model to concretize states into understandable patterns. Experimental results on 4 Atari games demonstrate that FCEN achieves high RL task performance while significantly enhancing the global and local explainability of RL agents. The FCEN demonstrates that by understanding the agent’s decision logic, a human can even enhance the agent’s performance by adjusting the model architecture with domain knowledge.In the last part of the research, the author provides a solution for the third research problem by modelling agent policies with FNN in a Multi-Agent System (MAS). This method models the information from other agents’ behaviour as the if clause of fuzzy logic, and the observation from the agent itself as the then clause of fuzzy logic. The trained agent thus has the ability to give humans transparent decision logic.
UR  - https://www.proquest.com/docview/3132879944?accountid=15181&bdid=109696&_bd=JWYEGiV3xbrMiDo64fX5s3xhVRQ%3D
ER  - 

TY  - Dissertations & Theses
T1  - Opportunity-Solution-Outcome Universal Design for Learning in Theory and Practice
AU  - Bowes, Danielle Marie
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798382607160
AB  - The "OPPORTUNITY-SOLUTION-OUTCOME UNIVERSAL DESIGN FOR LEARNING IN THEORY AND PRACTICE" presents the development of a robust framework tailored to facilitate universal learning that supports diverse learners and their ongoing development within organizational settings. This research addressed the identified gap in the literature with scalable solutions to support human and AI learners and educators, thereby advancing the field of Organizational Development (OD). The O-S-O UDL integrates a Universal Design for Learning (UDL), Cognitive Load Theory (CLT), Self-Determination Theory (SDT), individual human and artificial intelligence learning, learning organizations, OD, Socio-Technical Systems (STS) Theory, and Human Systems Integration (HSI).Employing a Design-Based Research (DBR) approach, the research began with autoethnography and literature review, followed by participant interviews to evaluate the O-S-O UDL in various contexts. Six professionals from diverse fields contributed to understanding the framework's features by applying the O-S-O UDL within semi-structured interviews, the study explored the lived experiences of six professionals a Regional Medical Education Coordinator, Chief Technological Innovator, National Healthcare Educator in Advanced Technological Simulations, Attending Neurosurgeon, Wawa Corporate Change Manager, and USA Today Best-Selling Author. The participants contributed to a deeper understanding of the O-S-O UDL’s features. The main findings of this study include the synthesis of twenty themes into O-S-O UDL Framework Principles and the subsequent Organizational Development (OD) uses within the O-S-O UDL Applied to Sociotechnical Systems (STS) Principles and the O-S-O UDL Applied to Human Systems Integration, rendering the O-S-O UDL applicable in dynamic organizational practice.
UR  - https://www.proquest.com/docview/3055591223?accountid=15181&bdid=109696&_bd=jSCQTlICvUPOhB%2BnT0amWaH5aqE%3D
ER  - 

TY  - Dissertations & Theses
T1  - When Artificial Intelligence Is Part of the Process: Global Leaders’ Beliefs and Algorithmic Decision-Making
AU  - Voda, Ruxandra A.
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798351446974
AB  - Leaders steer Artificial Intelligence's (AI) adoption across organizations to ensure competitiveness. Integrating algorithmic decision-making (ADM) enhances business prospects but comes with challenges. Understanding how human decision-making (HDM) and ADM blend, driving the combination, and addressing its obstacles represent leadership must-dos. With integrating ADM into strategic HDM, executives must understand how they function in relation to algorithmic advice. This paper offers a tool for increasing awareness of whether their belief that they receive algorithmic support influences them to make decisions they otherwise would not. Does the belief that a recommendation for a decision comes from an algorithm change their decision significantly?Within an experiment, global leaders from opposing culture clusters decided on their organization's survival, assisted by HDM or ADM-based advice. A pilot test established the normative decision choice. The recommended decision in the experiment was an inferior one. The results suggest that global leaders work fluently with ADM advice, placing it similarly with human counsel. Their decision-making is not necessarily influenced by the recommendation's nature but seems to be influenced by the existence of a recommendation (ADM or HDM). Results also indicate that global leaders followed their judgment and beliefs when rationalities diverged. The human-AI collaboration seems similar to human-human cooperation when the global leaders' self-efficacy is high.The paper builds on seminal global leadership and culture theories and algorithm aversion, appreciation, and trust in algorithms frameworks. These, combined with global leaders' digital literacy, seem to stand as viable guidelines for AI-based organizations' successful management.
UR  - https://www.proquest.com/docview/2715493330?accountid=15181&bdid=109696&_bd=jC2PeG%2BHgPjHkupwUGK71g9j0vw%3D
ER  - 

TY  - Dissertations & Theses
T1  - To Explore and Understand the Factors Influencing the Addition of Artificial Intelligence (AI) Skills to the U.S. Cybersecurity Workforce
AU  - Ragunathan, Ragu
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798382797922
AB  - Artificial Intelligence (AI) technologies are emerging and impacting industries, societies, and nations on a scale not seen before. The benefits of AI are entering the fields of healthcare, transportation, education, government, manufacturing, finance, military, and more. There is ongoing global competition among nations for leadership in AI, and the U.S. has launched multi-pronged AI initiatives. AI can strengthen cybersecurity, and AI skills for U.S. cybersecurity professionals is a path to a more robust U.S. cyber defense. This qualitative grounded theory study discusses and presents the factors that influence adding AI skills to the U.S. cybersecurity workforce, aiming at faster adoption of AI and enhanced cybersecurity. Its findings span trusted channels for AI information, integration of AI techniques into cybersecurity tools, inclusion of AI for cybersecurity in the U.S. laws, compliances and regulations, organizational awareness of AI for better teamwork, operational readiness, and explainability of AI techniques for cybersecurity practices, and the value of communicating to cybersecurity professionals the real-life use cases of AI for cyber defense and cyber-attacks combined with lessons learned. Furthermore, based on its findings, this study recommends actions to expedite adding AI skills to the U.S. cybersecurity workforce. It identifies future research questions for even deeper insights into AI skills and the adoption of AI techniques for U.S. cybersecurity. In conclusion, it emphasizes AI skills and techniques for expedited adoption in a non-disruptive, non-burdensome, and risk-free manner to blend into the environmental elements that control and define U.S. cybersecurity goals, objectives, and strategies. 
UR  - https://www.proquest.com/docview/3064676381?accountid=15181&bdid=109696&_bd=WpuFlzd3KUSjJOPjnYQiXODT5F8%3D
ER  - 

TY  - Dissertations & Theses
T1  - Documentation as a Tool for Algorithmic Accountability
AU  - Curtis, Taylor Lynn
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798304955331
AB  - This thesis argues that civil liability should rest on the deployer's understanding of system behavior, and that documentation is the necessary tool to accomplish this goal. This work begins by establishing the "hole'' in current approaches to AI risk regulation, the lack of a civil liability regime. It also highlights that civil liability is an already existing and effective regulatory tool that can be applied to AI. The rest of this thesis develops this argument by looking at what is necessary for such a framework to exist. It argues that an understanding of system behaviour is essential and achievable through documentation. It is divided into two substantive chapters. Firstly, Chapter 2 outlines how system behaviour can inform policy through documentation, linking the necessity of documentation to liability and proposing a concrete liability scheme based on documenting system understanding. Secondly, Chapter 3 discusses how documentation can alter a person's understanding of system behaviour, presenting a user study that demonstrates how system understanding can be achieved through documentation and structured data interaction. It argues that testing and system understanding are not insurmountable challenges and that by engaging in a relatively simple process, AI deployers can better understand the behaviour of their models. Overall, this thesis provides a methodical guide to understanding AI system behaviour and the establishment of a new pathway for effective regulation, arguing for the understanding of system behaviour and documentation at deployment as the path forward to achieve civil liability in AI.
UR  - https://www.proquest.com/docview/3172178811?accountid=15181&bdid=109696&_bd=uwkc6sDGNDSm7smZR1ATUraJHCg%3D
ER  - 

TY  - Dissertations & Theses
T1  - Leveraging Humans to Detect and Fix Representation Misalignment
AU  - Peng, Andi
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380096522
AB  - As robots are increasingly deployed in real-world environments, a key question becomes how to best teach them to accomplish tasks that end users want. A critical problem suffered by current robot reward and imitation learning approaches is that of representation misalignment, where the robot’s learned task representation does not fully capture the end user’s true task representation. In this work, we contend that because human users will be the ultimate evaluator of system performance in the world, it is crucial that we explicitly focus our efforts on leveraging them to detect and fix representation misalignment prior to attempting to learn their desired task. We advocate that current representation learning approaches can be studied under a single unifying formalism: the representation alignment problem. We mathematically operationalize this problem, define its desiderata, and situate current robot learning methods within this formalism. We then explore the feasibility of applying this formalism to robots trained end-to-end on visual input, where deployment failures can be caused by two types of error: errors due to an inability to infer the user’s true reward vs. errors due to knowing how to take correct actions in the desired state. We develop a human-in-the-loop framework—DFA (Diagnosis, Feedback, Adaptation)—to query for user feedback to perform efficient policy adaptation. In experiments with real human users in both discrete and continuous control domains, we show that our framework can help users diagnose the underlying source of representation misalignment more accurately than from robot behaviour alone. To conclude, we show how to leverage this feedback to improve model performance while minimizing human effort and discuss open challenges of using humans to detect and fix representation misalignment.
UR  - https://www.proquest.com/docview/2848806767?accountid=15181&bdid=109696&_bd=VrBdPx8KgLzaSu03k%2BGmNXL3X5k%3D
ER  - 

TY  - Dissertations & Theses
T1  - Automated Early Prediction of Cerebral Palsy: Interpretable Pose-Based Assessment for the Identification of Abnormal Infant Movements
AU  - McCay, Kevin D.
JF  - PQDT - Global
Y1  - 2022-01-01
DA  - 2022
SN  - 9798845434883
AB  - Cerebral Palsy (CP) is currently the most common chronic motor disability occurring in infants, affecting an estimated 1 in every 400 babies born in the UK each year. Techniques which can lead to an early diagnosis of CP have therefore been an active area of research, with some very promising results using tools such as the General Movements Assessment (GMA). By using video recordings of infant motor activity, assessors are able to classify an infant’s neurodevelopmental status based upon specific characteristics of the observed infant movement. However, these assessments are heavily dependent upon the availability of highly skilled assessors. As such, we explore the feasibility of the automated prediction of CP using machine learning techniques to analyse infant motion.We examine the viability of several new pose-based features for the analysis and classification of infant body movement from video footage. We extensively evaluate the effectiveness of the extracted features using several proposed classification frameworks, and also reimplement the leading methods from the literature for direct comparison using shared datasets to establish a new state-of-the-art. We introduce the RVI-38 video dataset, which we use to further inform the design, and establish the robustness of our proposed complementary pose-based motion features. Finally, given the importance of explainable AI for clinical applications, we propose a new classification framework which also incorporates a visualisation module to further aid with interpretability. Our proposed pose-based framework segments extracted features to detect movement abnormalities spatiotemporally, allowing us to identify and highlight body-parts exhibiting abnormal movement characteristics, subsequently providing intuitive feedback to clinicians.We suggest that our novel pose-based methods offer significant benefits over other approaches in both the analysis of infant motion and explainability of the associated data. Our engineered features, which are directly mapped to the assessment criteria in the clinical guidelines, demonstrate state-of-the-art performance across multiple datasets; and our feature extraction methods and associated visualisations significantly improve upon model interpretability.
UR  - https://www.proquest.com/docview/2714864790?accountid=15181&bdid=109696&_bd=t5%2F8LRMq25pv1hoJw7ZGdPKb4dA%3D
ER  - 

TY  - Dissertations & Theses
T1  - Explanation-Based Scientific Natural Language Inference
AU  - Valentino, Marco
JF  - PQDT - Global
Y1  - 2022-01-01
DA  - 2022
SN  - 9798371942531
AB  - Building systems that can explain and understand the world is a long-standing goal for Artificial Intelligence (AI). The ability to explain, in fact, constitutes an archetypal feature of human rationality, underpinning communication, learning, and generalisation, as well as one of the mediums enabling scientific discovery through the formulation of explanatory theories. As part of this long-term goal for AI, a large body of research in Natural Language Processing (NLP) focuses on the development and evaluation of explanation-based inference models, capable of reasoning through the interpretation and generation of natural language explanations.However, research in Explanation-based Natural Language Inference (NLI) presents several fundamental challenges. Firstly, the applied methodologies are still poorly informed by theories and accounts of explanations. Current work, in fact, rarely recur to formal characterisations of the nature and function of explanations, and are limited to generic explanatory properties. This gap between theory and practice poses the risk of slowing down progress in the field, missing the opportunity to formulate clearer hypotheses on inferential properties of explanations and well-defined evaluation methodologies. Secondly, Explanation-based NLI models still lack robustness and scalability for real-world applications. In particular, existing approaches suffer from several limitations when it comes to composing explanatory inference chains from large facts banks and performing abstraction for NLI in complex domains.This thesis focuses on scientific explanation as a rich theoretical and experimental framework for advancing research in Explanation-based NLI. In particular, the goal of the thesis is to investigate some of the fundamental challenges in the field from both a theoretical and an empirical perspective, attempting to derive a grounded epistemological-linguistic characterisation to inform the construction of more accurate and scalable Explanation-based NLI models in the scientific domain.Overall, the research described in the thesis can be summarised in the following scientific contributions:1. An extensive study on the notion of a scientific explanation from both a categorical and a corpus-based perspective aimed at deriving a grounded characterisation for explanation-based NLI. The study reveals that explanations cannot be entirely defined in terms of inductive or deductive arguments as their main function is to perform unification, fitting the event to be explained into a broader underlying regularity. Moreover, the study suggests that unification is an intrinsic property of existing corpora, emerging as explicit and recurring explanatory patterns in natural language.2. A novel computational model based on the notion of explanatory power as defined in the unificationist account of scientific explanation. Specifically, the model can be adopted to capture explicit explanatory patterns emerging in corpora of natural language explanations and flexibly integrated into explanation-based NLI architectures for downstream inference tasks.3. An empirical study on the impact of the explanatory power model on explanation-based NLI in the scientific domain, integrating it within sparse, dense and hybrid architectures, and performing a comprehensive evaluation in terms of inferential properties, accuracy and scalability.4. A systematic evaluation methodology to inspect and verify the logical properties of explanation-supporting corpora and benchmarks. The study, aimed at providing a critical quality assessment of gold standards for NLI, reveals that a majority of human-annotated explanations represent invalid arguments, ranging from being incomplete to containing identifiable logical errors.
UR  - https://www.proquest.com/docview/2778643096?accountid=15181&bdid=109696&_bd=HGU%2Fwkr40srzUF3ui2LzFWKhLXc%3D
ER  - 

TY  - Dissertations & Theses
T1  - A Generic Qualitative Inquiry on Cyber Professionals' Perceptions of Risk Tolerance Strategies on the Internet of Things Cyber Resilience
AU  - Wagner, Lily Y.
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798346875949
AB  - This research explores cybersecurity professionals' perceptions of risk tolerance strategies to enhance Internet of Things (IoT) cyber resilience. With the rapid proliferation of IoT systems, vulnerabilities that challenge traditional cybersecurity have emerged, particularly in critical infrastructure sectors. Using a generic qualitative inquiry approach, this study examines professionals' insights into the effectiveness of current strategies in balancing IoT risk and resilience, framed by the Theory of Planned Behavior (TPB), to assess attitudes, norms, and perceived control over cybersecurity practices. This methodology allowed for an in-depth examination of professionals’ experiences, drawing on a thematic analysis of interviews and field notes to capture shared perceptions. The sample comprised 12 cybersecurity and information technology (IT) professionals from various industries, aged mid-20s to late 60s, providing insights into IoT risk tolerance across the healthcare, finance, government, and agriculture sectors. The data analysis involved identifying significant phrases and patterns with ATLAS.ti, which revealed overarching themes capturing cybersecurity professionals' perspectives on IoT resilience. The findings indicate that current IoT risk-tolerance strategies are only partially effective, with professionals highlighting the need for continuous improvement, adequate resources, and interdepartmental collaboration. The conclusions emphasize the importance of organizational culture, leadership support, and resource availability while recommending IoT-specific standards, cross-departmental collaboration, and rigorous vendor compliance. Future research should examine sector-specific challenges, organizational size variations, and experience levels in IoT risk management.
UR  - https://www.proquest.com/docview/3148716226?accountid=15181&bdid=109696&_bd=r2svAZqmk75kttEN89gN%2BFhWT7w%3D
ER  - 

TY  - Dissertations & Theses
T1  - Human-Machine Collaboration in Real-World Machine-Learning Applications
AU  - Roberts, Claudia Veronica
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798371968654
AB  - Automation tools like machine learning are a necessity in our big data world. Thanks to the Internet and advancements in all facets of computer and storage technology, almost everyone has a voice in the Internet connected world. However, there are still very real physical limits in our physical world. This dichotomy—the seemingly limitless nature of technology enabled data colliding with the physical limits of the real world—has made automation tools a necessity, and predictive models powered by machine learning algorithms are one such tool.The promise of machine learning to accurately predict future human behavior and human preferences has lead practitioners and researchers alike to apply machine learning automation tools to tasks such as product recommendations and speculatory activities such as long term job applicant success. However, due to the mercurial nature of humans, developing mathematical intermediaries to attempt to model and predict human behavior is challenging and not a straight-forward task. One way of harnessing the power of machine-learning backed automation to help reduce the scale of many real-world applications in more challenging domain settings is by having humans and machines collaborating in non-trivial ways. In this dissertation, we delineate the various ways in which humans and machines collaborate in challenging real-world applications. Moreover, we highlight three specific ways in which we can use human-machine collaboration to keep or increase utility and reduce real-world harm when using these systems in the wild: (i) humans enabling computers with domain specific knowledge, (ii) computers providing humans with algorithmic explanations, (iii) humans and computers working together in decision making.
UR  - https://www.proquest.com/docview/2773882338?accountid=15181&bdid=109696&_bd=PnojeAsLzuRWTXE25ANeUkrczLg%3D
ER  - 

TY  - Dissertations & Theses
T1  - The Effects of Untrained Healthcare Professionals in Utilizing Artificial Intelligence
AU  - Dontu, Sravanthi
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798314895375
AB  - This qualitative study aimed to identify how the constructs of the Unified Theory of Acceptance and Use of Technology (UTAUT2) framework impact untrained healthcare professional’s utilization of Artificial Intelligence (AI) systems while also exploring reasons for the lack of AI training in healthcare. The research employed a descriptive phenomenological approach, conducting Zoom meeting interviews with 12 purposefully sampled participants (six male, six female), all of whom either were MDs or registered nurses. The study investigated how performance expectancy, effort expectancy, social influence, and facilitating conditions affect AI utilization in healthcare settings. Findings revealed significant gaps in AI training, with only one out of twelve participants reporting formal training. Poor IT support and unresolved technical issues were standard, indicating inadequate facilitating conditions for AI utilization. Social influence emerged as crucial, with many participants relying on peer reviews of AI results. Key concerns included liability issues, urban-rural disparities in AI deployment, and system integration challenges. The study also highlighted the need for government funding to implement AI systems in medical centers. Participants expressed reluctance to use AI results without review, often consulting colleagues. These findings suggest a pressing need for structured AI training programs, improved technical support, and strategies to address ethical and practical concerns. The research contributes to understanding the dynamics of AI utilization in healthcare, providing insights for developing strategies and training programs to enhance AI integration, ultimately aiming to improve efficiency and patient outcomes in the healthcare sector.
UR  - https://www.proquest.com/docview/3204519894?accountid=15181&bdid=109696&_bd=q18e5R8XVJtOD%2BY%2BJg9lS9TnrKQ%3D
ER  - 

TY  - Dissertations & Theses
T1  - Explainable Deep Learning Approach for Detecting Money Laundering Transactions in Banking System
AU  - Kute, Dattatray Vishnu
JF  - PQDT - Global
Y1  - 2022-01-01
DA  - 2022
SN  - 9798379785000
AB  - Money laundering has been a global issue since last few decades. The continuous change in technology, fraud patterns and regulatory landscape is making it even harder. To assist the compliance officers with investigation, Artificial Intelligence (AI) based decision support systems are developed which suffers from a high false negative rate of alerts without having much of transparency in the predictions. This thesis presents a study on the application of Convolutional Neural Network (CNN) method for predicting the suspicious money laundering transactions and explaining the predictions using SHapley Additive exPlanations (SHAP) XAI method. The results showed that the CNN model outperformed other models, indicating better handling of compliance risk. On the contrary, CNN model showed higher number of false positives compared to other models which indicates more operational efforts. From the Banks perspective, reducing the compliance risk is more important than operational efforts. Hence f_β score was calculated using β=3 to measure the performance. The scores were CNN - 78.23%, XGB – 62.09%, RF – 61.09%, and SVM - 30.86%. The SHAP XAI method applied on CNN model to interpret the predictions made by CNN model, could identify the key features influencing the predictions.
UR  - https://www.proquest.com/docview/2833602804?accountid=15181&bdid=109696&_bd=4U%2BpC0rIXc6CgQ%2FaaVjhfYFz7Yc%3D
ER  - 

TY  - Dissertations & Theses
T1  - The Science and Art of Human and Artificial Intelligence Collaboration
AU  - Groh, Matthew
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381957273
AB  - While artificial intelligence (AI) appears to be surpassing the performance of human experts on a wide variety of games and real-world tasks, these algorithms are prone to systematic and surprising failures when deployed. In contrast to today’s state-of-the-art algorithms, humans are highly capable of adapting to new contexts. The different strengths and weaknesses of humans and AI motivate a guiding research question for the emerging field of human-AI collaboration: When, where, why, and how does the combination of human problem solving and AI systems lead to a hybrid system that surpasses (or fails to surpass) the performance of either humans or the machine alone? This dissertation addresses various dimensions of this guiding question by conducting large-scale, digital experiments across three distinct tasks and domains: deepfake detection, dermatology diagnosis, and Wordle. First, the experiments in deepfake detection examine the similarities and differences between human and machine vision in identifying visual manipulations of people’s faces in videos and identify important performance trade-offs between hybrid systems and human or AI only systems for deepfake detection. Second, the experiments in dermatology diagnosis reveal that non-visual information is often essential for diagnosing skin disease, diagnostic accuracy disparities across skin color exist in image-only store-and-forward teledermatology, and clinical decision support based on a fair deep learning system can significantly increase physicians’ diagnostic accuracy in this experimental setting. Third, the experiment on Wordle demonstrates that digitally mediated expressions of empathy can counteract the negative effect of anger on human creative problem solving. In addition to these digital experiments, this dissertation presents two algorithmic audits on clinical dermatology images to reveal where systematic errors arise in state-of-the-art algorithms, examines how context influences automated affect recognition, and proposes methods for more effectively incorporating context in applied machine learning. Together, these contributions provide empirical evidence for why human-AI collaborations succeed and fail across a variety of tasks and domains, insights into how to design human-AI collaborations more effectively, and a framework for when and where hybrid systems should rely on human problem solving.
UR  - https://www.proquest.com/docview/3030965998?accountid=15181&bdid=109696&_bd=bx9jPKzutjuUb9nCfZyy12B8C14%3D
ER  - 

TY  - Dissertations & Theses
T1  - Subsurface Digital Twin and Emergence
AU  - Zhao, Yushi
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381956719
AB  - Subsurface characterization stands at the nexus of humanity’s growing demands for materials, energy, and safety amid the burgeoning population and rising living standards. However, challenges in subsurface characterization, rooted in conventional practices, functional silos, limited data density, and technological constraints, impede business efficacy and sustainable development. As societies’ expectations shift and industries evolve, a paradigm shift is required in the human-machine relationship and the way we organize work. To meet these challenges and ensure responsible human progress, a systematic solution is needed.This thesis investigates the concept of a subsurface digital twin as a boundary object that bridges disciplines, scales, and uncertainties, fostering collaboration and real-time informed decision-making. It explores the evolution of subsurface characterization from data-sparse and theory-dependent practices to a holistic digital twin framework. The thesis identifies critical technical and sociotechnical challenges, including data scarcity, over-reliance on empirical relationships, functional silos, and trust. The thesis demonstrates how a subsurface digital twin can enhance cross-functional collaboration and address critical challenges through real-world examples. It highlights the use of geoanalytics and machine learning to predict total organic carbon content and formation brittleness, showcasing the digital twin’s power in multidisciplinary workfows. Furthermore, it proposes a solution for uncertainty reduction through integration and laid out future steps for the development of the subsurface digital model, construction of pseudo/surrogate models for probabilistic simulation complex and time-consuming numerical simulations, and use of the digital twin to bridge workfows between data-rich and data-scarce regions across scales.The thesis outlines the design and value-creating functions of the subsurface digital twin system, facilitating adaptive resolution and agile implementation. It envisions a future where such digital twins revolutionize decision-making, from individual project optimization to enterprise-wide insights. The thesis underscores the importance of strategic investment in digital twins for long-term returns and as a cornerstone of the evolving human-machine relationship and advances the concept of a subsurface digital twin as a transformative approach to subsurface characterization, fostering collaboration, tackling challenges, and paving the way for sustainable progress in a rapidly changing world.
UR  - https://www.proquest.com/docview/3030902152?accountid=15181&bdid=109696&_bd=sDh1uXqIoGjbInmq9keoTXRXBss%3D
ER  - 

TY  - Dissertations & Theses
T1  - Explainable-AI in Multiview Facial Authentication Using Automatic Image Segmentation
AU  - Alsawwaf, Mohammad
JF  - PQDT - Global
Y1  - 2021-01-01
DA  - 2021
SN  - 9798380499170
AB  - Accurate authentication of a human user without considering specific proximity, context, and environmental conditions is a complex yet crucial task in application areas such as surveillance and proof of identity. While significant research has been conducted in the direction of human authentication, completing authentication with an explainable process is still far from being perfect under the currently deployed methods. This is mainly due to depending on machine learning to create behindthe-scene calculations that the user can’t fully understand. Another issue is the requirement of distance approximation to the device and neutral face expressions in many of the approaches deployed. For example, this applies to fingerprint, facial recognition, and retinal scan tests.In today’s world, it is becoming crucial to be able to complete the authentication steps without having to require specific proximity or neutral expressions. Multiview facial authentication relates to the ability to capture a face from the front or from the side and use these views for the authentication. This allows for an increase in the flexibility of the process.This explainable and flexible facial authentication system needed is highly dependent on the facial biometric features that can be extracted accurately to discriminate between one human and another. These biometric features should be insensitive to variation in angular facial views, illumination, and age. Extraction of a unique feature set that allows a human to be authenticated smoothly, even without proximity, under varying conditions is one of the main steps needed for this research.In this research, the proposed method includes a flexible authentication solution based on an accompanying explainable process. The explainable authentication approach uses a facial feature set that can be segmented automatically once a face has been successfully detected and captured. The proposed feature set is made up of a combination of facial minutiae and contours which are extracted from the human face after the detection step, using manual and automatic segmentation. Additionally, the proposed solution will include an authentication solution for faces from the side, referred to as a face-profile.The proposed approach uses the extracted feature sets and makes calculations to produce feature vectors. Then, weighted means are used to measure similarities between subjects. This research attempts to tackle the calculations part and the building of the feature vectors by implementing an eXplainable Artificial Intelligence (XAI) tactic. The contribution highlight of this reserch is in creating the vectors, using an explainable geometrical approach for the authentication process, and allowing face-profile to be used in authentication.Results from this investigation, in relation to the used datasets, determine that the used technique offers promising findings. Further research can provide a significant influence on how the facial authentication of humans can be attempted. The performance of the suggested system is measured using known universal calculations including precision and accuracy, false positive rates, and false negative rates.
UR  - https://www.proquest.com/docview/2877960633?accountid=15181&bdid=109696&_bd=5Xf2ITsxYextu0OP%2B5WrDhE5DBs%3D
ER  - 

TY  - Dissertations & Theses
T1  - Framing TRUST in Artificial Intelligence (AI) Ethics Communication: Analysis of AI Ethics Guiding Principles through the Lens of Framing Theory
AU  - Nagar, Namrata
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798845409294
AB  - With the fast proliferation of Artificial Intelligence (AI) technologies in our society, several corporations, governments, research institutions, and NGOs have produced and published AI ethics guiding documents. These include principles, guidelines, frameworks, assessment lists, training modules, blogs, and principle-to-practice strategies. The priorities, focus, and articulation of these innumerable documents vary to different extents. Though they all aim and claim to ensure AI usage for the common good, the actual AI system outcomes in various social applications have invigorated ethical dilemmas and scholarly debates. This study presents the analysis of AI ethics principles and guidelines text published by three pioneers from three different sectors—Microsoft Corporation, National Institute of Standards and Technology (NIST), AI HLEG set up by the European Commission through the lens of media and communication’s Framing Theory. The TRUST Framings extracted from recent academic AI literature are used as standard construct to study the ethics framings in the selected text. The institutional framing of AI principles and guidelines shapes the AI ethics of an institution in a soft (as there is no legal binding) but strong (incorporating their respective position/societal role’s priorities) way. The AI principles’ framing approach directly relates to the AI actor’s ethics that enjoins risk mitigation and problem resolution associated with AI development and deployment cycle. Thus, it has become important to examine institutional AI ethics communication. This paper brings forth a Comm-Tech perspective around the ethics of evolving technologies known under the umbrella term—Artificial Intelligence and the human moralities governing them.
UR  - https://www.proquest.com/docview/2721197134?accountid=15181&bdid=109696&_bd=otJ7J6ltFJu8OntM4jBSAvKLcOk%3D
ER  - 

TY  - Dissertations & Theses
T1  - An Exploratory Study of Risk Quantification Loss Event Frequency (LEF) Approaches Using the Factor Analysis of Information Risk (FAIR) Model in Non-Financial Risk Areas
AU  - Gowen, James L., Jr.
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798382395043
AB  - This study explored the limited use of non-financial banking quantitative risk assessment methodologies. Quantitative assessments have been avoided due to a lack of historical data or the need for complicated, expensive quantitative techniques. The purpose of this exploratory, quantitative, quasi-experimental study aims to develop a practical risk management model framework to help bank practitioners conduct quantitative risk assessments utilizing limited data through a novel Factor Analysis of Information Risk (FAIR) ontology framework. The research questions for the study are: What key factors in banking incidents, if any, influence vulnerability risk assessments derived from historical public incident data? Is there a difference between traditional quantitative assessments using an industry framework and quantitative assessments using an industry framework with a Bayesian approach? The study examined the banking industry's limited public data breach incidents to identify non-financial risk tendencies. The independent variables for this study are (1) the various types of data breach incidents and (2) their effects on the two dependent variables: loss event frequency (LEF) and loss magnitude (LM) risk calculation scores. The study found no association between the top data breach incident types. However, the study found statistically significant differences between the LEF methodologies even when utilizing the same framework. The findings show that various LEF quantitative approaches using the FAIR framework can use limited historical threat data (open source or internal) to analyze non-financial operational risk. The techniques from the study can provide banking risk practitioners with a practical quantitative model to supplement qualitative techniques for enhanced holistic banking risk forecasting.
UR  - https://www.proquest.com/docview/3051278826?accountid=15181&bdid=109696&_bd=R6D8Wb5zt57ZFkv8%2FWlRL2VKalU%3D
ER  - 

TY  - Dissertations & Theses
T1  - Robot Explanations: Preferences, Generation, and Communication
AU  - Han, Zhao
JF  - ProQuest Dissertations and Theses
Y1  - 2021-01-01
DA  - 2021
SN  - 9798460426928
AB  - During the last decade, robots have become increasingly ubiquitous. They have been moved outside of laboratories and deployed to environments where they have to interact with humans. Examples include public places such as warehouses, hotels, factories, retail stores, and streets, as well as the most anticipated places—private homes. In these increasingly unstructured environments, requirements for human-robot interaction and collaboration are more involved as the tasks that robots complete are increasingly complex. For people to build trust with robots, there is a pressing need for robots to explain their actions and behaviors explicitly, rather than in implicit and vague manners such as using eye gaze or arm movement. This dissertation centered around robot explanations and examined four interconnected aspects of the robot explanation process: from what explanations humans prefer, how to generate explanations, and how to communicate them explicitly, to explaining missing causal information of past actions due to environment change.The contributions of this work are fourfold. In a human-subjects study, we found strong evidence that people prefer verbal explanations coupled with non-verbal cues. To verbally explain, people prefer robots to get their attention first, then concisely explain, and are only willing to ask a few follow-up questions for more details. Then I contributed explanation generation algorithms using Behavior Trees (BTs), a simple yet powerful robot task sequence method for high-level and failure robot explanations. We framed BTs into semantic sets to generate explanations from the resulted shallow tree and demonstrated the algorithms with a complex mobile manipulation task and a taxi domain navigation task. BTs were also made dynamically modifiable for behavior insertion after users' follow-up questions. Thirdly, we contributed a complete projection mapping implementation solution for instant and salient robot communication: from how to choose an off-the-shelf projector, how to calibrate it, and the underlying principle, to all the code and files needed to readily integrate the solution into any ROS system. Finally, I investigated physical replays, verbal and projection markers for robots to help people infer missing causal information of a robot's past actions. We found that a multimodal approach, including all physical replay, verbal, and projection markers, provided better aid in inference-making, less mental workload, and more trustworthy robots.
UR  - https://www.proquest.com/docview/2582827819?accountid=15181&bdid=109696&_bd=Mb3z4GN0Wu%2BT%2FYABsfqsmaeXr5I%3D
ER  - 

TY  - Dissertations & Theses
T1  - Best Practices Policy for Commercial Artificial Intelligence Deployment: A Modified Delphi Study
AU  - McClean, Timothy W.
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798315738879
AB  - The problem is that it is unknown whether the North American logistics CIO/CEOs can reach a consensus on standard ethical principles and governance practices when developing AI systems. The purpose of this qualitative modified Delphi study is to find a consensus on ethical standards and a governance framework among logistics industry experts in the United States. The following research question guided this study. What is the minimum set of focused ethical guidelines for autonomous AI systems in the commercial transportation and logistics industry which could result in autonomous AI systems' employment of ethical principles? The study recruited ten industry CEO/CIO leaders from the United States East to the West Coast to take part in the study. The participants created a codified list of the top nine ethical principles for AI development and a suggested enforcement mechanism with a funding method for oversight. The theoretical frameworks for this study define the ideas and theories which provide the grounding of the research, unite them through logical connections, and relate these ideas to the study. The study included an innovative approach using a non-commercial AI/ML tool to double-check the thematic analysis against the standard manual thematic analysis. This approach compared the AI/ML results against a manual thematic analysis when supplying actor-network, technological determination, and motivational posturing theories as the conceptual frameworks. Study recommendations include recommendations for codifying a framework of the top nine identified ethical principles to monitor potential violations and a method to shut down systems that integrate this important AI technology innovation.
UR  - https://www.proquest.com/docview/3206719573?accountid=15181&bdid=109696&_bd=EHfKvOz3hW0hx108vi%2F9Dhr%2BSx8%3D
ER  - 

TY  - Dissertations & Theses
T1  - Creating Interpretable Data-Driven Approaches for Remote Health Monitoring
AU  - Ghods, Alireza
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380379212
AB  - The impending “age wave” underscores the urgent need for technology that supports remote health monitoring. This research focuses on developing technologies to improve clinician interpretability of remote health monitoring systems. These technologies visualize and summarize raw data and machine learning inferences. Unlike prior methods, this dissertation describes techniques developed in partnership with the target end user: clinicians. Keeping the clinician in the design loop improves technology trust, adoption, and patient outcomes.In this research, we introduce two visualization studies that illustrate and validate the human-in-the-loop approach, demonstrating that with this technique clinicians can successfully identify health events from unconventional data sources, such as smart homes and smartwatches. To complement these studies, the research also introduce methods to create machine learning models that are interpretable for humans. Here, we focus on discovering and communicating patterns that summarize complex data and justify machine learning decisions. All of the introduced algorithms yield improved objective performance on synthetic and real-world complex data. To evaluate their efficacy for remote health monitoring, the methods are assessed by clinician end users in a series of comparative surveys. In these experiments, the methods introduced in this dissertation show significantly enhanced interpretability. We discuss the implications and limitations of this work for remote health monitoring systems and offer ideas for continued work.
UR  - https://www.proquest.com/docview/2867107280?accountid=15181&bdid=109696&_bd=gGMuEzLwcNx5hz6AveWKNDhdceQ%3D
ER  - 

TY  - Dissertations & Theses
T1  - Privacy Attacks and Defenses Under Security Threats in Machine Learning
AU  - Zhou, Shuai
JF  - PQDT - Global
Y1  - 2024-01-01
DA  - 2024
SN  - 9798346573791
AB  - Machine learning has been increasingly adopted across various domains due to its outstanding performance. However, machine learning models exhibit some vulnerabilities against threats in the real world, including privacy risks and security concerns. In terms of privacy risks, malicious users can steal the private information of other users or model owners, including recovering training data, inferring membership, and cloning the trained model without authentication. In terms of security, the functionality of machine learning models might be disrupted by malicious users. Both types of attacks pose challenges to the widespread deployment of machine learning models. However, these attacks are often studied separately, and their relationship is not well understood. To gain a better understanding of threats in machine learning, this thesis explores the interaction between privacy and security threats, especially the change in performance of privacy attacks and defenses when security attacks are present. Specifically, the contributions can be summarized as follows:• This thesis reveals that adversarial examples have the potential to enhance the reconstruction of private training data, implying that security vulnerabilities would amplify the privacy leakages in machine learning. This insight enables a more precise assessment of privacy threats.• We emphasize that adversarial attacks can escalate the privilege of attackers targeting compromising privacy. We propose a universal privacy attack framework that enhances the existing label-only attacks by recovering the confidence vectors. This framework bridges the gap between label-only and confidence-based attacks.• We introduce an innovative defense to prevent the reconstruction of private data, which incorporates data poisoning techniques as a defensive strategy. This strategy leverages the security vulnerabilities inherent in the models of attackers to diminish their performance, thereby preventing privacy leakage. This approach represents a benign application of security attacks. • We investigate the benign use of privacy attacks, leading to a defense mechanism against model stealing attacks. By exploiting model inversion attacks for data reconstruction, we can uncover the hidden patterns in the model’s outputs, effectively creating a unique fingerprint for the model. This method allows us to detect the model stealing behaviors based on normal examples, eliminating the need for adversarial examples.
UR  - https://www.proquest.com/docview/3132879809?accountid=15181&bdid=109696&_bd=KRTwgiznDZaER3%2Fa%2FwCqPUe7yK4%3D
ER  - 

TY  - Dissertations & Theses
T1  - Training Human-AI Teams
AU  - Mozannar, Hussein
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798304946124
AB  - AI systems are augmenting humans' capabilities in settings such as healthcare and programming, forming human-AI teams. To enable more accurate and timely decisions, we need to optimize the performance of the human-AI team directly. In this thesis, we utilize a mathematical framing of the human-AI team and propose a set of methods that optimize the AI, the human, and the interface in which they communicate to enable better team performance. We first show how to provably train AI classifiers that complement humans and can defer the decision to humans when it is best to do so. However, in specific settings, AI cannot autonomously make decisions and thus only provides advice to humans. In that case, we build onboarding procedures that train humans to have an accurate mental model of the AI to enable appropriate reliance. Finally, we study how humans interact with large language models (LLMs) to write code. To understand current inefficiencies, we developed a taxonomy to categorize programmers' interactions with the LLM. Motivated by insight from the taxonomy, we leverage human feedback to know when to best display LLM suggestions.
UR  - https://www.proquest.com/docview/3170654288?accountid=15181&bdid=109696&_bd=HuIvQrLY0%2FcYZcr2BowxEjl7YyM%3D
ER  - 

TY  - Dissertations & Theses
T1  - MATLAB-Based Simulation of Autonomous Vehicle Detection and Pedestrian Safety at Signalized Intersection During Right Turns
AU  - Sulle, Methusela
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798315724056
AB  - Roadway intersections are among the most hazardous locations for pedestrians due to the complex interactions between vehicles and pedestrians, especially during right-turn maneuvers. Human drivers and traditional vehicles are both constrained by visibility and rely on intuition, eye contact, and quick judgment to respond to sudden pedestrian movement. Autonomous Vehicles (AVs), however, are dependent exclusively on sensor inputs and algorithmic processing, and thus are more prone to detection problems when line-of-sight is obstructed.This thesis examines the challenge that Autonomous Vehicles (AVs) face in detecting pedestrians at signalized intersections with limited sight distances and explores how Vehicle-to-Everything (V2X) communication can mitigate detection blind spots. Using a MATLAB simulation, this work simulated a real-world intersection in Jersey City, NJ, incorporating autonomous vehicle (AV) dynamics, pedestrians, and environmental occlusions.Findings revealed that under existing sight distance conditions (13 ft), AVs detected pedestrians at only 14% of points, with a collision probability of 1.0 for distances of less than 14.8 ft. Using the regulatory corner sight distance of 100 feet, detection was raised to 58%, which significantly reduced collision dangers. Additionally, V2V communication enabled occlusion-free, real-time pedestrian warning among vehicles, while V2I systems enhanced early warning through roadside infrastructure. Furthermore, V2P communication, facilitated by Class 1 Bluetooth, enabled occlusion-free, timely pedestrian detection.This research offers policy-critical results for infrastructure policy, AV design, and urban planning, recommending V2X integration and intersection design modifications as imperative steps toward safer pedestrian-AV interaction.
UR  - https://www.proquest.com/docview/3206152228?accountid=15181&bdid=109696&_bd=qoOcS%2FpT1bYi896n2M0EMtn8aj0%3D
ER  - 

TY  - Dissertations & Theses
T1  - Empowering Humans in Human-AI Decision Making
AU  - Lai, Vivian
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798845408556
AB  - Due to recent advances in Artificial Intelligence (AI), AI models are able to surpass human performance in various tasks unprecedentedly and are rapidly integrated into systems that assist humans in making decisions. However, deploying such systems into the real world requires an understanding of the potential risks and challenges we might face. How do we interpret and explain AI models’ predictions while being aware of their biases and weaknesses? In this thesis, I discuss my work that empowers humans to make better decisions with AI models through AI-backed interactive systems. I describe (1) how humans make decisions with models (Chapter 2), (2) how explanations differ across models and methods (Chapter 3), (3) how humans learn counterintuitive patterns from models (Chapter 4), and (4) how humans and imperfect models could collaborate effectively (Chapter 5). I conclude by discussing future research perspectives on making human-AI collaborations better and more accessible.
UR  - https://www.proquest.com/docview/2707594030?accountid=15181&bdid=109696&_bd=iirQw6V%2BKh4YN8ILbl10dfdQmZY%3D
ER  - 

TY  - Dissertations & Theses
T1  - Hybrid Service Delivery - Purposeful Integration and Design of Artificial Intelligence for the Automation and Augmentation of Online Service Encounters
AU  - Poser, Mathis
JF  - PQDT - Global
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381024067
AB  - The adoption rate of artificial intelligence (AI) and its application for the automation or augmentation of tasks and activities in organizations is steadily increasing. With its potential to enhance value creation by elevating operational efficiency and productivity, AI can be implemented to create a competitive advantage. To leverage this potential for value creation and to address the heightened expectations of service seekers (SKs), organizations apply AI-based solutions for the delivery of online intangible services. Represented as virtual agents or integrated in user interfaces, AI can be used to meet SKs’ needs for personalized, bidirectional, and chat-based interaction. By deploying conversational agents (CAs), which are provided with an identity and represented virtually, the availability, accessibility, and efficiency of text-based online service delivery can be increased by automating service encounters. In augmented service encounters, hybrid intelligence systems (HISs) can be used to combine the complementary capabilities of service employees (SEs) and AI, represented as a virtual agent, or integrated in a user interface, through collaboration. However, exploiting AI’s potential to automate and/or augment cognitive activities in online service delivery is not a self-fulfilling endeavor. First, the automation of service encounters is associated with limitations, as the bounded capabilities of CAs can lead to service failure. Second, HISs need to be improved in regard to collaboration between AI and SEs to increase the effectiveness of augmented service encounters. Third, the automation and/or augmentation of service encounters requires the renewal of socio-technical constellations of AI, SK, and SE to ensure successful online service delivery. By adapting and complementing CAs and HISs, the goal of this dissertation is the development of knowledge for the creation of human-centered AI-based solutions that are represented as virtual agents or embedded in user interfaces to allow hybrid online service encounters. In doing so, integration points for service processes and tasks are determined and knowledge for the design of AI-based solutions is generated. This enables, on the one hand, the realization of hybrid consecutive online service encounters with handover of SK requests to SEs to avoid CA failure. On the other hand, AI-based virtual agents and user interfaces can be constructed to support SEs in hybrid simultaneous online service encounters.This cumulative dissertation follows the ground rules of the design science research (DSR) paradigm to pursue the research goal. The conducted research activities refer to nine publications that interconnect the three cycles of DSR and address six research questions. Motivated by the different epistemological interests in the context of defining the problem space and producing the solution space, several research methods have been applied in this cumulative dissertation. Literature reviews were conducted to capture existing scientific  knowledge to identify research gaps and consider descriptive as well as prescriptive knowledge for the cumulative generation of knowledge contributions. The development of a taxonomy contributed to an improved understanding of the integration of AI-based solutions into online service delivery work systems. To generate the design knowledge, different qualitative methods were applied. Semi-structured interviews were conducted to identify real-world challenges as well as goodness criteria for solutions. In addition, interviews and focus groups were used for the evaluation of the design knowledge and its instantiation with prototypes. The collected data were examined by performing qualitative content analyses. Quantitative methods were applied to assess the influence of created design entities in the form of prototypes on individuals. The collected primary and secondary data were analyzed using different statistical methods.
UR  - https://www.proquest.com/docview/2901809645?accountid=15181&bdid=109696&_bd=nb67cfaYOlB4MC%2BCp66g4hyeEqU%3D
ER  - 

TY  - Dissertations & Theses
T1  - Toward Robust Stress Detection Using Real-World Data and Multimodal Learning
AU  - Hosseini, Seyedmajid
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798315731573
AB  - Predicting subjective human affect like stress from sensors poses significant machine learning challenges, especially when aiming for robust generalization in noisy, real-world environments with missing data. The prevalent reliance on controlled laboratory settings for training often results in models that fail to generalize to the complexities of real-world environments, characterized by sensor noise, variable user behavior, and contextual factors. Consequently, most published work, relying on small, controlled studies and traditional multivariate classifiers that treat each signal in isolation, cannot capture the crucial cross-modal dependencies or effectively compensate for the missing and noisy data inherent in real-world situations. This dissertation aims to bridge laboratory and real-world stress classification by first releasing two new multimodal datasets collected in both controlled and naturalistic environments. Second, this dissertation proposes a set of practical recommendations for robust stress detection in real-world settings, focusing on data preprocessing techniques and feature selection strategies resilient to noise and missing data. Third, proposing a robust multimodal learning framework that unifies attention-based intermediate fusion of physiological and contextual features to effectively capture complex temporal dependencies. Fourth, propose a missing-modality extension of the I2M2 product-of-experts model to maintain accurate stress prediction despite intermittent sensor failures. Together, these contributions enable accurate, privacy-preserving stress prediction with improved generalization to noisy and incomplete real-world sensor data. To overcome these challenges, we introduce lightweight fusion architectures with modality-aware attention blocks that learn robust joint representations by dynamically attending to informative modalities and mitigating the impact of noisy channels. Evaluated on the EmpathicSchool dataset for stress classification under a rigorous Leave-One-Subject-Out protocol, our approach outperformed strong multivariate baselines. Furthermore, the effectiveness of these architectural principles has been demonstrated in other domains such as drowsiness detection and image captioning, suggesting their broader applicability in multimodal time-series analysis. Across datasets, the proposed framework improved macro F1 by 6 – 12 percentage points over established multivariate and early/late-fusion networks; LOSO accuracy on the challenging drowsiness data rose from 64% (best baseline) to 74% with Uncertainty Weighted Conditional Variational Autoencoder. The UW-CVAE extension retained 90% of its accuracy when one modality was synthetically dropped.By pairing carefully curated real-world datasets with a modality-robust fusion strategy, this work demonstrates that reliable stress classification outside the lab is possible. The released resources and validated methods provide a foundation for next-generation affective computing systems that can perform well in noisy, dynamic environments.
UR  - https://www.proquest.com/docview/3206360415?accountid=15181&bdid=109696&_bd=eHxCL3lyavAuqKHkPOeHr6nq0qw%3D
ER  - 

TY  - Dissertations & Theses
T1  - Empowering Users on Social Media for Better Content Credibility
AU  - Jahanbakhsh, Farnaz
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798346385639
AB  - As misinformation raged on online social spaces and threatened people’s lives and even democracy, platforms rose as the authority on misinformation detection and moderation. However, concerns about freedom of speech and listening rights and the autonomy of individuals in deciding what content to consume, as well as the misalignment in incentives between the users and the platforms should give us pause in accepting this centralized moderation as the optimal solution. In this dissertation, I have explored an alternative approach to misinformation moderation-a democratized one that empowers every user to have a say in what content they consider misinforming, make decisions about what they want to do with such content, and help their friends avoid misinformation. I have investigated the following questions: 1) how to alter the design of social media platforms to enable users to have a say in what is misinformation and what a social media platform that provides this empowerment would look like, 2) how this user empowerment changes the accuracy of content that users share, 3) how to design tools that enable this user empowerment on the web and work on all platforms without needing support from them, 4) how to leverage AI not to impose "the truth" on users, but to help amplify users’ assessments, and 5) how to enable users beyond labeling content accuracy, and empower them to modify and "fix" online content.
UR  - https://www.proquest.com/docview/3132849909?accountid=15181&bdid=109696&_bd=vBo0%2FIXBhCFOXNjBRv205xNFDJE%3D
ER  - 

TY  - Dissertations & Theses
T1  - Scaling up Recognition in Expert Domains with Crowd-Source Annotations
AU  - Wang, Pei
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798368442457
AB  - The success of deep learning in image recognition is substantially driven by large-scale, well-curated data. On visual recognition of common objects, the data can be scalably annotated on online crowd-sourcing platforms because the labeling does not need any prior knowledge. However, the case is not true for images of expertise like biological or medical imaging in which labeling them needs background knowledge. Although data collection is still usually easy, the annotation is difficult. Existing self-supervised or semi-supervised solutions train a model that tries to learn from a small amount of labeled data and a large amount of unlabeled data. These solutions show good performances on common object recognition but have been found not to work effectively on fine-grained expert domains.In this thesis, we propose a new solution with crowd source annotations to address the problem. Inspired by the fact that supervised learning on as much as data can always perform better, our method tries to scale up the annotation. This is implemented by two different approaches, machine teaching and human filtering. Machine teaching first teaches humans with a short carefully designed course to learn the expertise knowledge so that they can label the data later. Human filtering simplifies the process to a binary selection procedure without preceding training. Beyond these two approaches, a unified explanation framework is developed to generate visualizations that are merged into two approaches, enabling easier and more accurate annotation results. Experiments show that both methods significantly outperform various alternative approaches in several benchmarks. They have also been found to be versatile and can benefit from more advanced machine learning techniques in the future. Overall, we believe that this thesis opens up a new direction to think about the expert domain classification problem, in general.
UR  - https://www.proquest.com/docview/2768687637?accountid=15181&bdid=109696&_bd=uOM%2FeOjZRpNnDCjBm%2F6WHrOOBJY%3D
ER  - 

TY  - Dissertations & Theses
T1  - Designing Time Bound Human-Robot Interfaces
AU  - Bermiss, Hassan E.
JF  - ProQuest Dissertations and Theses
Y1  - 2021-01-01
DA  - 2021
SN  - 979-8-209-99236-3
AB  - This exploratory case study examined psychological safety factors that can be used in robots’ human machine interfaces with a particular focus on the wellbeing of users who form intense or intimate but limited time relationships with robots. This study identified psychological safety parameters that could be used to develop programming guidelines for sexbots or other robots with intense human machine interfaces where attachments might develop, including military and healthcare applications. Twelve sex therapists were interviewed for their perspectives on a limited-period theoretical patient protocol that included the use of sexbots embedded with natural language processing and artificial intelligence technologies. Interviews focused on what robots should or should not say to patients in various phases of the sexual response cycle during a therapeutic encounter. The findings suggest patient screening, extensive human machine interface customization, therapist interface modification during the therapeutic period, and careful choices about optimizing algorithm goals should be considered when designing an intimate human machine interface. Participants identified risks such as dangerous learned behaviors, affectional attachments, unrealistic partner expectations, and trauma triggering as potential consequences of poorly managed robot verbal engines, some of which may apply to other robot applications. Practical implications include the need for utterance database relevance to patients and rhetorical off-ramps from sexual topics. The study identified potential extensions of attachment theory to artificial entities. Future research should consider ethical and data ownership, access, and protection issues for intimate human machine interfaces.
UR  - https://www.proquest.com/docview/2650005426?accountid=15181&bdid=109696&_bd=dP0c8ThAF5xhjB54sI8O2yHOSuo%3D
ER  - 

TY  - Dissertations & Theses
T1  - An Exploratory Study: Implications of Machine Learning and Artificial Intelligence in Risk Management
AU  - Scarpino, Joshua P.
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798357531148
AB  - Technology implementations should strive to be impartial, unbiased, and neutral when applied to risk management. Risk management is a critical component within every organization and will always be a crucial component in maintaining the balance between business goals and their impact on society. The aim of this study was to explore ideas around issues including biases, discrimination, privacy, risk thresholds, moral decision-making, and business benefits that challenge beliefs and foundational values when implementing Artificial Intelligence (AI) and Machine Learning (ML) technologies in risk management. A qualitative exploratory study used a phenomenological approach with a constructivist’s worldview to understand the lived experiences of the 15 participants; allowing insights into how to identify and prevent ethical and privacy concerns in the implementation of AI and ML within risk management. This study explored how AI and ML can have biases, promote discrimination, and present increasing privacy concerns. This research also assisted in understanding, exploring, and addressing the complex questions that bias, discrimination, and privacy can bring to any technological implementation. This research utilized participant interviews with industry experts to understand how these issues, personal beliefs, and values intersect with operational values and the implementation of AI and ML risk management. The research sought to explore how these critical components and concepts can help in identifying the implications associated with AI and ML applications in risk management. The research produced a taxonomy that organizations can focus on to ensure ethical AI and ML implementations across all industries. Furthermore, it highlighted critical areas where there is a lack of awareness within the AI and ML processes, presenting opportunities for organizations to begin conversations on how to address these fundamental issues. This research effort offers a framework to prioritize and address these fundamental issues by highlighting the gaps within AI and ML around education, oversight and accountability, risk evaluation, bias, and cultural considerations while ensuring that organizations are adequately considering these crucial areas. The results have revealed that it is critical for companies to truly ensure they understand the risk associated with AI and ML implementations. Additionally, organizations need to consider all options where alternatives could exist as this is critical to maintaining balance and equitable technology implementations. There is a lack of confidence across the industry on whether organizations truly understand the risk associated with the implementation of these technologies.
UR  - https://www.proquest.com/docview/2731478908?accountid=15181&bdid=109696&_bd=SUS8KC5Ssj8gzQ%2FfuitL%2FmXnsws%3D
ER  - 

TY  - Dissertations & Theses
T1  - U.S. AI Policy – A Balancing Act
AU  - Hetrick, Ryan T.
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381956931
AB  - Artificial intelligence policy is emerging as a critical component of U.S. strategy and strategies for countries around the world. What type of AI policy will allow the United States to continue to lead the world in AI innovation while doing it in an ethical and responsible manner? This work compares and contrasts 13 different countries and how each government approaches innovation, regulation, government funding, and law scope in the field of artificial intelligence. A significant portion of this analysis evaluates the tradeoffs that come with AI policies and their effects on society. Considering these tradeoffs, the U.S. needs to ensure that innovation in the field of artificial intelligence remains the top priority, while at the same time balancing the ethical deployment of AI to protect U.S. citizens. With China on the heels of the United States in terms of artificial intelligence capabilities, the United States needs to innovate more in the fields of foundation models, generative AI, human machine interaction, natural language processing (NLP), computer vision, and other emerging areas of artificial intelligence as well.This thesis takes an in-depth analysis of foundation models and generative artificial intelligence, while highlighting their importance and demonstrating their potential impact in the future. At the end of this body of work, there is a proposed Bill to U.S. lawmakers and Congress, titled “The Artificial Intelligence Startup, Innovation, Defense, Industry, and Academia Act (AI STIDIA Act)” that proposes a strategy for the United States to drive significant innovation in the field of artificial intelligence while deploying it in an ethical and responsible manner. The United States needs to prioritize ethical innovation in the field of artificial intelligence and cannot afford to emplace ineffective regulatory frameworks that curtails innovation. There will be a time when there is proper technology to extensively regulate artificial intelligence; however, there is not sufficient technology to extensively regulate AI as I publish this thesis. As the United States aims to generate the most innovative AI systems and create a culture that encourages the ethical deployment of AI, we should learn from past successes and failures when innovating technology. The United States needs to focus on creating AI technologies that enhances the wellbeing of U.S. citizens and people around the world.
UR  - https://www.proquest.com/docview/3031029945?accountid=15181&bdid=109696&_bd=8Nw8uCX0SFPG5MYQxRoANMho9r4%3D
ER  - 

TY  - Dissertations & Theses
T1  - Machine Learning and Inference Methods for Surrogate Modeling and Inexpensive Characterization of Elastodynamic Systems
AU  - Ogren, Alexander Charles
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798342110389
AB  - This thesis has two main focuses: (1) surrogate modeling of elastodynamic systems, and (2) inference methods for the inexpensive characterization of elastodynamic systems. Elastodynamics is the study of how and why materials move and deform when they are subject to time-varying loads, covering a wide range of applications from architected materials, to telecommunications, seismology, sound isolation, non-destructive evaluation, and medical imaging. Here, we explore how to more efficiently model elastodynamics, and what we can infer about our environment from observing them.The next generation of material engineering aims to leverage advanced multi-functional control over elastodynamic behaviors, but is currently limited by the large computational cost of purely physics-based modeling methods. Surrogate models aim to alleviate this cost by providing a data-driven approach to evaluate engineered material systems more efficiently. However, most current surrogate models lack certain useful traits, diminishing their potential for real-world use. This thesis begins by surveying the current state of surrogate modeling techniques, and establishes a set of state-of-the-art traits that greatly augment the utility of surrogate models, offering a perspective for the future direction of the field.Next, a data-driven surrogate model based on Gaussian process regression for the computation of dispersion relations is developed, GPR-dispersion. The model exhibits several of the aforementioned traits, including representation invariance, data efficiency, incorporating direct use of physical theories, and the provision of both uncertainty estimates on its predictions and gradients for compatibility with gradient-based design optimization methods. GPR-dispersion is evaluated in comparison against both deep learning and traditional physics-based models.The thesis then pivots to inference methods for the inexpensive characterization of material systems via partial observation of elastodynamic behaviors. Tissue stiffness is a tremendously important biomarker for a long list of health conditions, but often needs to be evaluated in a medical clinic with expensive equipment and highly trained workers. At-home health monitoring is a major next-generation goal of healthcare, but the trajectories of current consumer-grade sensor technology and biomarker inference methods have not yet fully intersected.Inspired by a related work (Visual Vibration Tomography), Visual Surface Wave Tomography(VSWT) is proposed. VSWT observes partial information about the surface waves of layered elastodynamic systems (such as biological tissue) through monocular video to infer subsurface constitutive and geometrical information. Simulated experiments are presented to evaluate the accuracy, sensitivity, and limits of the method under ideal conditions. Real-world experimental results are presented using phantom materials that emulate biological tissue to demonstrate a practical proof of concept.
UR  - https://www.proquest.com/docview/3122641300?accountid=15181&bdid=109696&_bd=zydRNL3QB4fE6Joa2Vc7%2FPQeANk%3D
DO  - https://doi.org/10.7907/dcbz-sj62
ER  - 

TY  - Dissertations & Theses
T1  - Mediating Trust and Influence in Human-Robot Teams via Multimodal Communication and Explanation for Mental Model Alignment
AU  - Tabrez, Aaquib
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798384051831
AB  - Effective communication is a foundational aspect of collaboration and teaming. Good communication enables and sustains the shared situational awareness necessary for adaptation and coordination during uncertain situations in human-robot teaming, and helps identify and remedy potential misunderstandings caused by mismatched expectations or behavior. Given the opaque nature of decision-making in  and robots, it is crucial that these agents can explain their decision-making rationale to both experts and novices for safe and trustworthy deployment in real-world applications. Furthermore, for autonomous agents to be effective and capable in human-robot teams, they should not only explain their decisions but also have the capacity to coach and convince their human collaborators.I argue that by leveraging multiple modalities of communication (such as visual and natural language), we can improve the safety and capability of human-robot teams, enabling appropriate trust, compliance, and reliance, especially in safety-critical, partially observable situations. Therefore, this doctoral thesis focuses on improving human-machine multimodal communication by employing explainable AI techniques to empower autonomous agents to: 1) communicate insights into their capabilities and limitations to human collaborators, 2) coach and influence human teammates’ behavior during joint task execution, and 3) successfully convince and mediate trust in human-robot interactions.
UR  - https://www.proquest.com/docview/3100484273?accountid=15181&bdid=109696&_bd=iFdx9m7M1aEX1MS2iUwYK1SRmOo%3D
ER  - 

TY  - Dissertations & Theses
T1  - The Predictive Relationship Between Big Data Technology Adoption and Employee Job Performance in Healthcare
AU  - Nnabuike, Gloria Chinonye
JF  - ProQuest Dissertations and Theses
Y1  - 2025-01-01
DA  - 2025
SN  - 9798314852163
AB  - The purpose of this quantitative correlational-predictive study is to determine if and to what extent perceived ease of use and perceived usefulness of Big Data technology adoption predict employee job performance for healthcare employees who use technology to manage big data in the United States. The study sampled 68 big-data technology healthcare professionals in the U.S., recruited via LinkedIn. It was guided by Ajzen’s Theory of Reasoned Action (TRA), Davis's Technology Acceptance Model (TAM), and Koopmans’s Individual-Work Performance (IWP) framework. Perceived ease of use (PEU) and perceived usefulness (PU) of Big Data technology adoption were measured using a revised TAM scale (PEU = .86, PU = .85). Employee job performance—task performance (TP), contextual performance (CP), and counterproductive work behavior (CWB)—was assessed using the IWPQ scale (TP = .78, CP = .86, CWB = .87). Multiple regression analysis examined the predictive relationships between PEU, PU, and job performance variables, with all eight assumptions met. Neither PEU nor PU, individually or combined, predicted TP, CP, or CWB (p > .05). The study emphasizes the need to address barriers to big data adoption, healthcare outcomes' impact on employee performance, and ethical complexities. It recommends future research expand beyond PEU and PU, integrating usability, security, and privacy into the framework and exploring broader impacts of big data technology adoption in healthcare contexts.
UR  - https://www.proquest.com/docview/3200068645?accountid=15181&bdid=109696&_bd=NMwYWXN01IKcEhLKbOng8lA1TC4%3D
ER  - 

TY  - Dissertations & Theses
T1  - User-Centered Deep Learning for Medical Image Analysis
AU  - Liang, Yuan
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798790654114
AB  - Medical imaging is a class of imaging technology to understand the human's body by non-invasively creating visual representations. Despite its ubiquitous use, diagnosis from medical imaging remains an uncertain and time-costly process for physicians. As such, there exists a call for automatic tools that can provide assistance to physicians in analyzing medical imaging data. Although current AI solutions can achieve promising diagnostic accuracy with deep learning in many applications, there has been a resistance to adopt AI-based diagnosis in clinics. We believe this is primarily due to the lack of designs that center the tools on the needs of physicians. In this dissertation, we explore user-centered deep learning for medical image analysis. In specific, we are interested in two research questions. First, how to use the concept of user-centered design to drive the development of deep learning (DL) algorithms? In specific, contrary to most existing computer aided diagnosis (CADx) systems, what are the ways of applying DL in clinical settings besides simply providing the predicted diagnostic results? This is because AI-based diagnosis can still raise ethics and safety concerns in the foreseeable future by considering the imperfectness of AI and the high stake of medical decision making. To answer this question, we perform formative studies to understand physician's needs, based on which we formulate novel deep learning tasks and provide pioneer solutions. Our works covers a wide range of medical domains of neural-radiology, dentistry, and forensics. Second, how to design the interactions between tools and their users so that they can be seamlessly integrated into users' workflow? To answer this question, we build interactive CADx systems with deep learning algorithms embedded, and perform comprehensive user studies to understand the designs. We experiment with a visualization tool for dentists to perform pre-surgical patient education, and a dental health monitoring tool for layman users. We conclude the dissertation by discussing the current major challenges for user-centered AI tools that we learnt from our studies.
UR  - https://www.proquest.com/docview/2638757200?accountid=15181&bdid=109696&_bd=rTKumu4TAyLTH6mqlnbkYjZE%2F5Q%3D
ER  - 

TY  - Dissertations & Theses
T1  - Explaining and Improving Deep Neural Networks Via Concept-Based Explanations
AU  - Wickramanayake, Sandareka Kumudu Kumari
JF  - PQDT - Global
Y1  - 2021-01-01
DA  - 2021
SN  - 9798352682401
AB  - Despite the outstanding performance, Deep Neural Networks (DNNs) remain black boxes to end-users. This opaque nature impedes user trust in DNNs and limits their adaption in the real world, especially in safety-critical domains such as healthcare. Therefore, elucidating the decisions made by DNNs in a way that is readily understandable to all users, including non-experts, is crucial to secure public trust. Concept-based explanations rationalize model decisions in terms of representations easily understandable to humans, such as super-pixels or word phrases. However, besides being easy to understand, the explanations must be descriptive and faithfully explain why a model makes its decisions. In this thesis, we explore two approaches to generate such explanations to rationalize the decisions of DNNs in computer vision, especially Convolutional Neural Networks (CNNs) and investigate using the insights gained via these explanations to improve the model accuracy. The first approach generates post-hoc linguistic explanations to rationalize a trained CNN’s decisions because linguistic explanations are easy to understand and descriptive. However, generating linguistic explanations that describe the features that truly contributed to the model decision is challenging. We propose Faithful Linguistic EXplanations (FLEX) framework to address this challenge. We derive a novel way to associate the features responsible for the decision with the words. We also propose a new decision-relevance metric that measures the faithfulness of a linguistic explanation to the model’s reasoning. Experiment results on two benchmark datasets demonstrate that the proposed framework can generate faithful explanations compared to state-of-the-art linguistic explanation methods.As the second approach, we explore developing a CNN with intrinsic interpretability. We introduce Comprehensible CNN (CCNN), which learns features consistent with human perception by learning the correspondence between visual features and word phrases. CCNN decisions are calculated as a linear combination of these features allowing CCNN to explain its decisions faithfully in word phrases. The proposed model employs an objective function that optimizes both the prediction accuracy and the semantics of the learned features. Experiment results demonstrate that CCNN can learn concepts consistent with human perception and their corresponding contributions to the model decision without compromising accuracy.Faithfully explaining DNNs allows us to use such explanations to understand possible shortcomings of the dataset used to train the model, particularly the under-represented regions in the dataset. In the third work, we propose BetteR Accuracy from Concept-based Explanation (BRACE) framework that utilizes concept-based explanations to automatically augment the dataset with new images that can cover these under-represented regions to improve the model performance. The framework is able to use the explanations generated by both interpretable classifiers and post-hoc explanations for black-box classifiers. Experiment results demonstrate that the proposed approach improves the accuracy of classifiers compared to state-of-the-art augmentation strategies. This thesis explores understanding DNNs via faithful human-understandable explanations and using insights gained via such explanations to improve classification accuracy. Improved understanding and performance will bolster public confidence in Artificial Intelligence (AI), leading to the increased adaption of AI systems in the real world.
UR  - https://www.proquest.com/docview/2723855716?accountid=15181&bdid=109696&_bd=2Vk7VJSQ87xBapPA8cmzHvSeMtU%3D
ER  - 

TY  - Dissertations & Theses
T1  - Trust, AI, and Synthetic Biometrics
AU  - Tinsley, Patrick
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798382726069
AB  - Artificial Intelligence-based image generation has recently seen remarkable advancements, largely driven by deep learning techniques, such as Generative Adversarial Networks (GANs). With the influx and development of generative models, so too have biometric re-identification models and presentation attack detection models seen a surge in discriminative performance. However, despite the impressive photo-realism of generated samples and the additive value to the data augmentation pipeline, the role and usage of machine learning models has received intense scrutiny and criticism, especially in the context of biometrics, often being labeled as untrustworthy. Problems that have garnered attention in modern machine learning include: humans’ and machines’ shared inability to verify the authenticity of (biometric) data, the inadvertent leaking of private biometric data through the image synthesis process, and racial bias in facial recognition algorithms. Given the arrival of these unwanted side effects, public trust has been shaken in the blind use and ubiquity of machine learning.However, in tandem with the advancement of generative AI, there are research efforts to re-establish trust in generative and discriminative machine learning models. Explainability methods based on aggregate model salience maps can elucidate the inner workings of a detection model, establishing trust in a post hoc manner. The CYBORG training strategy, originally proposed by Boyd et al. attempts to actively build trust into discriminative models by incorporating human salience into the training process. In doing so, CYBORG-trained machine learning models behave more similar to human annotators and generalize well to unseen types of synthetic data. Work in this dissertation also attempts to renew trust in generative models by training generative models on synthetic data in order to avoid identity leakage in models trained on authentic data. In this way, the privacy of individuals whose biometric data was seen during training is not compromised through the image synthesis procedure. Future development of privacy-aware image generation techniques will hopefully achieve the same degree of biometric utility in generative models with added guarantees of trustworthiness.
UR  - https://www.proquest.com/docview/3064730366?accountid=15181&bdid=109696&_bd=nPk5zOIaRZwYahXvRcLEgN7ctBU%3D
ER  - 

TY  - Dissertations & Theses
T1  - Using Qualitative Preferences to Guide Schedule Optimization
AU  - Wells, Tesla
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380097215
AB  - As robots and computers become more capable, we see more mixed human-robot-computer teams. One strategy for coordinating mixed teams is standardizing priorities across agents. However, the encoding of priorities may lead to discrepancies in interpretation across different types of agents. For example, a machine may have difficulty generating a plan that incorporates the subtleties of a natural language description. A human, given a purely quantitative encoding, may have difficulty intuiting trends or generating a plan without a computation aid. These discrepancies in priority interpretation may lead to a plan that only one class of agent deems optimal. In this case of distributed planning, differing interpretations may lead to significant differences in the plans produced by different classes of agents. What’s more, low fidelity interpretation of priorities can prohibit model correction or refinement. A robot may not recognize its learnt-physics-model conflicts with underlying assumptions in a natural language description of priorities. For humans, it is difficult to identify incorrect or unrefined priority models from the same streams of numerical relations computers find useful.Most strategies currently in use for bridging this gap involve machines learning human preferences from large data sets or require labor-intensive custom utility encodings to be written, explained, and revised by a trained expert. The former often homogenizes models of human preferences and fails to incorporate corrections accurately, efficiently, and in context. The latter prohibits the usage of robots or computer aids in casual or dynamic settings without the presence/supervision of a human trained in working with the system. In both settings, this inhibits average humans from having personalized, human-computer or human-robot interactions.In this thesis, we attempt to improve human-robot interactions by encoding utility as a series of qualitative, Ceteris Paribus preference statements over a design space. We posit this formalism is both readily understood by human agents and easily reasoned over by machines. Previous work establishes the ability to compute machine readable utility functions from said statements by efficiently generating topological orderings. We detail our implementation of a machine “agent” who uses said utility function to compute optimal schedules to Conditional Temporal Problem problems for a mixed-agent team. We then build off of the procedure for generating utility functions to generate admissible heuristics that increase performance.We show this encoding enables us to explain which human preferences differentiate feasibly scheduled, high-utility plans. We present a suit of algorithms capable of explaining model behavior according to five different relevance standards. Additionally, we construct and algorithm for identifying where additional preference specification would resolve assumptions used in underspecified areas of the model. These explanations not only improve human understanding, but also facilitate identification of inaccuracies in the machine’s utility model. We build off our explanations to presents options for model-repair. We show preference-addition and preference-relaxation informed by explanation results in specific, targeted plan changes.
UR  - https://www.proquest.com/docview/2848818901?accountid=15181&bdid=109696&_bd=xEd2iGERsvYjLWpXLUShhw%2FvHBQ%3D
ER  - 

TY  - Dissertations & Theses
T1  - Useful Interpretability for Real-World Machine Learning
AU  - Singh, Chandan
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798351476193
AB  - The recent surge in highly successful, but opaque, machine-learning models has given rise to a dire need for interpretability. This work addresses the problem of interpretability with novel definitions, methodology, and scientific investigations, ensuring that interpretations are useful by grounding them in the context of real-world problems and audiences. We begin by defining what we mean by interpretability and some desiderata surrounding it, emphasizing the underappreciated role of context. We then dive into novel methods for interpreting/improving neural network models, focusing on how to best score, use, and distill interactions. Next, we turn from neural networks to relatively simple rule-based models, where we investigate how to improve predictive performance while maintaining an extremely concise model. Finally, we conclude with work on open-source software and data for facilitating interpretable data science. In each case, we dive into a specific context which motivates the proposed methodology, ranging from cosmology to cell biology to medicine. Code for everything is available at github.com/csinva.
UR  - https://www.proquest.com/docview/2719022047?accountid=15181&bdid=109696&_bd=nIAQ6RjOYCrvJ3ofCYrnkY3J0pU%3D
ER  - 

TY  - Dissertations & Theses
T1  - Transparent Representation Learning for Graphs and Human-AI Collaboration
AU  - Kosan, Mert
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380158275
AB  - Graph data show relationships between entities in a variety of domains including but not limited to communication, social, and interaction networks. Representation learning makes graph data easier to use for graph tasks such as graph classification, link prediction, and clustering. The decisions on graphs depend on complex patterns combining rich structural and attribute data. Therefore, explaining these decisions made by representation learning models for high-stakes applications (e.g., anomaly detection and drug discovery) is critical for increasing transparency and guiding improvements. Moreover, human expertise can guide machine learning decisions, raising questions about the interactions between humans and artificial intelligence that require further analysis.This dissertation focuses on our research on two key topics: transparent representation learning on graphs and human-AI collaboration. Firstly, we present our proposed frameworks for graph anomaly detection, which have been developed to enhance accuracy and transparency. Subsequently, we scrutinize explainability in graph machine learning, where we discuss our novel post-hoc global counterfactual and robust ante-hoc graph explainers. Fairness is also a crucial aspect of transparent machine learning, and we propose a new individual fairness method for clustering. Finally, we investigate the impact of collaboration between humans and artificial intelligence on decision-making under risk and feedback loop systems.
UR  - https://www.proquest.com/docview/2858597521?accountid=15181&bdid=109696&_bd=DKVE9zIHmXofUryp4x9XCu76yD4%3D
ER  - 

TY  - Dissertations & Theses
T1  - Quantitative Correlation Analysis of Trust Issues in AI Systems
AU  - Singasani, Tejesh Reddy
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798346760924
AB  - As artificial intelligence (AI) systems become increasingly integrated into our daily lives, understanding the factors that influence trust in these systems has gained paramount importance. This dissertation explored trust issues related to AI systems through a quantitative correlational analysis, leveraging the technology acceptance model (TAM) and the extended privacy calculus model (EPCM). The study investigated the relationships between perceived privacy concerns, user attitudes, trust, and intentions to use AI systems. A diverse sample of AI device users was surveyed to gather insights into their perceptions. The research addressed five key questions: (a) the correlation between privacy concerns and trust; (b) privacy concerns and attitudes; (c) trust and attitudes; (d) trust and intentions to use; and (e) attitudes and intentions to use AI devices. Findings revealed significant correlations between perceived privacy concerns and trust, and between privacy concerns and user attitudes. Additionally, user attitudes significantly correlated with both trust and intentions to use AI devices, while trust influenced user intentions to use them. These results underscored the need for AI developers, policymakers, and organizations to prioritize privacy and user attitudes to foster trust in AI systems. This study contributes to the field of AI ethics and user behavior by providing valuable insights into trust issues, highlighting the importance of addressing privacy concerns and enhancing user trust to promote wider acceptance and use of AI systems. The findings have significant implications for the design, deployment, and regulation of AI technologies in contemporary society.
UR  - https://www.proquest.com/docview/3142708818?accountid=15181&bdid=109696&_bd=AdZtn%2B69RIWyfK49AxbUOwlehTM%3D
ER  - 

TY  - Dissertations & Theses
T1  - Quantitative Correlational Analysis of Factors Affecting Privacy Perceptions Towards AI Devices
AU  - Borra, Chandrakanth Reddy
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798342711999
AB  - This study explores the relationship between privacy concerns and adopting artificial intelligence (AI) devices, emphasizing the influence of user skills and technological factors on privacy perceptions. AI devices, which rely heavily on collecting and processing large amounts of data, often raise significant privacy concerns, potentially hindering their adoption. The study aims to examine how these privacy concerns affect users' trust in AI systems and ultimately influence their decision to adopt or reject such technologies. A quantitative correlational research design was employed, with data collected from AI users in the United States. The study examines critical variables, including the legal frameworks governing AI data usage, technological literacy, and user experience with AI devices. These variables were measured through surveys to understand better the extent of privacy concerns and their impact on users’ willingness to adopt AI technologies. Statistical results demonstrated a significant positive correlation between privacy concerns and AI device acceptance, highlighting privacy as a critical determinant in adoption. Users with higher privacy concerns were less likely to adopt AI devices, reinforcing the need for robust privacy measures. The findings of this study provide actionable insights for AI developers and policymakers, suggesting that addressing privacy issues through improved security protocols and legal frameworks could enhance user trust and increase the adoption of AI technologies. This research contributes to the growing body of knowledge on AI privacy issues and offers a foundation for future studies to promote user trust and acceptance of AI devices.Keywords: AI devices, Privacy Perception, Acceptance of AI devices.
UR  - https://www.proquest.com/docview/3121565788?accountid=15181&bdid=109696&_bd=an38h1U0GTXTxlKniFjDIgHHy9s%3D
ER  - 

TY  - Dissertations & Theses
T1  - Orthopedic Surgeons and Generative AI: Navigating the Perception– Adoption Gap Amidst Rapidly Advancing Digital Technologies in Healthcare
AU  - Schmidt, Rainer Matthias
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798384025078
AB  - The rapid advancement of generative artificial intelligence (AI) technologies, particularly large language models (LLMs), is poised to revolutionize the healthcare sector. This dissertation explores the complex landscape of generative AI adoption in orthopedic surgery, focusing on the perceptions and intentions of orthopedic surgeons, who play a pivotal role in determining the success and feasibility of AI integration in high-stakes environments. This study investigated factors influencing surgeons ' adoption of generative AI tools in their practice through a mixed methods approach grounded in the unified theory of acceptance and use of technology (UTAUT) and social cognitive theory. The research employs a quantitative survey of 177 orthopedic surgeons complemented by qualitative follow-up interviews to understand their familiarity with generative AI, the intrinsic and extrinsic factors shaping their views, and their beliefs about AI's role in augmenting or replacing human expertise. The findings reveal that, whereas orthopedic surgeons hold positive attitudes about the transformative potential of generative AI, their behavioral intentions and self-reported actual use remain in the early stages. Performance expectancy and facilitating conditions emerge as key determinants of adoption, underscoring the importance of perceived benefits and organizational support. Interestingly, the appeal of generative AI may differ based on the surgeon's career stage, with expert, later-career surgeons being drawn to its potential for streamlining their work and making their jobs easier. However, the study also highlights significant barriers to adoption, including systemic and institutional challenges, technological limitations, and concerns surrounding patient trust and ethics. The qualitative insights provide an indicative but nuanced understanding of surgeons' experiences and decision-making processes, revealing a cautious optimism tempered by the realities of integrating early-stage technologies into clinical practice.This dissertation contributes to the growing body of literature on AI adoption in healthcare, offering valuable insights for orthopedic surgeons navigating the integration of generative AI in their practice. The findings reveal that while surgeons hold positive attitudes about AI's potential, actual adoption remains in the early stages, with higher usage in clinical research and professional development than in direct patient care. Performance expectancy and facilitating conditions emerge as key determinants of adoption, underscoring the importance of perceived benefits and organizational support. Notably, the appeal of generative AI may differ based on career stage, with experienced surgeons viewing it as a means to streamline workflows. The study emphasizes the need for a collaborative approach to address challenges such as workflow integration, trust-building, and ethical concerns, ensuring that generative AI enhances rather than replaces the irreplaceable human touch in healthcare. By providing a strategic roadmap for navigating the early stages of AI adoption, this research paves the way for a future wherein surgeons and machines work together to improve surgical outcomes and transform patient care.
UR  - https://www.proquest.com/docview/3098123281?accountid=15181&bdid=109696&_bd=nNVUlM5WXzcqw5ZpmQ7lqQbvm10%3D
ER  - 

TY  - Dissertations & Theses
T1  - Context and Participation in Machine Learning
AU  - Suresh, Harini
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380096447
AB  - ML systems are shaped by human choices and norms, from problem conceptualization to deployment.  They are then used in complex socio-technical contexts, where they interact with and affect diverse populations.  However, development decisions are often made in isolation, without deeply taking into account the deployment context in which the system will be used.  And they are typically hidden to users in that context, who have few avenues to understand if or how they should use the system.  As a result, there are numerous examples of ML systems that in practice are harmful, poorly understood, or misused.  We propose an alternate approach to the development and deployment of ML systems that is focused on incorporating the participation of the people who use and are affected by the system.  We first develop two frameworks that lend clarity to the human choices that shape ML systems and the broad populations that these systems affect. These inform a prospective question: how can we shape new systems from the start to reflect context-specific needs and benefit justice and equity?  We address this question through an in-depth case study of co-designing ML tools to support activists who monitor gender-related violence.  Drawing from intersectional feminist theory and participatory design, we develop methods for data collection, annotation, modeling, and evaluation that prioritize sustainable partnerships and challenge power inequalities.  Then, we consider an alternative paradigm where we do not have full control over the development lifecycle, e.g., where a model has already been built and made available. In these cases, we show how deployment tools can give downstream stakeholders the information and agency to understand and hold ML systems accountable. We describe the design of two novel deployment tools that provide intuitive, useful, and context-relevant insight into model strengths and limitations. The first uses example-based visualizations and an interactive input editor to help users assess the reliability of individual model predictions.  The second, Kaleidoscope, enables context-specific evaluation, allowing downstream users to translate their implicit knowledge of "good model behavior'' for their context into explicitly-defined, semantically-meaningful tests.This dissertation demonstrates several ways that context-specific considerations and meaningful participation can shape the development and use of ML systems.  We hope that this is a step towards the broader goal of building ML-based systems that are grounded in societal context, are shaped by diverse viewpoints, and contribute to justice and equity.
UR  - https://www.proquest.com/docview/2848464755?accountid=15181&bdid=109696&_bd=cvN38ov%2B%2B7ISXcI%2BNYXi%2Bx7PMXI%3D
ER  - 

TY  - Dissertations & Theses
T1  - Learning to Calibrate Trust Through Explainability
AU  - Chesser, Amber F.
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380859462
AB  - Automation has been implemented in a range of machinery. Providing supplementary information about system processes (i.e., explainability) could mitigate over-reliance and enhance operator awareness of potential anomalies. Trust plays a critical role in humanautomation collaboration, as over-trust can lead to misuse or over-reliance, while under-trust can result in disuse or the failure to engage automation when it could enhance performance. Dynamic learned trust and situational trust fluctuate during an operator's interaction with a system and can be influenced by the rate of system failures or workload respectively. Design features like explainability can impact perceived usefulness and help users identify system errors and competencies. This study investigates the impact of explainability on user performance in a quality control task. Participants were randomly assigned to either receive training on system failures or not, with varying quality control inspection quotas to simulate various taskloads. The study used a mixed design and measured participants' use of system recommendations and accuracy over time. The results revealed that explainability enhanced accuracy in moderate to lower-quota environments, but this effect was contingent on participants’ receiving training. Explainability also increased reliance levels in lower-quota situations; however, as workload intensified, the time it took for users to determine suitable reliance diminished. While using explainability to assist users in fine-tuning reliance strategies and enhancing accuracy is advisable, it should not serve as a substitute for training, particularly for individuals in high workload environments.
UR  - https://www.proquest.com/docview/2894671056?accountid=15181&bdid=109696&_bd=yEC0hrRNLZwuTiCKyccX%2BJKi6rw%3D
ER  - 

TY  - Dissertations & Theses
T1  - Values by Design Imaginaries: Exploring Values Work in UX Practice
AU  - Wong, Richmond Yuet-Ming
JF  - ProQuest Dissertations and Theses
Y1  - 2020-01-01
DA  - 2020
SN  - 9798381708677
AB  - Recognizing the prevalence of initiatives to align technology with social values through design and “by design” (such as privacy by design, security by design, and governance by design), this dissertation explores the current and potential role of design techniques in attending to values, and analyzes user experience (UX) professionals’ “values work” practices—practices used to surface, advocate for, and attend to values—within large technology companies.The first part of the dissertation interrogates the relationship between values and design practices, looking at privacy as a case study. A review of human computer interaction literature about privacy and design suggests the importance of thinking about the purpose of design, who does the work of design, and on whose behalf is design work done. In order to better understand how design in the service of “values work” could be used towards purposes of exploration, critique and speculation, I create a set of speculative design fictions depicting a range of fictional products that suggest different sets of privacy harms. These designs serve as way to surface and foster reflection on values. The success of this design intervention in a laboratory setting sparked interest in understanding whether and how design approaches were used in values work within the technology industry.The second part of the dissertation seeks to understand the practices and strategies of UX professionals who already see addressing values as a part of their practice. I conducted interviews with UX professionals working at large technology companies, and field observations at meetups in the San Francisco Bay Area about technology design and values. These UX professionals report doing values work as a part of everyday configurations of UX work, such as when designing interfaces or conducting user research. More strikingly, UX professionals also report on engaging in a range of other activities aimed at shaping the organization, rather than a technical product or system. These practices are used by UX professionals to re-configure how values work is conducted at their organizations in several ways: by making more space for UX professionals’ values work; by getting others in the organization to adopt human-centered perspectives on values; and by changing the politics and strategies of the organization regarding values. Moreover, UX professionals’ values work practices occur within relations and systems of power. UX professionals often engage in tactics of soft resistance, seeking to subtly subvert existing practices towards more values-conscious ends while maintaining legibility as conducting business-as-usual within the organization. Together, these values work practices create social and organizational infrastructures to promote an alternative sociotechnical imaginary of large technology companies in a way that views these companies and their workers as more cognizant, proactive, and responsible for identifying and addressing social values, in particular reducing harms to users and other stakeholders.The last part of the dissertation reflects on the politics of using speculative design techniques in the service of values work. Experiences sharing speculative designs with others who interpreted the designs in ways that do not recognize their speculative, critical, and reflective nature, raises questions about how speculative design can be re-appropriated by or co-opted towards the very ends that are being critiqued and reflected upon. One approach to this dilemma might be to conduct speculative design work with and for specific groups of stakeholders, instead of for broad public discussion. Another approach might be to create organizational fictions that focus a designer’s and viewer’s attention more on practices and social relationships, compared to traditional speculative designs that focus attention on fictional products. Informed by the practices of UX professionals involved in values advocacy, the dissertation concludes by suggesting a new purpose for design, design for infrastructuring imaginaries, to complement the social practices of values advocacy. I reflect on the politics of choosing design as a mode of action when conducting values work, and reflect on implications that this work has for values in design researchers, practitioners, and stakeholders.
UR  - https://www.proquest.com/docview/2928157433?accountid=15181&bdid=109696&_bd=O6WKGZ77%2FGuu60TEYKVfjm6F9Rc%3D
ER  - 

TY  - Dissertations & Theses
T1  - Data-Driven Advances in Materials Discovery: From Crystal Structure Prediction to Synthesis Design and Property Analysis of BiFeO3 Thin Films
AU  - Baibakova, Viktoriia
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798310103511
AB  - Data-driven and data-centric approaches are revolutionizing materials discovery, significantly accelerating progress in the field of materials science. With the integration of artificial intelligence (AI) and machine learning (ML), these methods offer unprecedented speed and cost-efficiency compared to traditional techniques. Initiatives such as the U.S. Materials Genome Initiative utilize vast computational datasets to predict material properties and design new materials.A key element of this transformation lies in the development of robust data infrastructures. AI techniques, including generative models and large language models (LLMs), provide tools for addressing fundamental challenges, such as gathering data in various formats for materials discovery, exploration of structure-property relationships, and material behavior predictions. This paradigm shift is fostering interdisciplinary collaboration, where materials science and data science converge, setting the stage for breakthroughs in industries like electronics and energy storage.This thesis outlines a comprehensive three-step framework for data-driven materials discovery, blending computational and experimental methods. First, it presents a strategy for designing synthesis protocols to target specific crystal structures, supported by computational methods. Second, it explores the computational generation of solid crystal structures using advanced AI-based generative models. Finally, it integrates experimental data through machine learning-driven analysis of experimental characterization techniques, such as optical emissivity studies.The first step involves designing synthesis experiments for target materials. Using text mining and ab initio calculations, this research uncovers synthesis pathways for BiFeO3 (R3c) produced via the sol-gel method. By analyzing 340 experimental synthesis recipes, it identifies common design features and applies a chemical reaction network to predict preferential pathways. This combination of computational and data-mining approaches reveals precursor design rules critical to material synthesis.In the second step, generative AI techniques are applied to enhance computational workflows. A large language model (LLM) is trained to generate crystal structures from textual descriptions, assessing its ability to predict and innovate crystal sites based on input prompts. This method explores the feasibility of AI-generated crystal structure prediction, offering insights into the potential of AI for this task.Finally, the third step focuses on experimental characterization through text mining and image analysis, examining the correlation between thin-film material designs and their optical emissivity profiles. Machine learning techniques are used to analyze how macroscopic designs affect material properties, offering a new perspective on structure-property relationships in materials science.This thesis concludes by summarizing the findings of each approach, emphasizing the potential impact of AI and data-driven methods in accelerating materials discovery. By merging computational insights with experimental data, this work highlights the promise of interdisciplinary approaches in addressing critical challenges.The broader implications of this research align with global sustainability goals. Materials science, historically focused on fields such as energy storage, solar cells, catalysts, and water purification, now plays a pivotal role in combating climate change. With the urgency of decarbonization and the need to mitigate CO2 emissions—where industrial sector remains a significant contributor—materials innovation is crucial. AI-driven advancements offer the potential to revolutionize areas like electric vehicle production, battery efficiency, and the reduction of carbon footprints, driving a sustainable future.
UR  - https://www.proquest.com/docview/3175897397?accountid=15181&bdid=109696&_bd=wV3SPSaQVg%2FJR9jWP4QFQdRg%2Fwg%3D
ER  - 

TY  - Dissertations & Theses
T1  - An Exploratory Study of the Impacts of Artificial Intelligence and Machine Learning Technologies in the Supply Chain and Operations Field
AU  - Kalasani, Rohith Reddy
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380590068
AB  - This exploratory qualitative study investigates the potential impacts of Artificial Intelligence (AI) and Machine Learning (ML) technologies on an organization's supply chain and operations. The study examined the willingness of organizations to adopt these technologies and explored the knowledge gap between AI and ML in the supply chain industry. The study also analyzed the perspectives of employees and employers on how AI and ML can improve supply chain activities by analyzing data and identifying areas for improvement. Furthermore, the study examined how AI and ML can help predict demand, manage suppliers, reduce inventory and operating costs, improve customer service, and increase efficiency and productivity. The study provided evidence and added to the literature that AI and ML-enabled machines can make intelligent decisions and reduce errors, leading to optimal supply chain performance. Ultimately, this study aimed to provide supply chain professionals with insights into the potential benefits of AI and ML in supply chain and operations management.
UR  - https://www.proquest.com/docview/2878182412?accountid=15181&bdid=109696&_bd=fB5w2wVJDaCV60xue75w%2B5tNfik%3D
ER  - 

TY  - Dissertations & Theses
T1  - Deep Representation Learning for Biomedical Text Mining
AU  - Ebadi, Nima
JF  - ProQuest Dissertations and Theses
Y1  - 2021-01-01
DA  - 2021
SN  - 9798759960980
AB  - The rapidly evolving curation of biomedical publications has resulted in an information crisis, such that researchers and other individuals working in biomedical domain require specialized text mining tools to keep track of the ever-evolving literature landscape. With the advances in natural language processing (NLP) models, especially those that are based on deep representation learning, gaining valuable insights from large-scale biomedical corpora have become extremely popular among researchers. Inspired by the state-of-the-art models proposed in general NLP settings, many studies have tried to address the common biomedical natural language processing (BioNLP) tasks, such as named-entity recognition NER, information/relation extraction (IE/RE), and question answering. However, directly applying the advancements of general NLP field to biomedical text mining often yields unsatisfactory results due to scarcity of labeled biomedical documents, ambiguous relations of biomedical concepts–with other domains or with general English words–continuous distribution shift of word semantics, multiple synonyms across different domains, and reliability challenges of deep learning models in general. In this dissertation, we aim to address these challenges using deep representation learning algorithms customized for one or multiple common tasks in BioNLP domain. We study the under-investigate aspects of BioNLP problems such as interpretability and reliability in addition to challenges of scale, scarcity of labeled data, and changes of word distributions. This dissertation approach deep representation learning as a tool to address the requirements of the biomedical research community, rather than models that achieve competitive "accuracies". Furthermore, as the aforementioned challenges become more severe in the context of a pandemic, we constantly provide case studies of the current pandemic and provide exclusive solutions from which the literature could substantially benefit if such an emergency situation recur in the future (God forbid!).
UR  - https://www.proquest.com/docview/2615122795?accountid=15181&bdid=109696&_bd=rqIT5hgGD84%2BEzyD%2FtYP%2BY0JEsM%3D
ER  - 

TY  - Dissertations & Theses
T1  - Artificial Intelligence vs. Human Coaches: A Mixed Methods Randomized Controlled Experiment on Client Experiences and Outcomes
AU  - Barger, Amber
JF  - ProQuest Dissertations and Theses
Y1  - 2024-01-01
DA  - 2024
SN  - 9798382612850
AB  - The rise of artificial intelligence (AI) challenges us to explore whether human-to-human relationships can extend to AI, potentially reshaping the future of coaching. The purpose of this study was to examine client perceptions of being coached by a simulated AI coach, who was embodied as a vocally conversational live-motion avatar, compared to client perceptions of a human coach. It explored if and how client ratings of coaching process measures and outcome measures aligned between the two coach treatments. In this mixed methods randomized controlled trial (RCT), 81 graduate students enrolled in the study and identified a personally relevant goal to pursue. The study deployed an alternative-treatments between-subjects design, with one-third of participants receiving coaching from simulated AI coaches, another third engaging with seasoned human coaches, and the rest forming the control group. Both treatment groups had one 60-minute session guided by the CLEAR (contract, listen, explore, action, review) coaching model to support each person to gain clarity about their goal and identify specific behaviors that could help each make progress towards their goal. Quantitative data were captured through three surveys and qualitative input was captured through open-ended survey questions and 27 debrief interviews. The study utilized a Wizard of Oz technique from human-computer interaction research, ingeniously designed to sidestep the rapid obsolescence of technology by simulating an advanced AI coaching experience where participants unknowingly interacted with professional human coaches, enabling the assessment of responses to AI coaching in the absence of fully developed autonomous AI systems. The aim was to glean insights into client reactions to a future, fully autonomous AI with the expert capabilities of a human coach. Contrary to expectations from previous literature, participants did not rate professional human coaches higher than simulated AI coaches in terms of working alliance, session value, or outcomes, which included self-rated competence and goal achievement. In fact, both coached groups made significant progress compared to the control group, with participants convincingly engaging with their respective coaches, as confirmed by a novel believability index.The findings challenge prevailing assumptions about human uniqueness in relation to technology. The rapid advancement of AI suggests a revolutionary shift in coaching, where AI could take on a central and surprisingly effective role, redefining what we thought only human coaches could do and reshaping their role in the age of AI.
UR  - https://www.proquest.com/docview/3055851674?accountid=15181&bdid=109696&_bd=DkgP61SNhfUCgbZzeiUL%2FbqJDZ8%3D
ER  - 

TY  - Dissertations & Theses
T1  - An End-To-End Cloud-Based Solution for Optimal Attention Network Topology in Real-Time Applications
AU  - Sarraf, Saman
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798819323274
AB  - This study outlines the design and implementation of a cloud-based solution for real-time computer vision applications using an attention-based model known as a vision transformer. This study addresses the optimization of the solution by exploring optimal cloud components and vision transformer models to maintain or improve modeling performance while reducing the inference latency. First, the cloud components are investigated to explore the possibility of optimizing such components and selecting an optimal cloud architecture. The investigation reveals that the access to the core of cloud services is restricted; therefore, a widely-used architecture implemented in the Amazon Web Services environment for real-time computer vision use cases is selected. This finding leads the study to emphasize the effect of optimization for the vision transformer model. This study plans the vision transformer optimization by varying the input dimension and number of attention heads in the model to produce the same performance with confidence intervals of ±3% compared to an attention-based baseline known as ViT_224_16, ±5% compared to ResNet18 – the state-of-the-art baseline model. Furthermore, the optimized vision transform must meet the criteria of reducing the inference latency by at least +5% compared to the vision transformer baseline. UCF101 is a broadly used action recognition dataset including more than one million video clips selected for model development in this study. The modeling pipeline consists of a preprocessing module to extract frames from video clips, training vision transformer models, and deep learning baselines. Also, the pipeline includes a postprocessing step to aggregate the frame-level predictions to generate the video-level performance through a full classification report. The results against the testing dataset indicate that an optimal vision transformer model with an input dimension of 56×56×3 with eight attention heads produces an F1-score of 91.497%, where the performance of original attention-based and ResNet18 as the baselines are 94.152% and 95.749%, respectively. Also, the results reveal that the optimized vision transformer reduces the inference latency by 40.07%, with  55.63% faster training time than the original vision transformer considered as the baseline. This study shows that the vision transformer model is highly optimizable for inference latency while maintaining the model performance.
UR  - https://www.proquest.com/docview/2672315501?accountid=15181&bdid=109696&_bd=gVlyutbmT64r3EkWl0R500vyJcU%3D
ER  - 

TY  - Dissertations & Theses
T1  - Extensions and Applications of Deep Probabilistic Inference for Generative Models
AU  - Wu, Mike Hongfei
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798352600313
AB  - Despite the growth of data size, many applications for which we would like to apply learning algorithms to are limited by data quantity and quality. Generative models propose a framework to naturally combine prior beliefs with real world data. Core to the generative approach is the challenge of probabilistic inference, or estimating latent variables given observations. This challenge has led to a rich field of research spanning many statistical techniques. More recently, deep learning methods have been used to solve inference queries, aptly named deep inference. In my dissertation I will explore extensions to deep inference in response to real world challenges of sparsity and efficiency. I will present case studies of practical applications where deep inference achieves considerable improvements upon prior work.This dissertation is centered around three parts. We present the background for generative models and deep inference with an emphasis on modern variational methods. The first part will present new algorithms for generalizing inference to be robust to different notions of sparsity, such as multimodal data, missing data, or computational constraints. Second, we study meta-amortized inference, or "inferring how to infer". A doubly-amortized inference algorithm would be cheaply able to solve inference queries for a novel generative model. We will show a new algorithm to re-purpose masked language modeling to do just this.Third, we present two real-world applications of deep inference in education: (a) estimating student abilities under item response theory and related psychometric models, and (b) inferring educational feedback for students learning to solve programming questions. Together, these contributions showcase the richness and utility of deep inference in education, and more broadly in real world contexts.
UR  - https://www.proquest.com/docview/2723858207?accountid=15181&bdid=109696&_bd=cLrWfNlza8EyLnmY5w5EkfeQvkw%3D
ER  - 

TY  - Dissertations & Theses
T1  - Marionette: A Neurorobotics Toolchain for the Neuromorphic Control of Tendon-Driven Systems
AU  - Chakravarthi Raja, Suraj
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380363877
AB  - Most mammals begin to carry out complex movements shortly after birth, quickly improving their performance with limited experience. They do this while contending with the complexities of both their changing bodies and the physics of the world. This far exceeds the capabilities of even advanced robots which use deep learning techniques.Current data-driven robotic controllers have traditionally relied on large datasets to train neural networks from scratch and adjust thousands of parameters. This necessitates hundreds of hours of training on powerful GPU-based computers to perform even simple physical tasks in simulation. By contrast, mammals inherit prescribed neural circuits. This reduces the dimensionality of the exploration space, enabling fast learning of subscribed parameters for sensorimotor function from limited experience. This allows mammals to perform their actions adequately enough to survive in the real world. At the same time, tendon-driven robots could act as experimental platforms to test neuroscientific and biomechanical hypotheses that would otherwise be unfeasible to test due to biological limitations or ethical considerations.This dissertation begins to address this gap by bringing together robotics and sensorimotor neuroscience. Robotics can greatly benefit from the improved dexterity and faster lifelong learning afforded by our understanding of mammalian neuromechanics. Likewise, integrative systems modeling can enable us to test behavioral neuroscientific hypotheses, which are unable to be validated using reductionist techniques.The objective of this dissertation is to develop a standardized and modular collection of tools that enable interoperability of neuromechanical architecture of movement as a neuromorphic system that combines with state-of-the-art tendon-driven robotic systems.To this end, I first used an existing brittle implementation of the monosynaptic stretch reflex pathway accelerated on banks of Field-programmable Gate Arrays (FPGAs) and integrated it with my novel neuromechanical testbed for the actuation of tendons for robotic and human cadaveric specimens. This setup was then used to replicate human-like stretch reflexes, signal-dependent noise and even modulate voluntary movement in our specimens. This is, to my knowledge, the first physical robotic platform for the study of sensorimotor mechanisms in real life. Next, I extended the neuromechanical testbed to test and validate a passive implantable mechanism to restore grasp after tendon-transfer surgery. This was crucial to validating the broader application of my testbed in multiple neurorobotic and biomechanical studies. Additionally, this project allowed me to begin developing the modularity needed to interface with other tools and robotic systems. Third and finally, from my learnings while conducting these neuromechanical studies, I have developed a hardware architecture to accelerate neuromorphic systems which are modeled as differential equations. This FPGA-based edge-accelerator framework leverages High-Level Synthesis (HLS) and standardized I/O harnesses to stream data with sub-millisecond latency for real-time robotic control. This framework is packaged in a modular toolchain for tendon-driven robotics that can combine these accelerated implementations of neuromorphic models together with my modular real-time control, logging and monitoring tools. With a gentle learning curve, this C++ toolchain with a simple set of commands/functions remains accessible to neuroscientists with limited engineering experience. At the same time, its modular design enables it to integrate with modern robotic control, simulation and machine learning software.
UR  - https://www.proquest.com/docview/2866237863?accountid=15181&bdid=109696&_bd=KcoXy61uqj6%2F%2FBxKE26ZMwwSQ9w%3D
ER  - 

TY  - Dissertations & Theses
T1  - Sonic Mindfulness: A Qualitative Study of Sense of Agency and an Improvisational State of Mind in Free Form Musical Improvisation
AU  - Sol, William Ashley
JF  - ProQuest Dissertations and Theses
Y1  - 2021-01-01
DA  - 2021
SN  - 9798505540299
AB  - Study of musical improvisation has proven a productive avenue for exploring attributes of adapting dynamically in the moment. Research has focused on neuro-correlates and the experience of decision-making, with a recent argument by Dolan et al. (2018) for an improvisational state of mind. This qualitative study explored the question: How do musicians experience state of consciousness and sense of agency while freely improvising? Twenty-four experts in free form music performed a short free improvisation that was the subject of a semi-structured interview. Participants reported low sense of agency, minimal conscious thought, and decisions experienced as body-doing or subtle urges being acted upon through intuition, as well as moments of increased agency as the need arose to troubleshoot or steer toward a desired esthetic. Significant reports of focused, present-moment awareness suggest parallels between improvisation and techniques that utilize variations of mindfulness. Thematic analysis illuminated the experience of improvisational decision-making, complementing the neurocognitive perspective, while spotlighting rarely studied transpersonal dimensions of improvisation (e.g., reports that improvising music was occasionally associated with a trance state).  
UR  - https://www.proquest.com/docview/2544027757?accountid=15181&bdid=109696&_bd=A8%2BouOr2KlVTauqDp9pSZaTN93c%3D
ER  - 

TY  - Dissertations & Theses
T1  - Detecting Fine-Grained Semantic Divergences to Improve Translation Understanding Across Languages
AU  - Briakou, Eleftheria
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798380580946
AB  - One of the core goals of Natural Language Processing (NLP) is to develop computational representations and methods to compare and contrast text meaning across languages. Such methods are essential to many NLP tasks, such as question answering and information retrieval. One of the limitations of those methods is the lack of sensitivity to detecting fine-grained semantic divergences, i.e., fine-meaning differences in sentences that overlap in content. Yet, such differences abound even in parallel texts, i.e., texts in two different languages that are typically perceived as exact translations of each other. Detecting such fine-grained semantic divergences across languages matters for machine translation systems, as they yield challenging training samples and for humans, who can benefit from a nuanced understanding of the source.In this thesis, we focus on detecting fine-grained semantic divergences in parallel texts to improve machine and human translation understanding. In our first piece of work, we start by providing empirical evidence that such small meaning differences exist and can be reliably annotated both at a sentence and at a sub-sentential level. Then, we show that they can be automatically detected by fine-tuning large pre-trained language models without supervision by learning to rank synthetic divergences of varying granularity. In our second piece of work, we turn to analyzing the impact of fine-grained divergences on Neural Machine Translation (NMT) training and show that they negatively impact several aspects of NMT outputs, e.g., translation quality and confidence. Based on these findings, we present two orthogonal approaches to mitigating the negative impact of divergences and improve machine translation quality: first, we introduce a divergent-aware NMT framework that models divergences at training time; second, we present generation-based approaches for revising divergences in mined parallel texts to make the corresponding references more equivalent in meaning.After exploring how subtle meaning differences in parallel texts impact machine translation systems, we switch gears to understand how divergence detection can be used by humans directly. In our last piece of work, we extend our divergence detection methods to explain divergences from a human-centered perspective. We introduce a lightweight iterative algorithm that extracts contrastive phrasal highlights, i.e., highlights of segments indicating where divergences reside within bilingual texts, by explicitly formalizing the alignment between them. We show that our approach produces contrastive phrasal highlights that match human-provided rationales of divergences better than prior explainability approaches. Finally, based on extensive application-grounded evaluations, we show that contrastive phrasal highlights help bilingual speakers detect fine-grained meaning differences in human-translated texts, as well as critical errors due to local mistranslations in machine-translated texts.
UR  - https://www.proquest.com/docview/2874169427?accountid=15181&bdid=109696&_bd=em%2F9Qp79esT6SA3mUosI01spkbI%3D
ER  - 

TY  - Dissertations & Theses
T1  - Sense-Making Machines
AU  - Sarathy, Vasanth
JF  - ProQuest Dissertations and Theses
Y1  - 2020-01-01
DA  - 2020
SN  - 9798684655234
AB  - Although statistical machine learning techniques have led to significant advances in AI systems, they are still far from demonstrating fundamental intelligence capabilities possessed by human toddlers and even some animals. After decades of research and millennia of scientific and philosophical thought, the central goals of AI -- to explain and replicate human intelligence and creativity -- still remain unmet. In this thesis, I argue for instilling in AI systems, the ability to continually "make sense" of its changing world to guide behavior and understand perceptual information. Different from current mainstream AI approaches, I propose that agents maintain and update mental representations of the world that allow them to reason about symbolic concepts under uncertainty. I show that such representations and inference machinery are needed at all levels of cognitive processing -- from language interpretation, basic visual perception and action selection to high-level deliberation and even creative problem-solving. In the latter, sense-making becomes sense-breaking, in which I demonstrate how an agent can break its own assumptions and biases in order to discover novel ideas and solutions. Symbolic representations allow an agent to reason beyond statistical patterns, verify the veracity of their knowledge, recognize gaps in their understanding, raise questions, and explore the world to seek out answers. In doing so, such representations also allow artificial agents to provide us, humans, explanations of their behaviors, allow us to better interpret and understand their actions, andensure that they comply with our social norms.
UR  - https://www.proquest.com/docview/2457553435?accountid=15181&bdid=109696&_bd=ULIBtArkSpRG5fFraLHTFQBCT4k%3D
ER  - 

TY  - Dissertations & Theses
T1  - The Principal–Agent Alignment Problem in Artificial Intelligence
AU  - Hadfield-Menell, Dylan Jasper
JF  - ProQuest Dissertations and Theses
Y1  - 2021-01-01
DA  - 2021
SN  - 9798380621847
AB  - The field of artificial intelligence has seen serious progress in recent years, and has also caused serious concerns that range from the immediate harms caused by systems that replicate harmful biases to the more distant worry that effective goal-directed systems may, at a certain level of performance, be able to subvert meaningful control efforts. In this dissertation, I argue the following thesis:The use of incomplete or incorrect incentives to specify the target behavior for an autonomous system creates a value alignment problem between the principal(s), on whose behalf a system acts, and the system itself;This value alignment problem can be approached in theory and practice through the development of systems that are responsive to uncertainty about the principal’s true, unobserved, intended goal; andValue alignment problems can be modeled as a class of cooperative assistance games, which are computationally similar to the class of partially-observed Markov decision processes. This model captures the principal’s capacity to behave strategically in coordination with the autonomous system. It leads to distinct solutions to alignment problems, compared with more traditional approaches to preference learning like inverse reinforcement learning, and demonstrates the need for strategically robust alignment solutions.Chapter 2 goes over background knowledge needed for the work. Chapter 3 argues the first part of the thesis. First, in Section 3.1 we consider an order-following problem between a robot and a human. We show that improving on the human player’s performance requires that the robot deviate from the human’s orders. However, if the robot has an incomplete preference model (i.e., it fails to model properties of the world that the person cares about), then there is persistent misalignment in the sense that the robot takes suboptimal actions with positive probability indefinitely. Then, in Section 3.2, we consider the problem of optimizing an incomplete proxy metric and show that this phenomenon is a consequence of incompleteness and shared resources. That is, we provide general conditions under which optimizing any fixed incomplete representation of preferences will lead to arbitrarily large losses of utility for the human player. We identify dynamic incentive protocols and impact minimization as theoretical solutions to this problem.Next, Chapter 4 deals with the second part of the thesis. We first show, in Section 4.1, that uncertainty about utility evaluations creates incentives to get supervision from the human player. Then, in Section 4.2 and Section 4.3, we demonstrate how to use uncertainty about utility evaluations to implement reward learning approaches that penalize negative side-effects and support dynamic incentive protocols. Specifically, we show how to apply Bayesian inference to learn a distribution over potential true utility functions, given the observation of a proxy in a specific development context.Chapter 5 deals with the third part of the thesis. We introduce cooperative inverse reinforcement learning (CIRL), which formalizes the base case of assistance games. CIRL models dyadic value alignment between a human principal H and a robot assistant R. This game-theoretic framework models H’s incentive to be pedagogic. We show that pedagogical solutions to value alignment can be substantially more efficient than methods based on, e.g., imitation learning. Additionally, we provide theoretical results that support a family of efficient algorithms for CIRL that adapt standard approaches for solving POMDPs to compute pedagogical equilibria.Finally, Chapter 6 considers the final component of the thesis, the need for robust solutions that can handle strategy variation on the part of H. We introduce a setting where R assists H in solving a multi-armed bandit. As in Section 3.1, H’s actions tell R which of the k different arms to pull. However, this introduces the complication that H does not know which arm is optimal a priori. We show that this setting admits efficient strategies where H treats their actions as purely communicative. These communication solutions can achieve optimal learning performance, but perform arbitrarily poorly if the encoding strategy used by H is misaligned with R’s decoding strategy.We conclude with a discussion of related work in Chapter 7 and proposals for future work in Chapter 8.
UR  - https://www.proquest.com/docview/2884056303?accountid=15181&bdid=109696&_bd=wA%2BMt%2Bf2Bv0xlKa7%2Bly1e3HOvys%3D
ER  - 

TY  - Dissertations & Theses
T1  - The Dynamics of Attention in Digital Ecosystems
AU  - Epstein, Ziv
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381957938
AB  - With more than half of the population of earth now using social media, it has become a critical tool for individuals to gain information about the world around them and to connect with others, and for researchers to build a broad understanding of human behavior. However, myopic design patterns of these platforms, as well as the broader attention economy, have enabled the proliferation of misinformation and undermined collective intelligence. These situating factors motivate a guiding research question for the emerging field of platform design: how does attention operate in digital ecosystems, and how can we design platforms to mitigate misinformation and promote collective intelligence? This dissertation addresses key aspects of this question by arguing that attention operates in a two- stage process (“Try” and “Buy”) on social media. The Try phase (stage 1) involves initial exposure to content, and the Buy phase (stage 2) involves engagement with content conditional on having been exposed to it in the first place. Due to the difficulties of measuring Stage 1 exposure in standard survey experiment settings, most research has focused only on psychological determinants and design interventions for stage 2, neglecting the crucial role attention plays online. To understand the attentional dynamics of stage 2, I will discuss studies on a scalable accuracy prompts that “moves the spotlight of attention” towards peoples’ existing but latent capacity for discerning truth from falsehood. To understand the attentional dynamics of stage 1, I will discuss how social influence can impact the content users are exposed on social media environments, via a large scale field experiment with AI-generated hybrid animals (“GANimals”). To directly measure attentional exposure to content in addition to engagement, I introduce a new research tool called Yourfeed. Yourfeed is a digital environment that mirrors the attentional context of social media, and tracks both dwell time and engagement for each piece of content. Together, these contributions provide empirical evidence for how attention operates on social media, and highlights a suite of design interventions to promote high-quality interactions with these platforms.
UR  - https://www.proquest.com/docview/3031032280?accountid=15181&bdid=109696&_bd=UNjfozoGgI86Heb5bPItbSY5s4U%3D
ER  - 

TY  - Dissertations & Theses
T1  - Learning from Temporally-Structured Human Activities Data
AU  - Lipton, Zachary C.
JF  - ProQuest Dissertations and Theses
Y1  - 2017-01-01
DA  - 2017
SN  - 978-0-355-54519-7
AB  - Despite the extraordinary success of deep learning on diverse problems, these triumphs are too often confined to large, clean datasets and well-defined objectives. Face recognition systems train on millions of perfectly annotated images. Commercial speech recognition systems train on thousands of hours of painstakingly-annotated data. But for applications addressing human activity, data can be noisy, expensive to collect, and plagued by missing values. In electronic health records, for example, each attribute might be observed on a different time scale. Complicating matters further, deciding precisely what objective warrants optimization requires critical consideration of both algorithms and the application domain. Moreover, deploying human-interacting systems requires careful consideration of societal demands such as safety, interpretability, and fairness.The aim of this thesis is to address the obstacles to mining temporal patterns in human activity data. The primary contributions are: (1) the first application of RNNs to multivariate clinical time series data, with several techniques for bridging long-term dependencies and modeling missing data; (2) a neural network algorithm for forecasting surgery duration while simultaneously modeling heteroscedasticity; (3) an approach to quantitative investing that uses RNNs to forecast company fundamentals; (4) an exploration strategy for deep reinforcement learners that significantly speeds up dialogue policy learning; (5) an algorithm to minimize the number of catastrophic mistakes made by a reinforcement learner; (6) critical works addressing model interpretability and fairness in algorithmic decision-making.
UR  - https://www.proquest.com/docview/1985128636?accountid=15181&bdid=109696&_bd=VOkbHm%2F%2BcTETA%2BARFYgDNBXj%2BV0%3D
ER  - 

TY  - Dissertations & Theses
T1  - Privacy-Aware Artificial Intelligence in Systems Medicine
AU  - Matschinske, Julian Oskar
JF  - PQDT - Global
Y1  - 2023-01-01
DA  - 2023
SN  - 9798346771135
AB  - Bioinformatics is grappling with an explosion of data, creating both opportunities and challenges for scientific discovery and healthcare. This thesis stands at the crossroads of systems medicine and privacyaware artificial intelligence (AI), offering contributions that aim to harness the potential of this data-rich landscape. Central to the thesis are the web tools CoVex, sPLINK, FeatureCloud, and AIMe, each designed to address unique challenges, publicly and freely available to the research community.Within the ambit of systems medicine, CoVex emerges as a tool in the realm of infectious diseases and drug repurposing. Deploying network exploration and ranking algorithms like centrality measures, CoVex identifies intricate disease pathways and potential drug targets. Its purpose mainly lies in drug repurposing achieved by its capability to explore integrated virus-protein, protein-protein and protein-drug interaction networks to identify alternative applications for existing drugs, thereby accelerating the medical response to urgent challenges like the COVID-19 pandemic.Privacy-aware AI is the second major pillar of the thesis, with a focus on federated learning (FL) as an enabling technology. The tools sPLINK and FeatureCloud are introduced to demonstrate this approach. sPLINK, specialized for genome-wide association studies (GWAS), preserves data privacy without compromising analytical robustness. FeatureCloud expands upon this by serving as a versatile, FL platform, thereby facilitating large-scale analyses across multiple institutions while adhering to stringent data privacy norms. It employs and integrates state-of-the-art privacy-enhancing techniques (PETs), such as differential privacy (DP) and secure multiparty computation (SMPC), to protect sensitive patient data. Evaluation of FeatureCloud shows that the results are sufficiently close or even identical to centrally performed analyses, thereby demonstrating the efficacy and applicability of FL in a cross-silo context.The thesis also brings forth the AIMe registry, aiming to create a foundation for transparency, reproducibility, and reliability in biomedical AI. By setting standards and ensuring correct and complete reporting, AIMe acts as a central hub for vetting and disseminating AI tools, increasing validation and reproducibility of results reported in biomedical research.As we traverse an era defined by rapid data proliferation and stringent data protection laws, this thesis demonstrates that specialized tools and versatile platforms are valuable additions to the research landscape. CoVex, sPLINK, AIMe and FeatureCloud each have unique specializations, yet they all contribute to more efficient research in systems medicine: making integrated data quickly accessible to researchers, allowing large-scale analyses across distributed datasets, and ensuring valid and reproducible reporting of results.
UR  - https://www.proquest.com/docview/3143978621?accountid=15181&bdid=109696&_bd=Ral2v0%2BzUkhguCZ1DL%2BfF1%2BbdTQ%3D
ER  - 

TY  - Dissertations & Theses
T1  - How Do We Design Robots Equitably? Engaging Design Justice, Design Fictions, and Co-Design in Human-Robot Interaction Design and Policymaking Processes
AU  - Ostrowski, Anastasia K.
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381956696
AB  - Robots are in their infancy engaging in our social spaces and we are faced with new perplexing and complex questions and challenges, such as how will these technologies create or further entrench inequities in society, which stakeholders have a say in technology development or policy development, and whose voices are heard in technology design. There is limited engagement with these questions in human-robot interaction where there is a greater need to understand how human-centered design methodologies and frameworks, such as participatory design, co-design, and Design Justice, can be further developed in the field to promote more equitable robot design and policy design processes. Here, I explore areas that should be considered and incorporated in equitable innovative technology design including who designs the technology, how do we support co-designers in the design of technologies, and how can we leverage participatory design, co-design, and Design Justice principles to support equitable robot design and policy making. Overall, I consider how we can support and create equitable design boundaries, spaces, and processes in the design and policy making of robotics through the presentation of three sets of studies. The first set of studies focuses on empowering users through co-design of robots. The second set turns a reflexive lens onto human-robot interaction roboticists and designers, mapping the context of human-robot interaction design processes and the field’s attention to ethics, equity, and justice. The third set of studies explores ways of creating spaces to support equitable technology design and policy making through design workshops and a design justice pedagogy summit. From this work, I present design guidelines for more equitable co-design practices in human-robot interaction, tools to support roboticists and other innovative technology designers in engaging with equity and justice through their design processes, and further promote discussion around the policy and design ecosystems of innovative technology. While this dissertation specifically focuses on robot design, the methodologies and processes developed are broadly applicable to other innovative and often disruptive technologies.
UR  - https://www.proquest.com/docview/3030977740?accountid=15181&bdid=109696&_bd=u20y0xWwBOjmrJYpfN9T5wNE6dE%3D
ER  - 

TY  - Dissertations & Theses
T1  - Redistributing the Costs of Volumetric Denial-of-Service Mitigation
AU  - DeLaughter, Samuel
JF  - ProQuest Dissertations and Theses
Y1  - 2023-01-01
DA  - 2023
SN  - 9798381955477
AB  - Volumetric Denial-of-Service (DoS) attacks pose a severe and exponentially increasing threat to the Internet. Existing mitigations provide valuable stop-gaps but fail to address the root cause, and the overhead they incur is poorly understood. To combat these attacks we present a protocol-agnostic approach to DoS mitigation that moves overhead away from service bottlenecks, towards the network edge and onto attackers themselves. We observe that the vast majority of attacks rely on a small subset of packet types which are individually identical to legitimate packets, but generated far more often by attackers than by regular clients. Making such packets marginally more difficult to generate can significantly reduce flood volumes without harming legitimate clients. We design and implement two novel mitigations in TCP following this approach, to combat the ubiquitous SYN Flood attack. The first is largely a toy example illustrating how simple packet padding can rate-limit bandwidth-constrained attackers, while the second is a more robust approach using miniature proofs-of-work to restrict the common CPU-bound attacker. We also present a rigorous experimental methodology and novel suite of metrics for more accurately evaluating the efficacy and overhead of arbitrary DoS mitigations across changes in attack, client behavior, and network topology. We use this measurement framework to evaluate our proposed mitigations in a controlled network testbed. Both mitigations exhibit negligible overhead, and while their efficacy is subjective they succeed in completely nullifying potentially devastating SYN floods in certain contexts. Beyond our immediate findings in TCP, this work is broadly applicable to the design of DoS-resilient network protocols and internet architectures.
UR  - https://www.proquest.com/docview/3030902154?accountid=15181&bdid=109696&_bd=lVLk5g%2FgYD86tWCm3KZVLaJLGW8%3D
ER  - 

TY  - Dissertations & Theses
T1  - Understanding Social Media Influence, Semantic Network Analysis, and Thematic Campaign Campaign Classification Using Machine Learning
AU  - Johnson, Nathan
JF  - ProQuest Dissertations and Theses
Y1  - 2022-01-01
DA  - 2022
SN  - 9798802716540
AB  - Individuals and organizations have greater access to the world's population than ever before. The effects of Social Media Influence have already impacted the behaviour and actions of the world's population. This research employed mixed methods to investigate the mechanisms to further the understand of how Social Media Influence Campaigns (SMIC) impact the global community as well as develop tools and frameworks to conduct analysis. The research has qualitatively examined the perceptions of Social Media, specifically how leadership believe it will change and it's role within future conflict. This research has developed and tested semantic ontological modelling to provide insights into the nature of network related behaviour of SMICs. This research also developed exemplar data sets of SMICs. The insights gained from initial research were used to train Machine Learning classifiers to identify thematically related campaigns. This work has been conducted in close collaboration with Alliance Plus Network partner, University of New South Wales and the Australian Defence Force.
UR  - https://www.proquest.com/docview/2671558743?accountid=15181&bdid=109696&_bd=ehUTMa%2BE5uh4FRD9CxNLvjFxeG4%3D
ER  - 

TY  - Dissertations & Theses
T1  - Optimal Task Scheduling and Flight Planning for Multi-Task Unmanned Aerial Vehicles
AU  - Liu, Bin
JF  - PQDT - Global
Y1  - 2022-01-01
DA  - 2022
SN  - 9798380496902
AB  - Unmanned aerial vehicles (UAVs), also known as drones, play an important role in various areas due to their agility and versatility. Integrated with many embedded components, the UAV is capable of conducting multiple tasks simultaneously. Coordinating different tasks to a multi-task UAV can be challenging. The reason is that tasks may require different levels of commitment and tolerate different latencies. Another reason is that multi-tasking can give rise to difficulties in the UAV’s energy management, as many UAVs are battery-powered. In this thesis, we study the optimal flight planning, control, and routing for the multi-task UAV.The main contributions of this thesis can be summarized as follows.• This thesis presents a novel energy-efficient UAV flight planning framework, which integrates UAVs into intelligent transportation systems for energy-efficient, delay-sensitive delivery services. The UAV can dynamically choose actions from cruise speed, full speed, recharging at a roadside charging station, or hitchhiking and recharging on a collaborative vehicle. The objective is to minimize the energy consumption of the UAV and ensure timely delivery. We reveal the conditions under which the UAV’s flight planning changes in terms of the remaining flight distance or the elapsed time. Consequently, the optimal flight planning can be instantly made by comparing with the thresholds.• This thesis presents a new online control framework for multi-task UAVs, which allows a UAV to perform in-situ sensing while delivering goods. A new finite-horizon Markov decision process (FH-MDP) problem is formulated to ensure timely delivery, minimize the UAV’s energy consumption, and maximize its reward for in-situ sensing. We prove the monotonicity and subadditivity of the FH-MDP, such that the FH-MDP has an optimal, monotone deterministic Markovian policy. We find that the optimal policy consists of flight distancerelated and time-related thresholds at which the optimal action of the UAV switches. As a result, the optimal actions of the UAV can be obtained by comparing its state with the thresholds at a linear complexity.• This thesis presents a novel multi-task UAV routing framework, which aims to minimize the UAV’s energy consumption, maximize its sensing reward, and ensure its timely arrival at the destination. We interpret possible flight waypoints as location-dependent tasks, hence accommodating the waypoints and in-situ sensing in a unified process of task selection. We construct a weighted time-task graph, and transform the optimal routing of the UAV to a weighted routing problem, which can be optimally solved by the celebrated Bellman-Ford algorithm.
UR  - https://www.proquest.com/docview/2877961822?accountid=15181&bdid=109696&_bd=y%2BljaZwRW9Ph4Fr3FXiHR3ahrqg%3D
ER  - 

TY  - Dissertations & Theses
T1  - Exploring ethical intelligence through ancient wisdom and the lived experiences of senior business leaders
AU  - Opincar, John T.
JF  - ProQuest Dissertations and Theses
Y1  - 2012-01-01
DA  - 2012
SN  - 978-1-267-72798-5
AB  - Beginning in the early 1700s with the Mississippi and South Sea Companies scandal and continuing to the present day, senior business leaders' flawed ethical judgments have cost society multi-trillions of dollars of losses. The general problem is despite the risks of personal financial harm, public embarrassment, and long prison terms business leaders continue to make unsuitable ethical judgments. The specific problem this hermeneutic phenomenological study addressed was the ethical judging phenomenon occurring in the minds and hearts of business leaders was neither well documented nor fully understood. That ethical judging phenomenon was explored through the lived ethical judging experiences of 21 C-Suite executives, purposefully selected from the 2010 Fortune 1000, during semi-structured interviews. Thematic analysis, using Moustakas' (1994) modified Stevick-Colaizzi-Keen Method of Analysis of Phenomenological Data, identified 80 separate but interrelated themes, the most grounded of which were perceived hostile otherness, righteousness, discernment, leads by example, tone at the top, truthfulness, integrity, and hubris and entitlement. The themes coalesced into seven theme families that included cloisterization, cross-cultural ethics, distress, ethical judging, loss aversion – fear, organizational culture, and values and principles. Imaginative variation revealed seven potential structures that included an ethical fence, equity equation solver, heart sphere, internal compass, intuition, scope screen, and slippery slope. Analysis of the composite textural-structural description of the ethical judging phenomenon revealed convergence as it essence. A final model of ethical intelligence was presented along with four recommendations for legislators and regulators, eight recommendations for CEO's and boards of directors, and 10 recommendations for the research community.
UR  - https://www.proquest.com/docview/1170797273?accountid=15181&bdid=109696&_bd=MYSJciaOj7wO1t9ZWCbNLmTfPRQ%3D
ER  - 

TY  - Dissertations & Theses
T1  - Deep Learning for Optimization in Operations Research Enhancing Resource Allocation, Computational Efficiency, and Generalization
AU  - Liu, Chunrui
JF  - PQDT - Global
Y1  - 2023-01-01
DA  - 2023
SN  - 9798346572107
AB  - Operations research (OR) is indispensable in optimizing business operations. This thesis advances OR by integrating deep learning techniques, making significant strides in resource allocation, computational efficiency, generalization, and sentiment analysis.In resource allocation, the conventional reliance on static decision-making models is upended by our novel approach. Recognizing the limitations imposed by the traditional dynamic planning methods, we introduce the first dynamic multi-period retail shelf space allocation model. This model, which accounts for dynamic seasonal demand with a feedback mechanism, is adeptly solved using deep reinforcement learning. Our research reveals that strategies employing multi-period dynamic decision-making markedly outperform static single-period decisions, leading to heightened profitability and efficiency in retail operations.Turning to computational efficiency, this thesis shortens the fine-tuning durations of the deep PAC-Bayesian learning models. We pioneer an analytical approach, deriving a guaranteed PAC-Bayesian generalization bound through the novel application of the neural tangent kernel in conjunction with the PAC-Bayes $\mathcal{C}$-bound. This bound is ingeniously utilized as a proxy for efficient hyperparameter selection, markedly reducing computational time for searching optimized hyperparameters across various benchmarks. This advancement not only demystifies the theoretical aspects of deep PAC-Bayesian learning but also renders these models more accessible and practical for OR practitioners.For better generalization, this thesis introduces a theoretical framework using neural tangent kernel to demystify the learning rate annealing method, which ensures that gradient descent iterates converge to a flatter minimum during initial high learning rate phases, thereby enhancing model robustness and generalization capabilities in the presence of noisy data. Our comprehensive experiments validate the theoretical understanding of the learning rate annealing method's efficacy across diverse noise scenarios, highlighting the critical role of data separability in training dynamics and underscoring the benefits of our annealing strategy.Furthermore, the thesis expands the scope of OR to encompass sentiment analysis, an increasingly vital aspect of business analytics. Traditional nature language processing models in OR typically focus on detecting generic sentiments like positive or negative comments. However, employing large language models enables us to delve deeper, identifying specific emotional states such as `waiting anxiety' sentiment among retail investors in the stock market. This is particularly evident during the anticipatory phase preceding financial news releases. Our findings indicate that heightened pre-announcement anxiety sentiment correlates with a delayed market response post-release, implying that waiting anxiety can slow the speed of price formation. This insight provides managers with actionable intelligence for strategizing the timing of financial news releases.
UR  - https://www.proquest.com/docview/3132876715?accountid=15181&bdid=109696&_bd=3VOAFpugh9I1WmnHfQzP0QZ68%2Fw%3D
ER  - 

