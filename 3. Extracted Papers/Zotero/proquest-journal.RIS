TY  - Scholarly Journals
T1  - Abnormality Detection and Failure Prediction Using Explainable Bayesian Deep Learning: Methodology and Case Study with Industrial Data
AU  - Ahmad Kamal Mohd Nor
AU  - Ahmad Kamal Mohd Nor
AU  - Srinivasa Rao Pedapati
AU  - Masdi Muhammad
AU  - Leiva, Víctor
JF  - Mathematics
VL  - 10
IS  - 4
Y1  - 2022-01-01
DA  - 2022
SP  - 554
SN  - 22277390
AB  - Mistrust, amplified by numerous artificial intelligence (AI) related incidents, is an issue that has caused the energy and industrial sectors to be amongst the slowest adopter of AI methods. Central to this issue is the black-box problem of AI, which impedes investments and is fast becoming a legal hazard for users. Explainable AI (XAI) is a recent paradigm to tackle such an issue. Being the backbone of the industry, the prognostic and health management (PHM) domain has recently been introduced into XAI. However, many deficiencies, particularly the lack of explanation assessment methods and uncertainty quantification, plague this young domain. In the present paper, we elaborate a framework on explainable anomaly detection and failure prognostic employing a Bayesian deep learning model and Shapley additive explanations (SHAP) to generate local and global explanations from the PHM tasks. An uncertainty measure of the Bayesian model is utilized as a marker for anomalies and expands the prognostic explanation scope to include the model’s confidence. In addition, the global explanation is used to improve prognostic performance, an aspect neglected from the handful of studies on PHM-XAI. The quality of the explanation is examined employing local accuracy and consistency properties. The elaborated framework is tested on real-world gas turbine anomalies and synthetic turbofan failure prediction data. Seven out of eight of the tested anomalies were successfully identified. Additionally, the prognostic outcome showed a 19% improvement in statistical terms and achieved the highest prognostic score amongst best published results on the topic.
UR  - https://www.proquest.com/docview/2632954468?accountid=15181&bdid=109692&_bd=oFcJ%2BDvvirvgMfNXvmyMrMAet%2BA%3D
DO  - https://doi.org/10.3390/math10040554
ER  - 

TY  - Scholarly Journals
T1  - The Enlightening Role of Explainable Artificial Intelligence in Chronic Wound Classification
AU  - Salih Sarp
AU  - Kuzlu, Murat
AU  - Wilson, Emmanuel
AU  - Cali, Umit
AU  - Guler, Ozgur
JF  - Electronics
VL  - 10
IS  - 12
Y1  - 2021-01-01
DA  - 2021
SP  - 1406
SN  - 20799292
AB  - Artificial Intelligence (AI) has been among the most emerging research and industrial application fields, especially in the healthcare domain, but operated as a black-box model with a limited understanding of its inner working over the past decades. AI algorithms are, in large part, built on weights calculated as a result of large matrix multiplications. It is typically hard to interpret and debug the computationally intensive processes. Explainable Artificial Intelligence (XAI) aims to solve black-box and hard-to-debug approaches through the use of various techniques and tools. In this study, XAI techniques are applied to chronic wound classification. The proposed model classifies chronic wounds through the use of transfer learning and fully connected layers. Classified chronic wound images serve as input to the XAI model for an explanation. Interpretable results can help shed new perspectives to clinicians during the diagnostic phase. The proposed method successfully provides chronic wound classification and its associated explanation to extract additional knowledge that can also be interpreted by non-data-science experts, such as medical scientists and physicians. This hybrid approach is shown to aid with the interpretation and understanding of AI decision-making processes.
UR  - https://www.proquest.com/docview/2544961040?accountid=15181&bdid=109692&_bd=SVL2WsHQ2jCAdYYiosaa3vyN8Ag%3D
DO  - https://doi.org/10.3390/electronics10121406
ER  - 

TY  - Scholarly Journals
T1  - Info-CELS: Informative Saliency Map-Guided Counterfactual Explanation for Time Series Classification
AU  - Li, Peiyu
AU  - Bahri, Omar
AU  - Hosseinzadeh, Pouya
AU  - Soukaïna Filali Boubrahimi
AU  - Shah, Muhammad Hamdi
JF  - Electronics
VL  - 14
IS  - 7
Y1  - 2025-01-01
DA  - 2025
SP  - 1311
SN  - 20799292
AB  - As the demand for interpretable machine learning approaches continues to grow, there is an increasing necessity for human involvement in providing informative explanations for model decisions. This is necessary for building trust and transparency in AI-based systems, leading to the emergence of the Explainable Artificial Intelligence (XAI) field. Recently, a novel counterfactual explanation model, CELS, has been introduced. CELS learns a saliency map for the interests of an instance and generates a counterfactual explanation guided by the learned saliency map. While CELS represents the first attempt to exploit learned saliency maps not only to provide intuitive explanations for the reason behind the decision made by the time series classifier but also to explore post hoc counterfactual explanations, it exhibits limitations in terms of its high validity for the sake of ensuring high proximity and sparsity. In this paper, we present an enhanced approach that builds upon CELS. While the original model achieved promising results in terms of sparsity and proximity, it faced limitations in terms of validity. Our proposed method addresses this limitation by removing mask normalization to provide more informative and valid counterfactual explanations. Through extensive experimentation on datasets from various domains, we demonstrate that our approach outperforms the CELS model, achieving higher validity and producing more informative explanations.
UR  - https://www.proquest.com/docview/3188812881?accountid=15181&bdid=109692&_bd=9H1RRbVL2Vg5NESLVxHeUTMuR78%3D
DO  - https://doi.org/10.3390/electronics14071311
ER  - 

TY  - Scholarly Journals
T1  - Explainable Machine Learning in Critical Decision Systems: Ensuring Safe Application and Correctness
AU  - Wiggerthale, Julius
AU  - Wiggerthale, Julius
AU  - Reich, Christoph
AU  - Reich, Christoph
JF  - AI
VL  - 5
IS  - 4
Y1  - 2024-01-01
DA  - 2024
SP  - 2864
SN  - 26732688
AB  - Machine learning (ML) is increasingly used to support or automate decision processes in critical decision systems such as self driving cars or systems for medical diagnosis. These systems require decisions in which human lives are at stake and the decisions should therefore be well founded and very reliable. This need for reliability contrasts with the black-box nature of many ML models, making it difficult to ensure that they always behave as intended. In face of the high stakes involved, the resulting uncertainty is a significant challenge. Explainable artificial intelligence (XAI) addresses the issue by making black-box models more interpretable, often to increase user trust. However, many current XAI applications focus more on transparency and usability than on enhancing safety of ML applications. In this work, we therefore conduct a systematic literature review to examine how XAI can be leveraged to increase safety of ML applications in critical decision systems. We strive to find out for what purposes XAI is currently used in critical decision systems, what are the most common XAI techniques in critical decision systems and how XAI can be harnessed to increase safety of ML applications in critical decision systems. Using the SPAR-4-SLR protocol, we are able to answer these questions and provide a foundational resource for researchers and practitioners seeking to mitigate risks of ML applications. Essentially, we identify promising approaches of XAI which go beyond increasing trust to actively ensure correctness of decisions. Our findings propose a three-layered framework to enhance safety of ML in critical decision systems by means of XAI. The approach consists of Reliability, Validation and Verification. Furthermore, we point out gaps in research and propose future directions of XAI research for enhancing safety of ML applications in critical decision systems.
UR  - https://www.proquest.com/docview/3149498680?accountid=15181&bdid=109692&_bd=LEz7whuu5ovSjCOS%2BQB6Fq4dhXA%3D
DO  - https://doi.org/10.3390/ai5040138
ER  - 

TY  - Scholarly Journals
T1  - Enhancing Safety in Autonomous Maritime Transportation Systems with Real-Time AI Agents
AU  - Durlik Irmina
AU  - Miller Tymoteusz
AU  - Kostecka Ewelina
AU  - Kozlovska Polina
AU  - Ślączka Wojciech
JF  - Applied Sciences
VL  - 15
IS  - 9
Y1  - 2025-01-01
DA  - 2025
SP  - 4986
SN  - 20763417
AB  - The maritime transportation sector is undergoing a profound shift with the emergence of autonomous vessels powered by real-time artificial intelligence (AI) agents. This article investigates the pivotal role of these agents in enhancing the safety, efficiency, and sustainability of autonomous maritime systems. Following a structured literature review, we examine the architecture of real-time AI agents, including sensor integration, communication systems, and computational infrastructure. We distinguish maritime AI agents from conventional systems by emphasizing their specialized functions, real-time processing demands, and resilience in dynamic environments. Key safety mechanisms—such as collision avoidance, anomaly detection, emergency coordination, and fail-safe operations—are analyzed to demonstrate how AI agents contribute to operational reliability. The study also explores regulatory compliance, focusing on emission control, real-time monitoring, and data governance. Implementation challenges, including limited onboard computational power, legal and ethical constraints, and interoperability issues, are addressed with practical solutions such as edge AI and modular architectures. Finally, the article outlines future research directions involving smart port integration, scalable AI models, and emerging technologies like federated and explainable AI. This work highlights the transformative potential of AI agents in advancing autonomous maritime transportation.
UR  - https://www.proquest.com/docview/3203187799?accountid=15181&bdid=109692&_bd=sAW%2FyUgDYojowMaMhPrXNEWs8TM%3D
DO  - https://doi.org/10.3390/app15094986
ER  - 

TY  - Scholarly Journals
T1  - Eye tracking insights into physician behaviour with safe and unsafe explainable AI recommendations
AU  - Nagendran, Myura
AU  - Festor, Paul
AU  - Komorowski, Matthieu
AU  - Gordon, Anthony C.
AU  - Faisal, Aldo A.
JF  - NPJ Digital Medicine
VL  - 7
IS  - 1
Y1  - 2024-12-01
DA  - Dec 2024
SP  - 202
U1  - 202
SN  - 23986352
AB  - We studied clinical AI-supported decision-making as an example of a high-stakes setting in which explainable AI (XAI) has been proposed as useful (by theoretically providing physicians with context for the AI suggestion and thereby helping them to reject unsafe AI recommendations). Here, we used objective neurobehavioural measures (eye-tracking) to see how physicians respond to XAI with N = 19 ICU physicians in a hospital’s clinical simulation suite. Prescription decisions were made both pre- and post-reveal of either a safe or unsafe AI recommendation and four different types of simultaneously presented XAI. We used overt visual attention as a marker for where physician mental attention was directed during the simulations. Unsafe AI recommendations attracted significantly greater attention than safe AI recommendations. However, there was no appreciably higher level of attention placed onto any of the four types of explanation during unsafe AI scenarios (i.e. XAI did not appear to ‘rescue’ decision-makers). Furthermore, self-reported usefulness of explanations by physicians did not correlate with the level of attention they devoted to the explanations reinforcing the notion that using self-reports alone to evaluate XAI tools misses key aspects of the interaction behaviour between human and machine.
UR  - https://www.proquest.com/docview/3087465791?accountid=15181&bdid=109692&_bd=fnEN4uUaxfbzZzzwJxT%2F5jxvhZE%3D
DO  - https://doi.org/10.1038/s41746-024-01200-x
ER  - 

TY  - Scholarly Journals
T1  - Artificial Intelligence and Smart Technologies in Safety Management: A Comprehensive Analysis Across Multiple Industries
AU  - Park, Jiyoung
AU  - Kang, Dongheon
AU  - Kang, Dongheon
JF  - Applied Sciences
VL  - 14
IS  - 24
Y1  - 2024-01-01
DA  - 2024
SP  - 11934
SN  - 20763417
AB  - The integration of Artificial Intelligence (AI) and smart technologies into safety management is a pivotal aspect of the Fourth Industrial Revolution or Industry 4.0. This study conducts a systematic literature review to identify and analyze how AI and smart technologies enhance safety management across various sectors within the Safety 4.0 paradigm. Focusing on peer-reviewed journal articles that explicitly mention “Smart”, “AI”, or “Artificial Intelligence” in their titles, the research examines key safety management factors, such as accident prevention, risk management, real-time monitoring, and ethical implementation, across sectors, including construction, industrial safety, disaster and public safety, transport and logistics, energy and power, health, smart home and living, and other diverse industries. AI-driven solutions, such as predictive analytics, machine learning algorithms, IoT sensor integration, and digital twin models, are shown to proactively identify and mitigate potential hazards, optimize energy consumption, and enhance operational efficiency. For instance, in the energy and power sector, intelligent gas meters and automated fire suppression systems manage gas-related risks effectively, while in the health sector, AI-powered health monitoring devices and mental health support applications improve patient and worker safety. The analysis reveals a significant trend towards shifting from reactive to proactive safety management, facilitated by the convergence of AI with IoT and Big Data analytics. Additionally, ethical considerations and data privacy emerge as critical challenges in the adoption of AI technologies. The study highlights the transformative role of AI in enhancing safety protocols, reducing accident rates, and improving overall safety outcomes across industries. It underscores the need for standardized protocols, robust AI governance frameworks, and interdisciplinary research to address existing challenges and maximize the benefits of AI in safety management. Future research directions include developing explainable AI models, enhancing human–AI collaboration, and fostering global standardization to ensure the responsible and effective implementation of AI-driven safety solutions.
UR  - https://www.proquest.com/docview/3149516114?accountid=15181&bdid=109692&_bd=MRLOgIMugacYyi1PumDCH8Gpv48%3D
DO  - https://doi.org/10.3390/app142411934
ER  - 

TY  - Scholarly Journals
T1  - Exploring the Unseen: A Survey of Multi-Sensor Fusion and the Role of Explainable AI (XAI) in Autonomous Vehicles
AU  - De Jong Yeong
AU  - De Jong Yeong
AU  - Panduru, Krishna
AU  - Walsh, Joseph
JF  - Sensors
VL  - 25
IS  - 3
Y1  - 2025-01-01
DA  - 2025
SP  - 856
SN  - 14248220
AB  - Autonomous vehicles (AVs) rely heavily on multi-sensor fusion to perceive their environment and make critical, real-time decisions by integrating data from various sensors such as radar, cameras, Lidar, and GPS. However, the complexity of these systems often leads to a lack of transparency, posing challenges in terms of safety, accountability, and public trust. This review investigates the intersection of multi-sensor fusion and explainable artificial intelligence (XAI), aiming to address the challenges of implementing accurate and interpretable AV systems. We systematically review cutting-edge multi-sensor fusion techniques, along with various explainability approaches, in the context of AV systems. While multi-sensor fusion technologies have achieved significant advancement in improving AV perception, the lack of transparency and explainability in autonomous decision-making remains a primary challenge. Our findings underscore the necessity of a balanced approach to integrating XAI and multi-sensor fusion in autonomous driving applications, acknowledging the trade-offs between real-time performance and explainability. The key challenges identified span a range of technical, social, ethical, and regulatory aspects. We conclude by underscoring the importance of developing techniques that ensure real-time explainability, specifically in high-stakes applications, to stakeholders without compromising safety and accuracy, as well as outlining future research directions aimed at bridging the gap between high-performance multi-sensor fusion and trustworthy explainability in autonomous driving systems.
UR  - https://www.proquest.com/docview/3165914833?accountid=15181&bdid=109692&_bd=qeIsbq%2FLzWPH9RXSyDaBV1p1hDQ%3D
DO  - https://doi.org/10.3390/s25030856
ER  - 

TY  - Scholarly Journals
T1  - Building XAI-Based Agents for IoT Systems
AU  - Dobrovolskis, Algirdas
AU  - Kazanavičius, Egidijus
AU  - Kižauskienė, Laura
JF  - Applied Sciences
VL  - 13
IS  - 6
Y1  - 2023-01-01
DA  - 2023
SP  - 4040
SN  - 20763417
AB  - The technological maturity of AI solutions has been consistently increasing over the years, expanding its application scope and domains. Smart home systems have evolved to act as proactive assistants for their residents, autonomously detecting behavioral patterns, inferring needs, and making decisions pertaining to the management and control of various home subsystems. The implementation of explainable AI (XAI) solutions in this challenging domain can improve user experience and trust by providing clear and understandable explanations of the system’s behavior. The article discusses the increasing importance of explainable artificial intelligence (XAI) in smart home systems, which are becoming progressively smarter and more accessible to end-users, and presents an agent-based approach for developing explainable Internet of things (IoT) systems and an experiment conducted at the Centre of Real Time Computer Systems at the Kaunas University of Technology. The proposed method was adapted to build an explainable, rule-based smart home system for controlling light, heating, and ventilation. The results of this study serve as a demonstration of the feasibility and effectiveness of the proposed theoretical approach in real-world scenarios.
UR  - https://www.proquest.com/docview/2791593059?accountid=15181&bdid=109692&_bd=puBzuVXLNdo2WRhMMceMsvxibQk%3D
DO  - https://doi.org/10.3390/app13064040
ER  - 

TY  - Scholarly Journals
T1  - Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities
AU  - Retzlaff, Carl Orge
AU  - Das, Srijita
AU  - Wayllace, Christabel
AU  - Mousavi, Payam
AU  - Afshari, Mohammad
AU  - Yang, Tianpei
AU  - Saranti, Anna
AU  - Angerschmid, Alessa
AU  - Taylor, Matthew E
AU  - Holzinger, Andreas
JF  - The Journal of Artificial Intelligence Research
VL  - 79
Y1  - 2024-01-01
DA  - 2024
SP  - 359
U1  - 359-415
SN  - 10769757
AB  - Artificial intelligence (AI) and especially reinforcement learning (RL) have the potential to enable agents to learn and perform tasks autonomously with superhuman performance. However, we consider RL as fundamentally a Human-in-the-Loop (HITL) paradigm, even when an agent eventually performs its task autonomously.In cases where the reward function is challenging or impossible to define, HITL approaches are considered particularly advantageous.The application of Reinforcement Learning from Human Feedback (RLHF) in systems such as ChatGPT demonstrates the effectiveness of optimizing for user experience and integrating their feedback into the training loop. In HITL RL, human input is integrated during the agent’s learning process, allowing iterative updates and fine-tuning based on human feedback, thus enhancing the agent’s performance. Since the human is an essential part of this process, we argue that human-centric approaches are the key to successful RL, a fact that has not been adequately considered in the existing literature. This paper aims to inform readers about current explainability methods in HITL RL. It also shows how the application of explainable AI (xAI) and specific improvements to existing explainability approaches can enable a better human-agent interaction in HITL RL for all types of users, whether for lay people, domain experts, or machine learning specialists.Accounting for the workflow in HITL RL and based on software and machine learning methodologies, this article identifies four phases for human involvement for creating HITL RL systems: (1) Agent Development, (2) Agent Learning, (3) Agent Evaluation, and (4) Agent Deployment. We highlight human involvement, explanation requirements, new challenges, and goals for each phase.We furthermore identify low-risk, high-return opportunities for explainability research in HITL RL and present long-term research goals to advance the field. Finally, we propose a vision of human-robot collaboration that allows both parties to reach their full potential and cooperate effectively.
UR  - https://www.proquest.com/docview/3184964722?accountid=15181&bdid=109692&_bd=t3HMnKRxwgMheCIedayQ5TKIpV4%3D
DO  - https://doi.org/10.1613/jair.1.15348
ER  - 

TY  - Scholarly Journals
T1  - Provenance documentation to enable explainable and trustworthy AI: A literature review
AU  - Kale, Amruta
AU  - Nguyen, Tin
AU  - Harris, Frederick C, Jr
AU  - Li, Chenhao
AU  - Zhang, Jiyin
AU  - Ma, Xiaogang
JF  - Data Intelligence
VL  - 5
IS  - 1
Y1  - 2023-01-01
DA  - Winter 2023
SP  - 139
U1  - 139-162
SN  - 2641435X
AB  - Recently artificial intelligence (AI) and machine learning (ML) models have demonstrated remarkable progress with applications developed in various domains. It is also increasingly discussed that AI and ML models and applications should be transparent, explainable, and trustworthy. Accordingly, the field of Explainable AI (XAI) is expanding rapidly. XAI holds substantial promise for improving trust and transparency in AI-based systems by explaining how complex models such as the deep neural network (DNN) produces their outcomes. Moreover, many researchers and practitioners consider that using provenance to explain these complex models will help improve transparency in AI-based systems. In this paper, we conduct a systematic literature review of provenance, XAI, and trustworthy AI (TAI) to explain the fundamental concepts and illustrate the potential of using provenance as a medium to help accomplish explainability in AI-based systems. Moreover, we also discuss the patterns of recent developments in this area and offer a vision for research in the near future. We hope this literature review will serve as a starting point for scholars and practitioners interested in learning about essential components of provenance, XAI, and TAI.
UR  - https://www.proquest.com/docview/2890954773?accountid=15181&bdid=109692&_bd=PUyu8AUlIwXsaZyEved8Oi2NTMg%3D
DO  - https://doi.org/10.1162/dint_a_00119
ER  - 

TY  - Scholarly Journals
T1  - AI-Powered AOP: Enhancing Runtime Monitoring with Large Language Models and Statistical Learning
AU  - PDF
JF  - International Journal of Advanced Computer Science and Applications
VL  - 15
IS  - 11
Y1  - 2024-01-01
DA  - 2024
SN  - 2158107X
AB  - Modern software systems must adapt to dynamic artificial intelligence (AI) environments and evolving requirements. Aspect-oriented programming (AOP) effectively isolates crosscutting concerns (CCs) into single modules called aspects, enhancing quality metrics, and simplifying testing. However, AOP implementation can lead to unexpected program outputs and behavior changes. This paper proposes an AI-enhanced, adaptive monitoring framework for validating program behaviors during aspect weaving that integrates AOP interfaces (AOPIs) with large language models (LLMs), i.e. GPT-Codex AI, to dynamically generate and optimize monitoring aspects and statistical models in realtime. This enables intelligent run-time analysis, adaptive model checking, and natural language (NL) interaction. We tested the framework on ten diverse Java classes from JHotdraw 7.6 by extracting context and numerical data and building a dataset for analysis. By dynamically refining aspects and models based on observed behavior, its results showed that the framework maintained the integrity of the Java OOP class while providing predictive insights into potential conflicts and optimizations. Results demonstrate the framework’s efficacy in detecting subtle behavioral changes induced by aspect weaving, with a 94% accuracy in identifying potential conflicts and a 37% reduction in false positives compared to traditional static analysis techniques. Furthermore, the integration of explainable AI provides developers with clear, actionable explanations for flagged behaviors through NL interfaces, enhancing interpretability and trust in the system.
UR  - https://www.proquest.com/docview/3147965091?accountid=15181&bdid=109692&_bd=IAyJYH9cgtY5hUT5jmyYsdKYrn8%3D
DO  - https://doi.org/10.14569/IJACSA.2024.0151113
ER  - 

TY  - Scholarly Journals
T1  - Root Cause Analysis in Industrial Manufacturing: A Scoping Review of Current Research, Challenges and the Promises of AI-Driven Approaches
AU  - Pietsch, Dominik
AU  - Pietsch, Dominik
AU  - Pietsch, Dominik
AU  - Matthes, Marvin
AU  - Matthes, Marvin
AU  - Matthes, Marvin
AU  - Wieland, Uwe
AU  - Ihlenfeldt, Steffen
AU  - Munkelt, Torsten
JF  - Journal of Manufacturing and Materials Processing
VL  - 8
IS  - 6
Y1  - 2024-01-01
DA  - 2024
SP  - 277
SN  - 25044494
AB  - The manufacturing industry must maintain high-quality standards while meeting customer demands for customization, reduced carbon footprint, and competitive pricing. To address these challenges, companies are constantly improving their production processes using quality management tools. A crucial aspect of this improvement is the root cause analysis of manufacturing defects. In recent years, there has been a shift from traditional knowledge-driven approaches to data-driven approaches. However, there is a gap in the literature regarding a systematic overview of both methodological types, their overlaps, and the challenges they pose. To fill this gap, this study conducts a scoping literature review of root cause analysis in manufacturing, focusing on both data-driven and knowledge-driven approaches. For this, articles from IEEE Xplore, Scopus, and Web of Science are examined. This review finds that data-driven approaches have become dominant in recent years, with explainable artificial intelligence emerging as a particularly strong approach. Additionally, hybrid variants of root cause analysis, which combine expert knowledge and data-driven approaches, are also prevalent, leveraging the strengths of both worlds. Major challenges identified include dependence on expert knowledge, data availability, and management issues, as well as methodological difficulties. This article also evaluates the potential of artificial intelligence and hybrid approaches for the future, highlighting their promises in advancing root cause analysis in manufacturing.
UR  - https://www.proquest.com/docview/3149660596?accountid=15181&bdid=109692&_bd=BfuY%2FfDDMrlD6lke3pVkds0wB%2BU%3D
DO  - https://doi.org/10.3390/jmmp8060277
ER  - 

TY  - Scholarly Journals
T1  - A Blockchain-Assisted Federated Learning Framework for Secure and Self-Optimizing Digital Twins in Industrial IoT
AU  - Innocent Boakye Ababio
AU  - Innocent Boakye Ababio
AU  - Bieniek, Jan
AU  - Bieniek, Jan
AU  - Rahouti, Mohamed
AU  - Hayajneh, Thaier
AU  - Hayajneh, Thaier
AU  - Aledhari, Mohammed
AU  - Verma, Dinesh C
AU  - Chehri, Abdellah
JF  - Future Internet
VL  - 17
IS  - 1
Y1  - 2025-01-01
DA  - 2025
SP  - 13
SN  - 19995903
AB  - Optimizing digital twins in the Industrial Internet of Things (IIoT) requires secure and adaptable AI models. The IIoT enables digital twins, virtual replicas of physical assets, to improve real-time decision-making, but challenges remain in trust, data security, and model accuracy. This paper presents a novel framework combining blockchain technology and federated learning (FL) to address these issues. By deploying AI models on edge devices and using FL, data privacy is maintained while enabling collaboration across industrial assets. Blockchain ensures secure data management and transparency, while explainable AI (XAI) enhances interpretability. The framework improves transparency, control, security, privacy, and scalability for self-optimizing digital twins in IIoT. A real-world evaluation demonstrates the framework’s effectiveness in enhancing security, explainability, and optimization, offering improved efficiency and reliability for industrial operations.
UR  - https://www.proquest.com/docview/3159468475?accountid=15181&bdid=109692&_bd=iyRvA188jIfulRK16mT5ZR%2F5wio%3D
DO  - https://doi.org/10.3390/fi17010013
ER  - 

TY  - Scholarly Journals
T1  - Trustworthy AI for Whom? GenAI Detection Techniques of Trust Through Decentralized Web3 Ecosystems
AU  - Calzada, Igor
AU  - Calzada, Igor
AU  - Németh, Géza
AU  - Mohammed Salah Al-Radhi
JF  - Big Data and Cognitive Computing
VL  - 9
IS  - 3
Y1  - 2025-01-01
DA  - 2025
SP  - 62
SN  - 25042289
AB  - As generative AI (GenAI) technologies proliferate, ensuring trust and transparency in digital ecosystems becomes increasingly critical, particularly within democratic frameworks. This article examines decentralized Web3 mechanisms—blockchain, decentralized autonomous organizations (DAOs), and data cooperatives—as foundational tools for enhancing trust in GenAI. These mechanisms are analyzed within the framework of the EU’s AI Act and the Draghi Report, focusing on their potential to support content authenticity, community-driven verification, and data sovereignty. Based on a systematic policy analysis, this article proposes a multi-layered framework to mitigate the risks of AI-generated misinformation. Specifically, as a result of this analysis, it identifies and evaluates seven detection techniques of trust stemming from the action research conducted in the Horizon Europe Lighthouse project called ENFIELD: (i) federated learning for decentralized AI detection, (ii) blockchain-based provenance tracking, (iii) zero-knowledge proofs for content authentication, (iv) DAOs for crowdsourced verification, (v) AI-powered digital watermarking, (vi) explainable AI (XAI) for content detection, and (vii) privacy-preserving machine learning (PPML). By leveraging these approaches, the framework strengthens AI governance through peer-to-peer (P2P) structures while addressing the socio-political challenges of AI-driven misinformation. Ultimately, this research contributes to the development of resilient democratic systems in an era of increasing technopolitical polarization.
UR  - https://www.proquest.com/docview/3181353869?accountid=15181&bdid=109692&_bd=zx9SNaPyVmIEVijdJCFqCjwMv7E%3D
DO  - https://doi.org/10.3390/bdcc9030062
ER  - 

TY  - Scholarly Journals
T1  - The role of IoT and XAI convergence in the prediction, explanation, and decision of customer perceived value (CPV) in SMEs: a theoretical framework and research proposition perspective
JF  - Discover Internet of Things
VL  - 5
IS  - 1
Y1  - 2025-12-01
DA  - Dec 2025
SP  - 4
U1  - 4
SN  - 27307239
AB  - The goal of this study is to look at how the convergence of IoT and XAI (IoT-XAI) effects the explanation, prediction, and decision-making on customer perceived value (CPV) in SMEs, utilising CPV and IoT-XAI convergence theories. This study also investigates how customer-IoT interaction influences deep learning (DL) model prediction of CPV, as well as XAI explanation and decision making on CPV prediction. The literature on customer-IoT interaction, IoT physical objects, IoT data analysis, deep learning model, XAI, and CPV was reviewed to develop a theoretical framework for investigating the relationships between IoT and XAI convergence, and CPV prediction, explanation, and decision-making towards personalised marketing. The theoretical framework and research propositions are depicted in Fig. 1. Drawing on the theoretical framework used in this study, eight key research propositions were developed on the relationship between customers, IoT, DL, XAI, and CPV explanation and decision. According to the created theoretical framework and research propositions, customer-IoT interaction generates CPV data, which is then converted into structured CPV data by IoT analytics and fed into DL models for prediction. As a result, XAI models produce explanations and decisions based on DL-enabled CPV prediction, which guides personalise marketing. This paper explains how SMEs may leverage the convergence capabilities of IoT and XAI to generate CPV explanations and decisions to modify their personalize marketing methods. Article HighlightsThe results of the present investigation demonstrate that the combination of Explainable AI and the Internet of Things improves CPV prediction, interpretation, and decision-making, which results in personalised marketing optimisation.This study establishes that deep learning models like computer vision, natural language processing, and reinforcement support the Internet of Things and Explainable AI convergence assessment of CPV by taking in CPV data from the Internet of Things and analysing it to create CPV predictions. Explainable AI then interprets, makes decisions, and suggests CPV strategies based on the predictions.In terms of practical implications, Internet of Things and Explainable AI convergence can significantly boost personalisation effectiveness through CPV recommendations. In order to help business managers enhance their marketing strategy, Internet of Things and Explainable AI convergence highlights the logic behind CPV recommendations for product recommendations and targeted advertising. Business managers can more precisely tailor the personalisation process to each customer's preferences if they have a better understanding of the CPV factors (i.e., social values – acceptance, economic value – affordability and perceived cost, and functional value – performance) influencing the deep learning models' decisions.
UR  - https://www.proquest.com/docview/3154288439?accountid=15181&bdid=109692&_bd=dqv9qtqAfufPAJpjvGuErznrav0%3D
DO  - https://doi.org/10.1007/s43926-025-00092-x
ER  - 

TY  - Scholarly Journals
T1  - Explainable AI Frameworks: Navigating the Present Challenges and Unveiling Innovative Applications
AU  - Neeraj Anand Sharma
AU  - Neeraj Anand Sharma
AU  - Rishal Ravikesh Chand
AU  - Zain Buksh
AU  - A B M Shawkat Ali
AU  - Hanif, Ambreen
AU  - Beheshti, Amin
AU  - Beheshti, Amin
JF  - Algorithms
VL  - 17
IS  - 6
Y1  - 2024-01-01
DA  - 2024
SP  - 227
SN  - 19994893
AB  - This study delves into the realm of Explainable Artificial Intelligence (XAI) frameworks, aiming to empower researchers and practitioners with a deeper understanding of these tools. We establish a comprehensive knowledge base by classifying and analyzing prominent XAI solutions based on key attributes like explanation type, model dependence, and use cases. This resource equips users to navigate the diverse XAI landscape and select the most suitable framework for their specific needs. Furthermore, the study proposes a novel framework called XAIE (eXplainable AI Evaluator) for informed decision-making in XAI adoption. This framework empowers users to assess different XAI options based on their application context objectively. This will lead to more responsible AI development by fostering transparency and trust. Finally, the research identifies the limitations and challenges associated with the existing XAI frameworks, paving the way for future advancements. By highlighting these areas, the study guides researchers and developers in enhancing the capabilities of Explainable AI.
UR  - https://www.proquest.com/docview/3072235276?accountid=15181&bdid=109692&_bd=EHxbX8fo5385vXjTTv5FszmKAfQ%3D
DO  - https://doi.org/10.3390/a17060227
ER  - 

TY  - Scholarly Journals
T1  - Recent Applications of Explainable AI (XAI): A Systematic Literature Review
AU  - Saarela, Mirka
AU  - Saarela, Mirka
AU  - Podgorelec, Vili
JF  - Applied Sciences
VL  - 14
IS  - 19
Y1  - 2024-01-01
DA  - 2024
SP  - 8884
SN  - 20763417
AB  - This systematic literature review employs the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to investigate recent applications of explainable AI (XAI) over the past three years. From an initial pool of 664 articles identified through the Web of Science database, 512 peer-reviewed journal articles met the inclusion criteria—namely, being recent, high-quality XAI application articles published in English—and were analyzed in detail. Both qualitative and quantitative statistical techniques were used to analyze the identified articles: qualitatively by summarizing the characteristics of the included studies based on predefined codes, and quantitatively through statistical analysis of the data. These articles were categorized according to their application domains, techniques, and evaluation methods. Health-related applications were particularly prevalent, with a strong focus on cancer diagnosis, COVID-19 management, and medical imaging. Other significant areas of application included environmental and agricultural management, industrial optimization, cybersecurity, finance, transportation, and entertainment. Additionally, emerging applications in law, education, and social care highlight XAI’s expanding impact. The review reveals a predominant use of local explanation methods, particularly SHAP and LIME, with SHAP being favored for its stability and mathematical guarantees. However, a critical gap in the evaluation of XAI results is identified, as most studies rely on anecdotal evidence or expert opinion rather than robust quantitative metrics. This underscores the urgent need for standardized evaluation frameworks to ensure the reliability and effectiveness of XAI applications. Future research should focus on developing comprehensive evaluation standards and improving the interpretability and stability of explanations. These advancements are essential for addressing the diverse demands of various application domains while ensuring trust and transparency in AI systems.
UR  - https://www.proquest.com/docview/3116645255?accountid=15181&bdid=109692&_bd=4PUyvAX8iK4hJcRCiwlY6vsxv0s%3D
DO  - https://doi.org/10.3390/app14198884
ER  - 

TY  - Scholarly Journals
T1  - Explainable Boosting Machines Identify Key Metabolomic Biomarkers in Rheumatoid Arthritis
AU  - Yagin Fatma Hilal
AU  - Colak Cemil
AU  - Algarni Abdulmohsen
AU  - Algarni, Ali
AU  - Al-Hashem, Fahaid
AU  - Ardigò, Luca Paolo
JF  - Medicina
VL  - 61
IS  - 5
Y1  - 2025-01-01
DA  - 2025
SP  - 833
SN  - 1010660X
AB  - Background and Objectives: Rheumatoid arthritis (RA) is a chronic autoimmune disease characterised by joint inflammation and pain. Metabolomics approaches, which are high-throughput profiling of small molecule metabolites in plasma or serum in RA patients, have so far provided biomarker discovery in the literature for clinical subgroups, risk factors, and predictors of treatment response using classical statistical approaches or machine learning models. Despite these recent developments, an explainable artificial intelligence (XAI)-based methodology has not been used to identify RA metabolomic biomarkers and distinguish patients with RA. This study constructed a XAI-based EBM model using global plasma metabolomics profiling to identify metabolites predictive of RA patients and to develop a classification model that can distinguish RA patients from healthy controls. Materials and Methods: Global plasma metabolomics data were analysed from RA patients (49 samples) and healthy individuals (10 samples). SMOTE technique was used for class imbalance in data preprocessing. EBM, LightGBM, and AdaBoost algorithms were applied to generate a discriminatory model between RA and controls. Comprehensive performance metrics were calculated, and the interpretability of the optimal model was assessed using global and local feature descriptions. Results: A total of 59 samples were analysed, 49 from RA patients, and 10 from healthy subjects. The EBM generated better results than LightGBM and AdaBoost by attaining an AUC of 0.901 (95% CI: 0.847–0.955) with 87.8% sensitivity which helps prevent false negative early RA diagnosis. The primary biomarkers EBM-based XAI identified were N-acetyleucine, pyruvic acid, and glycerol-3-phosphate. EBM global explanation analysis indicated that elevated pyruvic acid levels were significantly correlated with RA, whereas N-acetyleucine exhibited a nonlinear relationship, implying possible protective effects at specific concentrations. Conclusions: This study underscores the promise of XAI and evidence-based medicine methodology in developing biomarkers for RA through metabolomics. The discovered metabolites offer significant insights into RA pathophysiology and may function as diagnostic biomarkers or therapeutic targets. Incorporating EBM methodologies integrated with XAI improves model transparency and increases the therapeutic applicability of predictive models for RA diagnosis/management. Furthermore, the transparent structure of the EBM model empowers clinicians to understand and verify the reasoning behind each prediction, thereby fostering trust in AI-assisted decision-making and facilitating the integration of metabolomic insights into routine clinical practice.
UR  - https://www.proquest.com/docview/3212073322?accountid=15181&bdid=109692&_bd=67oDJOz2bT0YctoUrwVcG97JdWM%3D
DO  - https://doi.org/10.3390/medicina61050833
ER  - 

TY  - Scholarly Journals
T1  - Second opinion machine learning for fast-track pathway assignment in hip and knee replacement surgery: the use of patient-reported outcome measures
AU  - Campagner, Andrea
AU  - Milella, Frida
AU  - Banfi, Giuseppe
AU  - Cabitza, Federico
JF  - BMC Medical Informatics and Decision Making
VL  - 24
Y1  - 2024-01-01
DA  - 2024
SP  - 1
U1  - 1-17
SN  - 14726947
AB  - BackgroundThe frequency of hip and knee arthroplasty surgeries has been rising steadily in recent decades. This trend is attributed to an aging population, leading to increased demands on healthcare systems. Fast Track (FT) surgical protocols, perioperative procedures designed to expedite patient recovery and early mobilization, have demonstrated efficacy in reducing hospital stays, convalescence periods, and associated costs. However, the criteria for selecting patients for FT procedures have not fully capitalized on the available patient data, including patient-reported outcome measures (PROMs).MethodsOur study focused on developing machine learning (ML) models to support decision making in assigning patients to FT procedures, utilizing data from patients’ self-reported health status. These models are specifically designed to predict the potential health status improvement in patients initially selected for FT. Our approach focused on techniques inspired by the concept of controllable AI. This includes eXplainable AI (XAI), which aims to make the model’s recommendations comprehensible to clinicians, and cautious prediction, a method used to alert clinicians about potential control losses, thereby enhancing the models’ trustworthiness and reliability.ResultsOur models were trained and tested using a dataset comprising 899 records from individual patients admitted to the FT program at IRCCS Ospedale Galeazzi-Sant’Ambrogio. After training and selecting hyper-parameters, the models were assessed using a separate internal test set. The interpretable models demonstrated performance on par or even better than the most effective ‘black-box’ model (Random Forest). These models achieved sensitivity, specificity, and positive predictive value (PPV) exceeding 70%, with an area under the curve (AUC) greater than 80%. The cautious prediction models exhibited enhanced performance while maintaining satisfactory coverage (over 50%). Further, when externally validated on a separate cohort from the same hospital-comprising patients from a subsequent time period-the models showed no pragmatically notable decline in performance.ConclusionsOur results demonstrate the effectiveness of utilizing PROMs as basis to develop ML models for planning assignments to FT procedures. Notably, the application of controllable AI techniques, particularly those based on XAI and cautious prediction, emerges as a promising approach. These techniques provide reliable and interpretable support, essential for informed decision-making in clinical processes.
UR  - https://www.proquest.com/docview/3091290062?accountid=15181&bdid=109692&_bd=XnTvhO5JyTQm6EJp8R8YToKUF0U%3D
DO  - https://doi.org/10.1186/s12911-024-02602-3
ER  - 

TY  - Scholarly Journals
T1  - Medical Professional Enhancement Using Explainable Artificial Intelligence in Fetal Cardiac Ultrasound Screening
AU  - Sakai, Akira
AU  - Komatsu, Masaaki
AU  - Komatsu, Masaaki
AU  - Komatsu, Reina
AU  - Matsuoka, Ryu
AU  - Yasutomi, Suguru
AU  - Ai Dozen
AU  - Kanto Shozu
AU  - Arakaki, Tatsuya
AU  - Machino, Hidenori
AU  - Asada, Ken
AU  - Kaneko, Syuzo
AU  - Sekizawa, Akihiko
AU  - Hamamoto, Ryuji
AU  - Hamamoto, Ryuji
JF  - Biomedicines
VL  - 10
IS  - 3
Y1  - 2022-01-01
DA  - 2022
SP  - 551
SN  - 22279059
AB  - Diagnostic support tools based on artificial intelligence (AI) have exhibited high performance in various medical fields. However, their clinical application remains challenging because of the lack of explanatory power in AI decisions (black box problem), making it difficult to build trust with medical professionals. Nevertheless, visualizing the internal representation of deep neural networks will increase explanatory power and improve the confidence of medical professionals in AI decisions. We propose a novel deep learning-based explainable representation “graph chart diagram” to support fetal cardiac ultrasound screening, which has low detection rates of congenital heart diseases due to the difficulty in mastering the technique. Screening performance improves using this representation from 0.966 to 0.975 for experts, 0.829 to 0.890 for fellows, and 0.616 to 0.748 for residents in the arithmetic mean of area under the curve of a receiver operating characteristic curve. This is the first demonstration wherein examiners used deep learning-based explainable representation to improve the performance of fetal cardiac ultrasound screening, highlighting the potential of explainable AI to augment examiner capabilities.
UR  - https://www.proquest.com/docview/2642347202?accountid=15181&bdid=109692&_bd=cL2sZ3srQP3S5%2FcgAPtLSEspNDg%3D
DO  - https://doi.org/10.3390/biomedicines10030551
ER  - 

TY  - Scholarly Journals
T1  - Explainable Artificial Intelligence (XAI) in Insurance
AU  - Owens, Emer
AU  - Owens, Emer
AU  - Sheehan, Barry
AU  - Mullins, Martin
AU  - Cunneen, Martin
AU  - Ressel, Juliane
AU  - Castignani, German
JF  - Risks
VL  - 10
IS  - 12
Y1  - 2022-01-01
DA  - 2022
SP  - 230
SN  - 22279091
AB  - Explainable Artificial Intelligence (XAI) models allow for a more transparent and understandable relationship between humans and machines. The insurance industry represents a fundamental opportunity to demonstrate the potential of XAI, with the industry’s vast stores of sensitive data on policyholders and centrality in societal progress and innovation. This paper analyses current Artificial Intelligence (AI) applications in insurance industry practices and insurance research to assess their degree of explainability. Using search terms representative of (X)AI applications in insurance, 419 original research articles were screened from IEEE Xplore, ACM Digital Library, Scopus, Web of Science and Business Source Complete and EconLit. The resulting 103 articles (between the years 2000–2021) representing the current state-of-the-art of XAI in insurance literature are analysed and classified, highlighting the prevalence of XAI methods at the various stages of the insurance value chain. The study finds that XAI methods are particularly prevalent in claims management, underwriting and actuarial pricing practices. Simplification methods, called knowledge distillation and rule extraction, are identified as the primary XAI technique used within the insurance value chain. This is important as the combination of large models to create a smaller, more manageable model with distinct association rules aids in building XAI models which are regularly understandable. XAI is an important evolution of AI to ensure trust, transparency and moral values are embedded within the system’s ecosystem. The assessment of these XAI foci in the context of the insurance industry proves a worthwhile exploration into the unique advantages of XAI, highlighting to industry professionals, regulators and XAI developers where particular focus should be directed in the further development of XAI. This is the first study to analyse XAI’s current applications within the insurance industry, while simultaneously contributing to the interdisciplinary understanding of applied XAI. Advancing the literature on adequate XAI definitions, the authors propose an adapted definition of XAI informed by the systematic review of XAI literature in insurance.
UR  - https://www.proquest.com/docview/2756776341?accountid=15181&bdid=109692&_bd=nKKGfqxroXimrHA%2FFOq7fyk7fzU%3D
DO  - https://doi.org/10.3390/risks10120230
ER  - 

TY  - Scholarly Journals
T1  - The Application of Explainable Artificial Intelligence to Low-Power Internet of Things Devices with Secure Communication Using Chaos-Based Cryptography
AU  - Dobrovolskis, Algirdas
AU  - Kazanavičius, Egidijus
JF  - Electronics
VL  - 14
IS  - 7
Y1  - 2025-01-01
DA  - 2025
SP  - 1255
SN  - 20799292
AB  - This paper investigates the feasibility of employing expert knowledge-based Explainable Artificial Intelligence (XAI) for smart house control through low-power Internet of Things (IoT) devices that possess limited computational capabilities. By integrating Explainable AI, we seek to enhance the transparency of the model’s decision-making process, thereby increasing its reliability for the end user. The Arduino Uno board was selected for IoT development because of its extensive popularity and affordability. A model of heating control has been developed using temperature sensors based on the presence of residents in the room. The operational prototype was evaluated by measuring the time taken between data input and decision-making, accompanied by an explanation, to identify any potential bottlenecks that may hinder the performance of the microcontroller. To enhance communication security, we developed a pseudo-random number generation function using chaos-based cryptography with hardware implementation, thus improving communication security without incurring additional computational costs. The method has demonstrated a time efficiency improvement of up to 67% for novice users, 58% for intermediate users, and 50% for expert users.
UR  - https://www.proquest.com/docview/3188812118?accountid=15181&bdid=109692&_bd=tso26JY69sX0GK2KAyTYqyqz6IY%3D
DO  - https://doi.org/10.3390/electronics14071255
ER  - 

TY  - Scholarly Journals
T1  - Explainable deep learning model for predicting money laundering transactions
AU  - Kute, Dattatray Vishnu
AU  - Pradhan, Biswajeet
AU  - Shukla, Nagesh
AU  - Alamri, Abdullah
JF  - International Journal on Smart Sensing and Intelligent Systems
VL  - 17
IS  - 1
Y1  - 2024-01-01
DA  - 2024
SN  - 11785608
AB  - Money laundering has been a global issue for decades. The ever-changing technology landscape, digital channels, and regulations make it increasingly difficult. Financial institutions use rule-based systems to detect suspicious money laundering transactions. However, it suffers from large false positives (FPs) that lead to operational efforts or misses on true positives (TPs) that increase the compliance risk. This paper presents a study of convolutional neural network (CNN) to predict money laundering and employs SHapley Additive exPlanations (SHAP) explainable artificial intelligence (AI) method to explain the CNN predictions. The results highlight the role of CNN in detecting suspicious transactions with high accuracy and SHAP’s role in bringing out the rationale of deep learning predictions.
UR  - https://www.proquest.com/docview/3084901072?accountid=15181&bdid=109692&_bd=mHlyehahvB98%2F%2BKwj40wWPdSQ8I%3D
DO  - https://doi.org/10.2478/ijssis-2024-0027
ER  - 

TY  - Scholarly Journals
T1  - Expert-driven explainable artificial intelligence models can detect multiple climate hazards relevant for agriculture
JF  - Communications Earth & Environment
VL  - 6
IS  - 1
Y1  - 2025-12-01
DA  - Dec 2025
SP  - 207
U1  - 207
SN  - 26624435
AB  - Concurrent climate extremes have severe consequences on societies, economies, and natural systems. Multi-hazard risk-oriented early warning systems are essential to reduce impacts, enhance preparedness, and boost adaptation. Yet, the growing volume and variety of spatio-temporal data combined with the increasing frequency of concurrent extremes pose challenges to the rapid detection and tracking of harmful events. Artificial intelligence offers an opportunity to deal with these challenges, especially when interpretability and explainability are ensured. Here, we show how expert-driven and explainable artificial intelligence models can probabilistically detect multiple agriculture-related hazards. The models are trained using the work of agro-climatic experts who, over decades, operationally identified multiple climate hazards affecting agriculture in Europe. The models identify the main drivers leading to the detection of affected areas while effectively dealing with large datasets to provide probabilistic results and uncertainty estimation. Results highlight the added value of expert-driven and explainable artificial intelligence models in supporting risk management as well as effective and sustainable adaptation, particularly when integrated into early warning systems and sectoral climate services. Grounded on expert-driven information, the models contribute to a better understanding of the complex dynamics behind the onset and spatio-temporal evolution of climate extremes and to enhanced trust in defining and communicating affected areas.Expert-driven explainable artificial intelligence models can identify agriculture-related hazards, enabling proactive risk management and climate change adaptation especially when integrated into early-warning systems and climate services, according to the results of an experiment designed to identify areas of concern for agriculture in Europe.
UR  - https://www.proquest.com/docview/3187965140?accountid=15181&bdid=109692&_bd=UzyvZ9fKAj2FskCtQhcJVoA2BSw%3D
DO  - https://doi.org/10.1038/s43247-024-01987-3
ER  - 

TY  - Scholarly Journals
T1  - Human-Centered Explainable Artificial Intelligence for Marine Autonomous Surface Vehicles
AU  - Veitch, Erik
AU  - Alsos, Ole Andreas
JF  - Journal of Marine Science and Engineering
VL  - 9
IS  - 11
Y1  - 2021-01-01
DA  - 2021
SP  - 1227
SN  - 20771312
AB  - Explainable Artificial Intelligence (XAI) for Autonomous Surface Vehicles (ASVs) addresses developers’ needs for model interpretation, understandability, and trust. As ASVs approach wide-scale deployment, these needs are expanded to include end user interactions in real-world contexts. Despite recent successes of technology-centered XAI for enhancing the explainability of AI techniques to expert users, these approaches do not necessarily carry over to non-expert end users. Passengers, other vessels, and remote operators will have XAI needs distinct from those of expert users targeted in a traditional technology-centered approach. We formulate a concept called ‘human-centered XAI’ to address emerging end user interaction needs for ASVs. To structure the concept, we adopt a model-based reasoning method for concept formation consisting of three processes: analogy, visualization, and mental simulation, drawing from examples of recent ASV research at the Norwegian University of Science and Technology (NTNU). The examples show how current research activities point to novel ways of addressing XAI needs for distinct end user interactions and underpin the human-centered XAI approach. Findings show how representations of (1) usability, (2) trust, and (3) safety make up the main processes in human-centered XAI. The contribution is the formation of human-centered XAI to help advance the research community’s efforts to expand the agenda of interpretability, understandability, and trust to include end user ASV interactions.
UR  - https://www.proquest.com/docview/2602103348?accountid=15181&bdid=109692&_bd=9rjKBbFHB7exTo4RASrgZsYx3eA%3D
DO  - https://doi.org/10.3390/jmse9111227
ER  - 

TY  - Scholarly Journals
T1  - A Survey of Explainable Artificial Intelligence for Smart Cities
AU  - Abdul Rehman Javed
AU  - Abdul Rehman Javed
AU  - Ahmed, Waqas
AU  - Pandya, Sharnil
AU  - Praveen Kumar Reddy Maddikunta
AU  - Alazab, Mamoun
AU  - Thippa Reddy Gadekallu
JF  - Electronics
VL  - 12
IS  - 4
Y1  - 2023-01-01
DA  - 2023
SP  - 1020
SN  - 20799292
AB  - The emergence of Explainable Artificial Intelligence (XAI) has enhanced the lives of humans and envisioned the concept of smart cities using informed actions, enhanced user interpretations and explanations, and firm decision-making processes. The XAI systems can unbox the potential of black-box AI models and describe them explicitly. The study comprehensively surveys the current and future developments in XAI technologies for smart cities. It also highlights the societal, industrial, and technological trends that initiate the drive towards XAI for smart cities. It presents the key to enabling XAI technologies for smart cities in detail. The paper also discusses the concept of XAI for smart cities, various XAI technology use cases, challenges, applications, possible alternative solutions, and current and future research enhancements. Research projects and activities, including standardization efforts toward developing XAI for smart cities, are outlined in detail. The lessons learned from state-of-the-art research are summarized, and various technical challenges are discussed to shed new light on future research possibilities. The presented study on XAI for smart cities is a first-of-its-kind, rigorous, and detailed study to assist future researchers in implementing XAI-driven systems, architectures, and applications for smart cities.
UR  - https://www.proquest.com/docview/2779528639?accountid=15181&bdid=109692&_bd=%2BBNXEVLvNEkEXqoyNjaoAdbljEw%3D
DO  - https://doi.org/10.3390/electronics12041020
ER  - 

TY  - Scholarly Journals
T1  - Towards Transparent Healthcare: Advancing Local Explanation Methods in Explainable Artificial Intelligence
AU  - Metta, Carlo
AU  - Metta, Carlo
AU  - Beretta, Andrea
AU  - Pellungrini, Roberto
AU  - Rinzivillo, Salvatore
AU  - Giannotti, Fosca
JF  - Bioengineering
VL  - 11
IS  - 4
Y1  - 2024-01-01
DA  - 2024
SP  - 369
SN  - 23065354
AB  - This paper focuses on the use of local Explainable Artificial Intelligence (XAI) methods, particularly the Local Rule-Based Explanations (LORE) technique, within healthcare and medical settings. It emphasizes the critical role of interpretability and transparency in AI systems for diagnosing diseases, predicting patient outcomes, and creating personalized treatment plans. While acknowledging the complexities and inherent trade-offs between interpretability and model performance, our work underscores the significance of local XAI methods in enhancing decision-making processes in healthcare. By providing granular, case-specific insights, local XAI methods like LORE enhance physicians’ and patients’ understanding of machine learning models and their outcome. Our paper reviews significant contributions to local XAI in healthcare, highlighting its potential to improve clinical decision making, ensure fairness, and comply with regulatory standards.
UR  - https://www.proquest.com/docview/3046719457?accountid=15181&bdid=109692&_bd=eJGzHTYnH5B1LwRW0luf9f%2FOu1I%3D
DO  - https://doi.org/10.3390/bioengineering11040369
ER  - 

TY  - Scholarly Journals
T1  - Exploring the Landscape of Explainable Artificial Intelligence (XAI): A Systematic Review of Techniques and Applications
AU  - Sayda Umma Hamida
AU  - Sayda Umma Hamida
AU  - Mohammad Jabed Morshed Chowdhury
AU  - Mohammad Jabed Morshed Chowdhury
AU  - Mohammad Jabed Morshed Chowdhury
AU  - Chakraborty, Narayan Ranjan
AU  - Chakraborty, Narayan Ranjan
AU  - Biswas, Kamanashis
AU  - Biswas, Kamanashis
AU  - Shahrab Khan Sami
AU  - Shahrab Khan Sami
JF  - Big Data and Cognitive Computing
VL  - 8
IS  - 11
Y1  - 2024-01-01
DA  - 2024
SP  - 149
SN  - 25042289
AB  - Artificial intelligence (AI) encompasses the development of systems that perform tasks typically requiring human intelligence, such as reasoning and learning. Despite its widespread use, AI often raises trust issues due to the opacity of its decision-making processes. This challenge has led to the development of explainable artificial intelligence (XAI), which aims to enhance user understanding and trust by providing clear explanations of AI decisions and processes. This paper reviews existing XAI research, focusing on its application in the healthcare sector, particularly in medical and medicinal contexts. Our analysis is organized around key properties of XAI—understandability, comprehensibility, transparency, interpretability, and explainability—providing a comprehensive overview of XAI techniques and their practical implications.
UR  - https://www.proquest.com/docview/3132880717?accountid=15181&bdid=109692&_bd=gPW0NMsiumeKrYWRsrQcPLkYoIg%3D
DO  - https://doi.org/10.3390/bdcc8110149
ER  - 

TY  - Scholarly Journals
T1  - Improving explainable AI with patch perturbation-based evaluation pipeline: a COVID-19 X-ray image analysis case study
AU  - Sun, Jimin
AU  - Shi, Wenqi
AU  - Giuste, Felipe O.
AU  - Vaghani, Yog S.
AU  - Tang, Lingzi
AU  - Wang, May D.
JF  - Scientific Reports (Nature Publisher Group)
VL  - 13
IS  - 1
Y1  - 2023-01-01
DA  - 2023
SP  - 19488
U1  - 19488
SN  - 20452322
AB  - Recent advances in artificial intelligence (AI) have sparked interest in developing explainable AI (XAI) methods for clinical decision support systems, especially in translational research. Although using XAI methods may enhance trust in black-box models, evaluating their effectiveness has been challenging, primarily due to the absence of human (expert) intervention, additional annotations, and automated strategies. In order to conduct a thorough assessment, we propose a patch perturbation-based approach to automatically evaluate the quality of explanations in medical imaging analysis. To eliminate the need for human efforts in conventional evaluation methods, our approach executes poisoning attacks during model retraining by generating both static and dynamic triggers. We then propose a comprehensive set of evaluation metrics during the model inference stage to facilitate the evaluation from multiple perspectives, covering a wide range of correctness, completeness, consistency, and complexity. In addition, we include an extensive case study to showcase the proposed evaluation strategy by applying widely-used XAI methods on COVID-19 X-ray imaging classification tasks, as well as a thorough review of existing XAI methods in medical imaging analysis with evaluation availability. The proposed patch perturbation-based workflow offers model developers an automated and generalizable evaluation strategy to identify potential pitfalls and optimize their proposed explainable solutions, while also aiding end-users in comparing and selecting appropriate XAI methods that meet specific clinical needs in real-world clinical research and practice.
UR  - https://www.proquest.com/docview/2887745384?accountid=15181&bdid=109692&_bd=%2FopRQDETY7WKJ8I%2BgFz9945Bi9Y%3D
DO  - https://doi.org/10.1038/s41598-023-46493-2
ER  - 

TY  - Scholarly Journals
T1  - To trust or not to trust an explanation: using LEAF to evaluate local linear XAI methods
AU  - Amparore, Elvio
AU  - Perotti, Alan
AU  - Bajardi, Paolo
JF  - PeerJ Computer Science
Y1  - 2021-04-16
DA  - Apr 16, 2021
SP  - n/a
SN  - 23765992
AB  - The main objective of eXplainable Artificial Intelligence (XAI) is to provide effective explanations for black-box classifiers. The existing literature lists many desirable properties for explanations to be useful, but there is a scarce consensus on how to quantitatively evaluate explanations in practice. Moreover, explanations are typically used only to inspect black-box models, and the proactive use of explanations as a decision support is generally overlooked. Among the many approaches to XAI, a widely adopted paradigm is Local Linear Explanations—with LIME and SHAP emerging as state-of-the-art methods. We show that these methods are plagued by many defects including unstable explanations, divergence of actual implementations from the promised theoretical properties, and explanations for the wrong label. This highlights the need to have standard and unbiased evaluation procedures for Local Linear Explanations in the XAI field. In this paper we address the problem of identifying a clear and unambiguous set of metrics for the evaluation of Local Linear Explanations. This set includes both existing and novel metrics defined specifically for this class of explanations. All metrics have been included in an open Python framework, named LEAF. The purpose of LEAF is to provide a reference for end users to evaluate explanations in a standardised and unbiased way, and to guide researchers towards developing improved explainable techniques.
UR  - https://www.proquest.com/docview/2513412843?accountid=15181&bdid=109692&_bd=YTOwf5NjbtxKmKDfv%2BXi2aH7RL4%3D
DO  - https://doi.org/10.7717/peerj-cs.479
ER  - 

TY  - Scholarly Journals
T1  - Current Challenges and Future Opportunities for XAI in Machine Learning-Based Clinical Decision Support Systems: A Systematic Review
AU  - Antoniadi, Anna Markella
AU  - Du, Yuhan
AU  - Guendouz, Yasmine
AU  - Lan, Wei
AU  - Mazo, Claudia
AU  - Becker, Brett A
AU  - Mooney, Catherine
AU  - Mooney, Catherine
JF  - Applied Sciences
VL  - 11
IS  - 11
Y1  - 2021-01-01
DA  - 2021
SP  - 5088
SN  - 20763417
AB  - Machine Learning and Artificial Intelligence (AI) more broadly have great immediate and future potential for transforming almost all aspects of medicine. However, in many applications, even outside medicine, a lack of transparency in AI applications has become increasingly problematic. This is particularly pronounced where users need to interpret the output of AI systems. Explainable AI (XAI) provides a rationale that allows users to understand why a system has produced a given output. The output can then be interpreted within a given context. One area that is in great need of XAI is that of Clinical Decision Support Systems (CDSSs). These systems support medical practitioners in their clinic decision-making and in the absence of explainability may lead to issues of under or over-reliance. Providing explanations for how recommendations are arrived at will allow practitioners to make more nuanced, and in some cases, life-saving decisions. The need for XAI in CDSS, and the medical field in general, is amplified by the need for ethical and fair decision-making and the fact that AI trained with historical data can be a reinforcement agent of historical actions and biases that should be uncovered. We performed a systematic literature review of work to-date in the application of XAI in CDSS. Tabular data processing XAI-enabled systems are the most common, while XAI-enabled CDSS for text analysis are the least common in literature. There is more interest in developers for the provision of local explanations, while there was almost a balance between post-hoc and ante-hoc explanations, as well as between model-specific and model-agnostic techniques. Studies reported benefits of the use of XAI such as the fact that it could enhance decision confidence for clinicians, or generate the hypothesis about causality, which ultimately leads to increased trustworthiness and acceptability of the system and potential for its incorporation in the clinical workflow. However, we found an overall distinct lack of application of XAI in the context of CDSS and, in particular, a lack of user studies exploring the needs of clinicians. We propose some guidelines for the implementation of XAI in CDSS and explore some opportunities, challenges, and future research needs.
UR  - https://www.proquest.com/docview/2635418950?accountid=15181&bdid=109692&_bd=bEwnV83eNQWwy0c6Ey0fYP%2FbRpM%3D
DO  - https://doi.org/10.3390/app11115088
ER  - 

TY  - Scholarly Journals
T1  - Fault Detection and Diagnosis in Industry 4.0: A Review on Challenges and Opportunities
AU  - Leite, Denis
AU  - Leite, Denis
AU  - Andrade, Emmanuel
AU  - Rativa, Diego
AU  - Maciel, Alexandre M A
JF  - Sensors
VL  - 25
IS  - 1
Y1  - 2025-01-01
DA  - 2025
SP  - 60
SN  - 14248220
AB  - Integrating Machine Learning (ML) in industrial settings has become a cornerstone of Industry 4.0, aiming to enhance production system reliability and efficiency through Real-Time Fault Detection and Diagnosis (RT-FDD). This paper conducts a comprehensive literature review of ML-based RT-FDD. Out of 805 documents, 29 studies were identified as noteworthy for presenting innovative methods that address the complexities and challenges associated with fault detection. While ML-based RT-FDD offers different benefits, including fault prediction accuracy, it faces challenges in data quality, model interpretability, and integration complexities. This review identifies a gap in industrial implementation outcomes that opens new research opportunities. Future Fault Detection and Diagnosis (FDD) research may prioritize standardized datasets to ensure reproducibility and facilitate comparative evaluations. Furthermore, there is a pressing need to refine techniques for handling unbalanced datasets and improving feature extraction for temporal series data. Implementing Explainable Artificial Intelligence (AI) (XAI) tailored to industrial fault detection is imperative for enhancing interpretability and trustworthiness. Subsequent studies must emphasize comprehensive comparative evaluations, reducing reliance on specialized expertise, documenting real-world outcomes, addressing data challenges, and bolstering real-time capabilities and integration. By addressing these avenues, the field can propel the advancement of ML-based RT-FDD methodologies, ensuring their effectiveness and relevance in industrial contexts.
UR  - https://www.proquest.com/docview/3153690361?accountid=15181&bdid=109692&_bd=TEKA3y81qHd487lKSCBGFDcSfxg%3D
DO  - https://doi.org/10.3390/s25010060
ER  - 

TY  - Scholarly Journals
T1  - A Scoping Review on the Progress, Applicability, and Future of Explainable Artificial Intelligence in Medicine
AU  - González-Alday, Raquel
AU  - González-Alday, Raquel
AU  - González-Alday, Raquel
AU  - García-Cuesta, Esteban
AU  - García-Cuesta, Esteban
AU  - Kulikowski, Casimir A
AU  - Maojo, Victor
JF  - Applied Sciences
VL  - 13
IS  - 19
Y1  - 2023-01-01
DA  - 2023
SP  - 10778
SN  - 20763417
AB  - Due to the success of artificial intelligence (AI) applications in the medical field over the past decade, concerns about the explainability of these systems have increased. The reliability requirements of black-box algorithms for making decisions affecting patients pose a challenge even beyond their accuracy. Recent advances in AI increasingly emphasize the necessity of integrating explainability into these systems. While most traditional AI methods and expert systems are inherently interpretable, the recent literature has focused primarily on explainability techniques for more complex models such as deep learning. This scoping review critically analyzes the existing literature regarding the explainability and interpretability of AI methods within the clinical domain. It offers a comprehensive overview of past and current research trends with the objective of identifying limitations that hinder the advancement of Explainable Artificial Intelligence (XAI) in the field of medicine. Such constraints encompass the diverse requirements of key stakeholders, including clinicians, patients, and developers, as well as cognitive barriers to knowledge acquisition, the absence of standardised evaluation criteria, the potential for mistaking explanations for causal relationships, and the apparent trade-off between model accuracy and interpretability. Furthermore, this review discusses possible research directions aimed at surmounting these challenges. These include alternative approaches to leveraging medical expertise to enhance interpretability within clinical settings, such as data fusion techniques and interdisciplinary assessments throughout the development process, emphasizing the relevance of taking into account the needs of final users to design trustable explainability methods.
UR  - https://www.proquest.com/docview/2876469042?accountid=15181&bdid=109692&_bd=AULsMp7XOiO1vByY7iRviPdy%2FxU%3D
DO  - https://doi.org/10.3390/app131910778
ER  - 

TY  - Scholarly Journals
T1  - Should AI models be explainable to clinicians?
AU  - Abgrall, Gwénolé
AU  - Holder, Andre L
AU  - Zaineb Chelly Dagdia
AU  - Zeitouni, Karine
AU  - Monnet, Xavier
JF  - Critical Care
VL  - 28
Y1  - 2024-01-01
DA  - 2024
SP  - 1
U1  - 1-8
SN  - 13648535
AB  - In the high-stakes realm of critical care, where daily decisions are crucial and clear communication is paramount, comprehending the rationale behind Artificial Intelligence (AI)-driven decisions appears essential. While AI has the potential to improve decision-making, its complexity can hinder comprehension and adherence to its recommendations. “Explainable AI” (XAI) aims to bridge this gap, enhancing confidence among patients and doctors. It also helps to meet regulatory transparency requirements, offers actionable insights, and promotes fairness and safety. Yet, defining explainability and standardising assessments are ongoing challenges and balancing performance and explainability can be needed, even if XAI is a growing field.
UR  - https://www.proquest.com/docview/3201585517?accountid=15181&bdid=109692&_bd=EnPcVu3nEn5dH08oyC1%2Buu%2FRmPg%3D
DO  - https://doi.org/10.1186/s13054-024-05005-y
ER  - 

TY  - Scholarly Journals
T1  - Transparency of Artificial Intelligence in Healthcare: Insights from Professionals in Computing and Healthcare Worldwide
AU  - Bernal, Jose
AU  - Mazo, Claudia
AU  - Mazo, Claudia
AU  - Mazo, Claudia
JF  - Applied Sciences
VL  - 12
IS  - 20
Y1  - 2022-01-01
DA  - 2022
SP  - 10228
SN  - 20763417
AB  - Although it is widely assumed that Artificial Intelligence (AI) will revolutionise healthcare in the near future, considerable progress must yet be made in order to gain the trust of healthcare professionals and patients. Improving AI transparency is a promising avenue for addressing such trust issues. However, transparency still lacks maturation and definitions. We seek to answer what challenges do experts and professionals in computing and healthcare identify concerning transparency of AI in healthcare? Here, we examine AI transparency in healthcare from five angles: interpretability, privacy, security, equity, and intellectual property. We respond to this question based on recent literature discussing the transparency of AI in healthcare and on an international online survey we sent to professionals working in computing and healthcare and potentially within AI. We collected responses from 40 professionals around the world. Overall, the survey results and current state of the art suggest key problems are a generalised lack of information available to the general public, a lack of understanding of transparency aspects covered in this work, and a lack of involvement of all stakeholders in the development of AI systems. We propose a set of recommendations, the implementation of which can enhance the transparency of AI in healthcare.
UR  - https://www.proquest.com/docview/2728422265?accountid=15181&bdid=109692&_bd=HRJlnkek6taeLZZ3gD3KEVGCtZk%3D
DO  - https://doi.org/10.3390/app122010228
ER  - 

TY  - Scholarly Journals
T1  - Explainable Image Classification: The Journey So Far and the Road Ahead
AU  - Kamakshi, Vidhya
AU  - Krishnan, Narayanan C
AU  - Krishnan, Narayanan C
JF  - AI
VL  - 4
IS  - 3
Y1  - 2023-01-01
DA  - 2023
SP  - 620
SN  - 26732688
AB  - Explainable Artificial Intelligence (XAI) has emerged as a crucial research area to address the interpretability challenges posed by complex machine learning models. In this survey paper, we provide a comprehensive analysis of existing approaches in the field of XAI, focusing on the tradeoff between model accuracy and interpretability. Motivated by the need to address this tradeoff, we conduct an extensive review of the literature, presenting a multi-view taxonomy that offers a new perspective on XAI methodologies. We analyze various sub-categories of XAI methods, considering their strengths, weaknesses, and practical challenges. Moreover, we explore causal relationships in model explanations and discuss approaches dedicated to explaining cross-domain classifiers. The latter is particularly important in scenarios where training and test data are sampled from different distributions. Drawing insights from our analysis, we propose future research directions, including exploring explainable allied learning paradigms, developing evaluation metrics for both traditionally trained and allied learning-based classifiers, and applying neural architectural search techniques to minimize the accuracy–interpretability tradeoff. This survey paper provides a comprehensive overview of the state-of-the-art in XAI, serving as a valuable resource for researchers and practitioners interested in understanding and advancing the field.
UR  - https://www.proquest.com/docview/2869218067?accountid=15181&bdid=109692&_bd=bnBKV6kNVhuCnKnzIyRaNW2Y5cU%3D
DO  - https://doi.org/10.3390/ai4030033
ER  - 

TY  - Scholarly Journals
T1  - Explainable Artificial Intelligence (XAI): Concepts and Challenges in Healthcare
AU  - Hulsen, Tim
JF  - AI
VL  - 4
IS  - 3
Y1  - 2023-01-01
DA  - 2023
SP  - 652
SN  - 26732688
AB  - Artificial Intelligence (AI) describes computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. Examples of AI techniques are machine learning, neural networks, and deep learning. AI can be applied in many different areas, such as econometrics, biometry, e-commerce, and the automotive industry. In recent years, AI has found its way into healthcare as well, helping doctors make better decisions (“clinical decision support”), localizing tumors in magnetic resonance images, reading and analyzing reports written by radiologists and pathologists, and much more. However, AI has one big risk: it can be perceived as a “black box”, limiting trust in its reliability, which is a very big issue in an area in which a decision can mean life or death. As a result, the term Explainable Artificial Intelligence (XAI) has been gaining momentum. XAI tries to ensure that AI algorithms (and the resulting decisions) can be understood by humans. In this narrative review, we will have a look at some central concepts in XAI, describe several challenges around XAI in healthcare, and discuss whether it can really help healthcare to advance, for example, by increasing understanding and trust. Finally, alternatives to increase trust in AI are discussed, as well as future research possibilities in the area of XAI.
UR  - https://www.proquest.com/docview/2869216448?accountid=15181&bdid=109692&_bd=b3HEAHsqcrzUgDdndEiOFtfkzUk%3D
DO  - https://doi.org/10.3390/ai4030034
ER  - 

TY  - Scholarly Journals
T1  - Explainable AI improves task performance in human–AI collaboration
JF  - Scientific Reports (Nature Publisher Group)
VL  - 14
IS  - 1
Y1  - 2024-01-01
DA  - 2024
SP  - 31150
U1  - 31150
SN  - 20452322
AB  - Artificial intelligence (AI) provides considerable opportunities to assist human work. However, one crucial challenge of human–AI collaboration is that many AI algorithms operate in a black-box manner where the way how the AI makes predictions remains opaque. This makes it difficult for humans to validate a prediction made by AI against their own domain knowledge. For this reason, we hypothesize that augmenting humans with explainable AI improves task performance in human–AI collaboration. To test this hypothesis, we implement explainable AI in the form of visual heatmaps in inspection tasks conducted by domain experts. Visual heatmaps have the advantage that they are easy to understand and help to localize relevant parts of an image. We then compare participants that were either supported by (a) black-box AI or (b) explainable AI, where the latter supports them to follow AI predictions when the AI is accurate or overrule the AI when the AI predictions are wrong. We conducted two preregistered experiments with representative, real-world visual inspection tasks from manufacturing and medicine. The first experiment was conducted with factory workers from an electronics factory, who performed  assessments of whether electronic products have defects. The second experiment was conducted with radiologists, who performed  assessments of chest X-ray images to identify lung lesions. The results of our experiments with domain experts performing real-world tasks show that task performance improves when participants are supported by explainable AI with heatmaps instead of black-box AI. We find that explainable AI as a decision aid improved the task performance by 7.7 percentage points (95% confidence interval [CI]: 3.3% to 12.0%, ) in the manufacturing experiment and by 4.7 percentage points (95% CI: 1.1% to 8.3%, ) in the medical experiment compared to black-box AI. These gains represent a significant improvement in task performance.
UR  - https://www.proquest.com/docview/3149653370?accountid=15181&bdid=109692&_bd=fu09olKdmjA%2FufHzMgN6O3j3ix8%3D
DO  - https://doi.org/10.1038/s41598-024-82501-9
ER  - 

TY  - Scholarly Journals
T1  - AI-Assisted Forecasting of a Mitigated Multiple Steam Generator Tube Rupture Scenario in a Typical Nuclear Power Plant
AU  - Spisak, Sonia
AU  - Diab, Aya
AU  - Diab, Aya
JF  - Energies
VL  - 18
IS  - 2
Y1  - 2025-01-01
DA  - 2025
SP  - 250
SN  - 19961073
AB  - This study is focused on developing a machine learning (ML) meta-model to predict the progression of a multiple steam generator tube rupture (MSGTR) accident in the APR1400 reactor. The accident was simulated using the thermal–hydraulic code RELAP5/SCDAPSIM/MOD3.4. The model incorporates a mitigation strategy executed through operator interventions. Following this, uncertainty quantification employing the Best Estimate Plus Uncertainty (BEPU) methodology was undertaken by coupling RELAP5/SCDAPSIM/MOD3.4 with the statistical software, DAKOTA 6.14.0. The analysis concentrated on critical safety parameters, including Reactor Coolant System (RCS) pressure and temperature, as well as reactor vessel upper head (RVUH) void fraction. These simulations generated a comprehensive dataset, which served as the foundation for training three ML architectures: Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM), and Convolutional LSTM (CNN+LSTM). Among these models, the CNN+LSTM hybrid configuration demonstrated superior performance, excelling in both predictive accuracy and computational efficiency. To bolster the model’s transparency and interpretability, Integrated Gradients (IGs)—an advanced Explainable AI (XAI) technique—was applied, elucidating the contribution of input features to the model’s predictions and enhancing its trustworthiness.
UR  - https://www.proquest.com/docview/3159624439?accountid=15181&bdid=109692&_bd=%2FfPoQvwRuIARShLpjQhCs%2F94Jlw%3D
DO  - https://doi.org/10.3390/en18020250
ER  - 

TY  - Scholarly Journals
T1  - Anomaly Detection and Analysis in Nuclear Power Plants
AU  - Chaudhary, Abhishek
AU  - Han, Junseo
AU  - Kim, Seongah
AU  - Kim, Aram
AU  - Choi, Sunoh
AU  - Choi, Sunoh
JF  - Electronics
VL  - 13
IS  - 22
Y1  - 2024-01-01
DA  - 2024
SP  - 4428
SN  - 20799292
AB  - Industries are increasingly adopting digital systems to improve control and accessibility by providing real-time monitoring and early alerts for potential issues. While digital transformation fuels exponential growth, it exposes these industries to cyberattacks. For critical sectors such as nuclear power plants, a cyberattack not only risks damaging the facility but also endangers human lives. In today’s digital world, enormous amounts of data are generated, and the analysis of these data can help ensure effectiveness, including security. In this study, we analyzed the data using a deep learning model for early detection of abnormal behavior. We first examined the Asherah Nuclear Power Plant simulator by initiating three different cyberattacks, each targeting a different system, thereby collecting and analyzing data from the simulator. Second, a Bi-LSTM model was used to detect anomalies in the simulator, which detected it before the plant’s protection system was activated in response to a threat. Finally, we applied explainable AI (XAI) to acquire insight into how distinctive features contribute to the detection of anomalies. XAI provides valuable explanations of model behavior by revealing how specific features influence anomaly detection during attacks. This research proposes an effective anomaly detection technique and interpretability to better understand counter-cyber threats in critical industries, such as nuclear plants.
UR  - https://www.proquest.com/docview/3133015140?accountid=15181&bdid=109692&_bd=LphPbhKguqDtUyhLJppi6KHxkIk%3D
DO  - https://doi.org/10.3390/electronics13224428
ER  - 

TY  - Scholarly Journals
T1  - A Survey on Artificial Intelligence (AI) and eXplainable AI in Air Traffic Management: Current Trends and Development with Future Research Trajectory
AU  - Degas, Augustin
AU  - Degas, Augustin
AU  - Islam, Mir Riyanul
AU  - Islam, Mir Riyanul
AU  - Hurter, Christophe
AU  - Barua, Shaibal
AU  - Rahman, Hamidur
AU  - Poudel, Minesh
AU  - Ruscio, Daniele
AU  - Mobyen Uddin Ahmed
AU  - Begum, Shahina
AU  - Rahman, Md Aquif
AU  - Bonelli, Stefano
AU  - Cartocci, Giulia
AU  - Gianluca Di Flumeri
AU  - Borghini, Gianluca
AU  - Babiloni, Fabio
AU  - Aricó, Pietro
JF  - Applied Sciences
VL  - 12
IS  - 3
Y1  - 2022-01-01
DA  - 2022
SP  - 1295
SN  - 20763417
AB  - Air Traffic Management (ATM) will be more complex in the coming decades due to the growth and increased complexity of aviation and has to be improved in order to maintain aviation safety. It is agreed that without significant improvement in this domain, the safety objectives defined by international organisations cannot be achieved and a risk of more incidents/accidents is envisaged. Nowadays, computer science plays a major role in data management and decisions made in ATM. Nonetheless, despite this, Artificial Intelligence (AI), which is one of the most researched topics in computer science, has not quite reached end users in ATM domain. In this paper, we analyse the state of the art with regards to usefulness of AI within aviation/ATM domain. It includes research work of the last decade of AI in ATM, the extraction of relevant trends and features, and the extraction of representative dimensions. We analysed how the general and ATM eXplainable Artificial Intelligence (XAI) works, analysing where and why XAI is needed, how it is currently provided, and the limitations, then synthesise the findings into a conceptual framework, named the DPP (Descriptive, Predictive, Prescriptive) model, and provide an example of its application in a scenario in 2030. It concludes that AI systems within ATM need further research for their acceptance by end-users. The development of appropriate XAI methods including the validation by appropriate authorities and end-users are key issues that needs to be addressed.
UR  - https://www.proquest.com/docview/2636122993?accountid=15181&bdid=109692&_bd=Geqo%2FxZu%2FrnHGOG8gfJDyNoaD1I%3D
DO  - https://doi.org/10.3390/app12031295
ER  - 

TY  - Scholarly Journals
T1  - A Multi-Component Framework for the Analysis and Design of Explainable Artificial Intelligence
AU  - Mi-Young, Kim
AU  - Mi-Young, Kim
AU  - Mi-Young, Kim
AU  - Atakishiyev, Shahin
AU  - Atakishiyev, Shahin
AU  - Housam Khalifa Bashier Babiker
AU  - Housam Khalifa Bashier Babiker
AU  - Nawshad Farruque
AU  - Nawshad Farruque
AU  - Goebel, Randy
AU  - Goebel, Randy
AU  - Zaïane, Osmar R
AU  - Zaïane, Osmar R
AU  - Mohammad-Hossein Motallebi
AU  - Mohammad-Hossein Motallebi
AU  - Rabelo, Juliano
AU  - Rabelo, Juliano
AU  - Syed, Talat
AU  - Syed, Talat
AU  - Yao, Hengshuai
AU  - Chun, Peter
JF  - Machine Learning and Knowledge Extraction
VL  - 3
IS  - 4
Y1  - 2021-01-01
DA  - 2021
SP  - 900
SN  - 25044990
AB  - The rapid growth of research in explainable artificial intelligence (XAI) follows on two substantial developments. First, the enormous application success of modern machine learning methods, especially deep and reinforcement learning, have created high expectations for industrial, commercial, and social value. Second, the emerging and growing concern for creating ethical and trusted AI systems, including compliance with regulatory principles to ensure transparency and trust. These two threads have created a kind of “perfect storm” of research activity, all motivated to create and deliver any set of tools and techniques to address the XAI demand. As some surveys of current XAI suggest, there is yet to appear a principled framework that respects the literature of explainability in the history of science and which provides a basis for the development of a framework for transparent XAI. We identify four foundational components, including the requirements for (1) explicit explanation knowledge representation, (2) delivery of alternative explanations, (3) adjusting explanations based on knowledge of the explainee, and (4) exploiting the advantage of interactive explanation. With those four components in mind, we intend to provide a strategic inventory of XAI requirements, demonstrate their connection to a basic history of XAI ideas, and then synthesize those ideas into a simple framework that can guide the design of AI systems that require XAI.
UR  - https://www.proquest.com/docview/2612800169?accountid=15181&bdid=109692&_bd=vgH51lS6SiIux%2B8M45Lk7xDX%2FME%3D
DO  - https://doi.org/10.3390/make3040045
ER  - 

TY  - Scholarly Journals
T1  - The Virtues of Interpretable Medical AI
AU  - Hatherley, Joshua
AU  - Sparrow, Robert
AU  - Howard, Mark
JF  - Cambridge Quarterly of Healthcare Ethics
VL  - 33
IS  - 3
Y1  - 2024-07-01
DA  - Jul 2024
SP  - 323
EP  - 332
U1  - 323-332
SN  - 09631801
AB  - Artificial intelligence (AI) systems have demonstrated impressive performance across a variety of clinical tasks. However, notoriously, sometimes these systems are “black boxes.” The initial response in the literature was a demand for “explainable AI.” However, recently, several authors have suggested that making AI more explainable or “interpretable” is likely to be at the cost of the accuracy of these systems and that prioritizing interpretability in medical AI may constitute a “lethal prejudice.” In this paper, we defend the value of interpretability in the context of the use of AI in medicine. Clinicians may prefer interpretable systems over more accurate black boxes, which in turn is sufficient to give designers of AI reason to prefer more interpretable systems in order to ensure that AI is adopted and its benefits realized. Moreover, clinicians may be justified in this preference. Achieving the downstream benefits from AI is critically dependent on how the outputs of these systems are interpreted by physicians and patients. A preference for the use of highly accurate black box AI systems, over less accurate but more interpretable systems, may itself constitute a form of lethal prejudice that may diminish the benefits of AI to—and perhaps even harm—patients.
UR  - https://www.proquest.com/docview/3171933498?accountid=15181&bdid=109692&_bd=ehKPEHPHZLrz%2FGWTjbz9L20%2F6oc%3D
DO  - https://doi.org/10.1017/S0963180122000664
ER  - 

TY  - Scholarly Journals
T1  - Solving the explainable AI conundrum by bridging clinicians’ needs and developers’ goals
AU  - Bienefeld, Nadine
AU  - Boss, Jens Michael
AU  - Lüthy, Rahel
AU  - Brodbeck, Dominique
AU  - Azzati, Jan
AU  - Blaser, Mirco
AU  - Willms, Jan
AU  - Keller, Emanuela
JF  - NPJ Digital Medicine
VL  - 6
IS  - 1
Y1  - 2023-12-01
DA  - Dec 2023
SP  - 94
U1  - 94
SN  - 23986352
AB  - Explainable artificial intelligence (XAI) has emerged as a promising solution for addressing the implementation challenges of AI/ML in healthcare. However, little is known about how developers and clinicians interpret XAI and what conflicting goals and requirements they may have. This paper presents the findings of a longitudinal multi-method study involving 112 developers and clinicians co-designing an XAI solution for a clinical decision support system. Our study identifies three key differences between developer and clinician mental models of XAI, including opposing goals (model interpretability vs. clinical plausibility), different sources of truth (data vs. patient), and the role of exploring new vs. exploiting old knowledge. Based on our findings, we propose design solutions that can help address the XAI conundrum in healthcare, including the use of causal inference models, personalized explanations, and ambidexterity between exploration and exploitation mindsets. Our study highlights the importance of considering the perspectives of both developers and clinicians in the design of XAI systems and provides practical recommendations for improving the effectiveness and usability of XAI in healthcare.
UR  - https://www.proquest.com/docview/2817273942?accountid=15181&bdid=109692&_bd=PFu4QOcvV1YS3B6SL9y3ePL%2BZJk%3D
DO  - https://doi.org/10.1038/s41746-023-00837-4
ER  - 

TY  - Scholarly Journals
T1  - A Systematic Review of Explainable Artificial Intelligence in Terms of Different Application Domains and Tasks
AU  - Islam, Mir Riyanul
AU  - Mobyen Uddin Ahmed
AU  - Barua, Shaibal
AU  - Begum, Shahina
JF  - Applied Sciences
VL  - 12
IS  - 3
Y1  - 2022-01-01
DA  - 2022
SP  - 1353
SN  - 20763417
AB  - Artificial intelligence (AI) and machine learning (ML) have recently been radically improved and are now being employed in almost every application domain to develop automated or semi-automated systems. To facilitate greater human acceptability of these systems, explainable artificial intelligence (XAI) has experienced significant growth over the last couple of years with the development of highly accurate models but with a paucity of explainability and interpretability. The literature shows evidence from numerous studies on the philosophy and methodologies of XAI. Nonetheless, there is an evident scarcity of secondary studies in connection with the application domains and tasks, let alone review studies following prescribed guidelines, that can enable researchers’ understanding of the current trends in XAI, which could lead to future research for domain- and application-specific method development. Therefore, this paper presents a systematic literature review (SLR) on the recent developments of XAI methods and evaluation metrics concerning different application domains and tasks. This study considers 137 articles published in recent years and identified through the prominent bibliographic databases. This systematic synthesis of research articles resulted in several analytical findings: XAI methods are mostly developed for safety-critical domains worldwide, deep learning and ensemble models are being exploited more than other types of AI/ML models, visual explanations are more acceptable to end-users and robust evaluation metrics are being developed to assess the quality of explanations. Research studies have been performed on the addition of explanations to widely used AI/ML models for expert users. However, more attention is required to generate explanations for general users from sensitive domains such as finance and the judicial system.
UR  - https://www.proquest.com/docview/2636123285?accountid=15181&bdid=109692&_bd=%2BIzLlfPj0%2BgLtTMqgucWM1VDeTo%3D
DO  - https://doi.org/10.3390/app12031353
ER  - 

TY  - Scholarly Journals
T1  - Designing for Confidence: The Impact of Visualizing Artificial Intelligence Decisions
AU  - Karran, Alexander John
AU  - Demazure, Théophile
AU  - Hudon, Antoine
AU  - Senecal, Sylvain
AU  - Léger, Pierre-Majorique
JF  - Frontiers in Neuroscience
Y1  - 2022-06-24
DA  - Jun 24, 2022
SP  - n/a
SN  - 16624548
AB  - Explainable Artificial Intelligence (XAI) aims to bring transparency to AI systems by translating, simplifying, and visualizing its decisions. While society remains sceptical about AI systems, studies show that transparent and explainable AI systems can help improve the Human-AI trust relationship. This manuscript presents two studies that assess three AI decision visualization attribution models that manipulate morphological clarity and two information presentation-order methods to determine each visualization's impact upon the Human-AI trust relationship through increased confidence and cognitive fit. The first study, N= 206 (Avg. age = 37.87 ± 10.51, Male = 123), utilized information presentation methods and visualizations delivered through an online experiment to explore trust in AI by asking participants to complete a visual decision-making task. The second study, N= 19 (24.9 ± 8.3 years old, Male = 10), utilized eye-tracking technology and the same stimuli presentation methods to investigate if cognitive load, inferred through pupillometry measures, mediated the confidence-trust relationship. The results indicate that low morphological clarity positively impacts Human-AI trust and that the presentation order of information within an interface in terms of adjacency further influences user trust in AI. We conclude that while adjacency and morphological clarity significantly affect cognitive load, cognitive load alone does not mediate the confidence-trust relationship. Our findings interpreted through a combination of cognitive fit, situation awareness, and ecological interface design have implications for the design of future AI systems, which may facilitate better collaboration between humans and AI-based decision agents.
UR  - https://www.proquest.com/docview/2680438671?accountid=15181&bdid=109692&_bd=gfaqH2E7cWLuKFaz7KdfalF%2FsCo%3D
DO  - https://doi.org/10.3389/fnins.2022.883385
ER  - 

TY  - Scholarly Journals
T1  - The Impact of Medical Explainable Artificial Intelligence on Nurses’ Innovation Behaviour: A Structural Equation Modelling Approach
AU  - Li, Xianmiao
AU  - Zong, Qilin
AU  - Cheng, Mengting
JF  - Journal of Nursing Management
VL  - 2024
Y1  - 2024-01-01
DA  - 2024
SN  - 0966-0429
SN  - 1365-2834
AB  - Aim: This study aims to investigate the influence of medical explainable artificial intelligence (XAI) on the innovation behaviour of nurses, as well as explore the dual-pathway mediating effect of AI self-efficacy and AI anxiety and organizational ethical climate as the moderating effect.Background: To address the practical application of medical AI technology, alleviate the scarcity of medical resources and fulfil the medical and health demands of the public, it is crucial to improve the innovation behaviour of nurses through the use of medical XAI.Methods: A cross-sectional survey was conducted involving 368 Chinese nurses working at tertiary and secondary hospitals in Anhui Province, Jiangsu Province, Zhejiang Province and Shanghai.Results: Implementing medical XAI significantly enhanced the innovation behaviour of nurses. Anxiety and self-efficacy regarding AI mediated the connection between medical XAI and the innovation behaviour of nurses. Furthermore, the organizational ethical climate positively moderated the relationship between medical XAI and AI self-efficacy.Conclusion: Medical XAI helps to enhance nurses’ AI self-efficacy and reduce AI anxiety, thereby enhancing nurses’ innovation behaviour. An organizational ethical climate enhances the positive relationship between medical XAI and AI self-efficacy.Implications for Nursing Management: Organizations and technology developers must augment the study about XAI and the system design of human-centred AI technology. The organizations aim to enhance the education and training of nurses in AI, specifically focussing on boosting nurses’ self-efficacy in utilizing AI technology. Moreover, they want to alleviate nurses’ fear of new technological advancements. Hospital administrators and leaders develop strategies to address the ethical atmosphere inside their organization.
UR  - https://www.proquest.com/docview/3113450597?accountid=15181&bdid=109692&_bd=mfkU2ibq0%2Fm8e0i32s9TIv9a91Y%3D
DO  - https://doi.org/10.1155/2024/8885760
ER  - 

TY  - Scholarly Journals
T1  - Affective Design Analysis of Explainable Artificial Intelligence (XAI): A User-Centric Perspective
AU  - Ezekiel Bernardo
AU  - Seva, Rosemary
JF  - Informatics
VL  - 10
IS  - 1
Y1  - 2023-01-01
DA  - 2023
SP  - 32
SN  - 22279709
AB  - Explainable Artificial Intelligence (XAI) has successfully solved the black box paradox of Artificial Intelligence (AI). By providing human-level insights on AI, it allowed users to understand its inner workings even with limited knowledge of the machine learning algorithms it uses. As a result, the field grew, and development flourished. However, concerns have been expressed that the techniques are limited in terms of to whom they are applicable and how their effect can be leveraged. Currently, most XAI techniques have been designed by developers. Though needed and valuable, XAI is more critical for an end-user, considering transparency cleaves on trust and adoption. This study aims to understand and conceptualize an end-user-centric XAI to fill in the lack of end-user understanding. Considering recent findings of related studies, this study focuses on design conceptualization and affective analysis. Data from 202 participants were collected from an online survey to identify the vital XAI design components and testbed experimentation to explore the affective and trust change per design configuration. The results show that affective is a viable trust calibration route for XAI. In terms of design, explanation form, communication style, and presence of supplementary information are the components users look for in an effective XAI. Lastly, anxiety about AI, incidental emotion, perceived AI reliability, and experience using the system are significant moderators of the trust calibration process for an end-user.
UR  - https://www.proquest.com/docview/2794660700?accountid=15181&bdid=109692&_bd=R62f%2B%2FejxntqkvAwsxFyd5Cewrg%3D
DO  - https://doi.org/10.3390/informatics10010032
ER  - 

TY  - Scholarly Journals
T1  - Exploring Local Explanation of Practical Industrial AI Applications: A Systematic Literature Review
AU  - Thi-Thu-Huong Le
AU  - Thi-Thu-Huong Le
AU  - Aji Teguh Prihatno
AU  - Oktian, Yustus Eko
AU  - Kang, Hyoeun
AU  - Kim, Howon
AU  - Kim, Howon
JF  - Applied Sciences
VL  - 13
IS  - 9
Y1  - 2023-01-01
DA  - 2023
SP  - 5809
SN  - 20763417
AB  - In recent years, numerous explainable artificial intelligence (XAI) use cases have been developed, to solve numerous real problems in industrial applications while maintaining the explainability level of the used artificial intelligence (AI) models to judge their quality and potentially hold the models accountable if they become corrupted. Therefore, understanding the state-of-the-art methods, pointing out recent issues, and deriving future directions are important to drive XAI research efficiently. This paper presents a systematic literature review of local explanation techniques and their practical applications in various industrial sectors. We first establish the need for XAI in response to opaque AI models and survey different local explanation methods for industrial AI applications. The number of studies is then examined with several factors, including industry sectors, AI models, data types, and XAI-based usage and purpose. We also look at the advantages and disadvantages of local explanation methods and how well they work in practical settings. The difficulties of using local explanation techniques are also covered, including computing complexity and the trade-off between precision and interpretability. Our findings demonstrate that local explanation techniques can boost industrial AI models’ transparency and interpretability and give insightful information about them. The efficiency of these procedures must be improved, and ethical concerns about their application must be resolved. This paper contributes to the increasing knowledge of local explanation strategies and offers guidance to academics and industry professionals who want to use these methods in practical settings.
UR  - https://www.proquest.com/docview/2812397522?accountid=15181&bdid=109692&_bd=StN%2FrGSm30F8BJsz3%2BXfnAXCTac%3D
DO  - https://doi.org/10.3390/app13095809
ER  - 

TY  - Scholarly Journals
T1  - Artificial Intelligence in Psychiatry: A Review of Biological and Behavioral Data Analyses
AU  - Baydili, İsmail
AU  - Tasci, Burak
AU  - Tasci, Burak
AU  - Tasci, Gülay
AU  - Tasci, Gülay
JF  - Diagnostics
VL  - 15
IS  - 4
Y1  - 2025-01-01
DA  - 2025
SP  - 434
SN  - 20754418
AB  - Artificial intelligence (AI) has emerged as a transformative force in psychiatry, improving diagnostic precision, treatment personalization, and early intervention through advanced data analysis techniques. This review explores recent advancements in AI applications within psychiatry, focusing on EEG and ECG data analysis, speech analysis, natural language processing (NLP), blood biomarker integration, and social media data utilization. EEG-based models have significantly enhanced the detection of disorders such as depression and schizophrenia through spectral and connectivity analyses. ECG-based approaches have provided insights into emotional regulation and stress-related conditions using heart rate variability. Speech analysis frameworks, leveraging large language models (LLMs), have improved the detection of cognitive impairments and psychiatric symptoms through nuanced linguistic feature extraction. Meanwhile, blood biomarker analyses have deepened our understanding of the molecular underpinnings of mental health disorders, and social media analytics have demonstrated the potential for real-time mental health surveillance. Despite these advancements, challenges such as data heterogeneity, interpretability, and ethical considerations remain barriers to widespread clinical adoption. Future research must prioritize the development of explainable AI models, regulatory compliance, and the integration of diverse datasets to maximize the impact of AI in psychiatric care.
UR  - https://www.proquest.com/docview/3170919251?accountid=15181&bdid=109692&_bd=O8kZftYy73RaoHW%2FJo0MWRrT8%2B0%3D
DO  - https://doi.org/10.3390/diagnostics15040434
ER  - 

TY  - Scholarly Journals
T1  - Reliable Autism Spectrum Disorder Diagnosis for Pediatrics Using Machine Learning and Explainable AI
AU  - Jeon, Insu
AU  - Kim, Minjoong
AU  - So, Dayeong
AU  - Eun Young Kim
AU  - Nam, Yunyoung
AU  - Kim, Seungsoo
AU  - Shim, Sehoon
AU  - Kim, Joungmin
AU  - Kim, Joungmin
AU  - Moon, Jihoon
AU  - Moon, Jihoon
JF  - Diagnostics
VL  - 14
IS  - 22
Y1  - 2024-01-01
DA  - 2024
SP  - 2504
SN  - 20754418
AB  - Background: As the demand for early and accurate diagnosis of autism spectrum disorder (ASD) increases, the integration of machine learning (ML) and explainable artificial intelligence (XAI) is emerging as a critical advancement that promises to revolutionize intervention strategies by improving both accuracy and transparency. Methods: This paper presents a method that combines XAI techniques with a rigorous data-preprocessing pipeline to improve the accuracy and interpretability of ML-based diagnostic tools. Our preprocessing pipeline included outlier removal, missing data handling, and selecting pertinent features based on clinical expert advice. Using R and the caret package (version 6.0.94), we developed and compared several ML algorithms, validated using 10-fold cross-validation and optimized by grid search hyperparameter tuning. XAI techniques were employed to improve model transparency, offering insights into how features contribute to predictions, thereby enhancing clinician trust. Results: Rigorous data-preprocessing improved the models’ generalizability and real-world applicability across diverse clinical datasets, ensuring a robust performance. Neural networks and extreme gradient boosting models achieved the best performance in terms of accuracy, precision, and recall. XAI techniques demonstrated that behavioral features significantly influenced model predictions, leading to greater interpretability. Conclusions: This study successfully developed highly precise and interpretable ML models for ASD diagnosis, connecting advanced ML methods with practical clinical application and supporting the adoption of AI-driven diagnostic tools by healthcare professionals. This study’s findings contribute to personalized intervention strategies and early diagnostic practices, ultimately improving outcomes and quality of life for individuals with ASD.
UR  - https://www.proquest.com/docview/3132921429?accountid=15181&bdid=109692&_bd=3JGR3EueT1khh1SPyFOpOfOplgY%3D
DO  - https://doi.org/10.3390/diagnostics14222504
ER  - 

TY  - Scholarly Journals
T1  - Chain of Thought Utilization in Large Language Models and Application in Nephrology
AU  - Miao, Jing
AU  - Thongprayoon, Charat
AU  - Thongprayoon, Charat
AU  - Suppadungsuk, Supawadee
AU  - Krisanapan, Pajaree
AU  - Radhakrishnan, Yeshwanter
AU  - Cheungpasitporn, Wisit
JF  - Medicina
VL  - 60
IS  - 1
Y1  - 2024-01-01
DA  - 2024
SP  - 148
SN  - 1010660X
AB  - Chain-of-thought prompting enhances the abilities of large language models (LLMs) significantly. It not only makes these models more specific and context-aware but also impacts the wider field of artificial intelligence (AI). This approach broadens the usability of AI, increases its efficiency, and aligns it more closely with human thinking and decision-making processes. As we improve this method, it is set to become a key element in the future of AI, adding more purpose, precision, and ethical consideration to these technologies. In medicine, the chain-of-thought prompting is especially beneficial. Its capacity to handle complex information, its logical and sequential reasoning, and its suitability for ethically and context-sensitive situations make it an invaluable tool for healthcare professionals. Its role in enhancing medical care and research is expected to grow as we further develop and use this technique. Chain-of-thought prompting bridges the gap between AI’s traditionally obscure decision-making process and the clear, accountable standards required in healthcare. It does this by emulating a reasoning style familiar to medical professionals, fitting well into their existing practices and ethical codes. While solving AI transparency is a complex challenge, the chain-of-thought approach is a significant step toward making AI more comprehensible and trustworthy in medicine. This review focuses on understanding the workings of LLMs, particularly how chain-of-thought prompting can be adapted for nephrology’s unique requirements. It also aims to thoroughly examine the ethical aspects, clarity, and future possibilities, offering an in-depth view of the exciting convergence of these areas.
UR  - https://www.proquest.com/docview/2918773812?accountid=15181&bdid=109692&_bd=hLC0I8iRGSBi6xr0Qt9rMdo6IdA%3D
DO  - https://doi.org/10.3390/medicina60010148
ER  - 

TY  - Scholarly Journals
T1  - Survey of Explainable AI Techniques in Healthcare
AU  - Chaddad, Ahmad
AU  - Chaddad, Ahmad
AU  - Peng, Jihao
AU  - Peng, Jihao
AU  - Xu, Jian
AU  - Xu, Jian
AU  - Bouridane, Ahmed
JF  - Sensors
VL  - 23
IS  - 2
Y1  - 2023-01-01
DA  - 2023
SP  - 634
SN  - 14248220
AB  - Artificial intelligence (AI) with deep learning models has been widely applied in numerous domains, including medical imaging and healthcare tasks. In the medical field, any judgment or decision is fraught with risk. A doctor will carefully judge whether a patient is sick before forming a reasonable explanation based on the patient’s symptoms and/or an examination. Therefore, to be a viable and accepted tool, AI needs to mimic human judgment and interpretation skills. Specifically, explainable AI (XAI) aims to explain the information behind the black-box model of deep learning that reveals how the decisions are made. This paper provides a survey of the most recent XAI techniques used in healthcare and related medical imaging applications. We summarize and categorize the XAI types, and highlight the algorithms used to increase interpretability in medical imaging topics. In addition, we focus on the challenging XAI problems in medical applications and provide guidelines to develop better interpretations of deep learning models using XAI concepts in medical image and text analysis. Furthermore, this survey provides future directions to guide developers and researchers for future prospective investigations on clinical topics, particularly on applications with medical imaging.
UR  - https://www.proquest.com/docview/2767296048?accountid=15181&bdid=109692&_bd=r39%2B9CzEEedoEwf8aQ9K2pQMUKE%3D
DO  - https://doi.org/10.3390/s23020634
ER  - 

TY  - Scholarly Journals
T1  - Integrating Explainable Artificial Intelligence in Extended Reality Environments: A Systematic Survey
AU  - Maathuis, Clara
AU  - Maathuis, Clara
AU  - Cidota, Marina Anca
AU  - Cidota, Marina Anca
AU  - Datcu, Dragoș
AU  - Marin, Letiția
JF  - Mathematics
VL  - 13
IS  - 2
Y1  - 2025-01-01
DA  - 2025
SP  - 290
SN  - 22277390
AB  - The integration of Artificial Intelligence (AI) within Extended Reality (XR) technologies has the potential to revolutionize user experiences by creating more immersive, interactive, and personalized environments. Nevertheless, the complexity and opacity of AI systems raise significant concerns regarding the transparency of data handling, reasoning processes, and decision-making mechanisms inherent in these technologies. To address these challenges, the implementation of explainable AI (XAI) methods and techniques becomes imperative, as they not only ensure compliance with prevailing ethical, social, and legal standards, norms, and principles, but also foster user trust and facilitate the broader adoption of AI solutions in XR applications. Despite the growing interest from both research and practitioner communities in this area, there is an important gap in the literature concerning a review of XAI methods specifically applied and tailored to XR systems. On this behalf, this research presents a systematic literature review that synthesizes current research on XAI approaches applied within the XR domain. Accordingly, this research aims to identify prevailing trends, assess the effectiveness of various XAI techniques, and highlight potential avenues for future research. It then contributes to the foundational understanding necessary for the development of transparent and trustworthy AI systems for XR systems using XAI technologies while enhancing the user experience and promoting responsible AI deployment.
UR  - https://www.proquest.com/docview/3159526256?accountid=15181&bdid=109692&_bd=4kKblQfKVESh8g0KgFT8rHdmttw%3D
DO  - https://doi.org/10.3390/math13020290
ER  - 

TY  - Scholarly Journals
T1  - Dermatologist-like explainable AI enhances trust and confidence in diagnosing melanoma
AU  - Chanda, Tirtha
AU  - Hauser, Katja
AU  - Hobelsberger, Sarah
AU  - Bucher, Tabea-Clara
AU  - Garcia, Carina Nogueira
AU  - Wies, Christoph
AU  - Kittler, Harald
AU  - Tschandl, Philipp
AU  - Navarrete-Dechent, Cristian
AU  - Podlipnik, Sebastian
AU  - Chousakos, Emmanouil
AU  - Crnaric, Iva
AU  - Majstorovic, Jovana
AU  - Alhajwan, Linda
AU  - Foreman, Tanya
AU  - Peternel, Sandra
AU  - Sarap, Sergei
AU  - Özdemir, İrem
AU  - Barnhill, Raymond L.
AU  - Llamas-Velasco, Mar
AU  - Poch, Gabriela
AU  - Korsing, Sören
AU  - Sondermann, Wiebke
AU  - Gellrich, Frank Friedrich
AU  - Heppt, Markus V.
AU  - Erdmann, Michael
AU  - Haferkamp, Sebastian
AU  - Drexler, Konstantin
AU  - Goebeler, Matthias
AU  - Schilling, Bastian
AU  - Utikal, Jochen S.
AU  - Ghoreschi, Kamran
AU  - Fröhling, Stefan
AU  - Krieghoff-Henning, Eva
AU  - Krieghoff-Henning, Eva
AU  - Salava, Alexander
AU  - Thiem, Alexander
AU  - Dimitrios, Alexandris
AU  - Ammar, Amr Mohammad
AU  - Vučemilović, Ana Sanader
AU  - Yoshimura, Andrea Miyuki
AU  - Ilieva, Andzelka
AU  - Gesierich, Anja
AU  - Reimer-Taschenbrecker, Antonia
AU  - Kolios, Antonios G. A.
AU  - Kalva, Arturs
AU  - Ferhatosmanoğlu, Arzu
AU  - Beyens, Aude
AU  - Pföhler, Claudia
AU  - Erdil, Dilara Ilhan
AU  - Jovanovic, Dobrila
AU  - Racz, Emoke
AU  - Bechara, Falk G.
AU  - Vaccaro, Federico
AU  - Dimitriou, Florentia
AU  - Rasulova, Gunel
AU  - Cenk, Hulya
AU  - Yanatma, Irem
AU  - Kolm, Isabel
AU  - Hoorens, Isabelle
AU  - Sheshova, Iskra Petrovska
AU  - Jocic, Ivana
AU  - Knuever, Jana
AU  - Fleißner, Janik
AU  - Thamm, Janis Raphael
AU  - Dahlberg, Johan
AU  - Lluch-Galcerá, Juan José
AU  - Figueroa, Juan Sebastián Andreani
AU  - Holzgruber, Julia
AU  - Welzel, Julia
AU  - Damevska, Katerina
AU  - Mayer, Kristine Elisabeth
AU  - Maul, Lara Valeska
AU  - Garzona-Navas, Laura
AU  - Bley, Laura Isabell
AU  - Schmitt, Laurenz
AU  - Reipen, Lena
AU  - Shafik, Lidia
AU  - Petrovska, Lidija
AU  - Golle, Linda
AU  - Jopen, Luise
AU  - Gogilidze, Magda
AU  - Burg, Maria Rosa
AU  - Morales-Sánchez, Martha Alejandra
AU  - Sławińska, Martyna
AU  - Mengoni, Miriam
AU  - Dragolov, Miroslav
AU  - Iglesias-Pena, Nicolás
AU  - Booken, Nina
AU  - Enechukwu, Nkechi Anne
AU  - Persa, Oana-Diana
AU  - Oninla, Olumayowa Abimbola
AU  - Theofilogiannakou, Panagiota
AU  - Kage, Paula
AU  - Neto, Roque Rafael Oliveira
AU  - Peralta, Rosario
AU  - Afiouni, Rym
AU  - Schuh, Sandra
AU  - Schnabl-Scheu, Saskia
AU  - Vural, Seçil
AU  - Hudson, Sharon
AU  - Saa, Sonia Rodriguez
AU  - Hartmann, Sören
AU  - Damevska, Stefana
AU  - Finck, Stefanie
AU  - Braun, Stephan Alexander
AU  - Hartmann, Tim
AU  - Welponer, Tobias
AU  - Sotirovski, Tomica
AU  - Bondare-Ansberga, Vanda
AU  - Ahlgrimm-Siess, Verena
AU  - Frings, Verena Gerlinde
AU  - Simeonovski, Viktor
AU  - Zafirovik, Zorica
AU  - Maul, Julia-Tatjana
AU  - Lehr, Saskia
AU  - Wobser, Marion
AU  - Debus, Dirk
AU  - Riad, Hassan
AU  - Pereira, Manuel P.
AU  - Lengyel, Zsuzsanna
AU  - Balcere, Alise
AU  - Tsakiri, Amalia
AU  - Braun, Ralph P.
AU  - Brinker, Titus J.
JF  - Nature Communications
VL  - 15
IS  - 1
Y1  - 2024-01-01
DA  - 2024
SP  - 524
U1  - 524
SN  - 20411723
AB  - Artificial intelligence (AI) systems have been shown to help dermatologists diagnose melanoma more accurately, however they lack transparency, hindering user acceptance. Explainable AI (XAI) methods can help to increase transparency, yet often lack precise, domain-specific explanations. Moreover, the impact of XAI methods on dermatologists’ decisions has not yet been evaluated. Building upon previous research, we introduce an XAI system that provides precise and domain-specific explanations alongside its differential diagnoses of melanomas and nevi. Through a three-phase study, we assess its impact on dermatologists’ diagnostic accuracy, diagnostic confidence, and trust in the XAI-support. Our results show strong alignment between XAI and dermatologist explanations. We also show that dermatologists’ confidence in their diagnoses, and their trust in the support system significantly increase with XAI compared to conventional AI. This study highlights dermatologists’ willingness to adopt such XAI systems, promoting future use in the clinic.Artificial intelligence has become popular as a cancer classification tool, but there is distrust of such systems due to their lack of transparency. Here, the authors develop an explainable AI system which produces text- and region-based explanations alongside its classifications which was assessed using clinicians’ diagnostic accuracy, diagnostic confidence, and their trust in the system.
UR  - https://www.proquest.com/docview/2914972049?accountid=15181&bdid=109692&_bd=DdHXzLkJf9LKXRFdLIxAA4U664U%3D
DO  - https://doi.org/10.1038/s41467-023-43095-4
ER  - 

TY  - Scholarly Journals
T1  - Explainability of Automated Fact Verification Systems: A Comprehensive Review
AU  - Vallayil, Manju
AU  - Vallayil, Manju
AU  - Vallayil, Manju
AU  - Parma Nand
AU  - Parma Nand
AU  - Wei Qi Yan
AU  - Allende-Cid, Héctor
JF  - Applied Sciences
VL  - 13
IS  - 23
Y1  - 2023-01-01
DA  - 2023
SP  - 12608
SN  - 20763417
AB  - The rapid growth in Artificial Intelligence (AI) has led to considerable progress in Automated Fact Verification (AFV). This process involves collecting evidence for a statement, assessing its relevance, and predicting its accuracy. Recently, research has begun to explore automatic explanations as an integral part of the accuracy analysis process. However, the explainability within AFV is lagging compared to the wider field of explainable AI (XAI), which aims at making AI decisions more transparent. This study looks at the notion of explainability as a topic in the field of XAI, with a focus on how it applies to the specific task of Automated Fact Verification. It examines the explainability of AFV, taking into account architectural, methodological, and dataset-related elements, with the aim of making AI more comprehensible and acceptable to general society. Although there is a general consensus on the need for AI systems to be explainable, there a dearth of systems and processes to achieve it. This research investigates the concept of explainable AI in general and demonstrates its various aspects through the particular task of Automated Fact Verification. This study explores the topic of faithfulness in the context of local and global explainability. This paper concludes by highlighting the gaps and limitations in current data science practices and possible recommendations for modifications to architectural and data curation processes, contributing to the broader goals of explainability in Automated Fact Verification.
UR  - https://www.proquest.com/docview/2899391939?accountid=15181&bdid=109692&_bd=F%2B6%2B9K6tq1ZZ8Jreoo0KBsuCL%2Fw%3D
DO  - https://doi.org/10.3390/app132312608
ER  - 

TY  - Scholarly Journals
T1  - Artificial Intelligence in Thoracic Surgery: A Review Bridging Innovation and Clinical Practice for the Next Generation of Surgical Care
AU  - Leivaditis Vasileios
AU  - Maniatopoulos Andreas Antonios
AU  - Lausberg Henning
AU  - Francesk, Mulita
AU  - Papatriantafyllou Athanasios
AU  - Liolis Elias
AU  - Beltsios Eleftherios
AU  - Adamou Antonis
AU  - Kontodimopoulos Nikolaos
AU  - Dahm Manfred
JF  - Journal of Clinical Medicine
VL  - 14
IS  - 8
Y1  - 2025-01-01
DA  - 2025
SP  - 2729
SN  - 20770383
AB  - Background: Artificial intelligence (AI) is rapidly transforming thoracic surgery by enhancing diagnostic accuracy, surgical precision, intraoperative guidance, and postoperative management. AI-driven technologies, including machine learning (ML), deep learning, computer vision, and robotic-assisted surgery, have the potential to optimize clinical workflows and improve patient outcomes. However, challenges such as data integration, ethical concerns, and regulatory barriers must be addressed to ensure AI’s safe and effective implementation. This review aims to analyze the current applications, benefits, limitations, and future directions of AI in thoracic surgery. Methods: This review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A comprehensive literature search was performed using PubMed, Scopus, Web of Science, and Cochrane Library for studies published up to January 2025. Relevant articles were selected based on predefined inclusion and exclusion criteria, focusing on AI applications in thoracic surgery, including diagnostics, robotic-assisted surgery, intraoperative guidance, and postoperative care. A risk of bias assessment was conducted using the Cochrane Risk of Bias Tool and ROBINS-I for non-randomized studies. Results: Out of 279 identified studies, 36 met the inclusion criteria for qualitative synthesis, highlighting AI’s growing role in diagnostic accuracy, surgical precision, intraoperative guidance, and postoperative care in thoracic surgery. AI-driven imaging analysis and radiomics have improved pulmonary nodule detection, lung cancer classification, and lymph node metastasis prediction, while robotic-assisted thoracic surgery (RATS) has enhanced surgical accuracy, reduced operative times, and improved recovery rates. Intraoperatively, AI-powered image-guided navigation, augmented reality (AR), and real-time decision-support systems have optimized surgical planning and safety. Postoperatively, AI-driven predictive models and wearable monitoring devices have enabled early complication detection and improved patient follow-up. However, challenges remain, including algorithmic biases, a lack of multicenter validation, high implementation costs, and ethical concerns regarding data security and clinical accountability. Despite these limitations, AI has shown significant potential to enhance surgical outcomes, requiring further research and standardized validation for widespread adoption. Conclusions: AI is poised to revolutionize thoracic surgery by enhancing decision-making, improving patient outcomes, and optimizing surgical workflows. However, widespread adoption requires addressing key limitations through multicenter validation studies, standardized AI frameworks, and ethical AI governance. Future research should focus on digital twin technology, federated learning, and explainable AI (XAI) to improve AI interpretability, reliability, and accessibility. With continued advancements and responsible integration, AI will play a pivotal role in shaping the next generation of precision thoracic surgery.
UR  - https://www.proquest.com/docview/3194615852?accountid=15181&bdid=109692&_bd=Zp7jB%2Fz30Fzvbbrn4SlHF2jzMwU%3D
DO  - https://doi.org/10.3390/jcm14082729
ER  - 

TY  - Scholarly Journals
T1  - Artificial Intelligence vs. Efficient Markets: A Critical Reassessment of Predictive Models in the Big Data Era
AU  - Pagliaro, Antonio
JF  - Electronics
VL  - 14
IS  - 9
Y1  - 2025-01-01
DA  - 2025
SP  - 1721
SN  - 20799292
AB  - This paper critically examines artificial intelligence applications in stock market forecasting, addressing significant gaps in the existing literature that often overlook the tension between theoretical market efficiency and empirical predictability. While numerous reviews catalog methodologies, they frequently fail to rigorously evaluate model performance across different market regimes or reconcile statistical significance with economic relevance. We analyze techniques ranging from traditional statistical models to advanced deep learning architectures, finding that ensemble methods like Extra Trees, Random Forest, and XGBoost consistently outperform single classifiers, achieving directional accuracy of up to 86% in specific market conditions. Our analysis reveals that hybrid approaches integrating multiple data sources demonstrate superior performance by capturing complementary market signals, yet many models showing statistical significance fail to generate economic value after accounting for transaction costs and market impact. By addressing methodological challenges including backtest overfitting, regime changes, and implementation constraints, we provide a novel comprehensive framework for rigorous model assessment that bridges the divide between academic research and practical implementation. This review makes three key contributions: (1) a reconciliation of the Efficient Market Hypothesis with AI-driven predictability through an adaptive market framework, (2) a multi-dimensional evaluation methodology that extends beyond classification accuracy to financial performance, and (3) an identification of promising research directions in explainable AI, transfer learning, causal modeling, and privacy-preserving techniques that address current limitations.
UR  - https://www.proquest.com/docview/3203194329?accountid=15181&bdid=109692&_bd=McnNSs8SEAj5aaJop3fQgpoD8Is%3D
DO  - https://doi.org/10.3390/electronics14091721
ER  - 

TY  - Scholarly Journals
T1  - Explainability and causability in digital pathology
AU  - Plass, Markus
AU  - Kargl, Michaela
AU  - Kiehl, Tim‐Rasmus
AU  - Regitnig, Peter
AU  - Geißler, Christian
AU  - Evans, Theodore
AU  - Zerbe, Norman
AU  - Carvalho, Rita
AU  - Holzinger, Andreas
AU  - Müller, Heimo
JF  - The Journal of Pathology: Clinical Research
VL  - 9
IS  - 4
Y1  - 2023-07-01
DA  - Jul 1, 2023
SP  - 251
EP  - 260
U1  - 251-260
SN  - 20564538
AB  - The current move towards digital pathology enables pathologists to use artificial intelligence (AI)‐based computer programmes for the advanced analysis of whole slide images. However, currently, the best‐performing AI algorithms for image analysis are deemed black boxes since it remains – even to their developers – often unclear why the algorithm delivered a particular result. Especially in medicine, a better understanding of algorithmic decisions is essential to avoid mistakes and adverse effects on patients. This review article aims to provide medical experts with insights on the issue of explainability in digital pathology. A short introduction to the relevant underlying core concepts of machine learning shall nurture the reader's understanding of why explainability is a specific issue in this field. Addressing this issue of explainability, the rapidly evolving research field of explainable AI (XAI) has developed many techniques and methods to make black‐box machine‐learning systems more transparent. These XAI methods are a first step towards making black‐box AI systems understandable by humans. However, we argue that an explanation interface must complement these explainable models to make their results useful to human stakeholders and achieve a high level of causability, i.e. a high level of causal understanding by the user. This is especially relevant in the medical field since explainability and causability play a crucial role also for compliance with regulatory requirements. We conclude by promoting the need for novel user interfaces for AI applications in pathology, which enable contextual understanding and allow the medical expert to ask interactive ‘what‐if’‐questions. In pathology, such user interfaces will not only be important to achieve a high level of causability. They will also be crucial for keeping the human‐in‐the‐loop and bringing medical experts' experience and conceptual knowledge to AI processes.
UR  - https://www.proquest.com/docview/2822158610?accountid=15181&bdid=109692&_bd=y%2F%2BmCjr7X%2FpgOh%2BZMujVBNLOh7g%3D
DO  - https://doi.org/10.1002/cjp2.322
ER  - 

TY  - Scholarly Journals
T1  - Transformative impact of explainable artificial intelligence: bridging complexity and trust
JF  - Discover Artificial Intelligence
VL  - 5
IS  - 1
Y1  - 2025-12-01
DA  - Dec 2025
SP  - 51
U1  - 51
SN  - 27310809
AB  - Artificial Intelligence and Deep Learning have gained widespread popularity in all sectors and industries from healthcare to finance and industrial management. Explainable Artificial Intelligence (XAI) is urgent need to bridge the gap between the needs of society interpretability, and trust while maximizing AI benefits. This review XAI methodologies is presented as a comprehensive analysis of three different types model including model-specific, model-agnostic, and hybrid, along with their applications. The review discussed sectors of healthcare, finance, and industrial management etc. where XAI can be utilized for better results and gain trust. The generic and prominent key challenges in terms of trade-offs between accuracy and interpretability, the existing scalability issues, and ethical considerations were focused. The paper also discussed future directions, such as domain-specific frameworks interdisciplinary collaborations and standardized evaluation metrics, to be proposed for advancing XAI research and applications. The review highlighted the potential of XAI for upbringing a society equipped with modern AI with precise results, high responsibility and more transparency.
UR  - https://www.proquest.com/docview/3203914903?accountid=15181&bdid=109692&_bd=AFdt8hydtQO42nPjZp%2FXDVKpSDs%3D
DO  - https://doi.org/10.1007/s44163-025-00281-1
ER  - 

TY  - Scholarly Journals
T1  - Towards responsible AI: an implementable blueprint for integrating explainability and social-cognitive frameworks in AI systems
JF  - AI Perspectives
VL  - 7
IS  - 1
Y1  - 2025-12-01
DA  - Dec 2025
SP  - 1
U1  - 1
SN  - 2523398X
AB  - Automation increasingly shapes modern society, requiring artificial intelligence (AI) systems to not only perform complex tasks but also provide clear, actionable explanations of their decisions, especially in high-stakes domains. However, most contemporary AI systems struggle to explain their runtime operations in specific instances, limiting their applicability in contexts demanding stringent outcome justification. Existing approaches have attempted to address this challenge but often fall short in terms of contextual relevance, human cognitive alignment, or scalability. This paper introduces System-of-Systems Machine Learning (SoS-ML) as a novel framework to advance explainable artificial intelligence (XAI) by addressing the limitations of current methods. Drawing from insights in philosophy, cognitive science, and social sciences, SoS-ML seeks to integrate human-like reasoning processes into AI, framing explanations as contextual inferences and justifications. The research demonstrates how SoS-ML addresses key challenges in XAI, such as enhancing explanation accuracy and aligning AI reasoning with human cognition. By leveraging a multi-agent, modular design, SoS-ML encourages collaboration among machine learning models, leading to more transparent, context-aware systems. The framework’s ability to generalize across domains is demonstrated through experiments on the Pima Indian Diabetes dataset and pie chart image-to-text interpretation, showcasing its transformative potential in improving both model accuracy and explainability. The findings emphasize SoS-ML’s role in advancing responsible AI, particularly in high-stakes environments where interpretability and social accountability are paramount.
UR  - https://www.proquest.com/docview/3156275077?accountid=15181&bdid=109692&_bd=PH8s3t5Hj2xufx%2FAqfxOYf1f%2FFo%3D
DO  - https://doi.org/10.1186/s42467-024-00016-5
ER  - 

TY  - Scholarly Journals
T1  - Explainable AI: A Neurally-Inspired Decision Stack Framework
AU  - Muhammad Salar Khan
AU  - Muhammad Salar Khan
AU  - Nayebpour, Mehdi
AU  - Meng-Hao, Li
AU  - El-Amine, Hadi
AU  - Koizumi, Naoru
AU  - Olds, James L
JF  - Biomimetics
VL  - 7
IS  - 3
Y1  - 2022-01-01
DA  - 2022
SP  - 127
SN  - 23137673
AB  - European law now requires AI to be explainable in the context of adverse decisions affecting the European Union (EU) citizens. At the same time, we expect increasing instances of AI failure as it operates on imperfect data. This paper puts forward a neurally inspired theoretical framework called “decision stacks” that can provide a way forward in research to develop Explainable Artificial Intelligence (X-AI). By leveraging findings from the finest memory systems in biological brains, the decision stack framework operationalizes the definition of explainability. It then proposes a test that can potentially reveal how a given AI decision was made.
UR  - https://www.proquest.com/docview/2716501657?accountid=15181&bdid=109692&_bd=ssjIaMg6hT8Ual3ZRRMf0adnpq4%3D
DO  - https://doi.org/10.3390/biomimetics7030127
ER  - 

TY  - Scholarly Journals
T1  - Cultural Bias in Explainable AI Research: A Systematic Analysis
AU  - Peters, Uwe
AU  - Carman, Mary
JF  - The Journal of Artificial Intelligence Research
VL  - 79
Y1  - 2024-01-01
DA  - 2024
SP  - 971
U1  - 971-1000
SN  - 10769757
AB  - For synergistic interactions between humans and artificial intelligence (AI) systems, AI outputs often need to be explainable to people. Explainable AI (XAI) systems are commonly tested in human user studies. However, whether XAI researchers consider potential cultural differences in human explanatory needs remains unexplored. We highlight psychological research that found significant differences in human explanations between many people from Western, commonly individualist countries and people from non-Western, often collectivist countries. We argue that XAI research currently overlooks these variations and that many popular XAI designs implicitly and problematically assume that Western explanatory needs are shared cross-culturally. Additionally, we systematically reviewed over 200 XAI user studies and found that most studies did not consider relevant cultural variations, sampled only Western populations, but drew conclusions about human-XAI interactions more generally. We also analyzed over 30 literature reviews of XAI studies. Most reviews did not mention cultural differences in explanatory needs or flag overly broad cross-cultural extrapolations of XAI user study results. Combined, our analyses provide evidence of a cultural bias toward Western populations in XAI research, highlighting an important knowledge gap regarding how culturally diverse users may respond to widely used XAI systems that future work can and should address.
UR  - https://www.proquest.com/docview/3184965217?accountid=15181&bdid=109692&_bd=w%2F2wxXIcRhGdzhBmuDAVJDI38qY%3D
DO  - https://doi.org/10.1613/jair.1.14888
ER  - 

TY  - Scholarly Journals
T1  - Explainable Artificial Intelligence to Support Work Safety in Forestry: Insights from Two Large Datasets, Open Challenges, and Future Work
AU  - Hoenigsberger, Ferdinand
AU  - Hoenigsberger, Ferdinand
AU  - Saranti, Anna
AU  - Saranti, Anna
AU  - Jalali, Anahid
AU  - Stampfer, Karl
AU  - Holzinger, Andreas
AU  - Holzinger, Andreas
JF  - Applied Sciences
VL  - 14
IS  - 9
Y1  - 2024-01-01
DA  - 2024
SP  - 3911
SN  - 20763417
AB  - Forestry work, which is considered one of the most demanding and dangerous professions in the world, is claiming more and more lives. In a country as small as Austria, more than 50 forestry workers are killed in accidents every year, and the number is increasing rapidly. This serves as a catalyst for us to implement more stringent measures for workplace safety in order to achieve the sustainability objective of SDG 3, which focuses on health and well-being. This study contributes to the analysis of occupational accidents and focuses on two large real-world datasets from both the Austrian Federal Forests (ÖBf) and the Austrian Workers’ Compensation Board (AUVA). Decision trees, random forests, and fully connected neural networks are used for the analysis. By exploring different interpretation methods, this study sheds light on the decision-making processes ranging from basic association to causal inference and emphasizes the importance of causal inference in providing actionable insights for accident prevention. This paper contributes to the topic of explainable AI, specifically in its application to occupational safety in forestry. As a result, it introduces novel aspects to decision support systems in this application domain.
UR  - https://www.proquest.com/docview/3053127898?accountid=15181&bdid=109692&_bd=rA4Rhvo%2FYDKaQBq9JppQa%2BGoZ7M%3D
DO  - https://doi.org/10.3390/app14093911
ER  - 

TY  - Scholarly Journals
T1  - Time-Based Stress and Procedural Justice: Can Transparency Mitigate the Effects of Algorithmic Compensation in Gig Work?
AU  - Semujanga, Benjamin
AU  - Parent-Rocheleau, Xavier
JF  - International Journal of Environmental Research and Public Health
VL  - 21
IS  - 1
Y1  - 2024-01-01
DA  - 2024
SP  - 86
SN  - 1661-7827
AB  - The gig economy has led to a new management style, using algorithms to automate managerial decisions. Algorithmic management has aroused the interest of researchers, particularly regarding the prevalence of precarious working conditions and the health issues related to gig work. Despite algorithmically driven remuneration mechanisms’ influence on work conditions, few studies have focused on the compensation dimension of algorithmic management. We investigate the effects of algorithmic compensation on gig workers in relation to perceptions of procedural justice and time-based stress, two important predictors of work-related health problems. Also, this study examines the moderating effect of algorithmic transparency in these relationships. Survey data were collected from 962 gig workers via a research panel. The results of hierarchical multiple regression analysis show that the degree of exposure to algorithmic compensation is positively related to time-based stress. However, contrary to our expectations, algorithmic compensation is also positively associated with procedural justice perceptions and our results indicate that this relation is enhanced at higher levels of perceived algorithmic transparency. Furthermore, transparency does not play a role in the relationship between algorithmic compensation and time-based stress. These findings suggest that perceived algorithmic transparency makes algorithmic compensation even fairer but does not appear to make it less stressful.
UR  - https://www.proquest.com/docview/2918765355?accountid=15181&bdid=109692&_bd=qL3Y4tRvzhOGJurPBnMyPOuKoFU%3D
DO  - https://doi.org/10.3390/ijerph21010086
ER  - 

TY  - Scholarly Journals
T1  - Explainable AI in Diagnostic Radiology for Neurological Disorders: A Systematic Review, and What Doctors Think About It
AU  - Hafeez, Yasir
AU  - Memon, Khuhed
AU  - AL-Quraishi, Maged S
AU  - Yahya, Norashikin
AU  - Elferik, Sami
AU  - Syed Saad Azhar Ali
AU  - Syed Saad Azhar Ali
JF  - Diagnostics
VL  - 15
IS  - 2
Y1  - 2025-01-01
DA  - 2025
SP  - 168
SN  - 20754418
AB  - Background: Artificial intelligence (AI) has recently made unprecedented contributions in every walk of life, but it has not been able to work its way into diagnostic medicine and standard clinical practice yet. Although data scientists, researchers, and medical experts have been working in the direction of designing and developing computer aided diagnosis (CAD) tools to serve as assistants to doctors, their large-scale adoption and integration into the healthcare system still seems far-fetched. Diagnostic radiology is no exception. Imagining techniques like magnetic resonance imaging (MRI), computed tomography (CT), and positron emission tomography (PET) scans have been widely and very effectively employed by radiologists and neurologists for the differential diagnoses of neurological disorders for decades, yet no AI-powered systems to analyze such scans have been incorporated into the standard operating procedures of healthcare systems. Why? It is absolutely understandable that in diagnostic medicine, precious human lives are on the line, and hence there is no room even for the tiniest of mistakes. Nevertheless, with the advent of explainable artificial intelligence (XAI), the old-school black boxes of deep learning (DL) systems have been unraveled. Would XAI be the turning point for medical experts to finally embrace AI in diagnostic radiology? This review is a humble endeavor to find the answers to these questions. Methods: In this review, we present the journey and contributions of AI in developing systems to recognize, preprocess, and analyze brain MRI scans for differential diagnoses of various neurological disorders, with special emphasis on CAD systems embedded with explainability. A comprehensive review of the literature from 2017 to 2024 was conducted using host databases. We also present medical domain experts’ opinions and summarize the challenges up ahead that need to be addressed in order to fully exploit the tremendous potential of XAI in its application to medical diagnostics and serve humanity. Results: Forty-seven studies were summarized and tabulated with information about the XAI technology and datasets employed, along with performance accuracies. The strengths and weaknesses of the studies have also been discussed. In addition, the opinions of seven medical experts from around the world have been presented to guide engineers and data scientists in developing such CAD tools. Conclusions: Current CAD research was observed to be focused on the enhancement of the performance accuracies of the DL regimens, with less attention being paid to the authenticity and usefulness of explanations. A shortage of ground truth data for explainability was also observed. Visual explanation methods were found to dominate; however, they might not be enough, and more thorough and human professor-like explanations would be required to build the trust of healthcare professionals. Special attention to these factors along with the legal, ethical, safety, and security issues can bridge the current gap between XAI and routine clinical practice.
UR  - https://www.proquest.com/docview/3159473733?accountid=15181&bdid=109692&_bd=LqMLJ6TfS4gHNm%2FAa72aWQz%2Bf0s%3D
DO  - https://doi.org/10.3390/diagnostics15020168
ER  - 

TY  - Scholarly Journals
T1  - Predicting and understanding human action decisions during skillful joint-action using supervised machine learning and explainable-AI
AU  - Auletta, Fabrizia
AU  - Kallen, Rachel W.
AU  - di Bernardo, Mario
AU  - Richardson, Michael J.
JF  - Scientific Reports (Nature Publisher Group)
VL  - 13
IS  - 1
Y1  - 2023-01-01
DA  - 2023
SP  - 4992
U1  - 4992
SN  - 20452322
AB  - This study investigated the utility of supervised machine learning (SML) and explainable artificial intelligence (AI) techniques for modeling and understanding human decision-making during multiagent task performance. Long short-term memory (LSTM) networks were trained to predict the target selection decisions of expert and novice players completing a multiagent herding task. The results revealed that the trained LSTM models could not only accurately predict the target selection decisions of expert and novice players but that these predictions could be made at timescales that preceded a player’s conscious intent. Importantly, the models were also expertise specific, in that models trained to predict the target selection decisions of experts could not accurately predict the target selection decisions of novices (and vice versa). To understand what differentiated expert and novice target selection decisions, we employed the explainable-AI technique, SHapley Additive explanation (SHAP), to identify what informational features (variables) most influenced modelpredictions. The SHAP analysis revealed that experts were more reliant on information about target direction of heading and the location of coherders (i.e., other players) compared to novices. The implications and assumptions underlying the use of SML and explainable-AI techniques for investigating and understanding human decision-making are discussed.
UR  - https://www.proquest.com/docview/2791456878?accountid=15181&bdid=109692&_bd=RBRIRzfrkz4tBjfmvu%2BgyFONFD4%3D
DO  - https://doi.org/10.1038/s41598-023-31807-1
ER  - 

TY  - Scholarly Journals
T1  - Uncertainty in XAI: Human Perception and Modeling Approaches
AU  - Chiaburu, Teodor
AU  - Haußer, Frank
AU  - Bießmann, Felix
AU  - Bießmann, Felix
JF  - Machine Learning and Knowledge Extraction
VL  - 6
IS  - 2
Y1  - 2024-01-01
DA  - 2024
SP  - 1170
SN  - 25044990
AB  - Artificial Intelligence (AI) plays an increasingly integral role in decision-making processes. In order to foster trust in AI predictions, many approaches towards explainable AI (XAI) have been developed and evaluated. Surprisingly, one factor that is essential for trust has been underrepresented in XAI research so far: uncertainty, both with respect to how it is modeled in Machine Learning (ML) and XAI as well as how it is perceived by humans relying on AI assistance. This review paper provides an in-depth analysis of both aspects. We review established and recent methods to account for uncertainty in ML models and XAI approaches and we discuss empirical evidence on how model uncertainty is perceived by human users of XAI systems. We summarize the methodological advancements and limitations of methods and human perception. Finally, we discuss the implications of the current state of the art in model development and research on human perception. We believe highlighting the role of uncertainty in XAI will be helpful to both practitioners and researchers and could ultimately support more responsible use of AI in practical applications.
UR  - https://www.proquest.com/docview/3072381028?accountid=15181&bdid=109692&_bd=RzuaPTf59sMkaCFCz1OG%2B29ZBUk%3D
DO  - https://doi.org/10.3390/make6020055
ER  - 

TY  - Scholarly Journals
T1  - Explainable Multi-Layer Dynamic Ensemble Framework Optimized for Depression Detection and Severity Assessment
AU  - Imans, Dillan
AU  - Abuhmed, Tamer
AU  - Abuhmed, Tamer
AU  - Alharbi, Meshal
AU  - El-Sappagh, Shaker
JF  - Diagnostics
VL  - 14
IS  - 21
Y1  - 2024-01-01
DA  - 2024
SP  - 2385
SN  - 20754418
AB  - Background: Depression is a pervasive mental health condition, particularly affecting older adults, where early detection and intervention are essential to mitigate its impact. This study presents an explainable multi-layer dynamic ensemble framework designed to detect depression and assess its severity, aiming to improve diagnostic precision and provide insights into contributing health factors. Methods: Using data from the National Social Life, Health, and Aging Project (NSHAP), this framework combines classical machine learning models, static ensemble methods, and dynamic ensemble selection (DES) approaches across two stages: detection and severity prediction. The depression detection stage classifies individuals as normal or depressed, while the severity prediction stage further classifies depressed cases as mild or moderate-severe. Finally, a confirmation depression scale prediction model estimates depression severity scores to support the two stages. Explainable AI (XAI) techniques are applied to improve model interpretability, making the framework more suitable for clinical applications. Results: The framework’s FIRE-KNOP DES algorithm demonstrated high efficacy, achieving 88.33% accuracy in depression detection and 83.68% in severity prediction. XAI analysis identified mental and non-mental health indicators as significant factors in the framework’s performance, emphasizing the value of these features for accurate depression assessment. Conclusions: This study emphasizes the potential of dynamic ensemble learning in mental health assessments, particularly in detecting and evaluating depression severity. The findings provide a strong foundation for future use of dynamic ensemble frameworks in mental health assessments, demonstrating their potential for practical clinical applications.
UR  - https://www.proquest.com/docview/3125991163?accountid=15181&bdid=109692&_bd=01xU53Xx2REblIpUIF2pLwJ9LeQ%3D
DO  - https://doi.org/10.3390/diagnostics14212385
ER  - 

TY  - Scholarly Journals
T1  - An Overview of the Empirical Evaluation of Explainable AI (XAI): A Comprehensive Guideline for User-Centered Evaluation in XAI
AU  - Sidra Naveed
AU  - Sidra Naveed
AU  - Stevens, Gunnar
AU  - Robin-Kern, Dean
JF  - Applied Sciences
VL  - 14
IS  - 23
Y1  - 2024-01-01
DA  - 2024
SP  - 11288
SN  - 20763417
AB  - Recent advances in technology have propelled Artificial Intelligence (AI) into a crucial role in everyday life, enhancing human performance through sophisticated models and algorithms. However, the focus on predictive accuracy has often resulted in opaque black-box models that lack transparency in decision-making. To address this issue, significant efforts have been made to develop explainable AI (XAI) systems that make outcomes comprehensible to users. Various approaches, including new concepts, models, and user interfaces, aim to improve explainability, build user trust, enhance satisfaction, and increase task performance. Evaluation research has emerged to define and measure the quality of these explanations, differentiating between formal evaluation methods and empirical approaches that utilize techniques from psychology and human–computer interaction. Despite the importance of empirical studies, evaluations remain underutilized, with literature reviews indicating a lack of rigorous evaluations from the user perspective. This review aims to guide researchers and practitioners in conducting effective empirical user-centered evaluations by analyzing several studies; categorizing their objectives, scope, and evaluation metrics; and offering an orientation map for research design and metric measurement.
UR  - https://www.proquest.com/docview/3143949724?accountid=15181&bdid=109692&_bd=5GY0Bnx6%2FYC%2F9lmrDUCvFW46HMs%3D
DO  - https://doi.org/10.3390/app142311288
ER  - 

TY  - Scholarly Journals
T1  - Explainable deep learning model for predicting money laundering transactions
AU  - Kute, Dattatray Vishnu
AU  - Pradhan, Biswajeet
AU  - Shukla, Nagesh
AU  - Alamri, Abdullah
JF  - International Journal on Smart Sensing and Intelligent Systems
IS  - 1
Y1  - 2024-01-01
DA  - 2024
SN  - 11785608
AB  - Money laundering has been a global issue for decades. The ever-changing technology landscape, digital channels, and regulations make it increasingly difficult. Financial institutions use rule-based systems to detect suspicious money laundering transactions. However, it suffers from large false positives (FPs) that lead to operational efforts or misses on true positives (TPs) that increase the compliance risk. This paper presents a study of convolutional neural network (CNN) to predict money laundering and employs SHapley Additive exPlanations (SHAP) explainable artificial intelligence (AI) method to explain the CNN predictions. The results highlight the role of CNN in detecting suspicious transactions with high accuracy and SHAP’s role in bringing out the rationale of deep learning predictions.
UR  - https://www.proquest.com/docview/3134903132?accountid=15181&bdid=109692&_bd=g3HGGAMyF5TxsbB9jSuZpslFzD0%3D
DO  - https://doi.org/10.2478/ijssis-2024-0027
ER  - 

TY  - Scholarly Journals
T1  - XAI Helps in Storm Surge Forecasts: A Case Study for the Southeastern Chinese Coasts
AU  - Han, Lei
AU  - Lu, Wenfang
AU  - Dong Changming
JF  - Journal of Marine Science and Engineering
VL  - 13
IS  - 5
Y1  - 2025-01-01
DA  - 2025
SP  - 896
SN  - 20771312
AB  - Storm surge forecasting presents a significant challenge for coastal resilience, particularly in typhoon-prone regions such as southeastern China, where compound flooding events lead to substantial socioeconomic losses. Although artificial intelligence (AI) models have shown strong potential in storm surge prediction, their inherent “black-box” nature limits both their interpretability and operational trust. In this study, we integrate a Vision Transformer (ViT) model with an explainable AI (XAI) method—specifically, Shapley value analysis (SHAP)—to develop an interpretable, high-performance storm surge forecasting framework. The baseline ViT model demonstrates excellent predictive skill, achieving spatiotemporal correlation coefficients exceeding 0.90 over a 12 h lead time. However, it exhibits systematic underestimations in topographically complex regions, such as semi-enclosed bays (e.g., up to 0.06 m). SHAP analysis reveals that the model primarily relies on the autocorrelation of historical surge levels rather than external wind forcing—contrary to the conventional physical understanding of storm surge dynamics. Guided by these insights, we introduce the surge time difference (ΔZ/Δt) as an explicit input feature to enhance the model’s physical representation. This modification yields substantial improvements: during the critical first hour of forecasting—a key window for disaster mitigation—the RMSE is reduced from 0.01 m to 0.005 m, while the correlation coefficient increases from 0.92 to 0.98. This study bridges the gap between data-driven forecasting and physical interpretability, offering a transparent and trustworthy framework for next-generation intelligent storm surge prediction.
UR  - https://www.proquest.com/docview/3212028175?accountid=15181&bdid=109692&_bd=P7LWSsMP5uIoyInv7BL6g8yHt4w%3D
DO  - https://doi.org/10.3390/jmse13050896
ER  - 

TY  - Scholarly Journals
T1  - A Review of AI-Based Cyber-Attack Detection and Mitigation in Microgrids
AU  - Beg, Omar A
AU  - Beg, Omar A
AU  - Beg, Omar A
AU  - Asad Ali Khan
AU  - Asad Ali Khan
AU  - Waqas Ur Rehman
AU  - Hassan, Ali
JF  - Energies
VL  - 16
IS  - 22
Y1  - 2023-01-01
DA  - 2023
SP  - 7644
SN  - 19961073
AB  - In this paper, the application and future vision of Artificial Intelligence (AI)-based techniques in microgrids are presented from a cyber-security perspective of physical devices and communication networks. The vulnerabilities of microgrids are investigated under a variety of cyber-attacks targeting sensor measurements, control signals, and information sharing. With the inclusion of communication networks and smart metering devices, the attack surface has increased in microgrids, making them vulnerable to various cyber-attacks. The negative impact of such attacks may render the microgrids out-of-service, and the attacks may propagate throughout the network due to the absence of efficient mitigation approaches.  AI-based techniques are being employed to tackle such data-driven cyber-attacks due to their exceptional pattern recognition and learning capabilities. AI-based methods for cyber-attack detection and mitigation that address the cyber-attacks in microgrids are summarized. A case study is presented showing the performance of AI-based cyber-attack mitigation in a distributed cooperative control-based AC microgrid. Finally, future potential research directions are provided that include the application of transfer learning and explainable AI techniques to increase the trust of AI-based models in the microgrid domain.
UR  - https://www.proquest.com/docview/2893048056?accountid=15181&bdid=109692&_bd=g5H8ivFLtBvkghC00uyvsB92iek%3D
DO  - https://doi.org/10.3390/en16227644
ER  - 

TY  - Scholarly Journals
T1  - An Exploration of Ethical Decision Making with Intelligence Augmentation
JF  - Social Sciences
VL  - 10
IS  - 2
Y1  - 2021-01-01
DA  - 2021
SP  - 57
SN  - 20760760
AB  - In recent years, the use of Artificial Intelligence agents to augment and enhance the operational decision making of human agents has increased. This has delivered real benefits in terms of improved service quality, delivery of more personalised services, reduction in processing time, and more efficient allocation of resources, amongst others. However, it has also raised issues which have real-world ethical implications such as recommending different credit outcomes for individuals who have an identical financial profile but different characteristics (e.g., gender, race). The popular press has highlighted several high-profile cases of algorithmic discrimination and the issue has gained traction. While both the fields of ethical decision making and Explainable AI (XAI) have been extensively researched, as yet we are not aware of any studies which have examined the process of ethical decision making with Intelligence augmentation (IA). We aim to address that gap with this study. We amalgamate the literature in both fields of research and propose, but not attempt to validate empirically, propositions and belief statements based on the synthesis of the existing literature, observation, logic, and empirical analogy. We aim to test these propositions in future studies.
UR  - https://www.proquest.com/docview/2488850750?accountid=15181&bdid=109692&_bd=nnjsfJajLSyKj2kVXX8o6S3YlXY%3D
DO  - https://doi.org/10.3390/socsci10020057
ER  - 

TY  - Scholarly Journals
T1  - Inhibitors and Enablers to Explainable AI Success: A Systematic Examination of Explanation Complexity and Individual Characteristics
AU  - Wienrich, Carolin
AU  - Wienrich, Carolin
AU  - Carolus, Astrid
AU  - Roth-Isigkeit, David
AU  - Hotho, Andreas
JF  - Multimodal Technologies and Interaction
VL  - 6
IS  - 12
Y1  - 2022-01-01
DA  - 2022
SP  - 106
SN  - 24144088
AB  - With the increasing adaptability and complexity of advisory artificial intelligence (AI)-based agents, the topics of explainable AI and human-centered AI are moving close together. Variations in the explanation itself have been widely studied, with some contradictory results. These could be due to users’ individual differences, which have rarely been systematically studied regarding their inhibiting or enabling effect on the fulfillment of explanation objectives (such as trust, understanding, or workload). This paper aims to shed light on the significance of human dimensions (gender, age, trust disposition, need for cognition, affinity for technology, self-efficacy, attitudes, and mind attribution) as well as their interplay with different explanation modes (no, simple, or complex explanation). Participants played the game Deal or No Deal while interacting with an AI-based agent. The agent gave advice to the participants on whether they should accept or reject the deals offered to them. As expected, giving an explanation had a positive influence on the explanation objectives. However, the users’ individual characteristics particularly reinforced the fulfillment of the objectives. The strongest predictor of objective fulfillment was the degree of attribution of human characteristics. The more human characteristics were attributed, the more trust was placed in the agent, advice was more likely to be accepted and understood, and important needs were satisfied during the interaction. Thus, the current work contributes to a better understanding of the design of explanations of an AI-based agent system that takes into account individual characteristics and meets the demand for both explainable and human-centered agent systems.
UR  - https://www.proquest.com/docview/2756751772?accountid=15181&bdid=109692&_bd=iX3C1b3sxYnPp7FDweLAPKFIJo0%3D
DO  - https://doi.org/10.3390/mti6120106
ER  - 

TY  - Scholarly Journals
T1  - A time-sensitive learning-to-rank approach for cloud simulation resource prediction
AU  - Xiao, Yuhao
AU  - Yao, Yiping
AU  - Chen, Kai
AU  - Tang, Wenjie
AU  - Zhu, Feng
JF  - Complex & Intelligent Systems
VL  - 9
IS  - 5
Y1  - 2023-10-01
DA  - Oct 2023
SP  - 5731
EP  - 5744
U1  - 5731-5744
SN  - 21994536
AB  - Predicting the computing resources required by simulation applications can provide a more reasonable resource-allocation scheme for efficient execution. Existing prediction methods based on machine learning, such as classification/regression, typically must accurately predict the runtime of simulation applications and select the optimal computing resource allocation scheme by sorting the length of the simulation runtime. However, the ranking results are easily affected by the simulation runtime prediction accuracy. This study proposes a time-sensitive learning-to-rank (LTR) approach for cloud simulations resource prediction. First, we use the Shapley additive explanation (SHAP) value from the field of explainable artificial intelligence (XAI) to analyze the impact of relevant factors on the simulation runtime and to extract the feature dimensions that significantly affect the simulation runtime. Second, by modifying the target loss function of the rankboost algorithm and training a time-sensitive LTR model based on simulation features, we can accurately predict the computing resource allocation scheme that maximizes the execution efficiency of simulation applications. Compared with the traditional machine learning prediction algorithm, the proposed method can improve the average sorting performance by 3%–48% and can accurately predict the computing resources required for the simulation applications to execute in the shortest amount of time.
UR  - https://www.proquest.com/docview/2867416591?accountid=15181&bdid=109692&_bd=1EZsXiysRKUvNstx0Tlqfbsyu2M%3D
DO  - https://doi.org/10.1007/s40747-023-01045-z
ER  - 

TY  - Scholarly Journals
T1  - Federated Learning of Explainable AI Models in 6G Systems: Towards Secure and Automated Vehicle Networking
AU  - Renda, Alessandro
AU  - Renda, Alessandro
AU  - Ducange, Pietro
AU  - Marcelloni, Francesco
AU  - Sabella, Dario
AU  - Filippou, Miltiadis C
AU  - Nardini, Giovanni
AU  - Stea, Giovanni
AU  - Virdis, Antonio
AU  - Micheli, Davide
AU  - Rapone, Damiano
AU  - Leonardo Gomes Baltar
JF  - Information
VL  - 13
IS  - 8
Y1  - 2022-01-01
DA  - 2022
SP  - 395
SN  - 20782489
AB  - This article presents the concept of federated learning (FL) of eXplainable Artificial Intelligence (XAI) models as an enabling technology in advanced 5G towards 6G systems and discusses its applicability to the automated vehicle networking use case. Although the FL of neural networks has been widely investigated exploiting variants of stochastic gradient descent as the optimization method, it has not yet been adequately studied in the context of inherently explainable models. On the one side, XAI permits improving user experience of the offered communication services by helping end users trust (by design) that in-network AI functionality issues appropriate action recommendations. On the other side, FL ensures security and privacy of both vehicular and user data across the whole system. These desiderata are often ignored in existing AI-based solutions for wireless network planning, design and operation. In this perspective, the article provides a detailed description of relevant 6G use cases, with a focus on vehicle-to-everything (V2X) environments: we describe a framework to evaluate the proposed approach involving online training based on real data from live networks. FL of XAI models is expected to bring benefits as a methodology for achieving seamless availability of decentralized, lightweight and communication efficient intelligence. Impacts of the proposed approach (including standardization perspectives) consist in a better trustworthiness of operations, e.g., via explainability of quality of experience (QoE) predictions, along with security and privacy-preserving management of data from sensors, terminals, users and applications.
UR  - https://www.proquest.com/docview/2706241895?accountid=15181&bdid=109692&_bd=UUNL4CvjV0Ih2m5cLs9AVpWltRg%3D
DO  - https://doi.org/10.3390/info13080395
ER  - 

TY  - Scholarly Journals
T1  - Machine Learning and Criminal Justice: A Systematic Review of Advanced Methodology for Recidivism Risk Prediction
AU  - Travaini, Guido Vittorio
AU  - Pacchioni, Federico
AU  - Bellumore, Silvia
AU  - Bosia, Marta
AU  - De Micco, Francesco
AU  - De Micco, Francesco
JF  - International Journal of Environmental Research and Public Health
VL  - 19
IS  - 17
Y1  - 2022-01-01
DA  - 2022
SP  - 10594
SN  - 1661-7827
AB  - Recent evolution in the field of data science has revealed the potential utility of machine learning (ML) applied to criminal justice. Hence, the literature focused on finding better techniques to predict criminal recidivism risk is rapidly flourishing. However, it is difficult to make a state of the art for the application of ML in recidivism prediction. In this systematic review, out of 79 studies from Scopus and PubMed online databases we selected, 12 studies that guarantee the replicability of the models across different datasets and their applicability to recidivism prediction. The different datasets and ML techniques used in each of the 12 studies have been compared using the two selected metrics. This study shows how each method applied achieves good performance, with an average score of 0.81 for ACC and 0.74 for AUC. This systematic review highlights key points that could allow criminal justice professionals to routinely exploit predictions of recidivism risk based on ML techniques. These include the presence of performance metrics, the use of transparent algorithms or explainable artificial intelligence (XAI) techniques, as well as the high quality of input data.
UR  - https://www.proquest.com/docview/2711297291?accountid=15181&bdid=109692&_bd=9ukkkzr4Tp8bWJ9ixhUOQBfwwiI%3D
DO  - https://doi.org/10.3390/ijerph191710594
ER  - 

TY  - Scholarly Journals
T1  - GDPR and Large Language Models: Technical and Legal Obstacles
AU  - Feretzakis Georgios
AU  - Vagena Evangelia
AU  - Kalodanis Konstantinos
AU  - Peristera Paraskevi
AU  - Kalles Dimitris
AU  - Anastasiou Athanasios
JF  - Future Internet
VL  - 17
IS  - 4
Y1  - 2025-01-01
DA  - 2025
SP  - 151
SN  - 19995903
AB  - Large Language Models (LLMs) have revolutionized natural language processing but present significant technical and legal challenges when confronted with the General Data Protection Regulation (GDPR). This paper examines the complexities involved in reconciling the design and operation of LLMs with GDPR requirements. In particular, we analyze how key GDPR provisions—including the Right to Erasure, Right of Access, Right to Rectification, and restrictions on Automated Decision-Making—are challenged by the opaque and distributed nature of LLMs. We discuss issues such as the transformation of personal data into non-interpretable model parameters, difficulties in ensuring transparency and accountability, and the risks of bias and data over-collection. Moreover, the paper explores potential technical solutions such as machine unlearning, explainable AI (XAI), differential privacy, and federated learning, alongside strategies for embedding privacy-by-design principles and automated compliance tools into LLM development. The analysis is further enriched by considering the implications of emerging regulations like the EU’s Artificial Intelligence Act. In addition, we propose a four-layer governance framework that addresses data governance, technical privacy enhancements, continuous compliance monitoring, and explainability and oversight, thereby offering a practical roadmap for GDPR alignment in LLM systems. Through this comprehensive examination, we aim to bridge the gap between the technical capabilities of LLMs and the stringent data protection standards mandated by GDPR, ultimately contributing to more responsible and ethical AI practices.
UR  - https://www.proquest.com/docview/3194606914?accountid=15181&bdid=109692&_bd=tdW5gzDMInWkn%2BqbO1VuSmZbUiU%3D
DO  - https://doi.org/10.3390/fi17040151
ER  - 

TY  - Scholarly Journals
T1  - Explainable AI for Chronic Kidney Disease Prediction in Medical IoT: Integrating GANs and Few-Shot Learning
AU  - Rezk, Nermeen Gamal
AU  - Alshathri Samah
AU  - Sayed Amged
AU  - Hemdan Ezz El-Din
JF  - Bioengineering
VL  - 12
IS  - 4
Y1  - 2025-01-01
DA  - 2025
SP  - 356
SN  - 23065354
AB  - According to recent global public health studies, chronic kidney disease (CKD) is becoming more and more recognized as a serious health risk as many people are suffering from this disease. Machine learning techniques have demonstrated high efficiency in identifying CKD, but their opaque decision-making processes limit their adoption in clinical settings. To address this, this study employs a generative adversarial network (GAN) to handle missing values in CKD datasets and utilizes few-shot learning techniques, such as prototypical networks and model-agnostic meta-learning (MAML), combined with explainable machine learning to predict CKD. Additionally, traditional machine learning models, including support vector machines (SVM), logistic regression (LR), decision trees (DT), random forests (RF), and voting ensemble learning (VEL), are applied for comparison. To unravel the “black box” nature of machine learning predictions, various techniques of explainable AI, such as SHapley Additive exPlanations (SHAP) and local interpretable model-agnostic explanations (LIME), are applied to understand the predictions made by the model, thereby contributing to the decision-making process and identifying significant parameters in the diagnosis of CKD. Model performance is evaluated using predefined metrics, and the results indicate that few-shot learning models integrated with GANs significantly outperform traditional machine learning techniques. Prototypical networks with GANs achieve the highest accuracy of 99.99%, while MAML reaches 99.92%. Furthermore, prototypical networks attain F1-score, recall, precision, and Matthews correlation coefficient (MCC) values of 99.89%, 99.9%, 99.9%, and 100%, respectively, on the raw dataset. As a result, the experimental results clearly demonstrate the effectiveness of the suggested method, offering a reliable and trustworthy model to classify CKD. This framework supports the objectives of the Medical Internet of Things (MIoT) by enhancing smart medical applications and services, enabling accurate prediction and detection of CKD, and facilitating optimal medical decision making.
UR  - https://www.proquest.com/docview/3194491714?accountid=15181&bdid=109692&_bd=wKhMUP6b%2B4YeGlW5ldVbfsh01g8%3D
DO  - https://doi.org/10.3390/bioengineering12040356
ER  - 

TY  - Scholarly Journals
T1  - A Survey on Medical Explainable AI (XAI): Recent Progress, Explainability Approach, Human Interaction and Scoring System
AU  - Ruey-Kai Sheu
AU  - Mayuresh Sunil Pardeshi
AU  - Mayuresh Sunil Pardeshi
JF  - Sensors
VL  - 22
IS  - 20
Y1  - 2022-01-01
DA  - 2022
SP  - 8068
SN  - 14248220
AB  - The emerging field of eXplainable AI (XAI) in the medical domain is considered to be of utmost importance. Meanwhile, incorporating explanations in the medical domain with respect to legal and ethical AI is necessary to understand detailed decisions, results, and current status of the patient’s conditions. Successively, we will be presenting a detailed survey for the medical XAI with the model enhancements, evaluation methods, significant overview of case studies with open box architecture, medical open datasets, and future improvements. Potential differences in AI and XAI methods are provided with the recent XAI methods stated as (i) local and global methods for preprocessing, (ii) knowledge base and distillation algorithms, and (iii) interpretable machine learning. XAI characteristics details with future healthcare explainability is included prominently, whereas the pre-requisite provides insights for the brainstorming sessions before beginning a medical XAI project. Practical case study determines the recent XAI progress leading to the advance developments within the medical field. Ultimately, this survey proposes critical ideas surrounding a user-in-the-loop approach, with an emphasis on human–machine collaboration, to better produce explainable solutions. The surrounding details of the XAI feedback system for human rating-based evaluation provides intelligible insights into a constructive method to produce human enforced explanation feedback. For a long time, XAI limitations of the ratings, scores and grading are present. Therefore, a novel XAI recommendation system and XAI scoring system are designed and approached from this work. Additionally, this paper encourages the importance of implementing explainable solutions into the high impact medical field.
UR  - https://www.proquest.com/docview/2728532042?accountid=15181&bdid=109692&_bd=aXj4KtyCNwq%2FvWgftQQLVSiMmvI%3D
DO  - https://doi.org/10.3390/s22208068
ER  - 

TY  - Scholarly Journals
T1  - Harnessing explainable artificial intelligence for patient-to-clinical-trial matching: A proof-of-concept pilot study using phase I oncology trials
AU  - Ghosh, Satanu
AU  - Hassan Mohammed Abushukair
AU  - Ganesan, Arjun
AU  - Pan, Chongle
AU  - Abdul Rafeh Naqash
AU  - Lu, Kun
JF  - PLoS One
VL  - 19
IS  - 10
Y1  - 2024-10-01
DA  - Oct 2024
SP  - e0311510
SN  - 19326203
AB  - This study aims to develop explainable AI methods for matching patients with phase 1 oncology clinical trials using Natural Language Processing (NLP) techniques to address challenges in patient recruitment for improved efficiency in drug development. A prototype system based on modern NLP techniques has been developed to match patient records with phase 1 oncology clinical trial protocols. Four criteria are considered for the matching: cancer type, performance status, genetic mutation, and measurable disease. The system outputs a summary matching score along with explanations of the evidence. The outputs of the AI system were evaluated against the ground truth matching results provided by the domain expert on a dataset of twelve synthesized dummy patient records and six clinical trial protocols. The system achieved a precision of 73.68%, sensitivity/recall of 56%, accuracy of 77.78%, and specificity of 89.36%. Further investigation into the misclassified cases indicated that ambiguity of abbreviation and misunderstanding of context are significant contributors to errors. The system found evidence of no matching for all false positive cases. To the best of our knowledge, no system in the public domain currently deploys an explainable AI-based approach to identify optimal patients for phase 1 oncology trials. This initial attempt to develop an AI system for patients and clinical trial matching in the context of phase 1 oncology trials showed promising results that are set to increase efficiency without sacrificing quality in patient-trial matching.
UR  - https://www.proquest.com/docview/3120495309?accountid=15181&bdid=109692&_bd=PZQo%2B3%2FAZTOtgA0LktYWRDrLgSI%3D
DO  - https://doi.org/10.1371/journal.pone.0311510
ER  - 

TY  - Scholarly Journals
T1  - Robotic Motion Intelligence Using Vector Symbolic Architectures and Blockchain-Based Smart Contracts
AU  - De Silva Daswin
AU  - Withanage Sudheera
AU  - Vidura, Sumanasena
AU  - Gunasekara Lakshitha
AU  - Moraliyage Harsha
AU  - Mills, Nishan
AU  - Manic Milos
JF  - Robotics
VL  - 14
IS  - 4
Y1  - 2025-01-01
DA  - 2025
SP  - 38
SN  - 22186581
AB  - The rapid adoption of artificial intelligence (AI) systems, such as predictive AI, generative AI, and explainable AI, is in contrast to the slower development and uptake of robotic AI systems. Dynamic environments, sensory processing, mechanical movements, power management, and safety are inherent complexities of robotic intelligence capabilities that can be addressed using novel AI approaches. The current AI landscape is dominated by machine learning techniques, specifically deep learning algorithms, that have been effective in addressing some of these challenges. However, these algorithms are subject to computationally complex processing and operational needs such as high data dependency. In this paper, we propose a computation-efficient and data-efficient framework for robotic motion intelligence (RMI) based on vector symbolic architectures (VSAs) and blockchain-based smart contracts. The capabilities of VSAs are leveraged for computationally efficient learning and noise suppression during perception, motion, movement, and decision-making tasks. As a distributed ledger technology, smart contracts address data dependency through a decentralized, distributed, and secure transactions ledger that satisfies contractual conditions. An empirical evaluation of the framework confirms its value and contribution towards addressing the practical challenges of robotic motion intelligence by significantly reducing the learnable parameters by 10 times while preserving sufficient accuracy compared to existing deep learning solutions.
UR  - https://www.proquest.com/docview/3194640158?accountid=15181&bdid=109692&_bd=FXTUtsytybOe5BUQsbspSadj5SU%3D
DO  - https://doi.org/10.3390/robotics14040038
ER  - 

TY  - Scholarly Journals
T1  - On Evaluating Black-Box Explainable AI Methods for Enhancing Anomaly Detection in Autonomous Driving Systems
AU  - Nazat, Sazid
AU  - Arreche, Osvaldo
AU  - Abdallah, Mustafa
AU  - Abdallah, Mustafa
JF  - Sensors
VL  - 24
IS  - 11
Y1  - 2024-01-01
DA  - 2024
SP  - 3515
SN  - 14248220
AB  - The recent advancements in autonomous driving come with the associated cybersecurity issue of compromising networks of autonomous vehicles (AVs), motivating the use of AI models for detecting anomalies on these networks. In this context, the usage of explainable AI (XAI) for explaining the behavior of these anomaly detection AI models is crucial. This work introduces a comprehensive framework to assess black-box XAI techniques for anomaly detection within AVs, facilitating the examination of both global and local XAI methods to elucidate the decisions made by XAI techniques that explain the behavior of AI models classifying anomalous AV behavior. By considering six evaluation metrics (descriptive accuracy, sparsity, stability, efficiency, robustness, and completeness), the framework evaluates two well-known black-box XAI techniques, SHAP and LIME, involving applying XAI techniques to identify primary features crucial for anomaly classification, followed by extensive experiments assessing SHAP and LIME across the six metrics using two prevalent autonomous driving datasets, VeReMi and Sensor. This study advances the deployment of black-box XAI methods for real-world anomaly detection in autonomous driving systems, contributing valuable insights into the strengths and limitations of current black-box XAI methods within this critical domain.
UR  - https://www.proquest.com/docview/3067441161?accountid=15181&bdid=109692&_bd=M5IdhQ0JEwrnxI9r6UgD0BcrFF4%3D
DO  - https://doi.org/10.3390/s24113515
ER  - 

TY  - Scholarly Journals
T1  - Explainable MRI-Based Ensemble Learnable Architecture for Alzheimer’s Disease Detection
AU  - Opeyemi Taiwo Adeniran
AU  - Blessing Ojeme
AU  - Ajibola, Temitope Ezekiel
AU  - Ojonugwa Oluwafemi Ejiga Peter
AU  - Ajala, Abiola Olayinka
AU  - Rahman, Md Mahmudur
AU  - Khalifa, Fahmi
AU  - Khalifa, Fahmi
JF  - Algorithms
VL  - 18
IS  - 3
Y1  - 2025-01-01
DA  - 2025
SP  - 163
SN  - 19994893
AB  - With the advancements in deep learning methods, AI systems now perform at the same or higher level than human intelligence in many complex real-world problems. The data and algorithmic opacity of deep learning models, however, make the task of comprehending the input data information, the model, and model’s decisions quite challenging. This lack of transparency constitutes both a practical and an ethical issue. For the present study, it is a major drawback to the deployment of deep learning methods mandated with detecting patterns and prognosticating Alzheimer’s disease. Many approaches presented in the AI and medical literature for overcoming this critical weakness are sometimes at the cost of sacrificing accuracy for interpretability. This study is an attempt at addressing this challenge and fostering transparency and reliability in AI-driven healthcare solutions. The study explores a few commonly used perturbation-based interpretability (LIME) and gradient-based interpretability (Saliency and Grad-CAM) approaches for visualizing and explaining the dataset, models, and decisions of MRI image-based Alzheimer’s disease identification using the diagnostic and predictive strengths of an ensemble framework comprising Convolutional Neural Networks (CNNs) architectures (Custom multi-classifier CNN, VGG-19, ResNet, MobileNet, EfficientNet, DenseNet), and a Vision Transformer (ViT). The experimental results show the stacking ensemble achieving a remarkable accuracy of 98.0% while the hard voting ensemble reached 97.0%. The findings present a valuable contribution to the growing field of explainable artificial intelligence (XAI) in medical imaging, helping end users and researchers to gain deep understanding of the backstory behind medical image dataset and deep learning model’s decisions.
UR  - https://www.proquest.com/docview/3181337963?accountid=15181&bdid=109692&_bd=DhM513kPVMBcpklCQjzenr6JTtQ%3D
DO  - https://doi.org/10.3390/a18030163
ER  - 

TY  - Scholarly Journals
T1  - Enhancing Explainable Artificial Intelligence: Using Adaptive Feature Weight Genetic Explanation (AFWGE) with Pearson Correlation to Identify Crucial Feature Groups
AU  - AlJalaud, Ebtisam
AU  - AlJalaud, Ebtisam
AU  - Hosny, Manar
JF  - Mathematics
VL  - 12
IS  - 23
Y1  - 2024-01-01
DA  - 2024
SP  - 3727
SN  - 22277390
AB  - The ‘black box’ nature of machine learning (ML) approaches makes it challenging to understand how most artificial intelligence (AI) models make decisions. Explainable AI (XAI) aims to provide analytical techniques to understand the behavior of ML models. XAI utilizes counterfactual explanations that indicate how variations in input features lead to different outputs. However, existing methods must also highlight the importance of features to provide more actionable explanations that would aid in the identification of key drivers behind model decisions—and, hence, more reliable interpretations—ensuring better accuracy. The method we propose utilizes feature weights obtained through adaptive feature weight genetic explanation (AFWGE) with the Pearson correlation coefficient (PCC) to determine the most crucial group of features. The proposed method was tested on four real datasets with nine different classifiers for evaluation against a nonweighted counterfactual explanation method (CERTIFAI) and the original feature values’ correlation. The results show significant enhancements in accuracy, precision, recall, and F1 score for most datasets and classifiers; this indicates the superiority of the feature weights selected via AFWGE with the PCC over CERTIFAI and the original data values in determining the most important group of features. Focusing on important feature groups elaborates the behavior of AI models and enhances decision making, resulting in more reliable AI systems.
UR  - https://www.proquest.com/docview/3144143822?accountid=15181&bdid=109692&_bd=BYb9LZ8%2BBArwf4Eg2L2zJTvxEJk%3D
DO  - https://doi.org/10.3390/math12233727
ER  - 

TY  - Scholarly Journals
T1  - Interpretability Is in the Mind of the Beholder: A Causal Framework for Human-Interpretable Representation Learning
AU  - Marconato, Emanuele
AU  - Passerini, Andrea
AU  - Teso, Stefano
AU  - Teso, Stefano
JF  - Entropy
VL  - 25
IS  - 12
Y1  - 2023-01-01
DA  - 2023
SP  - 1574
SN  - 10994300
AB  - Research on Explainable Artificial Intelligence has recently started exploring the idea of producing explanations that, rather than being expressed in terms of low-level features, are encoded in terms of interpretable concepts learned from data. How to reliably acquire such concepts is, however, still fundamentally unclear. An agreed-upon notion of concept interpretability is missing, with the result that concepts used by both post hoc explainers and concept-based neural networks are acquired through a variety of mutually incompatible strategies. Critically, most of these neglect the human side of the problem: a representation is understandable only insofar as it can be understood by the human at the receiving end. The key challenge in human-interpretable representation learning (hrl) is how to model and operationalize this human element. In this work, we propose a mathematical framework for acquiring interpretable representations suitable for both post hoc explainers and concept-based neural networks. Our formalization of hrl builds on recent advances in causal representation learning and explicitly models a human stakeholder as an external observer. This allows us derive a principled notion of alignment between the machine’s representation and the vocabulary of concepts understood by the human. In doing so, we link alignment and interpretability through a simple and intuitive name transfer game, and clarify the relationship between alignment and a well-known property of representations, namely disentanglement. We also show that alignment is linked to the issue of undesirable correlations among concepts, also known as concept leakage, and to content-style separation, all through a general information-theoretic reformulation of these properties. Our conceptualization aims to bridge the gap between the human and algorithmic sides of interpretability and establish a stepping stone for new research on human-interpretable representations.
UR  - https://www.proquest.com/docview/2904645312?accountid=15181&bdid=109692&_bd=pA9J4JtIwh%2BUpzxUetSEQUkmK6c%3D
DO  - https://doi.org/10.3390/e25121574
ER  - 

TY  - Scholarly Journals
T1  - Rough Set Theory and Soft Computing Methods for Building Explainable and Interpretable AI/ML Models
AU  - Naouali Sami
AU  - El Othmani Oussama
JF  - Applied Sciences
VL  - 15
IS  - 9
Y1  - 2025-01-01
DA  - 2025
SP  - 5148
SN  - 20763417
AB  - This study introduces a novel framework leveraging Rough Set Theory (RST)-based feature selection—MLReduct, MLSpecialReduct, and MLFuzzyRoughSet—to enhance machine learning performance on uncertain data. Applied to a private cardiovascular dataset, our MLSpecialReduct algorithm achieves a peak Random Forest accuracy of 0.99 (versus 0.85 without feature selection), while MLFuzzyRoughSet improves accuracy to 0.83, surpassing our MLVarianceThreshold (0.72–0.77), an adaptation of the traditional VarianceThreshold method. We integrate these RST techniques with preprocessing (discretization, normalization, encoding) and compare them against traditional approaches across classifiers like Random Forest and Naive Bayes. The results underscore RST’s edge in accuracy, efficiency, and interpretability, with MLSpecialReduct leading in minimal attribute reduction. Against baseline classifiers without feature selection and MLVarianceThreshold, our framework delivers significant improvements, establishing RST as a vital tool for explainable AI (XAI) in healthcare diagnostics and IoT systems. These findings open avenues for future hybrid RST-ML models, providing a robust, interpretable solution for complex data challenges.
UR  - https://www.proquest.com/docview/3203188947?accountid=15181&bdid=109692&_bd=URFNFNO%2BfdGFf83zvAUANg%2FKuPU%3D
DO  - https://doi.org/10.3390/app15095148
ER  - 

TY  - Scholarly Journals
T1  - Current Status and Future of Artificial Intelligence in Medicine
AU  - Basubrin Omar
JF  - Cureus
VL  - 17
IS  - 1
Y1  - 2025-01-01
DA  - 2025
SN  - 21688184
AB  - Artificial intelligence (AI) has rapidly emerged as a transformative force in medicine, revolutionizing various aspects of healthcare from diagnostics and treatment to public health and patient care. This narrative review synthesizes evidence from diverse study designs, exploring the current and future applications of AI in medicine. We highlight AI's role in improving diagnostic accuracy, optimizing treatment strategies, and enhancing patient care through personalized interventions and remote monitoring, drawing upon recent advancements and landmark studies. Emerging trends such as explainable AI and federated learning are also examined. While acknowledging the tremendous potential of AI in medicine, the review also addresses the barriers and ethical challenges that need to be overcome, including concerns about algorithmic bias, transparency, over-reliance, and the potential impact on the healthcare workforce. We emphasize the importance of establishing regulatory guidelines, fostering collaboration between clinicians and AI developers, and ensuring ongoing education for healthcare professionals. Despite these challenges, the future of AI in medicine holds immense promise, with the potential to significantly improve patient outcomes, transform healthcare delivery, and address healthcare disparities.
UR  - https://www.proquest.com/docview/3203887145?accountid=15181&bdid=109692&_bd=rcHqxZTro1RevwU24FPEh8ati7g%3D
DO  - https://doi.org/10.7759/cureus.77561
ER  - 

TY  - Scholarly Journals
T1  - Analysing the Effects of Scenario-Based Explanations on Automated Vehicle HMIs from Objective and Subjective Perspectives
AU  - Ma, Jun
AU  - Feng, Xuejing
AU  - Feng, Xuejing
JF  - Sustainability
VL  - 16
IS  - 1
Y1  - 2024-01-01
DA  - 2024
SP  - 63
SN  - 20711050
AB  - Automated vehicles (AVs) are recognized as one of the most effective measures to realize sustainable transport. These vehicles can reduce emissions and environmental pollution, enhance accessibility, improve safety, and produce economic benefits through congestion reduction and cost savings. However, the consumer acceptance of and trust in these vehicles are not ideal, which affects the diffusion speed of AVs on the market. Providing transparent explanations of AV behaviour is a method for building confidence and trust in AV technologies. In this study, we investigated the explainability of user interface information in an Automated Valet Parking (AVP) system—one of the first L4 automated driving systems with a large commercial landing. Specifically, we proposed a scenario-based explanation framework based on explainable AI and examined the effects of these explanations on drivers’ objective and subjective performance. The results of Experiment 1 indicated that the scenario-based explanations effectively improved drivers’ situational trust and user experience (UX), thereby enhancing the perception and understanding that drivers had of the system’s intelligence capabilities. These explanations significantly reduced the mental workload and elevated the user performance in objective evaluations. In Experiment 2, we uncovered distinct explainability preferences among new and frequent users. New users sought increased trust and transparency, benefiting from guided explanations. In contrast, frequent users emphasised efficiency and driving safety. The final experimental results confirmed that solutions customised for different segments of the population are significantly more effective, satisfying, and trustworthy than generic solutions. These findings demonstrate that the explanations for individual differences, based on our proposed scenario-based framework, have significant implications for the adoption and sustainability of AVs.
UR  - https://www.proquest.com/docview/2912827287?accountid=15181&bdid=109692&_bd=xcQTBJRRVy%2BuSnuLe4WgAWb89zw%3D
DO  - https://doi.org/10.3390/su16010063
ER  - 

TY  - Scholarly Journals
T1  - AI Advances in ICU with an Emphasis on Sepsis Prediction: An Overview
AU  - Stylianides, Charithea
AU  - Stylianides, Charithea
AU  - Nicolaou, Andria
AU  - Waqar Aziz Sulaiman
AU  - Christina-Athanasia Alexandropoulou
AU  - Panagiotopoulos, Ilias
AU  - Karathanasopoulou, Konstantina
AU  - Dimitrakopoulos, George
AU  - Kleanthous, Styliani
AU  - Politi, Eleni
AU  - Ntalaperas, Dimitris
AU  - Papageorgiou, Xanthi
AU  - Garcia, Fransisco
AU  - Antoniou, Zinonas
AU  - Ioannides, Nikos
AU  - Palazis, Lakis
AU  - Vavlitou, Anna
AU  - Pattichis, Marios S
AU  - Pattichis, Constantinos S
AU  - Panayides, Andreas S
JF  - Machine Learning and Knowledge Extraction
VL  - 7
IS  - 1
Y1  - 2025-01-01
DA  - 2025
SP  - 6
SN  - 25044990
AB  - Artificial intelligence (AI) is increasingly applied in a wide range of healthcare and Intensive Care Unit (ICU) areas to serve—among others—as a tool for disease detection and prediction, as well as for healthcare resources’ management. Since sepsis is a high mortality and rapidly developing organ dysfunction disease afflicting millions in ICUs and costing huge amounts to treat, the area can benefit from the use of AI tools for early and informed diagnosis and antibiotic administration. Additionally, resource allocation plays a crucial role when patient flow is increased, and resources are limited. At the same time, sensitive data use raises the need for ethical guidelines and reflective datasets. Additionally, explainable AI is applied to handle AI opaqueness. This study aims to present existing clinical approaches for infection assessment in terms of scoring systems and diagnostic biomarkers, along with their limitations, and an extensive overview of AI applications in healthcare and ICUs in terms of (a) sepsis detection/prediction and sepsis mortality prediction, (b) length of ICU/hospital stay prediction, and (c) ICU admission/hospitalization prediction after Emergency Department admission, each constituting an important factor towards either prompt interventions and improved patient wellbeing or efficient resource management. Challenges of AI applications in ICU are addressed, along with useful recommendations to mitigate them. Explainable AI applications in ICU are described, and their value in validating, and translating predictions in the clinical setting is highlighted. The most important findings and future directions including multimodal data use and Transformer-based models are discussed. The goal is to make research in AI advances in ICU and particularly sepsis prediction more accessible and provide useful directions on future work.
UR  - https://www.proquest.com/docview/3181643275?accountid=15181&bdid=109692&_bd=blb2iCjnygMLbL%2BW8X0o5QFjidw%3D
DO  - https://doi.org/10.3390/make7010006
ER  - 

TY  - Scholarly Journals
T1  - An Empirical Survey on Explainable AI Technologies: Recent Trends, Use-Cases, and Categories from Technical and Application Perspectives
AU  - Nagahisarchoghaei, Mohammad
AU  - Nagahisarchoghaei, Mohammad
AU  - Nasheen Nur
AU  - Nasheen Nur
AU  - Cummins, Logan
AU  - Nashtarin Nur
AU  - Mirhossein Mousavi Karimi
AU  - Nandanwar, Shreya
AU  - Bhattacharyya, Siddhartha
AU  - Rahimi, Shahram
AU  - Rahimi, Shahram
JF  - Electronics
VL  - 12
IS  - 5
Y1  - 2023-01-01
DA  - 2023
SP  - 1092
SN  - 20799292
AB  - In a wide range of industries and academic fields, artificial intelligence is becoming increasingly prevalent. AI models are taking on more crucial decision-making tasks as they grow in popularity and performance. Although AI models, particularly machine learning models, are successful in research, they have numerous limitations and drawbacks in practice. Furthermore, due to the lack of transparency behind their behavior, users need more understanding of how these models make specific decisions, especially in complex state-of-the-art machine learning algorithms. Complex machine learning systems utilize less transparent algorithms, thereby exacerbating the problem. This survey analyzes the significance and evolution of explainable AI (XAI) research across various domains and applications. Throughout this study, a rich repository of explainability classifications and summaries has been developed, along with their applications and practical use cases. We believe this study will make it easier for researchers to understand all explainability methods and access their applications simultaneously.
UR  - https://www.proquest.com/docview/2785188470?accountid=15181&bdid=109692&_bd=j9MFbBPNuHDyD4%2F%2Fp1%2FETWl4LoA%3D
DO  - https://doi.org/10.3390/electronics12051092
ER  - 

TY  - Scholarly Journals
T1  - Intelligent Supply Chain Management: A Systematic Literature Review on Artificial Intelligence Contributions
AU  - Teixeira, António R
AU  - Ferreira, José Vasconcelos
AU  - Ramos, Ana Luísa
JF  - Information
VL  - 16
IS  - 5
Y1  - 2025-01-01
DA  - 2025
SP  - 399
SN  - 20782489
AB  - This systematic literature review investigates the recent applications of artificial intelligence (AI) in supply chain management (SCM), particularly in the domains of resilience, process optimization, sustainability, and implementation challenges. The study is motivated by gaps identified in previous reviews, which often exclude literature published after 2020 and lack an integrated analysis of AI’s contributions across multiple supply chain phases. The review aims to provide an updated synthesis of AI technologies—such as machine learning, deep learning, and generative AI—and their practical implementation between 2021 and 2024. Following the PRISMA framework, a rigorous methodology was applied using the Scopus database, complemented by bibliometric and content analyses. A total of 66 studies were selected based on predefined inclusion criteria and evaluated for methodological quality and thematic relevance. The findings reveal a diverse classification of AI applications across strategic and operational SCM phases and highlight emerging techniques like explainable AI, neurosymbolic systems, and federated learning. The review also identifies persistent barriers such as data governance, ethical concerns, and scalability. Future research should focus on hybrid AI–human collaboration, transparency through explainable models, and integration with technologies such as IoT and blockchain. This review contributes to the literature by offering a structured synthesis of AI’s transformative impact on SCM and by outlining key research directions to guide future investigations and managerial practice.
UR  - https://www.proquest.com/docview/3211985935?accountid=15181&bdid=109692&_bd=i%2FiUZRchgPOwV084faW7F%2BMvCfQ%3D
DO  - https://doi.org/10.3390/info16050399
ER  - 

TY  - Scholarly Journals
T1  - Constructing Explainable Classifiers from the Start—Enabling Human-in-the Loop Machine Learning
AU  - Estivill-Castro, Vladimir
AU  - Gilmore, Eugene
AU  - Gilmore, Eugene
AU  - Hexel, René
JF  - Information
VL  - 13
IS  - 10
Y1  - 2022-01-01
DA  - 2022
SP  - 464
SN  - 20782489
AB  - Interactive machine learning (IML) enables the incorporation of human expertise because the human participates in the construction of the learned model. Moreover, with human-in-the-loop machine learning (HITL-ML), the human experts drive the learning, and they can steer the learning objective not only for accuracy but perhaps for characterisation and discrimination rules, where separating one class from others is the primary objective. Moreover, this interaction enables humans to explore and gain insights into the dataset as well as validate the learned models. Validation requires transparency and interpretable classifiers. The huge relevance of understandable classification has been recently emphasised for many applications under the banner of explainable artificial intelligence (XAI). We use parallel coordinates to deploy an IML system that enables the visualisation of decision tree classifiers but also the generation of interpretable splits beyond parallel axis splits. Moreover, we show that characterisation and discrimination rules are also well communicated using parallel coordinates. In particular, we report results from the largest usability study of a IML system, confirming the merits of our approach.
UR  - https://www.proquest.com/docview/2728486468?accountid=15181&bdid=109692&_bd=5hhN%2B8PrtgTHPSyqhbOkPzPTPAA%3D
DO  - https://doi.org/10.3390/info13100464
ER  - 

TY  - Scholarly Journals
T1  - Risk Analysis of Artificial Intelligence in Medicine with a Multilayer Concept of System Order
AU  - Moghadasi, Negin
AU  - Moghadasi, Negin
AU  - Valdez, Rupa S
AU  - Piran, Misagh
AU  - Moghaddasi, Negar
AU  - Linkov, Igor
AU  - Polmateer, Thomas L
AU  - Loose, Davis C
AU  - Lambert, James H
JF  - Systems
VL  - 12
IS  - 2
Y1  - 2024-01-01
DA  - 2024
SP  - 47
SN  - 20798954
AB  - Artificial intelligence (AI) is advancing across technology domains including healthcare, commerce, the economy, the environment, cybersecurity, transportation, etc. AI will transform healthcare systems, bringing profound changes to diagnosis, treatment, patient care, data, medicines, devices, etc. However, AI in healthcare introduces entirely new categories of risk for assessment, management, and communication. For this topic, the framing of conventional risk and decision analyses is ongoing. This paper introduces a method to quantify risk as the disruption of the order of AI initiatives in healthcare systems, aiming to find the scenarios that are most and least disruptive to system order. This novel approach addresses scenarios that bring about a re-ordering of initiatives in each of the following three characteristic layers: purpose, structure, and function. In each layer, the following model elements are identified: 1. Typical research and development initiatives in healthcare. 2. The ordering criteria of the initiatives. 3. Emergent conditions and scenarios that could influence the ordering of the AI initiatives. This approach is a manifold accounting of the scenarios that could contribute to the risk associated with AI in healthcare. Recognizing the context-specific nature of risks and highlighting the role of human in the loop, this study identifies scenario s.06—non-interpretable AI and lack of human–AI communications—as the most disruptive across all three layers of healthcare systems. This finding suggests that AI transparency solutions primarily target domain experts, a reasonable inclination given the significance of “high-stakes” AI systems, particularly in healthcare. Future work should connect this approach with decision analysis and quantifying the value of information. Future work will explore the disruptions of system order in additional layers of the healthcare system, including the environment, boundary, interconnections, workforce, facilities, supply chains, and others.
UR  - https://www.proquest.com/docview/2931061999?accountid=15181&bdid=109692&_bd=zD5sZUZfTuOuWET7%2FXVH9HSuaN4%3D
DO  - https://doi.org/10.3390/systems12020047
ER  - 

TY  - Scholarly Journals
T1  - Application-Wise Review of Machine Learning-Based Predictive Maintenance: Trends, Challenges, and Future Directions
AU  - Tsallis Christos
AU  - Papageorgas Panagiotis
AU  - Piromalis Dimitrios
AU  - Munteanu, Radu Adrian
JF  - Applied Sciences
VL  - 15
IS  - 9
Y1  - 2025-01-01
DA  - 2025
SP  - 4898
SN  - 20763417
AB  - This systematic literature review (SLR) provides a comprehensive application-wise analysis of machine learning (ML)-driven predictive maintenance (PdM) across industrial domains. Motivated by the digital transformation of industry 4.0, this study explores how ML techniques optimize maintenance by predicting faults, estimating remaining useful life (RUL), and reducing operational downtime. Sixty peer-reviewed articles published between 2020 and 2024 were selected using the preferred reporting items for systematic reviews and meta-analyses (PRISMA) 2020 guidelines, and were analyzed based on industrial sector, ML techniques, datasets, evaluation metrics, and implementation challenges. Results show that combining ML with diverse sensor data enhances predictive performance under varying operational conditions across manufacturing, energy, healthcare, and transportation. Frequently used open datasets include the commercial modular aero-propulsion system simulation (CMAPSS), the malfunctioning industrial machine investigation and inspection (MIMII), and the semiconductor manufacturing process (SECOM) datasets, though data heterogeneity and imbalance remain major barriers. Emerging paradigms such as hybrid modeling, digital twins, and physics-informed learning show promise but face issues like computational cost, interpretability, and limited scalability. The findings highlight future research needs in model generalizability, real-world validation, and explainable artificial intelligence (AI) to bridge gaps between ML innovations and industrial practice.
UR  - https://www.proquest.com/docview/3203187964?accountid=15181&bdid=109692&_bd=0Vihmp2ydzNs6nFP2R3gGxptIeM%3D
DO  - https://doi.org/10.3390/app15094898
ER  - 

TY  - Scholarly Journals
T1  - A Comprehensive Survey of Deep Learning Approaches in Image Processing
AU  - Trigka, Maria
AU  - Dritsas, Elias
JF  - Sensors
VL  - 25
IS  - 2
Y1  - 2025-01-01
DA  - 2025
SP  - 531
SN  - 14248220
AB  - The integration of deep learning (DL) into image processing has driven transformative advancements, enabling capabilities far beyond the reach of traditional methodologies. This survey offers an in-depth exploration of the DL approaches that have redefined image processing, tracing their evolution from early innovations to the latest state-of-the-art developments. It also analyzes the progression of architectural designs and learning paradigms that have significantly enhanced the ability to process and interpret complex visual data. Key advancements, such as techniques improving model efficiency, generalization, and robustness, are examined, showcasing DL’s ability to address increasingly sophisticated image-processing tasks across diverse domains. Metrics used for rigorous model evaluation are also discussed, underscoring the importance of performance assessment in varied application contexts. The impact of DL in image processing is highlighted through its ability to tackle complex challenges and generate actionable insights. Finally, this survey identifies potential future directions, including the integration of emerging technologies like quantum computing and neuromorphic architectures for enhanced efficiency and federated learning for privacy-preserving training. Additionally, it highlights the potential of combining DL with emerging technologies such as edge computing and explainable artificial intelligence (AI) to address scalability and interpretability challenges. These advancements are positioned to further extend the capabilities and applications of DL, driving innovation in image processing.
UR  - https://www.proquest.com/docview/3159619902?accountid=15181&bdid=109692&_bd=A%2FvYVeo%2B3xdJIezB%2FsENLAoieQc%3D
DO  - https://doi.org/10.3390/s25020531
ER  - 

TY  - Scholarly Journals
T1  - Leveraging Machine Learning to Enhance Road Safety: A Social Marketing Approach
AU  - Noonpakdee, Wasinee
AU  - Satitsamitpong, Manit
AU  - Senachai, Prarawan
AU  - Napontun, Kittipong
JF  - ABAC Journal
VL  - 45
IS  - 1
Y1  - 2025-01-01
DA  - Jan-Mar 2025
SP  - 82
U1  - 82-102
SN  - 08580855
AB  - Road safety remains a critical concern worldwide. This research aims to investigate the use of Explainable AI (XAI) techniques, particularly SHAP (Shapley Additive Explanations), to identify key factors influencing road accident severity and create a social marketing campaign encouraging people to change their behavior in relation to road safety. Several machine learning models were developed using data from Thailand’s Ministry of Transport. The results show that the Light Gradient Boosting Machine (LGBM) model achieved the highest accuracy of 0.85 and an F1 score of 0.83. SHAP analysis revealed that the most significant contributing factors were the number of motorcycle involvements, road code, and the total number of vehicles and people involved in the accident. A practical framework for promoting sustainable road safety was proposed, focusing on raising awareness, delivering emotionally impactful communication, and fostering immediate behavioral change. This research provides valuable insights for strategic road safety initiatives and demonstrates the effectiveness of integrating machine learning with XAI. The findings can guide government authorities, policymakers, insurance companies, and social marketing planners in improving road safety.
UR  - https://www.proquest.com/docview/3201815663?accountid=15181&bdid=109692&_bd=x%2F8Z6hwhFompv%2BctBcBIWJ17enE%3D
DO  - https://doi.org/10.59865/abacj.2024.70
ER  - 

TY  - Scholarly Journals
T1  - The Promise of Explainable AI in Digital Health for Precision Medicine: A Systematic Review
AU  - Allen, Ben
JF  - Journal of Personalized Medicine
VL  - 14
IS  - 3
Y1  - 2024-01-01
DA  - 2024
SP  - 277
SN  - 20754426
AB  - This review synthesizes the literature on explaining machine-learning models for digital health data in precision medicine. As healthcare increasingly tailors treatments to individual characteristics, the integration of artificial intelligence with digital health data becomes crucial. Leveraging a topic-modeling approach, this paper distills the key themes of 27 journal articles. We included peer-reviewed journal articles written in English, with no time constraints on the search. A Google Scholar search, conducted up to 19 September 2023, yielded 27 journal articles. Through a topic-modeling approach, the identified topics encompassed optimizing patient healthcare through data-driven medicine, predictive modeling with data and algorithms, predicting diseases with deep learning of biomedical data, and machine learning in medicine. This review delves into specific applications of explainable artificial intelligence, emphasizing its role in fostering transparency, accountability, and trust within the healthcare domain. Our review highlights the necessity for further development and validation of explanation methods to advance precision healthcare delivery.
UR  - https://www.proquest.com/docview/3003332137?accountid=15181&bdid=109692&_bd=ARBZ54jaclHnHV2KRYw5XT8kPpM%3D
DO  - https://doi.org/10.3390/jpm14030277
ER  - 

TY  - Scholarly Journals
T1  - Auguring Fake Face Images Using Dual Input Convolution Neural Network
AU  - Bhandari, Mohan
AU  - Neupane, Arjun
AU  - Mallik, Saurav
AU  - Mallik, Saurav
AU  - Gaur, Loveleen
AU  - Gaur, Loveleen
AU  - Qin, Hong
JF  - Journal of Imaging
VL  - 9
IS  - 1
Y1  - 2023-01-01
DA  - 2023
SP  - 3
SN  - 2313433X
AB  - Deepfake technology uses auto-encoders and generative adversarial networks to replace or artificially construct fine-tuned faces, emotions, and sounds. Although there have been significant advancements in the identification of particular fake images, a reliable counterfeit face detector is still lacking, making it difficult to identify fake photos in situations with further compression, blurring, scaling, etc. Deep learning models resolve the research gap to correctly recognize phony images, whose objectionable content might encourage fraudulent activity and cause major problems. To reduce the gap and enlarge the fields of view of the network, we propose a dual input convolutional neural network (DICNN) model with ten-fold cross validation with an average training accuracy of 99.36 ± 0.62, a test accuracy of 99.08 ± 0.64, and a validation accuracy of 99.30 ± 0.94. Additionally, we used ’SHapley Additive exPlanations (SHAP) ’ as explainable AI (XAI) Shapely values to explain the results and interoperability visually by imposing the model into SHAP. The proposed model holds significant importance for being accepted by forensics and security experts because of its distinctive features and considerably higher accuracy than state-of-the-art methods.
UR  - https://www.proquest.com/docview/2767221877?accountid=15181&bdid=109692&_bd=9yXVOilJI%2Bn1SEPeS%2BHscQXSOjg%3D
DO  - https://doi.org/10.3390/jimaging9010003
ER  - 

TY  - Scholarly Journals
T1  - Contrastive Explanations of Plans through Model Restrictions
AU  - Krarup, Benjamin
AU  - Krivic, Senka
AU  - Magazzeni, Daniele
AU  - Long, Derek
AU  - Cashmore, Michael
AU  - Smith, David E
JF  - The Journal of Artificial Intelligence Research
VL  - 72
Y1  - 2021-01-01
DA  - 2021
SP  - 533
U1  - 533-612
SN  - 10769757
AB  - In automated planning, the need for explanations arises when there is a mismatch between a proposed plan and the user’s expectation. We frame Explainable AI Planning as an iterative plan exploration process, in which the user asks a succession of contrastive questions that lead to the generation and solution of hypothetical planning problems that are restrictions of the original problem. The object of the exploration is for the user to understand the constraints that govern the original plan and, ultimately, to arrive at a satisfactory plan. We present the results of a user study that demonstrates that when users ask questions about plans, those questions are usually contrastive, i.e. “why A rather than B?”. We use the data from this study to construct a taxonomy of user questions that often arise during plan exploration. Our approach to iterative plan exploration is a process of successive model restriction. Each contrastive user question imposes a set of constraints on the planning problem, leading to the construction of a new hypothetical planning problem as a restriction of the original. Solving this restricted problem results in a plan that can be compared with the original plan, admitting a contrastive explanation. We formally define model-based compilations in PDDL2.1 for each type of constraint derived from a contrastive user question in the taxonomy, and empirically evaluate the compilations in terms of computational complexity. The compilations were implemented as part of an explanation framework supporting iterative model restriction. We demonstrate its benefits in a second user study.
UR  - https://www.proquest.com/docview/2590055891?accountid=15181&bdid=109692&_bd=f3Ehc86SHzGkksusqLRCbZPbGt4%3D
DO  - https://doi.org/10.1613/jair.1.12813
ER  - 

TY  - Scholarly Journals
T1  - Predicting the Volume of Response to Tweets Posted by a Single Twitter Account
AU  - Fiok, Krzysztof
AU  - Karwowski, Waldemar
AU  - Gutierrez, Edgar
AU  - Ahram, Tareq
JF  - Symmetry
VL  - 12
IS  - 6
Y1  - 2020-01-01
DA  - 2020
SP  - 1054
SN  - 20738994
AB  - Social media users, including organizations, often struggle to acquire the maximum number of responses from other users, but predicting the responses that a post will receive before publication is highly desirable. Previous studies have analyzed why a given tweet may become more popular than others, and have used a variety of models trained to predict the response that a given tweet will receive. The present research addresses the prediction of response measures available on Twitter, including likes, replies and retweets. Data from a single publisher, the official US Navy Twitter account, were used to develop a feature-based model derived from structured tweet-related data. Most importantly, a deep learning feature extraction approach for analyzing unstructured tweet text was applied. A classification task with three classes, representing low, moderate and high responses to tweets, was defined and addressed using four machine learning classifiers. All proposed models were symmetrically trained in a fivefold cross-validation regime using various feature configurations, which allowed for the methodically sound comparison of prediction approaches. The best models achieved F1 scores of 0.655. Our study also used SHapley Additive exPlanations (SHAP) to demonstrate limitations in the research on explainable AI methods involving Deep Learning Language Modeling in NLP. We conclude that model performance can be significantly improved by leveraging additional information from the images and links included in tweets.
UR  - https://www.proquest.com/docview/2418817931?accountid=15181&bdid=109692&_bd=lIrvepB48yiwYuO%2FYp%2Bf%2BF%2Bs4Eg%3D
DO  - https://doi.org/10.3390/sym12061054
ER  - 

