TY  - JOUR
AU  - Wu, Z.
AU  - Guo, C.
AU  - Chen, J.
AU  - Ding, S.
AU  - Zheng, Y.
TI  - Integrating temporal association rules into intelligent prediction system for metabolic dysfunction-associated fatty liver disease
PY  - 2025
T2  - Decision Support Systems
VL  - 195
C7  - 114467
DO  - 10.1016/j.dss.2025.114467
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005109273&doi=10.1016%2fj.dss.2025.114467&partnerID=40&md5=b891891b13f70b4cb1db8bc0e97cc883
AB  - Healthcare big data provides trajectory data on chronic disease onset, progression, and outcomes, essential for understanding metabolic dysfunction-associated fatty liver disease (MAFLD) patient health dynamics. However, constructing explainable predictive models for MAFLD using longitudinal healthcare big data remains challenging due to its complexity. While several high-performance machine learning models have shown promise, their “black box” nature limits interpretability and trust among clinical healthcare professionals. Most studies also rely on cross-sectional data, which lacks the depth of longitudinal data, hindering accurate health status tracking. This paper proposes an intelligent MAFLD prediction system integrating temporal association rules (TARs) through a “human-in-the-loop” approach. By analyzing TARs that capture disease dynamics, the system incorporates high-quality domain knowledge into its predictive model. To enhance explainability, we use the SHapley Additive exPlanations framework alongside clinically significant TARs. The system's effectiveness was validated on real-world data, showing improved MAFLD outcome prediction. Sensitivity analysis identified optimal TARs and robust model configurations. Finally, the online-deployed explainable prototype system demonstrates potential to boost trust and adoption among clinical healthcare professionals. Additionally, the system's effectiveness and their willingness to use it were further evaluated through the “human-on-the-loop” method. These findings suggest the system could serve as a valuable tool for clinical applications and advance information systems design. © 2025 Elsevier B.V.
KW  - Explainability
KW  - Healthcare big data
KW  - Intelligent prediction system
KW  - Metabolic dysfunction-associated fatty liver disease
KW  - Temporal association rule
KW  - Diseases
KW  - mHealth
KW  - Nutrition
KW  - Explainability
KW  - Fatty liver disease
KW  - Health care professionals
KW  - Healthcare big data
KW  - Intelligent prediction
KW  - Intelligent prediction system
KW  - Metabolic dysfunction-associated fatty liver disease
KW  - Prediction systems
KW  - Predictive models
KW  - Temporal association rule
KW  - Patient treatment
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Jha, D.
AU  - Durak, G.
AU  - Sharma, V.
AU  - Keles, E.
AU  - Cicek, V.
AU  - Zhang, Z.
AU  - Srivastava, A.
AU  - Rauniyar, A.
AU  - Hagos, D.H.
AU  - Tomar, N.K.
AU  - Miller, F.H.
AU  - Topcu, A.
AU  - Yazidi, A.
AU  - Håkegård, J.E.
AU  - Bagci, U.
TI  - A Conceptual Framework for Applying Ethical Principles of AI to Medical Practice
PY  - 2025
T2  - Bioengineering
VL  - 12
IS  - 2
C7  - 180
DO  - 10.3390/bioengineering12020180
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219200212&doi=10.3390%2fbioengineering12020180&partnerID=40&md5=1c943f063a75d5af2ad07c65713ae8cf
AB  - Artificial Intelligence (AI) is reshaping healthcare through advancements in clinical decision support and diagnostic capabilities. While human expertise remains foundational to medical practice, AI-powered tools are increasingly matching or exceeding specialist-level performance across multiple domains, paving the way for a new era of democratized healthcare access. These systems promise to reduce disparities in care delivery across demographic, racial, and socioeconomic boundaries by providing high-quality diagnostic support at scale. As a result, advanced healthcare services can be affordable to all populations, irrespective of demographics, race, or socioeconomic background. The democratization of such AI tools can reduce the cost of care, optimize resource allocation, and improve the quality of care. In contrast to humans, AI can potentially uncover complex relationships in the data from a large set of inputs and generate new evidence-based knowledge in medicine. However, integrating AI into healthcare raises several ethical and philosophical concerns, such as bias, transparency, autonomy, responsibility, and accountability. In this study, we examine recent advances in AI-enabled medical image analysis, current regulatory frameworks, and emerging best practices for clinical integration. We analyze both technical and ethical challenges inherent in deploying AI systems across healthcare institutions, with particular attention to data privacy, algorithmic fairness, and system transparency. Furthermore, we propose practical solutions to address key challenges, including data scarcity, racial bias in training datasets, limited model interpretability, and systematic algorithmic biases. Finally, we outline a conceptual algorithm for responsible AI implementations and identify promising future research and development directions. © 2025 by the authors.
KW  - artificial intelligence (AI)
KW  - ethical AI
KW  - philosophical AI
KW  - trustworthy AI
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Upadhyay, S.
AU  - Lakkaraju, H.
AU  - Gajos, K.Z.
TI  - Counterfactual Explanations May Not Be the Best Algorithmic Recourse Approach
PY  - 2025
T2  - International Conference on Intelligent User Interfaces, Proceedings IUI
SP  - 446
EP  - 462
DO  - 10.1145/3708359.3712095
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001919802&doi=10.1145%2f3708359.3712095&partnerID=40&md5=fd295bbe61f9ffea612ceef6a7fb8f6e
AB  - Algorithmic recourse is a rapidly developing subfield in explainable AI (XAI) concerned with providing individuals subject to adverse high-stakes algorithmic outcomes with explanations indicating how to reverse said outcomes. While XAI research in the machine learning community doesn't confine itself to counterfactual explanations, its algorithmic recourse subfield does, adopting the assumption that the optimal way to provide recourse is through counterfactual explanations. Though there has been extensive human-AI interaction research on explanations, translating these findings to the algorithmic recourse setting is non-obvious due to meaningful problem setting differences, leaving the question of whether counterfactuals are the most optimal explanation paradigm for recourse unanswered. While intuitively satisfying, the prescriptive nature of counterfactuals makes them vulnerable to poor outcomes when circumstances unknown to the decision-making and explanation generating algorithms affect re-application strategies. With these concerns in mind, we designed a series of experiments comparing different explanation methods in the recourse setting, explicitly incorporating scenarios where circumstances unknown to the decision-making and explanation algorithms affect re-application strategies. In Experiment 1, we compared counterfactuals with reason codes, a simple feature-based explanation, finding that they both yield comparable re-application success, and that reason codes led to better user outcomes when unknown circumstances had a high impact on re-application strategies. In Experiment 2, we sought to improve on reason code outcomes, comparing them to feature attributions, a more informative feature-based explanation, but found no improvements. Finally, in Experiment 3, we aimed to improve on reason code outcomes with a multiple counterfactual explanation condition, finding that multiple counterfactuals led to higher re-application success but still resulted in comparatively worse user outcomes in the face of high impact unknown circumstances. Taken together, these findings call into question whether the standard counterfactual paradigm is the best approach for the algorithmic recourse problem setting.  © 2025 Copyright held by the owner/author(s).
KW  - AI explanations
KW  - algorithmic recourse
KW  - counterfactual explanations
KW  - Adversarial machine learning
KW  - AI explanation
KW  - Algorithmic recourse
KW  - Algorithmics
KW  - Application strategies
KW  - Counterfactual explanation
KW  - Counterfactuals
KW  - Decisions makings
KW  - Feature-based
KW  - Subfields
KW  - Unknown circumstances
KW  - Contrastive Learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Wang, H.W.
AU  - Birnbaum, L.
AU  - Setlur, V.
TI  - Jupybara: Operationalizing a Design Space for Actionable Data Analysis and Storytelling with LLMs
PY  - 2025
T2  - Conference on Human Factors in Computing Systems - Proceedings 
C7  - 1005
DO  - 10.1145/3706598.3713913
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005747639&doi=10.1145%2f3706598.3713913&partnerID=40&md5=77202d525fd3e9c355322406bf2f9861
AB  - Mining and conveying actionable insights from complex data is a key challenge of exploratory data analysis (EDA) and storytelling. To address this challenge, we present a design space for actionable EDA and storytelling. Synthesizing theory and expert interviews, we highlight how semantic precision, rhetorical persuasion, and pragmatic relevance underpin effective EDA and storytelling. We also show how this design space subsumes common challenges in actionable EDA and storytelling, such as identifying appropriate analytical strategies and leveraging relevant domain knowledge. Building on the potential of LLMs to generate coherent narratives with commonsense reasoning, we contribute Jupybara, an AI-enabled assistant for actionable EDA and storytelling implemented as a Jupyter Notebook extension. Jupybara employs two strategies - design-space-aware prompting and multi-agent architectures - to operationalize our design space. An expert evaluation confirms Jupybara's usability, steerability, explainability, and reparability, as well as the effectiveness of our strategies in operationalizing the design space framework with LLMs. © 2025 Copyright held by the owner/author(s).
KW  - Actionable Insights
KW  - Data Science
KW  - Data Storytelling
KW  - Exploratory Data Analysis
KW  - Human-AI Collaboration
KW  - Large Language Model
KW  - Multi-Agent System
KW  - Pragmatics
KW  - Rhetoric
KW  - Semantics
KW  - Data reduction
KW  - Data Science
KW  - Information retrieval
KW  - Information theory
KW  - Actionable insight
KW  - Data storytelling
KW  - Design spaces
KW  - Exploratory data analysis
KW  - Human-AI collaboration
KW  - Language model
KW  - Large language model
KW  - Multiagent systems (MASs)
KW  - Pragmatic
KW  - Rhetoric
KW  - Data mining
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Kayser, M.
AU  - Menzat, B.
AU  - Emde, C.
AU  - Bercean, B.
AU  - Novak, A.
AU  - Espinosa, A.
AU  - Papiez, B.W.
AU  - Gaube, S.
AU  - Lukasiewicz, T.
AU  - Camburu, O.-M.
TI  - Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting
PY  - 2024
T2  - EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference
SP  - 18891
EP  - 18919
DO  - 10.18653/v1/2024.emnlp-main.1051
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217771428&doi=10.18653%2fv1%2f2024.emnlp-main.1051&partnerID=40&md5=f7283bfa04746abb74681980b88b3e9c
AB  - The growing capabilities of AI models are leading to their wider use, including in safety-critical domains.Explainable AI (XAI) aims to make these models safer to use by making their inference process more transparent.However, current explainability methods are seldom evaluated in the way they are intended to be used: by real-world end users.To address this, we conducted a large-scale user study with 85 healthcare practitioners in the context of human-AI collaborative chest X-ray analysis.We evaluated three types of explanations: visual explanations (saliency maps), natural language explanations, and a combination of both modalities.We specifically examined how different explanation types influence users depending on whether the AI advice and explanations are factually correct.We find that text-based explanations lead to significant over-reliance, which is alleviated by combining them with saliency maps.We also observe that the quality of explanations, that is, how much factually correct information they entail, and how much this aligns with AI correctness, significantly impacts the usefulness of the different explanation types. © 2024 Association for Computational Linguistics.
KW  - Visual languages
KW  - 'current
KW  - Clinical decision support
KW  - End-users
KW  - Inference process
KW  - Large-scales
KW  - Natural language explanations
KW  - Real-world
KW  - Safety-critical domain
KW  - Saliency map
KW  - User study
KW  - Computational linguistics
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Schmude, T.
AU  - Koesten, L.
AU  - Möller, T.
AU  - Tschiatschek, S.
TI  - Information that matters: Exploring information needs of people affected by algorithmic decisions
PY  - 2025
T2  - International Journal of Human Computer Studies
VL  - 193
C7  - 103380
DO  - 10.1016/j.ijhcs.2024.103380
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205992227&doi=10.1016%2fj.ijhcs.2024.103380&partnerID=40&md5=70bfe884417416cff76aa9b70c97a589
AB  - Every AI system that makes decisions about people has a group of stakeholders that are personally affected by these decisions. However, explanations of AI systems rarely address the information needs of this stakeholder group, who often are AI novices. This creates a gap between conveyed information and information that matters to those who are impacted by the system's decisions, such as domain experts and decision subjects. To address this, we present the “XAI Novice Question Bank”, an extension of the XAI Question Bank (Liao et al., 2020) containing a catalog of information needs from AI novices in two use cases: employment prediction and health monitoring. The catalog covers the categories of data, system context, system usage, and system specifications. We gathered information needs through task based interviews where participants asked questions about two AI systems to decide on their adoption and received verbal explanations in response. Our analysis showed that participants’ confidence increased after receiving explanations but that their understanding faced challenges. These included difficulties in locating information and in assessing their own understanding, as well as attempts to outsource understanding. Additionally, participants’ prior perceptions of the systems’ risks and benefits influenced their information needs. Participants who perceived high risks sought explanations about the intentions behind a system's deployment, while those who perceived low risks rather asked about the system's operation. Our work aims to support the inclusion of AI novices in explainability efforts by highlighting their information needs, aims, and challenges. We summarize our findings as five key implications that can inform the design of future explanations for lay stakeholder audiences. © 2024 The Authors
KW  - Affected stakeholders
KW  - Explainable AI
KW  - Information needs
KW  - Qualitative methods
KW  - Question-driven explanations
KW  - Understanding
KW  - Affected stakeholder
KW  - AI systems
KW  - Algorithmics
KW  - Explainable AI
KW  - Information need
KW  - Qualitative method
KW  - Question banks
KW  - Question-driven explanation
KW  - Stakeholder groups
KW  - Understanding
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Schmitt, V.
AU  - Csomor, B.P.
AU  - Meyer, J.
AU  - Villa-Areas, L.-F.
AU  - Jakob, C.
AU  - Polzehl, T.
AU  - Möller, S.
TI  - Evaluating Human-Centered AI Explanations: Introduction of an XAI Evaluation Framework for Fact-Checking
PY  - 2024
T2  - ACM International Conference Proceeding Series
SP  - 91
EP  - 100
DO  - 10.1145/3643491.3660283
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196375447&doi=10.1145%2f3643491.3660283&partnerID=40&md5=8891d866de5533b08b3730b8994bf302
AB  - The rapidly increasing amount of online information and the advent of Generative Artificial Intelligence (GenAI) make the manual verification of information impractical. Consequently, AI systems are deployed to detect disinformation and deepfakes. Prior studies have indicated that combining AI and human capabilities yields enhanced performance in detecting disinformation. Furthermore, the European Union (EU) AI Act mandates human supervision for AI applications in areas impacting essential human rights, like freedom of speech, necessitating that AI systems be transparent and provide adequate explanations to ensure comprehensibility. Extensive research has been conducted on incorporating explainability (XAI) attributes to augment AI transparency, yet these often miss a human-centric assessment. The effectiveness of such explanations also varies with the user's prior knowledge and personal attributes. Therefore, we developed a framework for validating XAI features for the collaborative human-AI fact-checking task. The framework allows the testing of XAI features with objective and subjective evaluation dimensions and follows human-centric design principles when displaying information about the AI system to the users. The framework was tested in a crowdsourcing experiment with 433 participants, including 406 crowdworkers and 27 journalists for the collaborative disinformation detection task. The tested XAI features increase the AI system's perceived usefulness, understandability, and trust. With this publication, the XAI evaluation framework is made open source. © 2024 Owner/Author.
KW  - blind trust in AI systems
KW  - Human-centered eXplanations
KW  - objective and subjective evaluation of eXplanations
KW  - Crowdsourcing
KW  - AI systems
KW  - Blind trust in AI system
KW  - European union
KW  - Evaluation framework
KW  - Human capability
KW  - Human-centered explanation
KW  - Objective and subjective evaluation of explanation
KW  - Objective and subjective evaluations
KW  - Online information
KW  - Performance
KW  - Subjective testing
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Bae, S.W.
AU  - Chung, T.
AU  - Zhang, T.
AU  - Dey, A.K.
AU  - Islam, R.
TI  - Enhancing Interpretable, Transparent, and Unobtrusive Detection of Acute Marijuana Intoxication in Natural Environments: Harnessing Smart Devices and Explainable AI to Empower Just-In-Time Adaptive Interventions: Longitudinal Observational Study
PY  - 2025
T2  - JMIR AI
VL  - 4
C7  - e52270
DO  - 10.2196/52270
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216964363&doi=10.2196%2f52270&partnerID=40&md5=f98ed1787527d160ce9d10b7247ea2c3
AB  - Background: Acute marijuana intoxication can impair motor skills and cognitive functions such as attention and information processing. However, traditional tests, like blood, urine, and saliva, fail to accurately detect acute marijuana intoxication in real time. Objective: This study aims to explore whether integrating smartphone-based sensors with readily accessible wearable activity trackers, like Fitbit, can enhance the detection of acute marijuana intoxication in naturalistic settings. No previous research has investigated the effectiveness of passive sensing technologies for enhancing algorithm accuracy or enhancing the interpretability of digital phenotyping through explainable artificial intelligence in real-life scenarios. This approach aims to provide insights into how individuals interact with digital devices during algorithmic decision-making, particularly for detecting moderate to intensive marijuana intoxication in real-world contexts. Methods: Sensor data from smartphones and Fitbits, along with self-reported marijuana use, were collected from 33 young adults over a 30-day period using the experience sampling method. Participants rated their level of intoxication on a scale from 1 to 10 within 15 minutes of consuming marijuana and during 3 daily semirandom prompts. The ratings were categorized as not intoxicated (0), low (1-3), and moderate to intense intoxication (4-10). The study analyzed the performance of models using mobile phone data only, Fitbit data only, and a combination of both (MobiFit) in detecting acute marijuana intoxication. Results: The eXtreme Gradient Boosting Machine classifier showed that the MobiFit model, which combines mobile phone and wearable device data, achieved 99% accuracy (area under the curve=0.99; F1-score=0.85) in detecting acute marijuana intoxication in natural environments. The F1-score indicated significant improvements in sensitivity and specificity for the combined MobiFit model compared to using mobile or Fitbit data alone. Explainable artificial intelligence revealed that moderate to intense self-reported marijuana intoxication was associated with specific smartphone and Fitbit metrics, including elevated minimum heart rate, reduced macromovement, and increased noise energy around participants. Conclusions: This study demonstrates the potential of using smartphone sensors and wearable devices for interpretable, transparent, and unobtrusive monitoring of acute marijuana intoxication in daily life. Advanced algorithmic decision-making provides valuable insight into behavioral, physiological, and environmental factors that could support timely interventions to reduce marijuana-related harm. Future real-world applications of these algorithms should be evaluated in collaboration with clinical experts to enhance their practicality and effectiveness. © Sang Won Bae, Tammy Chung, Tongze Zhang, Anind K Dey, Rahul Islam.
KW  - algorithmic decision-making process
KW  - artificial intelligence
KW  - cannabis
KW  - data collection
KW  - decision support
KW  - digital phenotyping
KW  - experience sampling
KW  - explainable artificial intelligence
KW  - eXtreme Gradient Boosting Machine classifier
KW  - Fitbit
KW  - intoxication
KW  - JITAI
KW  - just-in-time adaptive interventions
KW  - machine learning
KW  - marijuana
KW  - mHealth
KW  - passive sensing
KW  - smart devices
KW  - smartphone-based sensors
KW  - wearables
KW  - XAI
KW  - XGBoost
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Zace, D.
AU  - Semeraro, F.
AU  - Schnaubelt, S.
AU  - Montomoli, J.
AU  - Ristagno, G.
AU  - Fijačko, N.
AU  - Gamberini, L.
AU  - Bignami, E.G.
AU  - Greif, R.
AU  - Monsieurs, K.G.
AU  - Scapigliati, A.
TI  - Artificial intelligence in resuscitation: a scoping review
PY  - 2025
T2  - Resuscitation Plus
VL  - 24
C7  - 100973
DO  - 10.1016/j.resplu.2025.100973
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004939487&doi=10.1016%2fj.resplu.2025.100973&partnerID=40&md5=34ce60ae5e570d112d7ef54b77d1d1f2
AB  - Background: Artificial intelligence (AI) is increasingly applied in medicine, with growing interest in its potential to improve outcomes in cardiac arrest (CA). However, the scope and characteristics of current AI applications in resuscitation remain unclear. Methods: This scoping review aims to map the existing literature on AI applications in CA and resuscitation and identify research gaps for further investigation. PRISMA-ScR framework and ILCOR guidelines were followed. A systematic literature search across PubMed, EMBASE, and Cochrane identified AI applications in resuscitation. Articles were screened and classified by AI methodology, study design, outcomes, and implementation settings. AI-assisted data extraction was manually validated for accuracy. Results: Out of 4046 records, 197 studies met inclusion criteria. Most were retrospective (90%), with only 16 prospective studies and 2 randomised controlled trials. AI was predominantly applied in prediction of CA, rhythm classification, and post-resuscitation outcome prognostication. Machine learning was the most commonly used method (50% of studies), followed by deep learning and, less frequently, natural language processing. Reported performance was generally high, with AUROC values often exceeding 0.85; however, external validation was rare and real-world implementation limited. Conclusions: While AI applications in resuscitation demonstrate encouraging performance in prediction and decision support tasks, clear evidence of improved patient outcomes or routine clinical use remains limited. Future research should focus on prospective validation, equity in data sources, explainability, and seamless integration of AI tools into clinical workflows. © 2025 The Author(s)
KW  - Artificial intelligence
KW  - Cardiac arrest
KW  - Deep learning
KW  - Large language model
KW  - Machine learning
KW  - Resuscitation
KW  - Scoping review
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - BOOK
AU  - Duke, T.
AU  - Giudici, P.
TI  - Responsible AI in Practice: A Practical Guide to Safe and Human AI
PY  - 2025
T2  - Responsible AI in Practice: A Practical Guide to Safe and Human AI
SP  - 1
EP  - 212
DO  - 10.1007/979-8-8688-1166-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003352337&doi=10.1007%2f979-8-8688-1166-1&partnerID=40&md5=6178aa612a627eecc3fd97076690606b
AB  - This book is the first practical book on AI risk assessment and management. It will enable you to evaluate and implement safe and accurate AI models and applications. The book features risk assessment frameworks, statistical metrics and code, a risk taxonomy curated from real-world case studies, and insights into AI regulation and policy, and is an essential tool for AI governance teams, AI auditors, AI ethicists, machine learning (ML) practitioners, Responsible AI practitioners, and computer science and data science students building safe and trustworthy AI systems across businesses, organizations, and universities. The centerpiece of this book is a risk management and assessment framework titled “Safe Human-centered AI (SAFE-HAI),” which highlights AI risks across the following Responsible AI principles: accuracy, sustainability and robustness, explainability, transparency and accountability, fairness, privacy and human rights, human-centered AI, and AI governance. Using several statistical metrics such as Area Under Curve (AUC), Rank Graduation Accuracy, and Shapley values, you will learn to apply Lorenz curves to measure risk and inequality across the different principles and will be equipped with a taxonomy/scoring rubric to identify and mitigate identified risks. This book is a true practical guide and covers a real-world case study using the proposed SAFE-HAI framework. The book will help you adopt standards and voluntary codes of conduct in compliance with AI risk and safety policies and regulations, including those from the NIST (National Institute of Standards and Technology) and EU AI Act (European Commission). What You Will Learn • Know the key principles behind Responsible AI and associated risks • Become familiar with risk assessment frameworks, statistical metrics, and mitigation measures for identified risks • Be aware of the fundamentals of AI regulations and policies and how to adopt them • Understand AI governance basics and implementation guidelines Who This Book Is For AI governance teams, AI auditors, AI ethicists, machine learning (ML) practitioners, Responsible AI practitioners, and computer science and data science students building safe and trustworthy AI systems across businesses, organizations, and universities. © 2025 by Toju Duke and Paolo Giudici.
KW  - AI Governance
KW  - AI Risk Impact Assessment
KW  - AI Risk Mitigation
KW  - AI Safety
KW  - AI Safety Metrics
KW  - Artificial Intelligence
KW  - Data Ethics
KW  - Responsible AI
M3  - Book
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Stodt, J.
AU  - Reich, C.
AU  - Knahl, M.
TI  - Demystifying XAI: Requirements for Understandable XAI Explanations
PY  - 2024
T2  - Studies in Health Technology and Informatics
VL  - 316
SP  - 565
EP  - 569
DO  - 10.3233/SHTI240477
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202007827&doi=10.3233%2fSHTI240477&partnerID=40&md5=580c1b1eac99ebf2fcc968787d92d8ed
AB  - This paper establishes requirements for assessing the usability of Explainable Artificial Intelligence (XAI) methods, focusing on non-AI experts like healthcare professionals. Through a synthesis of literature and empirical findings, it emphasizes achieving optimal cognitive load, task performance, and task time in XAI explanations. Key components include tailoring explanations to user expertise, integrating domain knowledge, and using non-propositional representations for comprehension. The paper highlights the critical role of relevance, accuracy, and truthfulness in fostering user trust. Practical guidelines are provided for designing transparent and user-friendly XAI explanations, especially in high-stakes contexts like healthcare. Overall, the paper's primary contribution lies in delineating clear requirements for effective XAI explanations, facilitating human-AI collaboration across diverse domains. © 2024 The Authors.
KW  - Explanations
KW  - Non-AI Experts
KW  - Understandability Requirements
KW  - XAI
KW  - Artificial Intelligence
KW  - Comprehension
KW  - Humans
KW  - Economic and social effects
KW  - Cognitive loads
KW  - Domain knowledge
KW  - Empirical findings
KW  - Explanation
KW  - Health care professionals
KW  - Non-AI expert
KW  - Task performance
KW  - Understandability
KW  - Understandability requirement
KW  - XAI
KW  - artificial intelligence
KW  - comprehension
KW  - human
KW  - Requirements engineering
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Smirnov, A.
AU  - Ponomarev, A.
AU  - Levashova, T.
AU  - Teslya, N.
AU  - Shilov, N.
TI  - Platform Architecture for Human-AI Collaborative Decision Support
PY  - 2024
T2  - Lecture Notes in Networks and Systems
VL  - 1209 LNNS
SP  - 334
EP  - 345
DO  - 10.1007/978-3-031-77688-5_32
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214233879&doi=10.1007%2f978-3-031-77688-5_32&partnerID=40&md5=0d9809b7b2be44dc04c5262841cc3d99
AB  - Modern decision-maker typically uses several AI-based tools to obtain more information about the problem at hand and to evaluate possible solutions, besides, decision support in complex dynamic environment typically requires knowledge of several experts, therefore, collaboration between them. These trends naturally merge in human-AI collaborative systems, providing the means of collaboration of heterogeneous participants. However, collaborative human-AI decision support is connected with many challenges, both theoretical and technological. This paper addresses the technological side of the problem by presenting a platform for human-AI collaborative decision support systems. The platform provides a set of mechanisms and interfaces, simplifying the development of such systems: team formation and collaboration features, interfaces to define, deploy and manage AI agents, and a set of structured representations facilitating interaction between human experts and AI agents. Possible application of the platform is discussed on a use case in road safety analysis. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
KW  - Collaborative Decision Support Systems
KW  - Explainable AI
KW  - Human-AI Interaction
KW  - Neuro-Symbolic AI
KW  - Ontology
KW  - Service Ecosystem
KW  - Decision making
KW  - Collaborative decision support system
KW  - Collaborative decisions
KW  - Decision supports
KW  - Explainable AI
KW  - Human-AI interaction
KW  - Neuro-symbolic AI
KW  - Ontology's
KW  - Platform architecture
KW  - Service ecosystems
KW  - Support systems
KW  - Ontology
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Nadizar, G.
AU  - Rovito, L.
AU  - De Lorenzo, A.
AU  - Medvet, E.
AU  - Virgolin, M.
TI  - An Analysis of the Ingredients for Learning Interpretable Symbolic Regression Models with Human-in-The-loop and Genetic Programming
PY  - 2024
T2  - ACM Transactions on Evolutionary Learning and Optimization
VL  - 4
IS  - 1
C7  - 5
DO  - 10.1145/3643688
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187802221&doi=10.1145%2f3643688&partnerID=40&md5=0bae125cc568ddd03928ccdc26bd391f
AB  - Interpretability is a critical aspect to ensure a fair and responsible use of machine learning (ML) in high-stakes applications. Genetic programming (GP) has been used to obtain interpretable ML models because it operates at the level of functional building blocks: if these building blocks are interpretable, there is a chance that their composition (i.e., the entire ML model) is also interpretable. However, the degree to which a model is interpretable depends on the observer. Motivated by this, we study a recently-introduced human-in-The-loop system that allows the user to steer GP's generation process to their preferences, which shall be online-learned by an artificial neural network (ANN). We focus on the generation of ML models as analytical functions (i.e., symbolic regression) as this is a key problem in interpretable ML, and propose a two-fold contribution. First, we devise more general representations for the ML models for the ANN to learn upon, to enable the application of the system to a wider range of problems. Second, we delve into a deeper analysis of the system's components. To this end, we propose an incremental experimental evaluation, aimed at (1) studying the effectiveness by which an ANN can capture the perceived interpretability for simulated users, (2) investigating how the GP's outcome is affected across different simulated user feedback profiles, and (3) determining whether humans participants would prefer models that were generated with or without their involvement. Our results pose clarity on pros and cons of using a human-in-The-loop approach to discover interpretable ML models with GP. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
KW  - active learning
KW  - deep learning
KW  - evolutionary algorithms
KW  - evolutionary computation
KW  - Explainable artificial intelligence
KW  - explainable evolutionary computation
KW  - genetic programming
KW  - interpretable machine learning
KW  - neural networks
KW  - Deep learning
KW  - Functional programming
KW  - Genetic algorithms
KW  - Learning systems
KW  - Neural networks
KW  - Regression analysis
KW  - Reinforcement learning
KW  - Active Learning
KW  - Deep learning
KW  - Explainable artificial intelligence
KW  - Explainable evolutionary computation
KW  - Human-in-the-loop
KW  - Interpretable machine learning
KW  - Machine learning models
KW  - Machine-learning
KW  - Neural-networks
KW  - Symbolic regression
KW  - Genetic programming
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Reis, M.I.
AU  - Gonçalves, J.N.C.
AU  - Cortez, P.
AU  - Carvalho, M.S.
AU  - Fernandes, J.M.
TI  - A context-aware decision support system for selecting explainable artificial intelligence methods in business organizations
PY  - 2025
T2  - Computers in Industry
VL  - 165
C7  - 104233
DO  - 10.1016/j.compind.2024.104233
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213207820&doi=10.1016%2fj.compind.2024.104233&partnerID=40&md5=d9d852cc9cf2ab9f5ae845213406e6fe
AB  - Explainable Artificial Intelligence (XAI) methods are valuable tools for promoting understanding, trust, and efficient use of Artificial Intelligence (AI) systems in business organizations. However, the question of how organizations should select suitable XAI methods for a given task and business context remains a challenge, particularly when the number of methods available in the literature continues to increase. Here, we propose a context-aware decision support system (DSS) to select, from a given set of XAI methods, those with higher suitability to the needs of stakeholders operating in a given AI-based business problem. By including the human-in-the-loop, our DSS comprises an application-grounded analytical metric designed to facilitate the selection of XAI methods that align with the business stakeholders’ desiderata and promote a deeper understanding of the results generated by a given machine learning model. The proposed system was tested on a real supply chain demand problem, using real data and real users. The results provide evidence on the usefulness of our metric in selecting XAI methods based on the feedback and analytical maturity of stakeholders from the deployment context. We believe that our DSS is sufficiently flexible and understandable to be applied in a variety of business contexts, with stakeholders with varying degrees of AI literacy. © 2024
KW  - Business analytics
KW  - Decision support system
KW  - Explainable AI
KW  - Machine learning
KW  - Supply chain
KW  - Artificial intelligence methods
KW  - Artificial intelligence systems
KW  - Business analytics
KW  - Business contexts
KW  - Business organizations
KW  - Context-Aware
KW  - Decision supports
KW  - Explainable artificial intelligence
KW  - Machine-learning
KW  - Support systems
KW  - Adversarial machine learning
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Holzinger, A.
AU  - Zatloukal, K.
AU  - Müller, H.
TI  - Is human oversight to AI systems still possible?
PY  - 2025
T2  - New Biotechnology
VL  - 85
SP  - 59
EP  - 62
DO  - 10.1016/j.nbt.2024.12.003
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212069001&doi=10.1016%2fj.nbt.2024.12.003&partnerID=40&md5=6a1a81a65b7df5a73eed09a94b0d646c
AB  - The rapid proliferation of artificial intelligence (AI) systems across diverse domains raises critical questions about the feasibility of meaningful human oversight, particularly in high-stakes domains such as new biotechnology. As AI systems grow increasingly complex, opaque, and autonomous, ensuring responsible use becomes a formidable challenge. During our editorial work for the special issue “Artificial Intelligence for Life Sciences”, we placed increasing emphasis on the topic of “human oversight”. Consequently, in this editorial we briefly discuss the evolving role of human oversight in AI governance, focusing on the practical, technical, and ethical dimensions of maintaining control. It examines how the complexity of contemporary AI architectures, such as large-scale neural networks and generative AI applications, undermine human understanding and decision-making capabilities. Furthermore, it evaluates emerging approaches—such as explainable AI (XAI), human-in-the-loop systems, and regulatory frameworks—that aim to enable oversight while acknowledging their limitations. Through a comprehensive analysis, the picture emerged while complete oversight may no longer be viable in certain contexts, strategic interventions leveraging human-AI collaboration and trustworthy AI design principles can preserve accountability and safety. The discussion highlights the urgent need for interdisciplinary efforts to rethink oversight mechanisms in an era where AI may outpace human comprehension. © 2024 The Authors
KW  - Artificial intelligence
KW  - Biotechnology
KW  - Deep learning
KW  - Digital transformation
KW  - Machine learning
KW  - Artificial Intelligence
KW  - Biotechnology
KW  - Humans
KW  - Neural Networks, Computer
KW  - artificial intelligence
KW  - decision making
KW  - deep learning
KW  - Editorial
KW  - ethics
KW  - explainable artificial intelligence
KW  - generative artificial intelligence
KW  - human
KW  - human oversight
KW  - information processing
KW  - artificial neural network
KW  - biotechnology
M3  - Editorial
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Swaroop, S.
AU  - Buçinca, Z.
AU  - Gajos, K.Z.
AU  - Doshi-Velez, F.
TI  - Personalising AI Assistance Based on Overreliance Rate in AI-Assisted Decision Making
PY  - 2025
T2  - International Conference on Intelligent User Interfaces, Proceedings IUI
SP  - 1107
EP  - 1122
DO  - 10.1145/3708359.3712128
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001917996&doi=10.1145%2f3708359.3712128&partnerID=40&md5=fed388b7492f62098e7869b4fefe5e0b
AB  - Personalising decision-making assistance to different users and tasks can improve human-AI team performance, such as by appropriately impacting reliance on AI assistance. However, people are different in many ways, with many hidden qualities, and adapting AI assistance to these hidden qualities is difficult. In this work, we consider a hidden quality previously identified as important: overreliance on AI assistance. We would like to (i) quickly determine the value of this hidden quality, and (ii) personalise AI assistance based on this value. In our first study, we introduce a few probe questions (where we know the true answer) to determine if a user is an overrelier or not, finding that correctly-chosen probe questions work well. In our second study, we improve human-AI team performance, personalising AI assistance based on users' overreliance quality. Exploratory analysis indicates that people learn different strategies of using AI assistance depending on what AI assistance they saw previously, indicating that we may need to take this into account when designing adaptive AI assistance. We hope that future work will continue exploring how to infer and personalise to other important hidden qualities.  © 2025 Copyright held by the owner/author(s).
KW  - adaptive AI
KW  - AI-assisted decision-making
KW  - decision support systems
KW  - explainable AI
KW  - human-AI interaction
KW  - human-centered AI
KW  - overreliance
KW  - reinforcement learning
KW  - time pressure
KW  - Adaptive AI
KW  - AI-assisted decision-making
KW  - Decision supports
KW  - Decisions makings
KW  - Explainable AI
KW  - Human-AI interaction
KW  - Human-centered AI
KW  - Overreliance
KW  - Reinforcement learnings
KW  - Support systems
KW  - Time pressures
KW  - Reinforcement learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Park, J.
AU  - Kang, D.
TI  - Artificial Intelligence and Smart Technologies in Safety Management: A Comprehensive Analysis Across Multiple Industries
PY  - 2024
T2  - Applied Sciences (Switzerland)
VL  - 14
IS  - 24
C7  - 11934
DO  - 10.3390/app142411934
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213257089&doi=10.3390%2fapp142411934&partnerID=40&md5=c8b26628f3ae790fa40391dfc6c843b6
AB  - The integration of Artificial Intelligence (AI) and smart technologies into safety management is a pivotal aspect of the Fourth Industrial Revolution or Industry 4.0. This study conducts a systematic literature review to identify and analyze how AI and smart technologies enhance safety management across various sectors within the Safety 4.0 paradigm. Focusing on peer-reviewed journal articles that explicitly mention “Smart”, “AI”, or “Artificial Intelligence” in their titles, the research examines key safety management factors, such as accident prevention, risk management, real-time monitoring, and ethical implementation, across sectors, including construction, industrial safety, disaster and public safety, transport and logistics, energy and power, health, smart home and living, and other diverse industries. AI-driven solutions, such as predictive analytics, machine learning algorithms, IoT sensor integration, and digital twin models, are shown to proactively identify and mitigate potential hazards, optimize energy consumption, and enhance operational efficiency. For instance, in the energy and power sector, intelligent gas meters and automated fire suppression systems manage gas-related risks effectively, while in the health sector, AI-powered health monitoring devices and mental health support applications improve patient and worker safety. The analysis reveals a significant trend towards shifting from reactive to proactive safety management, facilitated by the convergence of AI with IoT and Big Data analytics. Additionally, ethical considerations and data privacy emerge as critical challenges in the adoption of AI technologies. The study highlights the transformative role of AI in enhancing safety protocols, reducing accident rates, and improving overall safety outcomes across industries. It underscores the need for standardized protocols, robust AI governance frameworks, and interdisciplinary research to address existing challenges and maximize the benefits of AI in safety management. Future research directions include developing explainable AI models, enhancing human–AI collaboration, and fostering global standardization to ensure the responsible and effective implementation of AI-driven safety solutions. © 2024 by the authors.
KW  - artificial intelligence
KW  - Industry 4.0
KW  - proactive safety
KW  - Safety 4.0
KW  - safety management
KW  - smart technologies
KW  - Ethical technology
KW  - Fire extinguishers
KW  - Fire hazards
KW  - Health hazards
KW  - International cooperation
KW  - Risk management
KW  - Artificial intelligence technologies
KW  - Comprehensive analysis
KW  - Industrial revolutions
KW  - Journal articles
KW  - Management IS
KW  - Proactive safety
KW  - Safety 4.0
KW  - Safety management
KW  - Smart technology
KW  - Systematic literature review
KW  - Gas hazards
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - CHAP
AU  - Skarzyńska, E.
AU  - Paliszkiewicz, J.
AU  - Dabrowski, I.
AU  - Mendel, M.
TI  - Risks, failures, and ethical dilemmas of AI technologies and trust
PY  - 2025
T2  - Trust in Generative Artificial Intelligence: Human-Robot Interaction and Ethical Considerations
SP  - 3
EP  - 11
DO  - 10.4324/9781003586937-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000006005&doi=10.4324%2f9781003586937-2&partnerID=40&md5=8b36efa3cc5879ea6a38fb6b0f1fde7c
AB  - This chapter explores the risks, failures, and ethical dilemmas associated with artificial intelligence (AI) technologies, with a focus on their implications for public trust. While AI promises significant benefits across sectors, from health care to commerce, its rapid adoption also introduces risks at both individual and systemic levels. Issues such as data quality, algorithmic bias, and privacy concerns can lead to malfunctions and injustices that undermine the reliability of AI systems. Additionally, the inherent complexity of AI models, often operating as "black boxes," complicates accountability and transparency, intensifying public concerns about trust and fairness. Ethical dilemmas arise in areas like algorithmic decision-making, where the lack of explainability and accountability questions the moral integrity of AI systems. This chapter emphasizes the need for sustainable and ethical AI design that aligns with societal values and legal standards to foster trust. International guidelines and ethical frameworks, such as those proposed by the European Union, highlight the importance of transparency, privacy, and accountability in AI. By addressing these ethical and operational challenges, stakeholders can contribute to a more responsible and trust-based integration of AI into society, ensuring its benefits while mitigating associated risks. © 2025 selection and editorial matter, Joanna Paliszkiewicz, Ireneusz Dabrowski and Leila Halawi. All rights reserved.
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Vasileiou, S.L.
AU  - Kumar, A.
AU  - Yeoh, W.
AU  - Son, T.C.
AU  - Toni, F.
TI  - Dialectical Reconciliation via Structured Argumentative Dialogues
PY  - 2024
T2  - Proceedings of the International Conference on Knowledge Representation and Reasoning
SP  - 777
EP  - 787
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210509975&partnerID=40&md5=f57a039e1cb926c8f257d8bbc5c6b471
AB  - We present a novel framework designed to extend model reconciliation approaches, commonly used in human-aware planning, for enhanced human-AI interaction. By adopting a structured argumentation-based dialogue paradigm, our framework enables dialectical reconciliation to address knowledge discrepancies between an explainer (AI agent) and an explainee (human user), where the goal is for the explainee to understand the explainer’s decision. We formally describe the operational semantics of our proposed framework, providing theoretical guarantees. We then evaluate the framework’s efficacy “in the wild” via computational and human-subject experiments. Our findings suggest that our framework offers a promising direction for fostering effective human-AI interactions in domains where explainability is important. © 2024 Proceedings of the International Conference on Knowledge Representation and Reasoning. All rights reserved.
KW  - Semantics
KW  - Argumentative dialogues
KW  - Human subject experiments
KW  - Human users
KW  - Human-aware
KW  - Model reconciliation
KW  - Operational semantics
KW  - Theoretical guarantees
KW  - Knowledge representation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Rong, Y.
AU  - Scheerer, D.
AU  - Kasneci, E.
TI  - Faithful Attention Explainer: Verbalizing Decisions Based on Discriminative Features
PY  - 2024
T2  - CEUR Workshop Proceedings
VL  - 3793
SP  - 33
EP  - 40
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208289099&partnerID=40&md5=ebc90b8dba5c7d316c6f9f2f4f81faf4
AB  - In recent years, model explanation methods have been designed to interpret model decisions faithfully and intuitively so that users can easily understand them. In this paper, we propose a framework, Faithful Attention Explainer (FAE), capable of generating faithful textual explanations regarding the attended-to features. Towards this goal, we deploy an attention module that takes the visual feature maps from the classifier for sentence generation. Furthermore, our method successfully learns the association between features and words, which allows a novel attention enforcement module for attention explanation. Our model achieves promising performance in caption quality metrics and a faithful decision-relevance metric on two datasets (CUB and ACT-X). In addition, we show that FAE can interpret gaze-based human attention, as human gaze indicates the discriminative features that humans use for decision-making, demonstrating the potential of deploying human gaze for advanced human-AI interaction. © 2022 Copyright for this paper by its authors.
KW  - Explainable AI (XAI)
KW  - Faithfulness
KW  - Saliency Map
KW  - Textual Explanations
KW  - Visual Explanation
KW  - Decision-based
KW  - Discriminative features
KW  - Explainable AI (XAI)
KW  - Faithfulness
KW  - Feature map
KW  - Modeling decisions
KW  - Saliency map
KW  - Textual explanation
KW  - Visual explanation
KW  - Visual feature
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Alami, J.
AU  - El Iskandarani, M.
AU  - Riggs, S.L.
TI  - The Effect of Workload and Task Priority on Multitasking Performance and Reliance on Level 1 Explainable AI (XAI) Use
PY  - 2025
T2  - Human Factors
DO  - 10.1177/00187208251323478
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000178288&doi=10.1177%2f00187208251323478&partnerID=40&md5=714eba7be42e2132ef998599f11c00f8
AB  - Objective: This study investigates the effects of workload and task priority on multitasking performance and reliance on Level 1 Explainable Artificial Intelligence (XAI) systems in high-stakes decision environments. Background: Operators in critical settings manage multiple tasks under varying levels of workload and priority, potentially leading to performance degradation. XAI offers opportunities to support decision making by providing insights into AI’s reasoning, yet its adoption and effectiveness in multitasking scenarios remain underexplored. Method: Thirty participants engaged in a simulated multitasking environment, involving UAV command and control tasks, with the assistance of a Level 1 (i.e., basic perceptual information) XAI system on one of the tasks. The study utilized a within-subjects experimental design, manipulating workload (low, medium, and high) and AI-supported-task priority (low and high) across six conditions. Participants’ accuracy, use of automatic rerouting, AI miss detection, false alert identification, and use of AI explanations were measured and analyzed across the different experimental conditions. Results: Workload significantly hindered performance on the AI-assisted task and increased reliance on the AI system especially when the AI-assisted task was given low priority. The use of AI explanations was significantly affected by task priority only. Conclusion: An increase in workload led to proper offloading by relying on the AI’s alerts, but it also led to a lower rate of alert verification despite the alert feature’s high false alert rate. Application: The findings from the present work help inform AI system designers on how to design their systems for high-stakes environments such that reliance on AI is properly calibrated. © 2025 Human Factors and Ergonomics Society.
KW  - automation
KW  - explanation
KW  - multitasking
KW  - performance
KW  - reliance
KW  - workload
KW  - Artificial intelligence
KW  - Decision making
KW  - AI systems
KW  - Decision environment
KW  - Explanation
KW  - Level-1
KW  - Multiple tasks
KW  - Performance
KW  - Performance degradation
KW  - Reliance
KW  - Task priorities
KW  - Workload
KW  - Ergonomics
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Correa, N.
AU  - Correa, A.
AU  - Zadrozny, W.
TI  - Generative AI for Consumer Communications: Classification, Summarization, Response Generation
PY  - 2024
T2  - IEEE Andescon, ANDESCON 2024 - Proceedings
DO  - 10.1109/ANDESCON61840.2024.10755794
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211963129&doi=10.1109%2fANDESCON61840.2024.10755794&partnerID=40&md5=a1e94f01496c7c7f1162d0ac0b9af204
AB  - Generative AI showed the unexpected power of large language models (LLMs) for understanding and generation of natural language text and other modalities at the end of 2022. This paper presents a novel generative AI system for text classification, summarization and response generation of consumer communications. The system uses the same foundation model and a uniform pipeline for the tasks proposed. Consumer communications are massive and served mainly via voice and text, and until recently could be handled only with human agents (customer service representatives). However, they must be handled with quality, consistency, speed and low cost, at scale. We limit our attention to financial consumer communications from the U.S. Consumer Financial Protection Bureau (CFPB), publicly available in a dataset of over 4.7 million complaints. Performance reaches 88 % accuracy (without fine-tuning) for classification and over 72 % for summarization and response generation. Artificial intelligence has great positive impacts for business and society, but its application and deployment also poses risks and unknowns. We thus address the important questions of risk, bias, interpretability, explainability, safety and regulatory compliance with the emerging legal frameworks. © 2024 IEEE.
KW  - AI safety
KW  - Generative AI
KW  - large language models
KW  - natural language processing
KW  - text classification
KW  - text summarization
KW  - vector embeddings
KW  - AI safety
KW  - Embeddings
KW  - Generative AI
KW  - Language model
KW  - Language processing
KW  - Large language model
KW  - Natural language processing
KW  - Natural languages
KW  - Text classification
KW  - Text Summarisation
KW  - Vector embedding
KW  - Natural language processing systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - BOOK
AU  - Soldatos, J.
TI  - Artificial intelligence in manufacturing: Enabling intelligent, flexible and cost-effective production through AI
PY  - 2024
T2  - Artificial Intelligence in Manufacturing: Enabling Intelligent, Flexible and Cost-Effective Production Through AI
SP  - 1
EP  - 505
DO  - 10.1007/978-3-031-46452-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201858561&doi=10.1007%2f978-3-031-46452-2&partnerID=40&md5=c04ead5b85b72172e3f3dd18867267ce
AB  - This open access book presents a rich set of innovative solutions for artificial intelligence (AI) in manufacturing. The various chapters of the book provide a broad coverage of AI systems for state of the art flexible production lines including both cyber-physical production systems (Industry 4.0) and emerging trustworthy and human-centered manufacturing systems (Industry 5.0). From a technology perspective, the book addresses a wide range of AI paradigms such as deep learning, reinforcement learning, active learning, agent-based systems, explainable AI, industrial robots, and AI-based digital twins. Emphasis is put on system architectures and technologies that foster human-AI collaboration based on trusted interactions between workers and AI systems. From a manufacturing applications perspective, the book illustrates the deployment of these AI paradigms in a variety of use cases spanning production planning, quality control, anomaly detection, metrology, workers' training, supply chain management, as well as various production optimization scenarios. This is an open access book. © The Author(s) 2024. All rights reserved.
KW  - Explainable AI
KW  - Industry 4.0
KW  - Industry 5.0
KW  - Intelligent agents
KW  - Open access
KW  - Open access
KW  - Reinforcement learning
M3  - Book
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Hua, D.
AU  - Petrina, N.
AU  - Sacks, A.J.
AU  - Young, N.
AU  - Cho, J.-G.
AU  - Smith, R.
AU  - Poon, S.K.
TI  - Towards human-AI collaboration in radiology: a multidimensional evaluation of the acceptability of AI for chest radiograph analysis in supporting pulmonary tuberculosis diagnosis
PY  - 2025
T2  - JAMIA Open
VL  - 8
IS  - 1
C7  - ooae151
DO  - 10.1093/jamiaopen/ooae151
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217842449&doi=10.1093%2fjamiaopen%2fooae151&partnerID=40&md5=1f6b229e6e45acf4b4d5b108161795d3
AB  - Objective: Artificial intelligence (AI) technology promises to be a powerful tool in addressing the global health challenges posed by tuberculosis (TB). However, evidence for its real-world impact is lacking, which may hinder safe, responsible adoption. This case study addresses this gap by assessing the technical performance, usability and workflow aspects, and health impact of implementing a commercial AI system (qXR by Qure.ai) to support Australian radiologists in diagnosing pulmonary TB. Materials and Methods: A retrospective diagnostic accuracy evaluation was conducted to establish the technical performance of qXR in detecting TB compared to a human radiologist and microbiological reference standard. A qualitative human factors assessment was performed to investigate the user experience and clinical decision-making process of radiologists using qXR. A task productivity analysis was completed to quantify how the radiological screening turnaround time is impacted. Results: qXR displays near-human performance satisfying the World Health Organization’s suggested accuracy profile. Radiologists reported high satisfaction with using qXR based on minimal workflow disruptions, respect for their professional autonomy, and limited increases in workload burden despite poor algorithm explainability. qXR delivers considerable productivity gains for normal cases and optimizes resource allocation through redistributing time from normal to abnormal cases. Discussion and Conclusion: This study provides preliminary evidence of how an AI system with reasonable diagnostic accuracy and a human-centered user experience can meaningfully augment the TB diagnostic workflow. Future research needs to investigate the impact of AI on clinician accuracy, its relationship with efficiency, and best practices for optimizing the impact of clinician-AI collaboration. © The Author(s) 2025. Published by Oxford University Press on behalf of the American Medical Informatics Association.
KW  - artificial intelligence
KW  - evaluation
KW  - human factors
KW  - pulmonary tuberculosis
KW  - technical performance
KW  - translational impact
KW  - algorithm
KW  - Article
KW  - artificial intelligence
KW  - Australian
KW  - biomedical technology assessment
KW  - case study
KW  - clinical decision making
KW  - clinician
KW  - comparative study
KW  - diagnostic accuracy
KW  - diagnostic test accuracy study
KW  - health impact assessment
KW  - human
KW  - image analysis
KW  - job experience
KW  - lung tuberculosis
KW  - microbiological examination
KW  - performance
KW  - predictive value
KW  - productivity
KW  - professional practice
KW  - quantitative analysis
KW  - radiologist
KW  - resource allocation
KW  - retrospective study
KW  - satisfaction
KW  - sensitivity and specificity
KW  - standard
KW  - thematic analysis
KW  - thorax radiography
KW  - turnaround time
KW  - usability
KW  - workflow
KW  - workload
KW  - World Health Organization
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Buschmeyer, K.
AU  - Zenner, J.
AU  - Hatfield, S.
TI  - Effectiveness of AI-based decision support systems in work environment: a systematic literature review
PY  - 2024
T2  - International Journal of Human Factors and Ergonomics
VL  - 11
IS  - 5
SP  - 1
EP  - 54
DO  - 10.1504/IJHFE.2024.142761
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210291373&doi=10.1504%2fIJHFE.2024.142761&partnerID=40&md5=f5cfa63c1738d0037c813af0218982f9
AB  - Artificial intelligence (AI) is being increasingly used in high-stakes working areas to augment experts in challenging decision-making situations. The AI support is intended to reduce the cognitive load on experts, which should ideally be reflected both in a greater sense of well-being when working on demanding tasks and in joint performance exceeding that of both the humans and AI alone. However, the extent and conditions of achievement (such as the AI accuracy and explainability) of these intended effects have not been systematically investigated. Therefore, we identified and reviewed 44 articles published since 2018 that have investigated the effects of AI-based decision support systems on experts in controlled experimental settings. The results suggest that, for optimal human-AI performance, which surpasses the performance of either alone, both must operate at similar and high levels. However, the effect on the psychological load remains unclear owing to limited research.  © The Author(s) 2024.
KW  - AI-based decision support systems
KW  - artificial intelligence
KW  - cognitive relief
KW  - decision making
KW  - task performance
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CHAP
AU  - Al E'mari, S.
AU  - Sanjalawe, Y.
AU  - Fataftah, F.
AU  - Hajjaj, R.
TI  - Foundations of autonomous cyber defense systems
PY  - 2025
T2  - AI-Driven Security Systems and Intelligent Threat Response Using Autonomous Cyber Defense
SP  - 1
EP  - 33
DO  - 10.4018/979-8-3373-0954-5.ch001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005382708&doi=10.4018%2f979-8-3373-0954-5.ch001&partnerID=40&md5=d43f677d12328488b0db1eb229e7dce1
AB  - Traditional cybersecurity methods are becoming inadequate due to the growing complexity and frequency of cyber threats. This chapter explores autonomous cyber defense systems-self-sustaining, intelligent frameworks that detect, analyze, and respond to threats in real time without human intervention. Leveraging AI, Machine Learning, Reinforcement Learning, NLP, and Explainable AI, these systems enable adaptive and scalable security operations. The chapter analyzes system architectures across varying autonomy levels-human-in-the-loop, on-the-loop, and out-of-the-loop-and discusses enabling technologies such as Cyber Threat Intelligence. It reviews modern threats including zero-day exploits, AI-driven malware, and APTs, highlighting the advantages of autonomous systems in resilience and responsiveness. Practical frameworks, deployment strategies, and real-world case studies are presented with performance and ethical evaluation. The chapter concludes with future directions such as quantum-resilient architectures and sustainable cybersecurity strategies. © 2025, IGI Global Scientific Publishing. All rights reserved.
KW  - Artificial life
KW  - Computer viruses
KW  - Computer worms
KW  - Cyber attacks
KW  - Embedded systems
KW  - Intelligent systems
KW  - Robotics
KW  - Adaptive security
KW  - Cyber security
KW  - Cyber threats
KW  - Cyber-defense
KW  - Defence systems
KW  - Human intervention
KW  - Learning reinforcements
KW  - Machine-learning
KW  - Real- time
KW  - Reinforcement learnings
KW  - Reinforcement learning
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CHAP
AU  - Tiwari, A.
AU  - Masilamani, M.
AU  - Rao, T.S.
AU  - Zope, S.
AU  - Deepak, S.A.
AU  - Karthick, L.
TI  - Shaping the future: Emerging trends and strategic predictions in big data and AI
PY  - 2025
T2  - AI and the Revival of Big Data
SP  - 125
EP  - 153
DO  - 10.4018/979-8-3693-8472-5.ch006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218162219&doi=10.4018%2f979-8-3693-8472-5.ch006&partnerID=40&md5=f7791f709e3ff5de8127accfe5889d43
AB  - Big Data and AI are changing global enterprises, cultures, and human- machine interaction. This chapter discusses this fast- changing world and the intricate interplay between these two massive forces. Industry- wide AI-powered automation, real- time data analytics for agile decision- making, quantum computing's disruptive potential and ramifications for AI capabilities, edge AI's impact on dispersed intelligence, and explainable AI (XAI)'s role in trust and transparency are future topics and Fast technological breakthroughs can innovate and solve global issues, but they also present huge challenges. This chapter forecasts data privacy and robust security in an increasingly interconnected world, algorithmic bias mitigation and AI system fairness and equity, the complex ethical issues surrounding massive AI adoption and its potential societal impacts, the changing regulatory landscape and the need for adaptive governance frameworks, and human- in- the- loop using cutting- edge research, industry best practices, and real- world case studies. © 2025, IGI Global Scientific Publishing. All rights reserved.
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Wood, N.G.
TI  - Explainable AI in the military domain
PY  - 2024
T2  - Ethics and Information Technology
VL  - 26
IS  - 2
C7  - 29
DO  - 10.1007/s10676-024-09762-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190401777&doi=10.1007%2fs10676-024-09762-w&partnerID=40&md5=35f96aaf468aba4f30c0e72f20244b3d
AB  - Artificial intelligence (AI) has become nearly ubiquitous in modern society, from components of mobile applications to medical support systems, and everything in between. In societally impactful systems imbued with AI, there has been increasing concern related to opaque AI, that is, artificial intelligence where it is unclear how or why certain decisions are reached. This has led to a recent boom in research on “explainable AI” (XAI), or approaches to making AI more explainable and understandable to human users. In the military domain, numerous bodies have argued that autonomous and AI-enabled weapon systems ought not incorporate unexplainable AI, with the International Committee of the Red Cross and the United States Department of Defense both explicitly including explainability as a relevant factor in the development and use of such systems. In this article, I present a cautiously critical assessment of this view, arguing that explainability will be irrelevant for many current and near-future autonomous systems in the military (which do not incorporate any AI), that it will be trivially incorporated into most military systems which do possess AI (as these generally possess simpler AI systems), and that for those systems with genuinely opaque AI, explainability will prove to be of more limited value than one might imagine. In particular, I argue that explainability, while indeed a virtue in design, is a virtue aimed primarily at designers and troubleshooters of AI-enabled systems, but is far less relevant for users and handlers actually deploying these systems. I further argue that human–machine teaming is a far more important element of responsibly using AI for military purposes, adding that explainability may undermine efforts to improve human–machine teamings by creating a prima facie sense that the AI, due to its explainability, may be utilized with little (or less) potential for mistakes. I conclude by clarifying that the arguments are not against XAI in the military, but are instead intended as a caution against over-inflating the value of XAI in this domain, or ignoring the limitations and potential pitfalls of this approach. © The Author(s) 2024.
KW  - AI
KW  - Artificial intelligence
KW  - Autonomous weapon systems
KW  - Explainability
KW  - Human–machine interaction
KW  - Autonomous weapon system
KW  - Explainability
KW  - Human machine interaction
KW  - Human users
KW  - Human-machine
KW  - International committee of the red cross
KW  - Military domains
KW  - Mobile applications
KW  - Support systems
KW  - Weapon system
KW  - Artificial intelligence
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Bergomi, L.
TI  - Fostering Human-AI interaction: development of a Clinical Decision Support System enhanced by eXplainable AI and Natural Language Processing
PY  - 2024
T2  - CEUR Workshop Proceedings
VL  - 3793
SP  - 321
EP  - 328
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208286837&partnerID=40&md5=71f74ced0cdd204f075646a5d0344846
AB  - Artificial Intelligence (AI) is increasingly integrated into Decision Support Systems (DSS). The explainability of AI-based systems becomes crucial in sensitive and critical domains, such as healthcare, where ethical considerations and reliability are paramount concerns. In the clinical setting, it is important to evaluate how humans and AI can collaborate on cognitive tasks. Collaboration protocols (HAI-CP) allow for the investigation of the usefulness of AI models and their impact on users (both positive and negative). Although research on the application of these methods is blooming, there is little understanding of the impact on clinical decision-making, especially for eXplainable AI (XAI) systems, due to the lack of user studies. Therefore, the goal of this proposal is to develop a clinical DSS enhanced by XAI and Natural Language Processing (NLP): their synergy can add value to the interaction between users and AI, fostering a more linguistically natural, comprehensible, trustworthy, and supporting interfacing, that blends into the existing workflows. This proposal explores potential solutions to tailor natural language explanations and data visualizations to the end-user, improving the comprehensibility of the reasons behind a decision, and increasing the user’s confidence in the decision; investigates and tests possible strategies to “get the patient-in-the-loop”; explores uncertainty quantification and counterfactual approaches, and finally assesses the impact on naturalistic (i.e., real-world) decision-making and long-term effects and biases. © 2024 Copyright for this paper by its authors.
KW  - Clinical decision making
KW  - Explainable artificial intelligence
KW  - Human-AI collaboration protocol
KW  - Natural language interaction
KW  - Clinical research
KW  - Decision making
KW  - Natural language processing systems
KW  - Visual languages
KW  - Clinical decision making
KW  - Clinical decision support systems
KW  - Collaboration protocols
KW  - Decision supports
KW  - Explainable artificial intelligence
KW  - Human-artificial intelligence collaboration protocol
KW  - Language processing
KW  - Natural language interaction
KW  - Natural languages
KW  - Support systems
KW  - Decision support systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Guzzi, J.
AU  - Giusti, A.
TI  - Human-in-the-loop testing of the explainability of robot navigation algorithms in extended reality
PY  - 2024
T2  - CEUR Workshop Proceedings
VL  - 3793
SP  - 297
EP  - 304
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208285748&partnerID=40&md5=d38e67e24dac709843e4247e6b876415
AB  - Developing robots to be deployed in spaces shared with people requires testing with humans-in-the-loop. In fact, only humans co-located with the robots are in a suitable position to judge the robots’ behaviors. This is especially important when people participate in the same task as the robots, like when navigating a shared environment: the actions of people and robots influence each other. To test the resulting dynamic system, we need to let real people experience the interaction, which, when done in simulation, translates to immersing real users using Virtual or Mixed Reality. We implement and demonstrate this solution where users experience variable legibility, predictability, and explainability of the robot navigation algorithms, depending on if and how the robots explicitly communicate their intentions. © 2024 Copyright for this paper by its authors.
KW  - Human-in-the loop
KW  - Social robotic navigation
KW  - Virtual Reality
KW  - Microrobots
KW  - Mixed reality
KW  - Social robots
KW  - Co-located
KW  - Human-in-the-loop
KW  - Mixed reality
KW  - Navigation algorithms
KW  - Robot behavior
KW  - Robot navigation
KW  - Robotic navigation
KW  - Social robotic navigation
KW  - Social robotics
KW  - Users' experiences
KW  - Virtual environments
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Herrera, F.
TI  - Reflections and attentiveness on eXplainable Artificial Intelligence (XAI). The journey ahead from criticisms to human–AI collaboration
PY  - 2025
T2  - Information Fusion
VL  - 121
C7  - 103133
DO  - 10.1016/j.inffus.2025.103133
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001229308&doi=10.1016%2fj.inffus.2025.103133&partnerID=40&md5=b07f7a8e854fe51b3ab1de35026413c7
AB  - The emergence of deep learning over the past decade has driven the development of increasingly complex AI models, amplifying the need for Explainable Artificial Intelligence (XAI). As AI systems grow in size and complexity, ensuring interpretability and transparency becomes essential, especially in high-stakes applications. With the rapid expansion of XAI research, addressing emerging debates and criticisms requires a comprehensive examination. This paper explores the complexities of XAI from multiple perspectives, proposing six key axes that shed light on its role in human–AI interaction and collaboration. First, it examines the imperative of XAI under the dominance of black-box AI models. Given the lack of definitional cohesion, the paper argues that XAI must be framed through the lens of audience and understanding, highlighting its different uses in AI–human interaction. The recent BLUE vs. RED XAI distinction is analyzed through this perspective. The study then addresses the criticisms of XAI, evaluating its maturity, current trajectory, and limitations in handling complex problems. The discussion then shifts to explanations as a bridge between AI models and human understanding, emphasizing the importance of usability of explanations in human–AI decision making. Key aspects such as AI reliance, human intuition, and emerging collaboration theories — including the human-algorithm centaur and co-intelligence paradigms — are explored in connection with XAI. The medical field is considered as a case study, given its extensive research on collaboration between doctors and AI through explainability. The paper proposes a framework to evaluate the maturity of XAI using three dimensions: practicality, auditability, and AI governance. Provide the final lessons learned focused on trends and questions to tackle in the near future. This is an in-depth exploration of the impact and urgency of XAI in the era of pervasive expansion of AI. Three Key reflections from this study include: (a) XAI must enhance cognitive engagement with explanations, (b) it must evolve to fully address why, what, and for what purpose explanations are needed, and (c) it plays a crucial role in building societal trust in AI. By advancing XAI in these directions, we can ensure that AI remains transparent, auditable, and accountable, and aligned with human needs. © 2025 Elsevier B.V.
KW  - AI governance
KW  - AI safety
KW  - Auditability
KW  - eXplainable Artificial Intelligence
KW  - Explanations
KW  - Human–AI collaboration
KW  - Human–AI decision-making
KW  - Maturity level of explainability
KW  - XAI audience
KW  - XAI criticisms
KW  - Decision making
KW  - Deep learning
KW  - AI governance
KW  - AI safety
KW  - Auditability
KW  - Decisions makings
KW  - Explainable artificial intelligence
KW  - Explanation
KW  - Human–AI collaboration
KW  - Human–AI decision-making
KW  - Maturity level of explainability
KW  - Maturity levels
KW  - XAI audience
KW  - XAI criticism
KW  - Economic and social effects
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Francis, A.
AU  - Pérez-D'Arpino, C.
AU  - Li, C.
AU  - Xia, F.
AU  - Alahi, A.
AU  - Alami, R.
AU  - Bera, A.
AU  - Biswas, A.
AU  - Biswas, J.
AU  - Chandra, R.
AU  - Chiang, H.-T.L.
AU  - Everett, M.
AU  - Ha, S.
AU  - Hart, J.
AU  - How, J.P.
AU  - Karnan, H.
AU  - Lee, T.-W.E.
AU  - Manso, L.J.
AU  - Mirsky, R.
AU  - Pirk, S.
AU  - Singamaneni, P.T.
AU  - Stone, P.
AU  - Taylor, A.V.
AU  - Trautman, P.
AU  - Tsoi, N.
AU  - Vázquez, M.
AU  - Xiao, X.
AU  - Xu, P.
AU  - Yokoyama, N.
AU  - Toshev, A.
AU  - Martín-Martín, R.
TI  - Principles and Guidelines for Evaluating Social Robot Navigation Algorithms
PY  - 2025
T2  - ACM Transactions on Human-Robot Interaction
VL  - 14
IS  - 2
C7  - 34
DO  - 10.1145/3700599
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003641956&doi=10.1145%2f3700599&partnerID=40&md5=46df44bda948ff8c9c3f089b67ce4774
AB  - A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this article, we pave the road toward common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributions include (a) a definition of a socially navigating robot as one that respects the principles of safety, comfort, legibility, politeness, social competency, agent understanding, proactivity, and responsiveness to context, (b) guidelines for the use of metrics, development of scenarios, benchmarks, datasets, and simulators to evaluate social navigation, and (c) a design of a social navigation metrics framework to make it easier to compare results from different simulators, robots, and datasets.  © 2025 Copyright held by the owner/author(s).
KW  - benchmarks
KW  - datasets
KW  - robot navigation
KW  - simulators
KW  - social robotics
KW  - Industrial robots
KW  - Intelligent robots
KW  - Benchmark
KW  - Dataset
KW  - Human agent
KW  - Navigation algorithms
KW  - Robot navigation
KW  - Robotic agents
KW  - Social navigation
KW  - Social robotics
KW  - Social robots
KW  - Static environment
KW  - Social robots
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Rheem, H.
AU  - Lee, J.
AU  - Lee, J.D.
AU  - Szczerba, J.F.
AU  - Tsimhoni, O.
TI  - Explaining Automated Vehicle Behavior at an Appropriate Abstraction Level and Timescale to Maintain Common Ground
PY  - 2025
T2  - Journal of Cognitive Engineering and Decision Making
VL  - 19
IS  - 2
SP  - 174
EP  - 199
DO  - 10.1177/15553434251318477
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002267951&doi=10.1177%2f15553434251318477&partnerID=40&md5=f362e36f02e93d456142d2077dedc4ca
AB  - Automation is becoming increasingly complex, playing a larger role in driving and expanding its operational design domain to dynamic urban roads. Explainable AI (XAI) research in computer science aims to craft explanations of automation that help people understand the behavior of complex algorithms. However, many XAI approaches rely on fixed-format explanations, which may not effectively support drivers with varying levels of automation knowledge and tasks with different timescales. Maintaining common ground is a multilevel process, in which individuals and automation must adjust communication format and abstraction based on knowledge and time constraints. We first draw on existing research to suggest that common ground is a shared understanding between drivers and automation that requires constant maintenance. We applied the abstraction hierarchy (AH) modeling method, which describes complex systems across multiple abstraction levels to match drivers’ cognitive capacity. We modified it to translate vehicle and traffic data into multilevel explanations of automation behavior. We expanded the model into the abstraction–decomposition space, naming it the Driver–Automation Teaming model, designed to generate explanations that account for task timescale. With this modified model, we developed three human–machine interface concepts to demonstrate how it can improve XAI’s support for driver–automation collaboration. © 2025, Human Factors and Ergonomics Society.
KW  - abstraction hierarchy
KW  - automated driving
KW  - common ground
KW  - communication
KW  - eXplainable AI
KW  - human-autonomy teaming
KW  - human–machine interface
KW  - work domain analysis
KW  - Cognitive systems
KW  - Ergonomics
KW  - Hierarchical systems
KW  - Large scale systems
KW  - Man machine systems
KW  - Abstraction hierarchy
KW  - Abstraction level
KW  - Automated driving
KW  - Automated vehicles
KW  - Common ground
KW  - Explainable AI
KW  - Human Machine Interface
KW  - Human-autonomy teaming
KW  - Time-scales
KW  - Work domain analysis
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bhakte, A.
AU  - Kumar Kumawat, P.
AU  - Srinivasan, R.
TI  - Explainable AI methodology for understanding fault detection results during Multi-Mode operations
PY  - 2024
T2  - Chemical Engineering Science
VL  - 299
C7  - 120493
DO  - 10.1016/j.ces.2024.120493
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199017993&doi=10.1016%2fj.ces.2024.120493&partnerID=40&md5=3c51ab8dbbb58054f34cd72e4e0bf48c
AB  - Multi-mode operations are prevalent in the chemical industry. Various methods have been proposed for monitoring multi-mode operations. Of these, AI-based approaches such as Deep Neural Networks (DNN) are becoming popular due to their higher accuracy. However, the lack of transparency of DNNs hinders their widespread adoption in safety–critical applications such as process monitoring. This work addresses this limitation by proposing an Explainable AI (XAI) methodology for multi-mode operations. The proposed methodology encompasses a supervisory system that identifies the current operational mode. This information is used by an Integrated Gradient (IG) based XAI method to configure mode-specific baselines and thus generate DNN explanations corresponding to each operational mode. The ability of this methodology to generate reliable explanations to aid plant operators is illustrated through a simulated CSTR process, the Tennessee-Eastman process, and the pilot-scale Multiphase Flow Facility case study. © 2024 Elsevier Ltd
KW  - Deep Learning
KW  - Explainable Artificial Intelligence
KW  - Multi-mode operations
KW  - Process Monitoring
KW  - Chemical industry
KW  - Deep neural networks
KW  - Fault detection
KW  - Process control
KW  - 'current
KW  - Deep learning
KW  - Explainable artificial intelligence
KW  - Faults detection
KW  - Gradient based
KW  - High-accuracy
KW  - Multimode operations
KW  - Operational modes
KW  - Safety critical applications
KW  - Supervisory systems
KW  - Process monitoring
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Moghadasi, N.
AU  - Valdez, R.S.
AU  - Piran, M.
AU  - Moghaddasi, N.
AU  - Linkov, I.
AU  - Polmateer, T.L.
AU  - Loose, D.C.
AU  - Lambert, J.H.
TI  - Risk Analysis of Artificial Intelligence in Medicine with a Multilayer Concept of System Order
PY  - 2024
T2  - Systems
VL  - 12
IS  - 2
C7  - 47
DO  - 10.3390/systems12020047
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185656044&doi=10.3390%2fsystems12020047&partnerID=40&md5=1811cd6d3f43ddd8acaad33822bbb357
AB  - Artificial intelligence (AI) is advancing across technology domains including healthcare, commerce, the economy, the environment, cybersecurity, transportation, etc. AI will transform healthcare systems, bringing profound changes to diagnosis, treatment, patient care, data, medicines, devices, etc. However, AI in healthcare introduces entirely new categories of risk for assessment, management, and communication. For this topic, the framing of conventional risk and decision analyses is ongoing. This paper introduces a method to quantify risk as the disruption of the order of AI initiatives in healthcare systems, aiming to find the scenarios that are most and least disruptive to system order. This novel approach addresses scenarios that bring about a re-ordering of initiatives in each of the following three characteristic layers: purpose, structure, and function. In each layer, the following model elements are identified: 1. Typical research and development initiatives in healthcare. 2. The ordering criteria of the initiatives. 3. Emergent conditions and scenarios that could influence the ordering of the AI initiatives. This approach is a manifold accounting of the scenarios that could contribute to the risk associated with AI in healthcare. Recognizing the context-specific nature of risks and highlighting the role of human in the loop, this study identifies scenario s.06—non-interpretable AI and lack of human–AI communications—as the most disruptive across all three layers of healthcare systems. This finding suggests that AI transparency solutions primarily target domain experts, a reasonable inclination given the significance of “high-stakes” AI systems, particularly in healthcare. Future work should connect this approach with decision analysis and quantifying the value of information. Future work will explore the disruptions of system order in additional layers of the healthcare system, including the environment, boundary, interconnections, workforce, facilities, supply chains, and others. © 2024 by the authors.
KW  - interpretable and explainable AI
KW  - risk communication
KW  - risk management
KW  - scenario-based preferences
KW  - systems engineering
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Schneider, L.
AU  - Grote, G.
AU  - Boos, D.
AU  - Cakir, Y.
TI  - Designing Targeted Explanations for Heterogeneous Stakeholders - AI for Collaborative Use in the Railways
PY  - 2025
T2  - Conference on Human Factors in Computing Systems - Proceedings 
C7  - 180
DO  - 10.1145/3706599.3719927
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005733282&doi=10.1145%2f3706599.3719927&partnerID=40&md5=98cc3782160fdc3a65aaa12771bec3fb
AB  - The opaqueness of AI continues to represent a significant challenge when designing safe and reliable high risk-AI-systems, particularly for collaborative use by multiple heterogeneous stakeholders. Explainability approaches are used to address this opacity by increasing transparency and interpretability of AI-based systems and thereby establishing human control over the AI-system. In order to be effective, these explainability approaches need to be targeted to needs of all involved stakeholders. We show with an example use case from the railway sector how work design can be combined with technology design for successful human-system integration. Through observation of work processes and interviews with stakeholders at an early stage of system development we can support the design of targeted explanations for high-risk AI systems used in complex collaborative settings by gaining in depth understanding of the respective work systems. We identified five relevant explainability approaches for a visual train inspection decision support system. Exploratory testing then showed preferences for some explanations over others, fostering the discussion on choosing appropriate and effective explanations for visual tasks and informing the design of a final visual train inspection interface prototype for future research and application in the railways. © 2025 Copyright held by the owner/author(s).
KW  - Accountability
KW  - AI Ethics
KW  - AI in high-risk domains
KW  - Control
KW  - Explainability
KW  - Human-in-the-loop
KW  - Socio-technical systems design
KW  - Stakeholder Networks
KW  - Work Design
KW  - XAI
KW  - Economics
KW  - Human engineering
KW  - Man machine systems
KW  - Accountability
KW  - AI ethic
KW  - AI in high-risk domain
KW  - Explainability
KW  - High-risk domains
KW  - Human-in-the-loop
KW  - Socio-technical system design
KW  - Sociotechnical systems
KW  - Stakeholder network
KW  - Work design
KW  - XAI
KW  - Railroad transportation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - El Arab, R.A.
AU  - Almoosa, Z.
AU  - Alkhunaizi, M.
AU  - Abuadas, F.H.
AU  - Somerville, J.
TI  - Artificial intelligence in hospital infection prevention: an integrative review
PY  - 2025
T2  - Frontiers in Public Health
VL  - 13
C7  - 1547450
DO  - 10.3389/fpubh.2025.1547450
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003294593&doi=10.3389%2ffpubh.2025.1547450&partnerID=40&md5=9dd8b8faffed79f158f94908458e3500
AB  - Background: Hospital-acquired infections (HAIs) represent a persistent challenge in healthcare, contributing to substantial morbidity, mortality, and economic burden. Artificial intelligence (AI) offers promising potential for improving HAIs prevention through advanced predictive capabilities. Objective: To evaluate the effectiveness, usability, and challenges of AI models in preventing, detecting, and managing HAIs. Methods: This integrative review synthesized findings from 42 studies, guided by the SPIDER framework for inclusion criteria. We assessed the quality of included studies by applying the TRIPOD checklist to individual predictive studies and the AMSTAR 2 tool for reviews. Results: AI models demonstrated high predictive accuracy for the detection, surveillance, and prevention of multiple HAIs, with models for surgical site infections and urinary tract infections frequently achieving area-under-the-curve (AUC) scores exceeding 0.80, indicating strong reliability. Comparative data suggest that while both machine learning and deep learning approaches perform well, some deep learning models may offer slight advantages in complex data environments. Advanced algorithms, including neural networks, decision trees, and random forests, significantly improved detection rates when integrated with EHRs, enabling real-time surveillance and timely interventions. In resource-constrained settings, non-real-time AI models utilizing historical EHR data showed considerable scalability, facilitating broader implementation in infection surveillance and control. AI-supported surveillance systems outperformed traditional methods in accurately identifying infection rates and enhancing compliance with hand hygiene protocols. Furthermore, Explainable AI (XAI) frameworks and interpretability tools such as Shapley additive explanations (SHAP) values increased clinician trust and facilitated actionable insights. AI also played a pivotal role in antimicrobial stewardship by predicting the emergence of multidrug-resistant organisms and guiding optimal antibiotic usage, thereby reducing reliance on second-line treatments. However, challenges including the need for comprehensive clinician training, high integration costs, and ensuring compatibility with existing workflows were identified as barriers to widespread adoption. Discussion: The integration of AI in HAI prevention and management represents a potentially transformative shift in enhancing predictive capabilities and supporting effective infection control measures. Successful implementation necessitates standardized validation protocols, transparent data reporting, and the development of user-friendly interfaces to ensure seamless adoption by healthcare professionals. Variability in data sources and model validations across studies underscores the necessity for multicenter collaborations and external validations to ensure consistent performance across diverse healthcare environments. Innovations in non-real-time AI frameworks offer viable solutions for scaling AI applications in low- and middle-income countries (LMICs), addressing the higher prevalence of HAIs in these regions. Conclusions: Artificial Intelligence stands as a transformative tool in the fight against hospital-acquired infections, offering advanced solutions for prevention, surveillance, and management. To fully realize its potential, the healthcare sector must prioritize rigorous validation standards, comprehensive data quality reporting, and the incorporation of interpretability tools to build clinician confidence. By adopting scalable AI models and fostering interdisciplinary collaborations, healthcare systems can overcome existing barriers, integrating AI seamlessly into infection control policies and ultimately enhancing patient safety and care quality. Further research is needed to evaluate cost-effectiveness, real-world applications, and strategies (e.g., clinician training and the integration of explainable AI) to improve trust and broaden clinical adoption. Copyright © 2025 El Arab, Almoosa, Alkhunaizi, Abuadas and Somerville.
KW  - artificial intelligence
KW  - explainable AI
KW  - hospital-acquired infections
KW  - infection control
KW  - infection prevention
KW  - infection surveillance
KW  - predictive analytics
KW  - Artificial Intelligence
KW  - Cross Infection
KW  - Humans
KW  - Infection Control
KW  - artificial intelligence
KW  - cross infection
KW  - human
KW  - infection control
KW  - prevention and control
KW  - procedures
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Pyle, C.
AU  - Roemmich, K.
AU  - Andalibi, N.
TI  - U.S. Job-Seekers' Organizational Justice Perceptions of Emotion AI-Enabled Interviews
PY  - 2024
T2  - Proceedings of the ACM on Human-Computer Interaction
VL  - 8
IS  - CSCW2
C7  - 454
DO  - 10.1145/3686993
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209351616&doi=10.1145%2f3686993&partnerID=40&md5=2b237b49b906da6a163229e97664b981
AB  - Emotion AI is increasingly used to automatically evaluate asynchronous hiring interviews. Although touted for increasing hiring fit and reducing bias, it is unclear how job-seekers perceive emotion AI-enabled asynchronous interviews. This gap is striking, given job-seekers' marginalized position in hiring and how job-seekers with marginalized identities may be particularly vulnerable to this technology's potential harms. Addressing this gap, we conducted exploratory interviews with 14 U.S.-based participants with direct, recent experience with emotion AI-enabled asynchronous interviews. While participants acknowledged the asynchronous, virtual modality's potential benefits to employers and job-seekers, they perceived harms to job-seekers associated with automatic emotion inferences that our analysis maps to distributive, procedural, and interactional injustices. We find that social identity can inform job-seekers' perceptions of emotion AI, extending prior work's understandings of the factors contributing to job-seekers' perceptions of AI (broadly) in hiring. Moreover, our results suggest that emotion AI use may reconfigure demands for emotional labor in hiring and that deploying this technology in its current state may unjustly risk harmful outcomes for job-seekers - or, at the very least, perceptions thereof, which shape behaviors and attitudes. Accordingly, we recommend against the present adoption of emotion AI in hiring, identifying opportunities for the design of future asynchronous hiring interview platforms to be meaningfully transparent, contestable, and privacy-preserving. We emphasize that only a subset of perceived harms we surface may be alleviated by these efforts; some injustices may only be resolved by removing emotion AI-enabled features.  © 2024 ACM.
KW  - affective computing
KW  - algorithmic decision-making
KW  - algorithms
KW  - contestability
KW  - emotion recognition
KW  - employment
KW  - explainability
KW  - future of work
KW  - hiring
KW  - justice
KW  - organizational justice
KW  - privacy
KW  - privacy harms
KW  - relational ethics
KW  - transparency
KW  - workplace
KW  - Decision making
KW  - Emotion Recognition
KW  - Affective Computing
KW  - Algorithmic decision-making
KW  - Algorithmics
KW  - Contestability
KW  - Decisions makings
KW  - Emotion recognition
KW  - Explainability
KW  - Future of works
KW  - Hiring
KW  - Justice
KW  - Organisational
KW  - Organizational justice
KW  - Privacy
KW  - Privacy harm
KW  - Relational ethic
KW  - Workplace
KW  - Differential privacy
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Chevalier, O.
AU  - Dubey, G.
AU  - Benkabbou, A.
AU  - Majbar, M.A.
AU  - Souadka, A.
TI  - Comprehensive overview of artificial intelligence in surgery: a systematic review and perspectives
PY  - 2025
T2  - Pflugers Archiv European Journal of Physiology
VL  - 477
IS  - 4
SP  - 617
EP  - 626
DO  - 10.1007/s00424-025-03076-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000449165&doi=10.1007%2fs00424-025-03076-6&partnerID=40&md5=9a6429b4acbff6963b2684450d76015c
AB  - The rapid integration of artificial intelligence (AI) into surgical practice necessitates a comprehensive evaluation of its applications, challenges, and physiological impact. This systematic review synthesizes current AI applications in surgery, with a particular focus on machine learning (ML) and its role in optimizing preoperative planning, intraoperative decision-making, and postoperative patient management. Using PRISMA guidelines and PICO criteria, we analyzed key studies addressing AI’s contributions to surgical precision, outcome prediction, and real-time physiological monitoring. While AI has demonstrated significant promise—from enhancing diagnostics to improving intraoperative safety—many surgeons remain skeptical due to concerns over algorithmic unpredictability, surgeon autonomy, and ethical transparency. This review explores AI’s physiological integration into surgery, discussing its role in real-time hemodynamic assessments, AI-guided tissue characterization, and intraoperative physiological modeling. Ethical concerns, including algorithmic opacity and liability in high-stakes scenarios, are critically examined alongside AI’s potential to augment surgical expertise. We conclude that longitudinal validation, improved AI explainability, and adaptive regulatory frameworks are essential to ensure safe, effective, and ethically sound integration of AI into surgical decision-making. Future research should focus on bridging AI-driven analytics with real-time physiological feedback to refine precision surgery and patient safety strategies. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.
KW  - Decision support systems in surgery
KW  - Ethical implications of AI
KW  - Human-AI collaboration
KW  - Machine learning in surgery
KW  - Surgical artificial intelligence
KW  - Artificial Intelligence
KW  - Humans
KW  - Machine Learning
KW  - artificial intelligence
KW  - augmented reality
KW  - automation
KW  - decision making
KW  - decision support system
KW  - deep learning
KW  - digital twin
KW  - error
KW  - explainable artificial intelligence
KW  - follow up
KW  - information security
KW  - intraoperative monitoring
KW  - machine learning
KW  - mortality risk
KW  - patient care
KW  - postoperative care
KW  - postoperative period
KW  - Preferred Reporting Items for Systematic Reviews and Meta-Analyses
KW  - Review
KW  - surgery
KW  - surgical technology
KW  - systematic review
KW  - tissue characterization
KW  - human
KW  - machine learning
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Wen, Y.
AU  - Li, S.
AU  - Zuo, R.
AU  - Yuan, L.
AU  - Mao, H.
AU  - Liu, P.
TI  - SkillTree: Explainable Skill-Based Deep Reinforcement Learning for Long-Horizon Control Tasks
PY  - 2025
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
VL  - 39
IS  - 20
SP  - 21491
EP  - 21500
DO  - 10.1609/aaai.v39i20.35451
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003996345&doi=10.1609%2faaai.v39i20.35451&partnerID=40&md5=4ff4ce71fc37744ca3ffdb64d58ec49b
AB  - Deep reinforcement learning (DRL) has achieved remarkable success in various domains, yet its reliance on neural networks results in a lack of transparency, which limits its practical applications in safety-critical and human-agent interaction domains. Decision trees, known for their notable explainability, have emerged as a promising alternative to neural networks. However, decision trees often struggle in long-horizon continuous control tasks with high-dimensional observation space due to their limited expressiveness. To address this challenge, we propose SkillTree, a novel hierarchical framework that reduces the complex continuous action space of challenging control tasks into discrete skill space. By integrating the differentiable decision tree within the high-level policy, SkillTree generates discrete skill embeddings that guide low-level policy execution. Furthermore, through distillation, we obtain a simplified decision tree model that improves performance while further reducing complexity. Experiment results validate SkillTree’s effectiveness across various robotic manipulation tasks, providing clear skill-level insights into the decision-making process. The proposed approach not only achieves performance comparable to neural network based methods in complex long-horizon control tasks but also significantly enhances the transparency and explainability of the decision-making process. © 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
KW  - Deep neural networks
KW  - Reinforcement learning
KW  - Robots
KW  - Continuous control
KW  - Control task
KW  - Decision-making process
KW  - High-dimensional
KW  - Higher-dimensional
KW  - Horizon control
KW  - Human-agent interaction
KW  - Interaction domain
KW  - Neural-networks
KW  - Reinforcement learnings
KW  - Deep reinforcement learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Shi, H.
AU  - Li, S.
AU  - Huang, Z.
AU  - Li, S.
AU  - Li, A.
AU  - Zhou, Y.
TI  - Manned and Unmanned Aerial Vehicles Cooperative Combat Framework Based on Large Language Models
PY  - 2024
T2  - ICAS Proceedings
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208791772&partnerID=40&md5=d2c9fbfebd7c89d9ba78b9232dc7074b
AB  - The collaborative combat of manned and unmanned aerial vehicles is one of the principal directions for the future development of aerial combat systems, facing the challenge of excessive decision-making and operational burdens on pilots during UAV control. This paper proposes a novel framework for the collaborative combat of manned and unmanned aerial vehicles, incorporating large language model(LLM) into the process of aircraft pilots commanding and controlling multiple unmanned aerial vehicles (UAVs). The framework utilizes LLM for complex semantic understanding and monitoring of task instruction execution. It allows pilots to issue task instructions to UAVs using non-standard natural language. The received natural language task instructions are matched with the preloaded policy library of the designed task executor in UAVs, and an appropriate policy is selected for execution. During task execution, UAVs provide feedback on the task execution status to manned aircraft at key nodes, and continue task execution upon confirmation by manned aircraft until task completion or receipt of new task instructions. The framework is tested in typical beyond-visual-range combat scenarios of manned and unmanned aerial vehicle collaboration. It exhibits good human-machine interaction, robustness, trustworthiness, explainability, and effectively reducing the decision-making and operational burdens on pilots. The research findings of this paper can be widely applied to various task scenarios where humans and robots collaborate to accomplish tasks, providing a feasible technical route for the collaborative combat of manned and unmanned aerial vehicles. © 2024, International Council of the Aeronautical Sciences. All rights reserved.
KW  - Human- machine cooperation
KW  - Large language model agent
KW  - Large language model control
KW  - Manned and unmanned aerial vehicles cooperative
KW  - Natural language contro
KW  - Collaborative robots
KW  - Fighter aircraft
KW  - Man machine systems
KW  - Military helicopters
KW  - Problem oriented languages
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Human-machine cooperation
KW  - Language model
KW  - Large language model agent
KW  - Large language model control
KW  - Manned and unmanned aerial vehicle cooperative
KW  - Model agents
KW  - Modelling controls
KW  - Natural language contro
KW  - Natural languages
KW  - Semantics
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Fregosi, C.
AU  - Cabitza, F.
TI  - A Frictional Design Approach: Towards Judicial AI and its Possible Applications
PY  - 2024
T2  - CEUR Workshop Proceedings
VL  - 3825
SP  - 23
EP  - 28
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210318631&partnerID=40&md5=57237a3a4d79500d5013d2a24a1b6b7a
AB  - Decision support systems (DSS) are increasingly being integrated into high-stakes domains like healthcare, law, and finance, where critical decisions have significant consequences. Traditional DSS often provide a single, clear-cut recommendation, which can lead to automation bias and diminish the user’s sense of agency. However, there is a growing concern about the over-reliance on these systems and the potential for deskilling among users. The knowledge gap we aim to address is the development of decision support systems that effectively encourage critical reflection and maintain user engagement and responsibility in decision-making processes. In this workshop contribution, we report on the development of Judicial AI, a novel approach inspired by Frictional AI. Judicial AI diverges from traditional DSS by offering multiple, contrasting explanations to support different potential outcomes. This design encourages users to engage in deeper cognitive processing, thereby promoting critical reflection, reducing automation bias, and preserving the user’s sense of agency. This ongoing study employs a two-arm experiment to investigate the effects of this approach in the context of content classification tasks, comparing it with the traditional protocol. The expected outcomes of this ongoing study suggest that the Judicial protocol could not only mitigate automation bias but also safeguard users’ sense of agency and promote long-term skill retention. © 2024 Copyright for this paper by its authors.
KW  - eXplainable AI (XAI)
KW  - Frictional AI
KW  - Human-AI Decision making process
KW  - Judicial AI
KW  - Artificial intelligence
KW  - Decision making
KW  - Automation bias
KW  - Critical reflections
KW  - Decision supports
KW  - Decision-making process
KW  - Explainable AI (XAI)
KW  - Frictional AI
KW  - Human-AI decision making process
KW  - Judicial AI
KW  - Sense of agencies
KW  - Support systems
KW  - Decision support systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Senoner, J.
AU  - Schallmoser, S.
AU  - Kratzwald, B.
AU  - Feuerriegel, S.
AU  - Netland, T.
TI  - Explainable AI improves task performance in human–AI collaboration
PY  - 2024
T2  - Scientific Reports
VL  - 14
IS  - 1
C7  - 31150
DO  - 10.1038/s41598-024-82501-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213519314&doi=10.1038%2fs41598-024-82501-9&partnerID=40&md5=82e9bf118cc37b240571021c3ee2c53a
AB  - Artificial intelligence (AI) provides considerable opportunities to assist human work. However, one crucial challenge of human–AI collaboration is that many AI algorithms operate in a black-box manner where the way how the AI makes predictions remains opaque. This makes it difficult for humans to validate a prediction made by AI against their own domain knowledge. For this reason, we hypothesize that augmenting humans with explainable AI improves task performance in human–AI collaboration. To test this hypothesis, we implement explainable AI in the form of visual heatmaps in inspection tasks conducted by domain experts. Visual heatmaps have the advantage that they are easy to understand and help to localize relevant parts of an image. We then compare participants that were either supported by (a) black-box AI or (b) explainable AI, where the latter supports them to follow AI predictions when the AI is accurate or overrule the AI when the AI predictions are wrong. We conducted two preregistered experiments with representative, real-world visual inspection tasks from manufacturing and medicine. The first experiment was conducted with factory workers from an electronics factory, who performed assessments of whether electronic products have defects. The second experiment was conducted with radiologists, who performed assessments of chest X-ray images to identify lung lesions. The results of our experiments with domain experts performing real-world tasks show that task performance improves when participants are supported by explainable AI with heatmaps instead of black-box AI. We find that explainable AI as a decision aid improved the task performance by 7.7 percentage points (95% confidence interval [CI]: 3.3% to 12.0%,) in the manufacturing experiment and by 4.7 percentage points (95% CI: 1.1% to 8.3%,) in the medical experiment compared to black-box AI. These gains represent a significant improvement in task performance. © The Author(s) 2024.
KW  - Decision-making
KW  - Explainable AI
KW  - Human-centered AI
KW  - Human–AI collaboration
KW  - Task performance
KW  - Adult
KW  - Algorithms
KW  - Artificial Intelligence
KW  - Female
KW  - Humans
KW  - Male
KW  - Task Performance and Analysis
KW  - adult
KW  - algorithm
KW  - artificial intelligence
KW  - female
KW  - human
KW  - male
KW  - task performance
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Alt, B.
AU  - Zahn, J.
AU  - Kienle, C.
AU  - Dvorak, J.
AU  - May, M.
AU  - Katic, D.
AU  - Jäkel, R.
AU  - Kopp, T.
AU  - Beetz, M.
AU  - Lanza, G.
TI  - Human-AI Interaction in Industrial Robotics: Design and Empirical Evaluation of a User Interface for Explainable AI-Based Robot Program Optimization
PY  - 2024
T2  - IFAC-PapersOnLine
VL  - 58
IS  - 27
SP  - 591
EP  - 596
DO  - 10.1016/j.procir.2024.10.134
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213028346&doi=10.1016%2fj.procir.2024.10.134&partnerID=40&md5=5646b09d73b1828b533e3392837b42a1
AB  - While recent advances in deep learning have demonstrated its transformative potential, its adoption for real-world manufacturing applications remains limited. We present an Explanation User Interface (XUI) for a state-of-the-art deep learning-based robot program optimizer which provides both naive and expert users with different user experiences depending on their skill level, as well as Explainable AI (XAI) features to facilitate the application of deep learning methods in real-world applications. To evaluate the impact of the XUI on task performance, user satisfaction and cognitive load, we present the results of a preliminary user survey and propose a study design for a large-scale follow-up study. © 2024 The Authors.
KW  - deep learning
KW  - explainable artificial intelligence
KW  - explanation user interfaces
KW  - industrial robotics
KW  - manufacturing
KW  - user study
KW  - Contrastive Learning
KW  - Deep learning
KW  - Human robot interaction
KW  - Industrial robots
KW  - Microrobots
KW  - Robot applications
KW  - Robot learning
KW  - Robot programming
KW  - Deep learning
KW  - Design evaluation
KW  - Empirical evaluations
KW  - Explainable artificial intelligence
KW  - Explanation user interface
KW  - Industrial robotics
KW  - Real-world
KW  - Robot programs
KW  - Robotic design
KW  - User study
KW  - Smart manufacturing
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Masaeid, T.A.
AU  - Alkhalidi, M.M.
AU  - Ali, A.A.A.A.
AU  - Almaazmi, S.M.G.A.
AU  - Alami, R.
TI  - Artificial Intelligence-Augmented Decision-Making: Examining the Interplay Between Machine Learning Algorithms and Human Judgment in Organizational Leadership
PY  - 2025
T2  - Journal of Ecohumanism
VL  - 4
IS  - 1
SP  - 4683
EP  - 4699
DO  - 10.62754/joe.v4i1.6364
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218781082&doi=10.62754%2fjoe.v4i1.6364&partnerID=40&md5=e46d6bd01c86d5bcb4358ac65da3fd66
AB  - The paper discusses the bright and dark sides of the relationship between human judgment and AI-driven machine learning (ML) algorithms. While discussing important issues, such as algorithm aversion, automation bias, and trust, it probes into how AI improves decision-making efficiency through predictive accuracy, resource optimisation, and data-driven insights. Even as AI can revolutionise decision-making, its effective integration must balance algorithmic output and human judgment. The most critical challenges include automation bias resulting from over-reliance on advice given by AI and algorithm aversion driven by concerns related to AI failures. Open systems, explainable AI (XAI) frameworks, and user-centered design can help to engender confidence in AI systems and alleviate these issues. Accountability, equity, and prejudice concerns raise further ethical considerations with AI. The study proposed several tactics that might mitigate such challenges: audits of ethics, adherence to legal policy, and integration of the AI systems with the company’s values. It underlines the human-AI collaboration that will be increasingly necessary, as well as hybrid models for decision-making that bring algorithmic accuracy to human intuition. It follows the case study review and empirical findings with practical lessons for organisational leaders on ethics, best deployment practices for AI, and tactical ways to engender better collaboration and trust. The conclusion outlines the need to enhance the explainability features of AI, study cognitive dynamics in decision processes, and work out ethical schemata guiding leading positions for AI. Beyond providing a roadmap for organisations to leverage the interaction of human judgment and machine intelligence to drive and achieve more ethical and effective leadership outcomes, this paper tries to contribute to the ongoing debate on AI-augmented decision-making. © 2025, Creative Publishing House. All rights reserved.
KW  - AI-augmented decision-making
KW  - algorithm aversion
KW  - algorithm machine learning (ML) algorithms
KW  - and human-AI collaboration
KW  - Cognitive Bias
KW  - Data-driven insights
KW  - Ethical AI
KW  - ethical frameworks
KW  - Explainable AI (XAI)
KW  - human judgment
KW  - human-AI collaboration
KW  - Hybrid Decision-Making Models
KW  - strategies of leaders
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Kathrine Kollerup, N.
AU  - Johansen, S.S.
AU  - Tolsgaard, M.G.
AU  - Lønborg Friis, M.
AU  - Skov, M.B.
AU  - van Berkel, N.
TI  - Clinical needs and preferences for AI-based explanations in clinical simulation training
PY  - 2025
T2  - Behaviour and Information Technology
VL  - 44
IS  - 5
SP  - 954
EP  - 974
DO  - 10.1080/0144929X.2024.2334852
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001929911&doi=10.1080%2f0144929X.2024.2334852&partnerID=40&md5=8b213c5f791534515483335248862ad2
AB  - Medical training is a key element in maintaining and improving today's healthcare standards. Given the nature of medical work, students must master not only theory but also develop their hands-on abilities and skills in clinical practice. Medical simulators play an increasing role in supporting the active learning of these students due to their ability to present a large variety of tasks allowing students to train and experiment indefinitely without causing any patient harm. While the criticality of explainable AI systems has been extensively discussed in the literature, the medical training context presents unique user needs for explanations. In this paper, we explore the potential gap of current limitations within simulation-based training, and the role Artificial Intelligence (AI) holds in supporting the needs of medical students in training. Through contextual inquiries and interviews with clinicians in training (N = 9) and subsequent validation with medical experts (N = 4), we obtain an understanding of the shortcomings in current simulation-based training and offer recommendations for future AI-driven training. Our results stress the need for continuous and actionable feedback that resembles the interaction between clinical supervisor and resident in real-world training scenarios while adjusting training material to the residents' skills and prior performance. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
KW  - AI support systems
KW  - clinical
KW  - Explanations
KW  - feedback
KW  - learning
KW  - simulation
KW  - Artificial intelligence
KW  - Artificial intelligence support system
KW  - Clinical
KW  - Clinical simulation
KW  - Explanation
KW  - Intelligence-support systems
KW  - Learning
KW  - Medical training
KW  - Simulation
KW  - Simulation training
KW  - Simulation-based training
KW  - Students
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - BOOK
AU  - Paramasivan, P.
AU  - Rajest, S.S.
AU  - Chinnusamy, K.
AU  - Regin, R.
AU  - Joseph, F.J.J.
TI  - Cross-industry AI applications
PY  - 2024
T2  - Cross-Industry AI Applications
SP  - 1
EP  - 389
DO  - 10.4018/979-8-3693-5951-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199666214&doi=10.4018%2f979-8-3693-5951-8&partnerID=40&md5=f56330ee69a6dd84e3145f7ea5e1c97b
AB  - The rise of Artificial Intelligence (AI) amidst the backdrop of a world that is changing at lightning speed presents a whole new set of challenges. One of our biggest hurdles is more transparency in AI solutions. It's a complex issue, but one that we need to address if we want to ensure that the benefits of AI are accessible to everyone. Across diverse sectors such as healthcare, surveillance, and human-computer interaction, the inability to understand and evaluate AI's decision-making processes hinders progress and raises concerns about accountability. Cross-Industry AI Applications is a groundbreaking solution to illuminate the mysteries of AI-driven human behavior analysis. This pioneering book addresses the necessity of transparency and explainability in AI systems, particularly in analyzing human behavior. Compiling insights from leading experts and academics offers a comprehensive exploration of state-of-the-art methodologies, benchmarks, and systems for understanding and predicting human behavior using AI. Each chapter delves into novel approaches and real-world applications, from facial expressions to gesture recognition, providing invaluable knowledge for scholars and professionals alike. With its focus on explainable AI and its implications for decision-making, Cross-Industry AI Applications serves as a roadmap for navigating the complexities of human behavior analysis in the age of AI. By offering clarity and insight into AI algorithms and their impact on diverse industries, this book empowers readers to harness the full potential of AI while ensuring accountability and ethical practice. Join the forefront of AI research and innovation - order your copy today and embark on a transformative journey into the future of human-AI interaction. © 2024 by IGI Global. All rights reserved.
M3  - Book
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CHAP
AU  - Dewasiri, N.J.
AU  - Dharmarathna, D.G.
AU  - Choudhary, M.
TI  - Leveraging artificial intelligence for enhanced risk management in banking: A systematic literature review
PY  - 2024
T2  - Artificial Intelligence Enabled Management: An Emerging Economy Perspective
SP  - 197
EP  - 213
DO  - 10.1515/9783111172408013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193833743&doi=10.1515%2f9783111172408013&partnerID=40&md5=9e88db9ae791d08b3a3910e5677f34d0
AB  - This systematic review delves into the transformative role of Artificial Intelligence (AI) in the banking industry's risk management practices. AI, encompassing machine learning, data analytics, and natural language processing, has enhanced risk assessment, mitigation, and decision-making processes. The findings emphasise AI's capacity to identify and assess risks, enabling proactive risk management effectively. Applications like credit scoring models, fraud detection systems, and stress testing tools play instrumental roles in optimising risk management processes. At the same time, the importance of data quality, governance, and transparency cannot be overstated in successfully implementing AI-driven risk management strategies. The implications of AI in banking are profound, offering data-driven procedures, equitable lending practices, and enhanced operational efficiency. However, data privacy concerns, model interpretability issues, and regulatory compliance complexities must be addressed carefully. Emerging trends in AI for risk management encompass Explainable AI, AI-enabled regulatory Compliance, AI for Cybersecurity Risk Management, and Natural Language Processing for Unstructured Data Analysis, along with the optimisation of efficiency through Robotic Process Automation in Risk Operations. Future research should focus on ethical considerations, dynamic stress testing models, AI's role in climate-related risk analysis, human-AI collaboration, cybersecurity risk prediction, and the development of robust regulatory frameworks for AI integration in risk management. AI stands poised to revolutionise banking risk management. Still, responsible and ethical integration is paramount, necessitating collaborative efforts to harness its full potential while ensuring trust and stability within the sector. © 2024 Walter de Gruyter GmbH, Berlin/Boston.
KW  - Artificial Intelligence (AI)
KW  - Banking Industry
KW  - Data Analytics
KW  - Regulatory Compliance
KW  - Risk Management
KW  - Artificial intelligence
KW  - Climate models
KW  - Data Analytics
KW  - Decision making
KW  - Efficiency
KW  - Ethical technology
KW  - Information management
KW  - Learning algorithms
KW  - Natural language processing systems
KW  - Regulatory compliance
KW  - Risk analysis
KW  - Risk assessment
KW  - Artificial intelligence
KW  - Banking industry
KW  - Cyber security
KW  - Data analytics
KW  - Language processing
KW  - Natural languages
KW  - Risks management
KW  - Stress Testing
KW  - Systematic literature review
KW  - Systematic Review
KW  - Risk management
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 5
ER  -

TY  - CONF
AU  - Alt, B.
AU  - Zahn, J.
AU  - Kienle, C.
AU  - Dvorak, J.
AU  - May, M.
AU  - Katic, D.
AU  - Jäkel, R.
AU  - Kopp, T.
AU  - Beetz, M.
AU  - Lanza, G.
TI  - Human-AI Interaction in Industrial Robotics: Design and Empirical Evaluation of a User Interface for Explainable AI-Based Robot Program Optimization
PY  - 2024
T2  - Procedia CIRP
VL  - 130
SP  - 591
EP  - 596
DO  - 10.1016/j.procir.2024.10.134
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215011175&doi=10.1016%2fj.procir.2024.10.134&partnerID=40&md5=1789c67601d69f87e07eb33e98bf5243
AB  - While recent advances in deep learning have demonstrated its transformative potential, its adoption for real-world manufacturing applications remains limited. We present an Explanation User Interface (XUI) for a state-of-the-art deep learning-based robot program optimizer which provides both naive and expert users with different user experiences depending on their skill level, as well as Explainable AI (XAI) features to facilitate the application of deep learning methods in real-world applications. To evaluate the impact of the XUI on task performance, user satisfaction and cognitive load, we present the results of a preliminary user survey and propose a study design for a large-scale follow-up study. © 2024 The Authors. Published by Elsevier B.V.
KW  - deep learning
KW  - explainable artificial intelligence
KW  - explanation user interfaces
KW  - industrial robotics
KW  - manufacturing
KW  - user study
KW  - Contrastive Learning
KW  - Industrial robots
KW  - Microrobots
KW  - Robot programming
KW  - Deep learning
KW  - Design evaluation
KW  - Empirical evaluations
KW  - Explainable artificial intelligence
KW  - Explanation user interface
KW  - Industrial robotics
KW  - Real-world
KW  - Robot programs
KW  - Robotic design
KW  - User study
KW  - Smart manufacturing
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CHAP
AU  - Adhvaryu, R.
AU  - Agal, S.
AU  - Odedra, N.D.
AU  - Rusho, M.A.
AU  - Pande, S.D.
TI  - An in-depth exploration of data analysis and processing through the prism of explainable artificial intelligence paradigms
PY  - 2024
T2  - Explainable Artificial Intelligence for Biomedical and Healthcare Applications
SP  - 249
EP  - 266
DO  - 10.1201/9781003220107-15
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210683744&doi=10.1201%2f9781003220107-15&partnerID=40&md5=bc9bdd3e4b9c7eedf6943404d6cfcd50
AB  - This extensive study investigates the complex problem of data processing and analysis through the lens of explainable artificial intelligence (XAI), a highly advanced kind of artificial intelligence. The purpose of this study is to perform a detailed examination of the many functions that XAI plays in shining light on the often-obscure processes that underpin AI-driven data analytics. The major premise of this study is that the openness and understandability given by XAI may significantly improve the performance and accuracy of data interpretation for sophisticated artificial intelligence models. Although the abstract covers a wide range of XAI applications, its major focus is on the relevance of these applications in terms of democratizing data-driven insights and encouraging human–AI trust. The current discussion primarily focuses on the ethical implications, issues associated with the deployment of XAI in various settings, and the prognosis for this rapidly growing business. This article introduces XAI as a cutting-edge technology that has the potential to change data analysis. People and organizations navigating the contemporary world's data-rich environment depend on XAI. © 2025 selection and editorial matter, Aditya Khamparia and Deepak Gupta; individual chapters, the contributors.
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Zhang, Z.T.
AU  - Argın, S.K.
AU  - Bilen, M.B.
AU  - Urgun, D.
AU  - Deniz, S.M.
AU  - Liu, Y.
AU  - Hassib, M.
TI  - Measuring the effect of mental workload and explanations on appropriate AI reliance using EEG
PY  - 2024
T2  - Behaviour and Information Technology
DO  - 10.1080/0144929X.2024.2431055
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210474451&doi=10.1080%2f0144929X.2024.2431055&partnerID=40&md5=ba19d0ef535559167d943a8b7df57fb8
AB  - AI is anticipated to improve human decision-making across various domains, often in high-stakes, difficult tasks. However, human reliance on AI recommendations is often inappropriate. A common approach to address this is to provide explanations about the AI output to decision makers, but results have been mixed so far. It often remains unclear when people can rely appropriately on AI and when explanations can help. In this work, we conducted a lab experiment (N = 34) to investigate how the appropriateness of human reliance on (explainable) AI depends on the mental workload induced by different decision difficulties. Instead of self-assessments, we used EEG (Emotiv Epoc Flex head cap, 32 wet electrodes) to more directly measure participants' mental workload. We found that the difficulty of a decision, indicated by the induced mental workload, strongly influences participants' ability to rely appropriately on AI, as assessed through relative self-reliance, relative AI reliance, and decision accuracy with and without AI. While reliance was appropriate for low mental workload decisions, participants were prone to overreliance in high mental workload decisions. Explanations had no significant effect in either case. Our results imply that alternatives to the common ‘recommend-and-explain’ approach should be explored to assist human decision-making in challenging tasks. © 2024 Informa UK Limited, trading as Taylor & Francis Group.
KW  - appropriate reliance
KW  - decision difficulty
KW  - EEG
KW  - explainable AI
KW  - Human-AI decision-making
KW  - mental workload
KW  - Appropriate reliance
KW  - Decision difficulty
KW  - Decision makers
KW  - Decisions makings
KW  - Explainable AI
KW  - Human decision-making
KW  - Human-AI decision-making
KW  - Lab. experiment
KW  - Mental workload
KW  - Self-assessment
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CHAP
AU  - Ghela, S.
AU  - Bhagavatula, A.
AU  - Tripathy, B.K.
TI  - Unveiling the Power of Explainable AI: Real-World Applications and Implications
PY  - 2024
T2  - Explainable, Interpretable, and Transparent AI Systems
SP  - 1
EP  - 19
DO  - 10.1201/9781003442509-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201888075&doi=10.1201%2f9781003442509-1&partnerID=40&md5=436b8693897d7a3c22398a09d6a09dfd
AB  - Artificial intelligence (AI), a mainstay of modern decision-making, is a black box that is widely used but sometimes poorly understood. Despite the numerous potential advantages of AI in algorithmic decision-making in various domains, the significance of trusting these systems has just recently come to light. The field of explainable artificial intelligence (XAI) tries to make a model less mysterious by explaining its behavior and/or predictions and enables the primary stakeholders to examine the model and place more trust in it. “If algorithm results are low-impact, like the songs recommended by a music service, society probably doesn’t need regulators plumbing the depths of how those recommendations are made. However, the primary stakeholders may not be able to live with much more important decisions borne out of AI systems, perhaps literally in the case of a recommended medical treatment or a rejected application for a mortgage loan.” Through this chapter, we aim to dispel the magic behind this black box by exploring and understanding various applications of XAI. © 2024 CRC Press.
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Tutul, A.A.
AU  - Nirjhar, E.H.
AU  - Chaspari, T.
TI  - Investigating Trust in Human-AI Collaboration for a Speech-Based Data Analytics Task
PY  - 2025
T2  - International Journal of Human-Computer Interaction
VL  - 41
IS  - 5
SP  - 2936
EP  - 2954
DO  - 10.1080/10447318.2024.2328910
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000377267&doi=10.1080%2f10447318.2024.2328910&partnerID=40&md5=e1dc9dc890242b65d3eaf0dedd70a0fc
AB  - Complex real-world problems can benefit from the collaboration between humans and artificial intelligence (AI) to achieve reliable decision-making. We investigate trust in a human-in-the-loop decision-making task, in which participants with background on psychological sciences collaborate with an explainable AI system for estimating one’s anxiety level from speech. The AI system relies on the explainable boosting machine (EBM) model which takes prosodic features as the input and estimates the anxiety level. Trust in AI is quantified via self-reported (i.e., administered via a questionnaire) and behavioral (i.e., computed as user-AI agreement) measures, which are positively correlated with each other. Results indicate that humans and AI depict differences in performance depending on the characteristics of the specific case under review. Overall, human annotators’ trust in the AI increases over time, with momentary decreases after the AI partner makes an error. Annotators further differ in terms of appropriate trust calibration in the AI system, with some annotators over-trusting and some under-trusting the system. Personality characteristics (i.e., agreeableness, conscientiousness) and overall propensity to trust machines further affect the level of trust in the AI system, with these findings approaching statistical significance. Results from this work will lead to a better understanding of human-AI collaboration and will guide the design of AI algorithms toward supporting better calibration of user trust. © 2024 Taylor & Francis Group, LLC.
KW  - Explainable AI
KW  - human trust
KW  - transparency
KW  - trust calibration
KW  - Behavioral research
KW  - Data Analytics
KW  - Decision making
KW  - Anxiety levels
KW  - Artificial intelligence systems
KW  - Data analytics
KW  - Decisions makings
KW  - Explainable artificial intelligence
KW  - Human trust
KW  - Human-in-the-loop
KW  - Machine modelling
KW  - Real-world problem
KW  - Trust calibration
KW  - Calibration
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - CHAP
AU  - Jain, B.
AU  - Mamodiya, U.
TI  - Al-Enhanced Security Information and Event Management
PY  - 2025
T2  - Deep Learning Innovations for Securing Critical Infrastructures
SP  - 109
EP  - 118
DO  - 10.4018/979-8-3373-0563-9.ch007
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005131847&doi=10.4018%2f979-8-3373-0563-9.ch007&partnerID=40&md5=9429a6aa3e590c058c9dc06bb99385c7
AB  - AI-enhanced Security Information and Event Management (SIEM) systems are the new paradigm of cybersecurity. SIEM itself is combined with a full extent of power in terms of sophisticated artificial intelligence through machine learning, natural language processing, and predictive analytics, so now the system offers real-time monitoring, further advancement of anomaly detection, fully automated incident response, as well as minimization in detection as well as in response with reduced false positives. These AI-augmented SIEM systems will, in a proactive way, defend organizations from both known as well as unknown threats through integrating user behaviour analytics, context-aware insights, and predictive modelling. The architecture, primary features, and the benefits of AI-based SIEM have been explored by the authors considering data quality problems, computational overheads, and explainability issues. Some examples of use cases like cloud security, IoT protection, and phishing prevention are given below to validate real-world applicability. © 2025 by IGI Global Scientific Publishing. All rights reserved.
KW  - Anomaly detection
KW  - Cyber attacks
KW  - Data Analytics
KW  - Natural language processing systems
KW  - Prediction models
KW  - Anomaly detection
KW  - Cyber security
KW  - Fully automated
KW  - Language processing
KW  - Machine-learning
KW  - Natural languages
KW  - Power
KW  - Real time monitoring
KW  - Security information and event management systems
KW  - Security information and event managements
KW  - Information management
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Kirkby, A.
AU  - Baumgarth, C.
AU  - Henseler, J.
TI  - Welcome, new brand colleague! A conceptual framework for efficient and effective human–AI co-creation for creative brand voice
PY  - 2025
T2  - Journal of Brand Management
DO  - 10.1057/s41262-025-00387-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005096759&doi=10.1057%2fs41262-025-00387-y&partnerID=40&md5=33752cfc2e1071856c177218623b93a7
AB  - The rapid advancement of artificial intelligence (AI) capabilities has extended into creative realms, presenting opportunities for creative collaboration between human brand professionals and AI in support of brand voice efforts. However, there remains little clarity regarding the implementation of this creative interaction. With a conceptual approach, the current research proposes a three-level framework of human–AI co-creation for creative brand voice that highlights key factors that can facilitate brand efficiency and effectiveness at the individual (AI task roles, co-creation teaming, knowledge and skills), organisational (infrastructure and brand voice database, socialisation), and societal (responsibility and accountability, AI transparency, brand voice copyright) levels. Each level presents different challenges and insights. At the individual level, it is critical to consider operational processes; at the organisational level, managing the interactions is key; and at the societal level, external influences must be accounted for, to manage the brand. This research contribution in turn offers theoretical guidance, aligned with a high-level brand management perspective, on how to pursue efficiency and effectiveness at three defined levels, as well as relevant avenues for further research. © The Author(s) 2025.
KW  - Artificial intelligence
KW  - Brand management
KW  - Brand voice
KW  - Co-creation
KW  - Creativity
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Huang, B.
AU  - Niyomsilp, E.
TI  - The impact of artificial intelligence on organizational decision-making processes
PY  - 2025
T2  - Edelweiss Applied Science and Technology
VL  - 9
IS  - 4
SP  - 794
EP  - 808
DO  - 10.55214/25768484.v9i4.6081
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003212632&doi=10.55214%2f25768484.v9i4.6081&partnerID=40&md5=6a3f5e76a4e1d8f329ea949f9560e96a
AB  - Using a mixed-methods approach, data was collected through quantitative surveys (N=258) and qualitative interviews with AI practitioners and decision-makers across multiple industries. Findings indicate that AI significantly improves decision efficiency by automating analytical tasks, reducing human cognitive biases, and enabling real-time insights. However, challenges persist, particularly in algorithmic transparency, ethical governance, and compliance with regulatory standards. Key findings reveal that AI integration positively influences decision effectiveness (β=0.156, p=0.031), but human oversight (β=0.381, p<0.001) and regulatory compliance (β=0.314, p<0.001) play crucial mediating roles. Ethical and security challenges necessitate stronger AI governance frameworks, as organizations struggle with bias mitigation, legal accountability, and AI explainability. Industry experts emphasize the need for a hybrid Human-AI collaboration model, ensuring AI remains an augmentation rather than a replacement for human decision-makers. This study contributes to AI governance literature by highlighting the importance of ethical AI deployment, transparent decision systems, and regulatory adherence. Future research should explore AI’s impact in high-risk sectors, develop proactive AI compliance strategies, and examine cross-national AI regulatory frameworks to enhance responsible AI adoption globally. © 2025 by the authors.
KW  - AI-driven decision-making
KW  - Artificial intelligence
KW  - Ethical AI
KW  - Human oversight
KW  - Organizational strategy
KW  - Regulatory compliance
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Gardezi, M.
AU  - Joshi, B.
AU  - Rizzo, D.M.
AU  - Ryan, M.
AU  - Prutzer, E.
AU  - Brugler, S.
AU  - Dadkhah, A.
TI  - Artificial intelligence in farming: Challenges and opportunities for building trust
PY  - 2024
T2  - Agronomy Journal
VL  - 116
IS  - 3
SP  - 1217
EP  - 1228
DO  - 10.1002/agj2.21353
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160595564&doi=10.1002%2fagj2.21353&partnerID=40&md5=0e212f1b94a6b3c4266bc0b761dc0b51
AB  - Artificial intelligence (AI) represents technologies with human-like cognitive abilities to learn, perform, and make decisions. AI in precision agriculture (PA) enables farmers and farm managers to deploy highly targeted and precise farming practices based on site-specific agroclimatic field measurements. The foundational and applied development of AI has matured considerably over the last 30 years. The time is now right to engage seriously with the ethics and responsible practice of AI for the well-being of farmers and farm managers. In this paper, we identify and discuss both challenges and opportunities for improving farmers’ trust in those providing AI solutions for PA. We highlight that farmers’ trust can be moderated by how the benefits and risks of AI are perceived, shared, and distributed. We propose four recommendations for improving farmers’ trust. First, AI developers should improve model transparency and explainability. Second, clear responsibility and accountability should be assigned to AI decisions. Third, concerns about the fairness of AI need to be overcome to improve human-machine partnerships in agriculture. Finally, regulation and voluntary compliance of data ownership, privacy, and security are needed, if AI systems are to become accepted and used by farmers. © 2023 The Authors. Agronomy Journal published by Wiley Periodicals LLC on behalf of American Society of Agronomy.
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 27
ER  -

TY  - JOUR
AU  - Wang, K.
AU  - Hou, W.
AU  - Hong, L.
AU  - Guo, J.
TI  - Smart Transparency: A User-Centered Approach to Improving Human–Machine Interaction in High-Risk Supervisory Control Tasks
PY  - 2025
T2  - Electronics (Switzerland)
VL  - 14
IS  - 3
C7  - 420
DO  - 10.3390/electronics14030420
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217748678&doi=10.3390%2felectronics14030420&partnerID=40&md5=f5fe1ee1b352439fd7c34c1f436eb569
AB  - In supervisory control tasks, particularly in high-risk fields, operators need to collaborate with automated intelligent agents to manage dynamic, time-sensitive, and uncertain information. Effective human–agent collaboration relies on transparent interface communication to align with the operator’s cognition and enhance trust. This paper proposes a human-centered adaptive transparency information design framework (ATDF), which dynamically adjusts the display of transparency information based on the operator’s needs and the task type. This ensures that information is accurately conveyed at critical moments, thereby enhancing trust, task performance, and interface usability. Additionally, the paper introduces a novel user research method, Heu–Kano, to explore the prioritization of transparency needs and presents a model based on eye-tracking and machine learning to identify different types of human–agent interactions. This research provides new insights into human-centered explainability in supervisory control tasks. © 2025 by the authors.
KW  - activity recognition
KW  - adaptive design
KW  - eye tracking
KW  - supervisory control tasks
KW  - transparency
KW  - user research
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - de Brito Duarte, R.
TI  - Explainable AI as a Crucial Factor for Improving Human-AI Decision-Making Processes
PY  - 2024
T2  - CEUR Workshop Proceedings
VL  - 3793
SP  - 345
EP  - 352
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208285005&partnerID=40&md5=bf4190e06bf68832eb03cfa2221f266c
AB  - A crucial aspect of AI-assisted decision making involves providing explanations for AI recommendations and predictions. Despite the optimism surrounding eXplainable AI (XAI) to improve transparency and trustworthiness, several studies have highlighted its shortcomings. My doctoral research aims to develop and validate a framework for human-AI decision making where explanations are central, serving as an enhancement factor for AI-assisted decision tasks. I hypothesize that a robust framework will elucidate underlying mechanisms and investigate the effects of AI explanations on decision outcomes. This research will advance our understanding of the combination of AI and human capabilities, informing the design of AI-assisted decision tasks for real-world scenarios. © 2024 Copyright for this paper by its authors.
KW  - eXplainable AI
KW  - Human-AI Decision Making
KW  - Human-AI Interaction
KW  - Decision outcome
KW  - Decision task
KW  - Decision-making process
KW  - Decisions makings
KW  - Doctoral research
KW  - Enhancement factor
KW  - Explainable AI
KW  - Human capability
KW  - Human-AI decision making
KW  - Human-AI interaction
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Naiseh, M.
AU  - Webb, C.
AU  - Underwood, T.
AU  - Ramchurn, G.
AU  - Walters, Z.
AU  - Thavanesan, N.
AU  - Vigneswaran, G.
TI  - XAI for Group-AI Interaction: Towards Collaborative and Inclusive Explanations
PY  - 2024
T2  - CEUR Workshop Proceedings
VL  - 3793
SP  - 249
EP  - 256
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208284036&partnerID=40&md5=035ba61682ad3e0add621ac5c7bcab8d
AB  - The increasing integration of Machine Learning (ML) into decision-making across various sectors has raised concerns about ethics, legality, explainability, and safety, highlighting the necessity of human oversight. In response, eXplainable AI (XAI) has emerged as a means to enhance transparency by providing insights into ML model decisions and offering humans an understanding of the underlying logic. Despite its potential, existing XAI models often lack practical usability and fail to improve human-AI performance, as they may introduce issues such as overreliance. This underscores the need for further research in Human-Centered XAI to improve the usability of current XAI methods. Notably, much of the current research focuses on one-to-one interactions between the XAI and individual decision-makers, overlooking the dynamics of many-to-one relationships in real-world scenarios where groups of humans collaborate using XAI in collective decision-making. In this late-breaking work, we draw upon current work in Human-Centered XAI research and discuss how XAI design could be transitioned to group-AI interaction. We discuss four potential challenges in the transition of XAI from human-AI interaction to group-AI interaction. This paper contributes to advancing the field of Human-Centered XAI and facilitates the discussion on group-XAI interaction, calling for further research in this area. © 2024 Copyright for this paper by its authors.
KW  - Explainable AI
KW  - Group-AI Interaction
KW  - Interaction Design
KW  - Usability engineering
KW  - 'current
KW  - Decisions makings
KW  - Explainable AI
KW  - Group-AI interaction
KW  - Human oversight
KW  - Interaction design
KW  - Machine learning models
KW  - Machine-learning
KW  - Modeling decisions
KW  - Performance
KW  - Adversarial machine learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lash, M.T.
TI  - HEX: Human-in-the-loop explainability via deep reinforcement learning
PY  - 2024
T2  - Decision Support Systems
VL  - 187
C7  - 114304
DO  - 10.1016/j.dss.2024.114304
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207096278&doi=10.1016%2fj.dss.2024.114304&partnerID=40&md5=1f4595348adc8a14a0bcc3f608948df5
AB  - The use of machine learning (ML) models in decision-making contexts, particularly those used in high-stakes decision-making, are fraught with issue and peril since a person – not a machine – must ultimately be held accountable for the consequences of decisions made using such systems. Machine learning explainability (MLX) promises to provide decision-makers with prediction-specific rationale, assuring them that the model-elicited predictions are made for the right reasons and are thus reliable. Few works explicitly consider this key human-in-the-loop (HITL) component, however. In this work we propose HEX, a human-in-the-loop deep reinforcement learning approach to MLX. HEX incorporates 0-distrust projection to synthesize decider-specific explainers that produce explanations strictly in terms of a decider's preferred explanatory features using any classification model. Our formulation explicitly considers the decision boundary of the ML model in question using a proposed explanatory point mode of explanation, thus ensuring explanations are specific to the ML model in question. We empirically evaluate HEX against other competing methods, finding that HEX is competitive with the state-of-the-art and outperforms other methods in human-in-the-loop scenarios. We conduct a randomized, controlled laboratory experiment utilizing actual explanations elicited from both HEX and competing methods. We causally establish that our method increases decider's trust and tendency to rely on trusted features. © 2024 The Author(s)
KW  - Behavioral machine learning
KW  - Decision support
KW  - Deep reinforcement learning
KW  - Explainability
KW  - Human-in-the-loop
KW  - Interpretability
KW  - Machine learning
KW  - Adversarial machine learning
KW  - Contrastive Learning
KW  - Federated learning
KW  - Reinforcement learning
KW  - Behavioral machine learning
KW  - Decision supports
KW  - Decisions makings
KW  - Explainability
KW  - Human-in-the-loop
KW  - Interpretability
KW  - Machine learning models
KW  - Machine-learning
KW  - Reinforcement learnings
KW  - Deep reinforcement learning
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Swaroop, S.
AU  - Buçinca, Z.
AU  - Gajos, K.Z.
AU  - Doshi-Velez, F.
TI  - Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure
PY  - 2024
T2  - ACM International Conference Proceeding Series
SP  - 138
EP  - 154
DO  - 10.1145/3640543.3645206
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190963645&doi=10.1145%2f3640543.3645206&partnerID=40&md5=c49de85436de6cf2a0706957a570f022
AB  - In settings where users both need high accuracy and are time-pressured, such as doctors working in emergency rooms, we want to provide AI assistance that both increases decision accuracy and reduces decision-making time. Current literature focusses on how users interact with AI assistance when there is no time pressure, finding that different AI assistances have different benefits: some can reduce time taken while increasing overreliance on AI, while others do the opposite. The precise benefit can depend on both the user and task. In time-pressured scenarios, adapting when we show AI assistance is especially important: relying on the AI assistance can save time, and can therefore be beneficial when the AI is likely to be right. We would ideally adapt what AI assistance we show depending on various properties (of the task and of the user) in order to best trade off accuracy and time. We introduce a study where users have to answer a series of logic puzzles. We find that time pressure affects how users use different AI assistances, making some assistances more beneficial than others when compared to no-time-pressure settings. We also find that a user's overreliance rate is a key predictor of their behaviour: overreliers and not-overreliers use different AI assistance types differently. We find marginal correlations between a user's overreliance rate (which is related to the user's trust in AI recommendations) and their personality traits (Big Five Personality traits). Overall, our work suggests that AI assistances have different accuracy-time tradeoffs when people are under time pressure compared to no time pressure, and we explore how we might adapt AI assistances in this setting. © 2024 ACM.
KW  - AI-assisted decision-making
KW  - decision support systems
KW  - explainable AI
KW  - human-AI interaction
KW  - human-centered AI
KW  - overreliance
KW  - time pressure
KW  - Artificial intelligence
KW  - Behavioral research
KW  - Decision support systems
KW  - Economic and social effects
KW  - AI-assisted decision-making
KW  - Decision accuracies
KW  - Decisions makings
KW  - Explainable AI
KW  - High-accuracy
KW  - Human-AI interaction
KW  - Human-centered AI
KW  - Overreliance
KW  - Personality traits
KW  - Time pressures
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 9
ER  -

TY  - CHAP
AU  - Li, B.
AU  - Liu, Y.
TI  - Ethical Issues In Translation Education in the AI Era
PY  - 2025
T2  - Translation Studies in the Age of Artificial Intelligence
SP  - 210
EP  - 229
DO  - 10.4324/9781003482369-11
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004130250&doi=10.4324%2f9781003482369-11&partnerID=40&md5=9f65c0d5cfa7ae96e94869707ad1eba0
AB  - This chapter critically examines the ethical challenges arising from the integration of artificial intelligence (AI), particularly large language models (LLMs) and generative AI (GenAI), into translation education. It highlights how rapid technological advancements have outpaced the development of comprehensive ethical frameworks and pedagogical approaches within the field. The authors discuss the digital inequalities that result from disparities in access to AI tools, which can lead to unfair advantages among students and exacerbate existing socioeconomic and geographical divides. Issues of transparency in AI-assisted assessment are explored, emphasizing the need to shift from product-focused evaluation to process-oriented approaches that consider the human–AI interaction. The chapter proposes strategies for teaching AI-related ethics, including developing digital reflexivity, critical thinking, and ethical decision-making skills. It advocates for an integrated approach to professional ethics training, incorporating real-world scenarios, collaboration between academia and industry, and the development of discipline-specific AI guidelines. By addressing these ethical complexities, the chapter aims to prepare future translators for the evolving demands of an AI-augmented professional landscape. © 2025 selection and editorial matter, Sanjun Sun, Kanglong Liu and Riccardo Moratto.
KW  - Decision making
KW  - Personnel training
KW  - Teaching
KW  - Artificial intelligence tools
KW  - Critical thinking
KW  - Digital inequalities
KW  - Ethical decision making
KW  - Ethical issues
KW  - Language model
KW  - Pedagogical approach
KW  - Process-oriented approaches
KW  - Socio-economics
KW  - Technological advancement
KW  - Students
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Chen, Y.
AU  - Liu, Z.
TI  - WordDecipher: Enhancing Digital Workspace Communication with Explainable AI for Non-native English Speakers
PY  - 2024
T2  - ACM International Conference Proceeding Series
SP  - 7
EP  - 10
DO  - 10.1145/3690712.3690715
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209357161&doi=10.1145%2f3690712.3690715&partnerID=40&md5=9ec092ab17621d8bc88bcb6614dfa2d0
AB  - Non-native English speakers (NNES) face challenges in digital workspace communication (e.g., emails, Slack messages), often inadvertently translating expressions from their native languages, which can lead to awkward or incorrect usage. Current AI-assisted writing tools are equipped with fluency enhancement and rewriting suggestions; however, NNES may struggle to grasp the subtleties among various expressions, making it challenging to choose the one that accurately reflects their intent. Such challenges are exacerbated in high-stake text-based communications, where the absence of non-verbal cues heightens the risk of misinterpretation. By leveraging the latest advancements in large language models (LLM) and word embeddings, we propose WordDecipher, an explainable AI-assisted writing tool to enhance digital workspace communication for NNES. WordDecipher not only identifies the perceived social intentions detected in users' writing, but also generates rewriting suggestions aligned with users' intended messages, either numerically or by inferring from users' writing in their native language. Then, WordDecipher provides an overview of nuances to help NNES make selections. Through a usage scenario, we demonstrate how WordDecipher can significantly enhance an NNES's ability to communicate her request, showcasing its potential to transform workspace communication for NNES. © 2024 Owner/Author.
KW  - AI-assisted writing tools
KW  - Explainable AI
KW  - Human-AI interaction
KW  - Large language models
KW  - Technical writing
KW  - Translation (languages)
KW  - 'current
KW  - AI-assisted writing tool
KW  - Explainable AI
KW  - Human-AI interaction
KW  - Language model
KW  - Large language model
KW  - Native language
KW  - Non-native
KW  - Text-based communication
KW  - Writing tools
KW  - Digital elevation model
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Bach, T.A.
AU  - Kristiansen, J.K.
AU  - Babic, A.
AU  - Jacovi, A.
TI  - Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review
PY  - 2024
T2  - IEEE Access
VL  - 12
SP  - 106385
EP  - 106414
DO  - 10.1109/ACCESS.2024.3437190
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200246892&doi=10.1109%2fACCESS.2024.3437190&partnerID=40&md5=183c41ba0a8496dd303ff57e9c446d8c
AB  - Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, existing research on HAII is limited, fragmented, and inconsistent. We present here a survey of that literature and recommendations for research best practices that should improve the field. We divided our investigation into the following areas: 1) terms used to describe HAII, 2) primary roles of AI-enabled systems, 3) factors that influence HAII, and 4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, seven factors influence HAII: user characteristics (e.g., user personality), user perceptions and attitudes (e.g., user biases), user expectations and experience (e.g., mismatched user expectations and experience), AI interface and features (e.g., interactive design), AI output (e.g., perceived accuracy), explainability and interpretability (e.g., level of detail, user understanding), and usage of AI (e.g., heterogeneity of environments). HAII is most measured with user-related subjective metrics (e.g., user perceptions, trust, and attitudes), and AI-assisted decision-making is the most common primary role of AI-enabled systems. Based on this review, we conclude that there are substantial research gaps in HAII. Researchers and developers need to codify HAII terminology, involve users throughout the AI lifecycle (especially during development), and tailor HAII in safety-critical industries to the users and environments. © 2013 IEEE.
KW  - Artificial intelligence
KW  - humans
KW  - measurement
KW  - methods
KW  - safety
KW  - safety-critical
KW  - society
KW  - survey
KW  - systematic literature review
KW  - technology readiness level
KW  - user
KW  - Accident prevention
KW  - Artificial intelligence
KW  - Decision making
KW  - Ergonomics
KW  - Job analysis
KW  - User interfaces
KW  - Human
KW  - Medical services
KW  - Method
KW  - Safety-critical
KW  - Society
KW  - Systematic literature review
KW  - Task analysis
KW  - Technology readiness levels
KW  - User
KW  - Terminology
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Chen, H.
AU  - Dreizin, D.
AU  - Gomez, C.
AU  - Zapaishchykova, A.
AU  - Unberath, M.
TI  - Interpretable Severity Scoring of Pelvic Trauma Through Automated Fracture Detection and Bayesian Inference
PY  - 2025
T2  - IEEE Transactions on Medical Imaging
VL  - 44
IS  - 1
SP  - 130
EP  - 141
DO  - 10.1109/TMI.2024.3428836
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199332242&doi=10.1109%2fTMI.2024.3428836&partnerID=40&md5=66c2bbbc772ba42fabceb8ba614e5ab0
AB  - Pelvic ring disruptions result from blunt injury mechanisms and are potentially lethal mainly due to associated injuries and massive pelvic hemorrhage. The severity of pelvic fractures in trauma victims is frequently assessed by grading the fracture according to the Tile AO/OTA classification in whole-body Computed Tomography (CT) scans. Due to the high volume of whole-body CT scans generated in trauma centers, the overall information content of a single whole-body CT scan and low manual CT reading speed, an automatic approach to Tile classification would provide substantial value, e.g., to prioritize the reading sequence of the trauma radiologists or enable them to focus on other major injuries in multi-trauma patients. In such a high-stakes scenario, an automated method for Tile grading should ideally be transparent such that the symbolic information provided by the method follows the same logic a radiologist or orthopedic surgeon would use to determine the fracture grade. This paper introduces an automated yet interpretable pelvic trauma decision support system to assist radiologists in fracture detection and Tile grading. To achieve interpretability despite processing high-dimensional whole-body CT images, we design a neurosymbolic algorithm that operates similarly to human interpretation of CT scans. The algorithm first detects relevant pelvic fractures on CTs with high specificity using Faster-RCNN. To generate robust fracture detections and associated detection (un)certainties, we perform test-time augmentation of the CT scans to apply fracture detection several times in a self-ensembling approach. The fracture detections are interpreted using a structural causal model based on clinical best practices to infer an initial Tile grade. We apply a Bayesian causal model to recover likely co-occurring fractures that may have been rejected initially due to the highly specific operating point of the detector, resulting in an updated list of detected fractures and corresponding final Tile grade. Our method is transparent in that it provides fracture location and types, as well as information on important counterfactuals that would invalidate the system's recommendation. Our approach achieves an AUC of 0.89/0.74 for translational and rotational instability,which is comparable to radiologist performance. Despite being designed for human-machine teaming, our approach does not compromise on performance compared to previous black-box methods.  © 1982-2012 IEEE.
KW  - Bayesian inference
KW  - deep learning
KW  - explainable machine learning
KW  - human-computer interaction
KW  - Algorithms
KW  - Bayes Theorem
KW  - Fractures, Bone
KW  - Humans
KW  - Pelvic Bones
KW  - Tomography, X-Ray Computed
KW  - Whole Body Imaging
KW  - Automation
KW  - Bayesian networks
KW  - Classification (of information)
KW  - Decision support systems
KW  - Deep learning
KW  - Grading
KW  - Human computer interaction
KW  - Inference engines
KW  - Three dimensional displays
KW  - Bayes method
KW  - Bayesian inference
KW  - Computed tomography
KW  - Deep learning
KW  - Explainable machine learning
KW  - Features extraction
KW  - Injury
KW  - Machine-learning
KW  - Three-dimensional display
KW  - age
KW  - Article
KW  - automation
KW  - Bayes theorem
KW  - Bayesian learning
KW  - best practice
KW  - causality
KW  - clinical significance
KW  - comparative study
KW  - computer assisted tomography
KW  - convolutional neural network
KW  - cross validation
KW  - data interpretation
KW  - decision support system
KW  - diastatic sacral fracture
KW  - disease severity
KW  - explainable artificial intelligence
KW  - false negative result
KW  - false positive result
KW  - frequency analysis
KW  - human
KW  - pelvis fracture
KW  - prediction
KW  - sacrum fracture
KW  - whole body CT
KW  - algorithm
KW  - diagnostic imaging
KW  - fracture
KW  - injury
KW  - pelvic girdle
KW  - procedures
KW  - whole body imaging
KW  - x-ray computed tomography
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Teixeira, B.
AU  - Valina, L.
AU  - Pinto, T.
AU  - Reis, A.
AU  - Barroso, J.
AU  - Vale, Z.
TI  - Exploring Clustering to Improve Interpretability in Complex Energy Forecasting Models
PY  - 2024
T2  - 2024 International Conference on Smart Energy Systems and Technologies: Driving the Advances for Future Electrification, SEST 2024 - Proceedings
DO  - 10.1109/SEST61601.2024.10694413
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207641777&doi=10.1109%2fSEST61601.2024.10694413&partnerID=40&md5=3321b2c79a5409af0349c742679e1113
AB  - Explainable Artificial Intelligence (XAI) aims to enhance the interpretability of Artificial Intelligence (AI) systems for humans. The goal is to ensure that algorithmic decisions and underlying data are understandable to non-technical stakeholders. Advanced Machine Learning (ML) models, such as deep neural networks, enable AI systems to process vast data and extract intricate patterns, akin to the human brain, but this complicates XAI development. Complex ML models require substantial data for training, exacerbating the challenge. Consequently, this paper proposes a novel approach to improve XAI for complex ML models, particularly those with large data needs. Using K-Means clustering, the paper proposes to identify relevant data instances to create similarity clusters. This filtering process focuses XAI on essential information, even with complex models, reducing the data set to find patterns and explanations, so that, using the same approach, only the best explanations are filtered efficiently. The paper proposes to implement and test this model with a case study on ML for PV generation forecasting in buildings. Results show that the proposed approach is able to generate explanations that are very similar to those generated when using the entire available data, in only a portion of the execution time, leveraging from the identification of a small number of representative data points. This approach, therefore, enhances the efficiency of XAI by achieving promising results with a smaller dataset. It also facilitates the development of more understandable and fastly provided solutions, which is essential for real-world XAI users such as electric mobility users that need PV forecasting explanations as support for their vehicles charging management. © 2024 IEEE.
KW  - Artificial intelligence
KW  - clustering
KW  - explainable artificial intelligence
KW  - large data sets
KW  - Artificial intelligence systems
KW  - Clusterings
KW  - Complex energy
KW  - Complex machines
KW  - Energy forecasting
KW  - Explainable artificial intelligence
KW  - Forecasting models
KW  - Interpretability
KW  - Large datasets
KW  - Machine learning models
KW  - Deep neural networks
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ling, S.
AU  - Zhang, Y.
AU  - Du, N.
TI  - More Is Not Always Better: Impacts of AI-Generated Confidence and Explanations in Human–Automation Interaction
PY  - 2024
T2  - Human Factors
VL  - 66
IS  - 12
SP  - 2606
EP  - 2620
DO  - 10.1177/00187208241234810
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187416241&doi=10.1177%2f00187208241234810&partnerID=40&md5=dad37dcdec9bb41b5514269b0f2a4f90
AB  - Objective: The study aimed to enhance transparency in autonomous systems by automatically generating and visualizing confidence and explanations and assessing their impacts on performance, trust, preference, and eye-tracking behaviors in human–automation interaction. Background: System transparency is vital to maintaining appropriate levels of trust and mission success. Previous studies presented mixed results regarding the impact of displaying likelihood information and explanations, and often relied on hand-created information, limiting scalability and failing to address real-world dynamics. Method: We conducted a dual-task experiment involving 42 university students who operated a simulated surveillance testbed with assistance from intelligent detectors. The study used a 2 (confidence visualization: yes vs. no) × 3 (visual explanations: none, bounding boxes, bounding boxes and keypoints) mixed design. Task performance, human trust, preference for intelligent detectors, and eye-tracking behaviors were evaluated. Results: Visual explanations using bounding boxes and keypoints improved detection task performance when confidence was not displayed. Meanwhile, visual explanations enhanced trust and preference for the intelligent detector, regardless of the explanation type. Confidence visualization did not influence human trust in and preference for the intelligent detector. Moreover, both visual information slowed saccade velocities. Conclusion: The study demonstrated that visual explanations could improve performance, trust, and preference in human–automation interaction without confidence visualization partially by changing the search strategies. However, excessive information might cause adverse effects. Application: These findings provide guidance for the design of transparent automation, emphasizing the importance of context-appropriate and user-centered explanations to foster effective human–machine collaboration. © 2024 Human Factors and Ergonomics Society.
KW  - explainable artificial intelligence
KW  - eye-tracking analysis
KW  - human–automation interaction
KW  - task performance
KW  - transparency
KW  - trust
KW  - Adult
KW  - Artificial Intelligence
KW  - Eye-Tracking Technology
KW  - Female
KW  - Humans
KW  - Male
KW  - Man-Machine Systems
KW  - Task Performance and Analysis
KW  - Trust
KW  - Young Adult
KW  - Automation
KW  - Behavioral research
KW  - Eye movements
KW  - Eye tracking
KW  - Visualization
KW  - Bounding-box
KW  - Explainable artificial intelligence
KW  - Eye-tracking
KW  - Eye-tracking analysis
KW  - Human-automation interactions
KW  - Keypoints
KW  - Performance tracking
KW  - Task performance
KW  - Tracking behavior
KW  - Trust
KW  - adult
KW  - artificial intelligence
KW  - eye-tracking technology
KW  - female
KW  - human
KW  - male
KW  - man machine interaction
KW  - task performance
KW  - trust
KW  - young adult
KW  - Transparency
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Ehsan, U.
AU  - Liao, Q.V.
AU  - Passi, S.
AU  - Riedl, M.O.
AU  - Daumé, H.I.I.I.
TI  - Seamful XAI: Operationalizing Seamful Design in Explainable AI
PY  - 2024
T2  - Proceedings of the ACM on Human-Computer Interaction
VL  - 8
IS  - CSCW1
C7  - 119
DO  - 10.1145/3637396
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193368846&doi=10.1145%2f3637396&partnerID=40&md5=4effec7a23b279cc9d5ee5d8c11d7a1b
AB  - Mistakes in AI systems are inevitable, arising from both technical limitations and sociotechnical gaps. While black-boxing AI systems can make the user experience seamless, hiding the seams risks disempowering users to mitigate fallouts from AI mistakes. Instead of hiding these AI imperfections, can we leverage them to help the user? While Explainable AI (XAI) has predominantly tackled algorithmic opaqueness, we propose that seamful design can foster AI explainability by revealing and leveraging sociotechnical and infrastructural mismatches. We introduce the concept of Seamful XAI by (1) conceptually transferring “seams” to the AI context and (2) developing a design process that helps stakeholders anticipate and design with seams. We explore this process with 43 AI practitioners and real end-users, using a scenario-based co-design activity informed by real-world use cases. We found that the Seamful XAI design process helped users foresee AI harms, identify underlying reasons (seams), locate them in the AI's lifecycle, learn how to leverage seamful information to improve XAI and user agency. We share empirical insights, implications, and reflections on how this process can help practitioners anticipate and craft seams in AI, how seamfulness can improve explainability, empower end-users, and facilitate Responsible AI. © 2024 Copyright held by the owner/author(s).
KW  - explainable AI
KW  - human-AI interaction
KW  - responsible AI
KW  - Seamful design
KW  - Design
KW  - User interfaces
KW  - AI systems
KW  - Design-process
KW  - End-users
KW  - Explainable AI
KW  - Human-AI interaction
KW  - Responsible AI
KW  - Seamful designs
KW  - Sociotechnical
KW  - Technical limitations
KW  - Users' experiences
KW  - Life cycle
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Durlik, I.
AU  - Miller, T.
AU  - Kostecka, E.
AU  - Kozlovska, P.
AU  - Ślączka, W.
TI  - Enhancing Safety in Autonomous Maritime Transportation Systems with Real-Time AI Agents
PY  - 2025
T2  - Applied Sciences (Switzerland)
VL  - 15
IS  - 9
C7  - 4986
DO  - 10.3390/app15094986
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004899698&doi=10.3390%2fapp15094986&partnerID=40&md5=b901117d459fd7ab933550ee5238e884
AB  - The maritime transportation sector is undergoing a profound shift with the emergence of autonomous vessels powered by real-time artificial intelligence (AI) agents. This article investigates the pivotal role of these agents in enhancing the safety, efficiency, and sustainability of autonomous maritime systems. Following a structured literature review, we examine the architecture of real-time AI agents, including sensor integration, communication systems, and computational infrastructure. We distinguish maritime AI agents from conventional systems by emphasizing their specialized functions, real-time processing demands, and resilience in dynamic environments. Key safety mechanisms—such as collision avoidance, anomaly detection, emergency coordination, and fail-safe operations—are analyzed to demonstrate how AI agents contribute to operational reliability. The study also explores regulatory compliance, focusing on emission control, real-time monitoring, and data governance. Implementation challenges, including limited onboard computational power, legal and ethical constraints, and interoperability issues, are addressed with practical solutions such as edge AI and modular architectures. Finally, the article outlines future research directions involving smart port integration, scalable AI models, and emerging technologies like federated and explainable AI. This work highlights the transformative potential of AI agents in advancing autonomous maritime transportation. © 2025 by the authors.
KW  - AI agents
KW  - anomaly detection
KW  - autonomous maritime systems
KW  - collision avoidance
KW  - computational infrastructure
KW  - cybersecurity
KW  - data governance
KW  - emergency response
KW  - ethical AI
KW  - human-machine interaction
KW  - maritime safety
KW  - real-time processing
KW  - regulatory compliance
KW  - smart port integration
KW  - sustainability
KW  - Autonomous vehicles
KW  - Ethical technology
KW  - Inland waterways
KW  - Interoperability
KW  - Marine safety
KW  - Anomaly detection
KW  - Artificial intelligence agent
KW  - Autonomous maritime system
KW  - Collisions avoidance
KW  - Computational infrastructure
KW  - Cyber security
KW  - Data governances
KW  - Emergency response
KW  - Ethical artificial intelligence
KW  - Human machine interaction
KW  - Maritime safety
KW  - Maritime systems
KW  - Realtime processing
KW  - Smart port integration
KW  - Emergency services
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Xu, K.
AU  - Shi, J.
TI  - Visioning a two-level human-machine communication framework: initiating conversations between explainable AI and communication
PY  - 2024
T2  - Communication Theory
VL  - 34
IS  - 4
SP  - 216
EP  - 229
DO  - 10.1093/ct/qtae016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208079357&doi=10.1093%2fct%2fqtae016&partnerID=40&md5=c27000af0b72a218c27af8c1fab74c9d
AB  - Amid mounting interest in artificial intelligence (AI) technology, communication scholars have sought to understand humans' perceptions of and attitudes toward AI's predictions, recommendations, and decisions. Meanwhile, scholars in the nascent but growing field of explainable AI (XAI) have aimed to clarify AI's operational mechanisms and make them interpretable, visible, and transparent. In this conceptual article, we suggest that a conversation between human-machine communication (HMC) and XAI is advantageous and necessary. Following the introduction of these two areas, we demonstrate how research on XAI can inform the HMC scholarship regarding the human-in-the-loop approach and the message production explainability. Next, we expound upon how communication scholars' focuses on message sources, receivers, features, and effects can reciprocally benefit XAI research. At its core, this article proposes a two-level HMC framework and posits that bridging the two fields can guide future AI research and development. © 2024 The Author(s). Published by Oxford University Press on behalf of International Communication Association. All rights reserved. 
KW  - artificial intelligence
KW  - explainable AI
KW  - human-AI interaction
KW  - human-in-the-loop approach
KW  - human-machine communication
KW  - artificial intelligence
KW  - future prospect
KW  - machine learning
KW  - perception
KW  - prediction
KW  - research and development
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Akhtar, M.
AU  - Nehal, N.
AU  - Gull, A.
AU  - Parveen, R.
AU  - Khan, S.
AU  - Khan, S.
AU  - Ali, J.
TI  - Explicating the transformative role of artificial intelligence in designing targeted nanomedicine
PY  - 2025
T2  - Expert Opinion on Drug Delivery
DO  - 10.1080/17425247.2025.2502022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005808512&doi=10.1080%2f17425247.2025.2502022&partnerID=40&md5=6014989a250924a74848cd50b0daef00
AB  - Introduction: Artificial intelligence (AI) has emerged as a transformative force in nanomedicine, revolutionizing drug delivery, diagnostics, and personalized treatment. While nanomedicine offers precise targeted drug delivery and reduced toxic effects, its clinical translation is hindered by biological complexity, unpredictable in vivo behavior, and inefficient trial-and-error approaches. Areas covered: This review covers the application of AI and Machine Learning (ML) across the nanomedicine development pipeline, starting from drug and target identification to nanoparticle design, toxicity prediction, and personalized dosing. Different AI/ML models like QSAR, MTK-QSBER, and Alchemite, along with data sources and high-throughput screening methods, have been explored. Real-world applications are critically discussed, including AI-assisted drug repurposing, controlled-release formulations, and cancer-specific delivery systems. Expert opinion: AI has emerged as an essential component in designing next-generation nanomedicine. Efficiently handling multidimensional datasets, optimizing formulations, and personalizing treatment regimens, it has sped up the innovation process. However, challenges like data heterogeneity, model transparency, and regulatory gaps remain. Addressing these hurdles through interdisciplinary efforts and emerging innovations like explainable AI and federated learning will pave the way for the clinical translation of AI-driven nanomedicine. © 2025 Informa UK Limited, trading as Taylor & Francis Group.
KW  - Artificial intelligence
KW  - deep learning
KW  - machine learning
KW  - nanomedicine
KW  - personalized treatments
KW  - nanoparticle
KW  - artificial intelligence
KW  - controlled release formulation
KW  - deep learning
KW  - drug delivery system
KW  - drug development
KW  - drug repositioning
KW  - explainable artificial intelligence
KW  - federated learning
KW  - high throughput screening
KW  - human
KW  - machine learning
KW  - nanomedicine
KW  - nonhuman
KW  - personalized medicine
KW  - prediction
KW  - quantitative structure activity relation
KW  - review
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ding, Y.
AU  - Jia, L.
AU  - Du, N.
TI  - Watch Out for Explanations: Information Type and Error Type Affect Trust and Situational Awareness in Automated Vehicles
PY  - 2025
T2  - IEEE Transactions on Human-Machine Systems
DO  - 10.1109/THMS.2025.3558437
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004029939&doi=10.1109%2fTHMS.2025.3558437&partnerID=40&md5=4fb457bb40672d28aa2f284880af76d3
AB  - Trust and situational awareness (SA) are critical for the acceptance and safety of automated vehicles (AVs). While AV explanations with different information types have been studied to enhance drivers' trust and SA, their effectiveness remains unclear when AVs make errors that do not trigger takeover requests. This study investigated the effects of information type, error type, and their interaction on drivers' trust in AVs, SA, and their relationships. We recruited 300 participants in an online video study with a 3 (information type: why, how, why + how) × 3 (error type: false alarm, miss, correct [no error]) mixed design. How information describes the vehicle's action, while why information refers to the reason for the vehicle's action. Linear mixed models showed that false alarms and misses were associated with lower SA compared with correct scenarios, but possibly due to different reasons. Compared with correct scenarios, both false alarms and misses were associated with lower trust, with misses even lower than false alarms, possibly due to the varying severity of potential consequences. Compared with why and why + how information, how information was generally associated with lower SA and a higher potential of overtrust in false alarms. Trust and SA had a negative linear relationship in misses and false alarms, while no correlations were found in correct scenarios. To mitigate potential overtrust and misinterpretation of situations when AVs make errors, it is crucial to maintain higher SA. We recommend including why information in AV explanations and deploying AV decision systems that are less miss-prone. © 2013 IEEE.
KW  - Automated vehicles (AVs)
KW  - explainable artificial intelligence
KW  - human factors
KW  - human–machine interface
KW  - situational awareness (SA)
KW  - trust
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Bhat, M.
TI  - Designing AI Interfaces for Transparent Decision-Making and Ethical Reflection
PY  - 2025
T2  - International Conference on Intelligent User Interfaces, Proceedings IUI
SP  - 211
EP  - 214
DO  - 10.1145/3708557.3716150
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001674262&doi=10.1145%2f3708557.3716150&partnerID=40&md5=89f46aa0ab298d97ba11c4a6b594bb79
AB  - As artificial intelligence (AI) systems increasingly mediate high-stakes decisions in domains such as healthcare, finance, and education, ensuring transparency and ethical accountability in AI interfaces is critical. However, existing interfaces often obscure algorithmic processes, leading to overtrust, disengagement, or misinterpretation of AI-generated outputs. My research explores how interface design—including presentation modes, interactive explainability tools, and speculative design interventions—can shape user perceptions, foster critical reflection, and enhance AI literacy. By integrating controlled experiments, participatory design, and qualitative analysis, my work aims to develop AI interfaces that not only communicate algorithmic decisions effectively but also encourage users to critically assess the ethical implications of AI technologies. I examine the trade-offs between transparency, usability, and engagement, investigating how interfaces can balance cognitive load while making ethical considerations more salient. Through this doctoral consortium, I seek mentorship and feedback on designing adaptive transparency mechanisms and mitigating overtrust in engaging AI interfaces. © 2025 Copyright held by the owner/author(s).
KW  - Agency
KW  - AI Literacy
KW  - AI Transparency
KW  - Cognitive Interaction
KW  - Control
KW  - Design Theory
KW  - Explainable AI
KW  - Interaction Design
KW  - Technology-Mediated Learning
KW  - Trust
KW  - User Research
KW  - Decision making
KW  - Agency
KW  - Artificial intelligence literacy
KW  - Artificial intelligence transparency
KW  - Cognitive interaction
KW  - Design theory
KW  - Explainable artificial intelligence
KW  - Interaction design
KW  - Technology-mediated learning
KW  - Trust
KW  - User research
KW  - Ethical technology
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Kabir, M.N.
TI  - Unleashing Human Potential: A Framework for Augmenting Co-Creation with Generative AI
PY  - 2024
T2  - Proceedings of the 4th International Conference on AI Research, ICAIR 2024
SP  - 183
EP  - 193
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215684757&partnerID=40&md5=ff69a3dee02e00bc61fe3cb623c98a75
AB  - This paper redefines the traditional view of automation as a threat to human labor, advocating for human-AI co-creation as a strategic imperative for organizations. We propose a comprehensive six-stage framework for co-creation, emphasizing iterative feedback loops and continuous improvement to integrate generative AI into workflows effectively. Drawing from a multi-disciplinary perspective, we explore critical enablers of successful human-AI collaboration, including user-centered interface design, explainable AI systems, and fostering a culture of trust and experimentation. Real-world case studies, such as AI-enhanced visual design and creative writing, illustrate the transformative potential of co-creation across various sectors. We also propose a multifaceted measurement framework encompassing quantitative metrics (e.g., productivity gains, time-to-market acceleration) and qualitative indicators (e.g., employee well-being, skill development) to assess the impact of co-creation comprehensively. This research offers a strategic roadmap for organizations to embrace generative AI as a tool for collaboration and augmentation, thereby unlocking new levels of creativity, productivity, and employee empowerment. © Proceedings of the 4th International Conference on AI Research, ICAIR 2024.
KW  - Augmentation
KW  - Collaboration
KW  - Future of work
KW  - Generative AI
KW  - Human-AI co-creation
KW  - Innovation
KW  - Empowerment of personnel
KW  - Augmentation
KW  - Co-creation
KW  - Collaboration
KW  - Future of works
KW  - Generative AI
KW  - Human labor
KW  - Human potential
KW  - Human-AI co-creation
KW  - Innovation
KW  - Strategic imperative
KW  - User centered design
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Lee, C.P.
AU  - Lee, M.K.
AU  - Mutlu, B.
TI  - The AI-DEC: A Card-based Design Method for User-centered AI Explanations
PY  - 2024
T2  - Proceedings of the 2024 ACM Designing Interactive Systems Conference, DIS 2024
SP  - 1010
EP  - 1028
DO  - 10.1145/3643834.3661576
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200342630&doi=10.1145%2f3643834.3661576&partnerID=40&md5=a72a8356a87061be4791ae4264632231
AB  - Increasing evidence suggests that many deployed AI systems do not sufficiently support end-user interaction and information needs. Engaging end-users in the design of these systems can reveal user needs and expectations, yet effective ways of engaging end-users in the AI explanation design remain under-explored. To address this gap, we developed a design method, called AI-DEC, that defines four dimensions of AI explanations that are critical for the integration of AI systems—communication content, modality, frequency, and direction—and offers design examples for end-users to design AI explanations that meet their needs. We evaluated this method through co-design sessions with workers in healthcare, finance, and management industries who regularly use AI systems in their daily work. Findings indicate that the AI-DEC effectively supported workers in designing explanations that accommodated diverse levels of performance and autonomy needs, which varied depending on the AI system’s workplace role and worker values. We discuss the implications of using the AI-DEC for the user-centered design of AI explanations in real-world systems. © 2024 Copyright held by the owner/author(s).
KW  - Design cards
KW  - human-AI interaction
KW  - user-centered design
KW  - Human engineering
KW  - Human resource management
KW  - AI systems
KW  - Design card
KW  - Design method
KW  - End-users
KW  - Human-AI interaction
KW  - User information
KW  - User interaction
KW  - User need
KW  - User-centred
KW  - Workers'
KW  - User centered design
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Olsen, H.P.
AU  - Hildebrandt, T.T.
AU  - Wiesener, C.
AU  - Larsen, M.S.
AU  - Ammitzbøll Flügge, A.W.
TI  - The Right to Transparency in Public Governance: Freedom of Information and the Use of Artificial Intelligence by Public Agencies
PY  - 2024
T2  - Digital Government: Research and Practice
VL  - 5
IS  - 1
C7  - 8
DO  - 10.1145/3632753
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187807869&doi=10.1145%2f3632753&partnerID=40&md5=5792158c41ccf799f9327f5887c2f171
AB  - What information should and can be transparent for artificial intelligence (AI) algorithms? This article examines the socio- technical and legal perspectives of transparency in relation to algorithmic decision-making in public administration. We show how transparency in AI can be understood in light of the various technologies and the challenges one may encounter. Despite some first steps in that direction, there exists so far no mature standard for documenting AI models. From a legal perspective, this article examined the applicable freedom of information (FOI) regimes across different jurisdictions, with a particular focus on Denmark and other Scandinavian countries. Despite notable differences, our findings show that the FOI regimes generally only grant access to existing documents, and that access can be denied on the basis of the wide proprietary interests and internal documents exemptions. This is why we ultimately conclude that the European data-protection framework and the proposed EU AI Act -with their far-reaching duties to document the functioning of AI systems -provide promising new avenues for research and insights into transparency in AI.  Copyright © 2024 held by the owner/author(s).
KW  - administrative decision-making
KW  - algorithm
KW  - freedom of information
KW  - Transparency
KW  - Artificial intelligence
KW  - Decision making
KW  - Public administration
KW  - Administrative decision making
KW  - Algorithmics
KW  - Artificial intelligence algorithms
KW  - Decisions makings
KW  - Denmark
KW  - Freedom of informations
KW  - Intelligence models
KW  - Public agencies
KW  - Sociotechnical
KW  - Various technologies
KW  - Transparency
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Baron, S.
AU  - Latham, A.J.
AU  - Varga, S.
TI  - Explainable AI and stakes in medicine: A user study
PY  - 2025
T2  - Artificial Intelligence
VL  - 340
C7  - 104282
DO  - 10.1016/j.artint.2025.104282
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215221249&doi=10.1016%2fj.artint.2025.104282&partnerID=40&md5=698931cb5d5e166bca5edfb0ca5952ed
AB  - The apparent downsides of opaque algorithms have led to a demand for explainable AI (XAI) methods by which a user might come to understand why an algorithm produced the particular output it did, given its inputs. Patients, for example, might find that the lack of explanation of the process underlying the algorithmic recommendations for diagnosis and treatment hinders their ability to provide informed consent. This paper examines the impact of two factors on user perceptions of explanations for AI systems in medical contexts. The factors considered were the stakes of the decision—high versus low—and the decision source—human versus AI. 484 participants were presented with vignettes in which medical diagnosis and treatment plan recommendations were made by humans or by AI. Separate vignettes were used for high stakes scenarios involving life-threatening diseases, and low stakes scenarios involving mild diseases. In each vignette, an explanation for the decision was given. Four explanation types were tested across separate vignettes: no explanation, counterfactual, causal and a novel ‘narrative-based’ explanation, not previously considered. This yielded a total of 16 conditions, of which each participant saw only one. Individuals were asked to evaluate the explanations they received based on helpfulness, understanding, consent, reliability, trust, interests and likelihood of undergoing treatment. We observed a main effect for stakes on all factors and a main effect for decision source on all factors except for helpfulness and likelihood to undergo treatment. While we observed effects for explanation on helpfulness, understanding, consent, reliability, trust and interests, we by and large did not see any differences between the effects of explanation types. This suggests that the effectiveness of explanations may not depend on type of explanation but instead, on the stakes and decision source. © 2025
KW  - Causation
KW  - Explainable AI (XAI)
KW  - Explainable ML
KW  - Human-AI interaction
KW  - Human-centered XAI
KW  - User study
KW  - AI systems
KW  - Algorithmics
KW  - Causation
KW  - Explainable AI (XAI)
KW  - Explainable ML
KW  - Human-AI interaction
KW  - Human-centered XAI
KW  - Main effect
KW  - User perceptions
KW  - User study
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Festor, P.
AU  - Nagendran, M.
AU  - Gordon, A.C.
AU  - Faisal, A.A.
AU  - Komorowski, M.
TI  - Safety of human-AI cooperative decision-making within intensive care: A physical simulation study
PY  - 2025
T2  - PLOS Digital Health
VL  - 4
IS  - 2
C7  - e0000726
DO  - 10.1371/journal.pdig.0000726
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219010067&doi=10.1371%2fjournal.pdig.0000726&partnerID=40&md5=6ec44ec1e817d51d9df9510d5ff4b7c3
AB  - The safety of Artificial Intelligence (AI) systems is as much one of human decision-making as a technological question. In AI-driven decision support systems, particularly in high-stakes settings such as healthcare, ensuring the safety of human-AI interactions is paramount, given the potential risks of following erroneous AI recommendations. To explore this question, we ran a safety-focused clinician-AI interaction study in a physical simulation suite. Physicians were placed in a simulated intensive care ward, with a human nurse (played by an experimenter), an ICU data chart, a high-fidelity patient mannequin and an AI recommender system on a display. Clinicians were asked to prescribe two drugs for the simulated patients suffering from sepsis and wore eye-tracking glasses to allow us to assess where their gaze was directed. We recorded clinician treatment plans before and after they saw the AI treatment recommendations, which could be either ‘safe’ or ‘unsafe’. 92% of clinicians rejected unsafe AI recommendations vs 29% of safe ones. Physicians paid increased attention (+37% gaze fixations) to unsafe AI recommendations vs safe ones. However, visual attention on AI explanations was not greater in unsafe scenarios. Similarly, clinical information (patient monitor, patient chart) did not receive more attention after an unsafe versus safe AI reveal suggesting that the physicians did not look back to these sources of information to investigate why the AI suggestion might be unsafe. Physicians were only successfully persuaded to change their dose by scripted comments from the bedside nurse 5% of the time. Our study emphasises the importance of human oversight in safety-critical AI and the value of evaluating human-AI systems in high-fidelity settings that more closely resemble real world practice. © 2025 Festor et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CHAP
AU  - Hemalatha, P.
AU  - Manikandan, J.
AU  - Balaji, B.
AU  - Sujitha, V.
TI  - Enlightened XAI: Illuminating Ethics and Equitable Explainability
PY  - 2025
T2  - Explainable Artificial Intelligence in the Healthcare Industry
SP  - 593
EP  - 617
DO  - 10.1002/9781394249312.ch25
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001368718&doi=10.1002%2f9781394249312.ch25&partnerID=40&md5=51e2ad00ea791d96645366f5b2dbdcf1
AB  - As artificial intelligence (AI) algorithms become increasingly integral parts of society, their opaqueness and inaccessibility raise significant ethical concerns. Explainable AI (XAI) addresses these challenges by providing insight into how AI models make decisions. In this chapter, we explore some ethical considerations surrounding XAI systems and the associated fairness concerns. This paper highlights the significance of transparency and interpretability for artificial intelligence algorithms and their associated risks such as biased decision making. Additionally, this chapter addresses the challenges of attaining fairness within XAI systems and their need to address algorithmic biases. Furthermore, frameworks and guidelines that ensure such systems’ responsible development and deployment uphold ethical principles while providing greater fairness are also reviewed. This chapter focuses on understanding the impact of artificial intelligence (AI) on society and individuals and emphasizes ethical guidelines in AI development. Additionally, its contents examine the potential consequences of biased AI decision making and real-world examples involving fairness issues related to AI development. This study investigates the trade-off between fairness and interpretability, assessing fairness in explainable AI models and successful implementations of ethical XAI through case studies and best practices. Also included are practical measures for incorporating ethics and fairness into XAI projects and public concerns about AI systems used for healthcare, finance, or criminal justice purposes. This chapter discusses emerging trends and research directions in ethical AI, placing special emphasis on interdisciplinary when developing ethical XAI applications while emphasizing fairness and transparency as vital ingredients of shaping its future, calling for responsible use and development of XAI so that its beneficial societal effects may be maximized. © 2025 Scrivener Publishing LLC.
KW  - Artificial intelligence
KW  - biased decision making
KW  - compliance
KW  - ethical XAI
KW  - explainable AI
KW  - human-AI collaboration
KW  - interpretability
KW  - k-nearest neighbors
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Bertrand, A.
AU  - Eagan, J.R.
AU  - Maxwell, W.
AU  - Brand, J.
TI  - AI is Entering Regulated Territory: Understanding the Supervisors' Perspective on Model Justifiability in Financial Crime Detection
PY  - 2024
T2  - Conference on Human Factors in Computing Systems - Proceedings
C7  - 480
DO  - 10.1145/3613904.3642326
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194900040&doi=10.1145%2f3613904.3642326&partnerID=40&md5=566d068d921812b1dade8d701c693dfe
AB  - Artificial intelligence (AI) has the potential to bring significant benefits to highly regulated industries such as healthcare or banking. Adoption, however, remains low. AI's entry into complex sociotechno-legal systems raises issues of transparency, specifically for regulators. However, the perspective of supervisors, regulators who monitor compliance with applicable financial regulations, has rarely been studied. This paper focuses on understanding the needs of supervisors in anti-money laundering (AML) to better inform the design of AI justifications and explanations in highly regulated fields. Through scenario-based workshops with 13 supervisors and 6 banking professionals, we outline the auditing practices and socio-technical context of the supervisor. By combining the workshops' insights with an analysis of compliance requirements, we identify the AML obligations that conflict with AI opacity. We then formulate seven needs that supervisors have for model justifiability. We discuss the role of explanations as reliable evidence on which to base justifications. © 2024 Copyright held by the owner/author(s)
KW  - AI regulation
KW  - anti-money laundering
KW  - explainability
KW  - highly-regulated environment
KW  - justifiability
KW  - Artificial intelligence
KW  - Laundering
KW  - Regulatory compliance
KW  - Anti-money laundering
KW  - Artificial intelligence regulation
KW  - Explainability
KW  - Financial crime detection
KW  - Financial regulations
KW  - Highly-regulated environment
KW  - Justifiability
KW  - Legal system
KW  - Scenario-based
KW  - Sociotechnical
KW  - Supervisory personnel
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Marmolejo-Ramos, F.
AU  - Marrone, R.
AU  - Korolkiewicz, M.
AU  - Gabriel, F.
AU  - Siemens, G.
AU  - Joksimovic, S.
AU  - Yamada, Y.
AU  - Mori, Y.
AU  - Rahwan, T.
AU  - Sahakyan, M.
AU  - Sonna, B.
AU  - Meirmanov, A.
AU  - Bolatov, A.
AU  - Som, B.
AU  - Ndukaihe, I.
AU  - Arinze, N.C.
AU  - Kundrát, J.
AU  - Skanderová, L.
AU  - Ngo, V.-G.
AU  - Nguyen, G.
AU  - Lacia, M.
AU  - Kung, C.-C.
AU  - Irmayanti, M.
AU  - Muktadir, A.
AU  - Samosir, F.T.
AU  - Liuzza, M.T.
AU  - Giorgini, R.
AU  - Khatin-Zadeh, O.
AU  - Banaruee, H.
AU  - Özdoğru, A.A.
AU  - Ariyabuddhiphongs, K.
AU  - Rakchai, W.
AU  - Trujillo, N.
AU  - Valencia, S.M.
AU  - Janyan, A.
AU  - Kostov, K.
AU  - Montoro, P.R.
AU  - Hinojosa, J.
AU  - Medeiros, K.
AU  - Hunt, T.E.
AU  - Posada, J.
AU  - Freitag, R.M.K.
AU  - Tejada, J.
TI  - Factors influencing trust in algorithmic decision-making: an indirect scenario-based experiment
PY  - 2024
T2  - Frontiers in Artificial Intelligence
VL  - 7
C7  - 1465605
DO  - 10.3389/frai.2024.1465605
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218185895&doi=10.3389%2ffrai.2024.1465605&partnerID=40&md5=f769d97ad80e7c133e065080735bfcb9
AB  - Algorithms are involved in decisions ranging from trivial to significant, but people often express distrust toward them. Research suggests that educational efforts to explain how algorithms work may help mitigate this distrust. In a study of 1,921 participants from 20 countries, we examined differences in algorithmic trust for low-stakes and high-stakes decisions. Our results suggest that statistical literacy is negatively associated with trust in algorithms for high-stakes situations, while it is positively associated with trust in low-stakes scenarios with high algorithm familiarity. However, explainability did not appear to influence trust in algorithms. We conclude that having statistical literacy enables individuals to critically evaluate the decisions made by algorithms, data and AI, and consider them alongside other factors before making significant life decisions. This ensures that individuals are not solely relying on algorithms that may not fully capture the complexity and nuances of human behavior and decision-making. Therefore, policymakers should consider promoting statistical/AI literacy to address some of the complexities associated with trust in algorithms. This work paves the way for further research, including the triangulation of data with direct observations of user interactions with algorithms or physiological measures to assess trust more accurately. Copyright © 2025 Marmolejo-Ramos, Marrone, Korolkiewicz, Gabriel, Siemens, Joksimovic, Yamada, Mori, Rahwan, Sahakyan, Sonna, Meirmanov, Bolatov, Som, Ndukaihe, Arinze, Kundrát, Skanderová, Ngo, Nguyen, Lacia, Kung, Irmayanti, Muktadir, Samosir, Liuzza, Giorgini, Khatin-Zadeh, Banaruee, Özdoğru, Ariyabuddhiphongs, Rakchai, Trujillo, Valencia, Janyan, Kostov, Montoro, Hinojosa, Medeiros, Hunt, Posada, Freitag and Tejada.
KW  - AI
KW  - algorithms
KW  - data
KW  - explainability
KW  - statistical literacy
KW  - trust
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Qian, P.
AU  - Unhelkar, V.
TI  - Interactively Explaining Robot Policies to Humans in Integrated Virtual and Physical Training Environments
PY  - 2024
T2  - ACM/IEEE International Conference on Human-Robot Interaction
SP  - 847
EP  - 851
DO  - 10.1145/3610978.3640656
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188086000&doi=10.1145%2f3610978.3640656&partnerID=40&md5=5b6debbfeacabce47138a84f6f1a5847
AB  - Policy summarization is a computational paradigm for explaining the behavior and decision-making processes of autonomous robots to humans. It summarizes robot policies via exemplary demonstrations, aiming to improve human understanding of robotic behaviors. This understanding is crucial, especially since users often make critical decisions about robot deployment in the real world. Previous research in policy summarization has predominantly focused on simulated robots and environments, overlooking its application to physically embodied robots. Our work fills this gap by combining current policy summarization methods with a novel, interactive user interface that involves physical interaction with robots. We conduct human-subject experiments to assess our explanation system, focusing on the impact of different explanation modalities in policy summarization. Our findings underscore the unique advantages of combining virtual and physical training environments to effectively communicate robot behavior to human users. © 2024 Copyright held by the owner/author(s)
KW  - AI-Assisted Human Training
KW  - Explainable AI
KW  - Value Alignment
KW  - Behavioral research
KW  - Decision making
KW  - E-learning
KW  - Robots
KW  - Virtual reality
KW  - AI-assisted human training
KW  - Computational paradigm
KW  - Decision-making process
KW  - Explainable AI
KW  - Human understanding
KW  - Physical training
KW  - Real-world
KW  - Robotic behavior
KW  - Value alignment
KW  - Virtual training
KW  - User interfaces
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bennett, S.J.
AU  - Catanzariti, B.
AU  - Tollon, F.
TI  - “Everybody knows what a pothole is”: representations of work and intelligence in AI practice and governance
PY  - 2025
T2  - AI and Society
DO  - 10.1007/s00146-024-02162-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217191777&doi=10.1007%2fs00146-024-02162-0&partnerID=40&md5=954d7f430adc3167b79a83382ad768d4
AB  - In this paper, we empirically and conceptually examine how distributed human–machine networks of labour comprise a form of underlying intelligence within Artificial Intelligence (AI), considering the implications of this for Responsible Artificial Intelligence (R-AI) innovation. R-AI aims to guide AI research, development and deployment in line with certain normative principles, for example fairness, privacy, and explainability; notions implicitly shaped by comparisons of AI with individualised notions of human intelligence. However, as critical scholarship on AI demonstrates, this is a limited framing of the nature of intelligence, both of humans and AI. Furthermore, it dismisses the skills and labour central to developing AI systems, involving a distributed network of human-directed practices and reasoning. We argue that inequities in the agency and recognition of different types of practitioners across these networks of AI development have implications beyond RAI, with narrow framings concealing considerations which are important within broader discussions of AI intelligence. Drawing from interactive workshops conducted with AI practitioners, we explore practices of data acquisition, cleaning, and annotation, as the point where practitioners interface with domain experts and data annotators. Despite forming a crucial part of AI design and development, this type of data work is frequently framed as a tedious, unskilled, and low-value process. In exploring these practices, we examine the political role of the epistemic framings that underpin AI development and how these framings can shape understandings of distributed intelligence, labour practices, and annotators’ agency within data structures. Finally, we reflect on the implications of our findings for developing more participatory and equitable approaches to machine learning applications in the service of R-AI. © The Author(s) 2025.
KW  - Artificial intelligence
KW  - Automation
KW  - Intelligence
KW  - Labour
KW  - Machine learning
KW  - Responsible AI
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Fischer, I.
TI  - Evaluating the ethics of machines assessing humans
PY  - 2024
T2  - Journal of Information Technology Teaching Cases
VL  - 14
IS  - 2
SP  - 273
EP  - 281
DO  - 10.1177/20438869231178844
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162942545&doi=10.1177%2f20438869231178844&partnerID=40&md5=3b1154ec5dabd1d6bcb8ba51c3c5fbac
AB  - The case focusses on AQA (formerly known as Assessment and Qualification Alliance), one of England’s largest exam boards, and its evaluation of whether Artificial Intelligence (AI) could be deployed, in principle, for marking high-stakes assessments. At a time when generative AI, such as ChatGPT, has gained popularity, the case offers insights into the challenges and risks of algorithmic decision making and algorithmic fairness, such as accuracy and explainability. The case allows students to explore the role of ethics when developing an AI-based tool in an area that they all know very well: Most students will have had to sit high-stakes assessments in the past and are still likely to be assessed on an ongoing basis as a current student. Students will thus be able to relate to the case and have their own stake(s) and opinion(s) about whether they would want to be assessed by AI. The case is also more broadly applicable in raising general awareness of the challenges and potential risks involved when using AI for decision making and it encourages students to consider the wider consequences for all stakeholders that are triggered by the use of digital technology. © Association for Information Technology Trust 2023.
KW  - algorithmic decision-making
KW  - artificial intelligence
KW  - artificial intelligence ethics
KW  - assessments
KW  - automated essay scoring
KW  - education
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Selimovic, M.
AU  - Almisreb, A.A.
AU  - Ismail, N.
AU  - Amanzholova, S.
TI  - AI in Cancer Research: Challenges, Applications, and Future Directions
PY  - 2025
T2  - Lecture Notes in Networks and Systems
VL  - 1273
SP  - 201
EP  - 216
DO  - 10.1007/978-3-031-82881-2_13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004638353&doi=10.1007%2f978-3-031-82881-2_13&partnerID=40&md5=467d4809cd21291312b7aed608f4d966
AB  - The era of cancer diagnosis and treatment can change dramatically through Artificial Intelligence (AI). The present paper looks into AI's transformative attributes alongside the barriers and constraints that need to be solved for clinical integration to be successful. We discuss how AI-assisted solutions can help target therapeutic regime investigation and speed up medical decision-making in different medical fields related to oncology. Next, we highlight the challenges that harnessing AI faces like data standardization, ethical matters, and the level of accuracy of the data and adaptability of AI models. In summary, we underline the necessity of international cooperation, the importance of XAI (explainable AI) for establishing trust, and the conduct of the research in a manner that involves clinical trials and real-time demonstration in patients to guarantee database safety and usefulness in cancer care. AI can change the treatment world of cancers by combatting these obstacles and moving onto new ways. Such possibilities entail more accurate diagnosis, effective therapy, and hopefully, improved outcomes for the patients in the end. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
KW  - Artificial intelligence (AI)
KW  - Cancer diagnosis
KW  - Clinical decision support systems
KW  - Clinical trials
KW  - Data standardization
KW  - Ethical considerations
KW  - Explainable AI (XAI)
KW  - Personalized medicine
KW  - Real-world validation
KW  - Biopsy
KW  - Cardiography
KW  - Diseases
KW  - Ethical technology
KW  - Artificial intelligence
KW  - Cancer diagnosis
KW  - Clinical decision support systems
KW  - Clinical trial
KW  - Data standardization
KW  - Ethical considerations
KW  - Explainable artificial intelligence (XAI)
KW  - Personalized medicines
KW  - Real-world
KW  - Real-world validation
KW  - Theranostics
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Mohammadi, A.
AU  - Maghsoudi, M.
TI  - Bridging perspectives on artificial intelligence: a comparative analysis of hopes and concerns in developed and developing countries
PY  - 2025
T2  - AI and Society
C7  - 114155
DO  - 10.1007/s00146-025-02331-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002085433&doi=10.1007%2fs00146-025-02331-9&partnerID=40&md5=18a021830a2be5284c21331fb200a867
AB  - Artificial intelligence (AI) is transforming industries, generating both enthusiasm and concern. While AI-driven innovations enhance productivity, healthcare, and education, significant ethical issues persist, including misinformation, algorithmic bias, and job displacement. This study examines public perceptions of AI by analyzing large-scale social media discourse and integrating sentiment analysis with expert insights via the Delphi method to assess global perspectives. Findings reveal notable differences across socio-economic contexts. In high-income countries, discussions emphasize AI ethics, governance, and automation risks, whereas in low-income regions, economic challenges and accessibility barriers dominate concerns. Public trust in AI is significantly influenced by governance frameworks, transparency in algorithmic decision-making, and regulatory oversight. Recent advancements in AI governance highlight the increasing role of explainable AI (XAI) and algorithmic fairness, alongside regulatory developments tailored to societal needs. Algorithmic nudging has emerged as a tool for guiding user behavior while maintaining autonomy, and research on user sensemaking in fairness and transparency underscores the importance of interpretability tools in fostering trust and acceptance of AI-driven decisions. These insights emphasize the need for adaptive, context-specific policies that ensure ethical AI deployment while mitigating risks. By bridging public sentiment analysis with governance research, this study provides a comprehensive understanding of AI’s societal impact. Findings offer practical implications for policymakers, industry leaders, and researchers, contributing to the development of inclusive, transparent, and accountable AI governance frameworks that align with public expectations. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.
KW  - AI governance
KW  - Ethical AI
KW  - Global AI strategies
KW  - Public sentiment
KW  - Socio-economic contexts
KW  - Algorithmics
KW  - Artificial intelligence governance
KW  - Comparative analyzes
KW  - Ethical artificial intelligence
KW  - Ethical issues
KW  - Global artificial intelligence strategy
KW  - Public sentiments
KW  - Sentiment analysis
KW  - Socio-economic context
KW  - Socio-economics
KW  - Decision making
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Weng, L.
AU  - Liu, S.
AU  - Zhu, H.
AU  - Sun, J.
AU  - Kam-Kwai, W.
AU  - Han, D.
AU  - Zhu, M.
AU  - Chen, W.
TI  - Towards an understanding and explanation for mixed-initiative artificial scientific text detection
PY  - 2024
T2  - Information Visualization
VL  - 23
IS  - 3
SP  - 272
EP  - 291
DO  - 10.1177/14738716241240156
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190283229&doi=10.1177%2f14738716241240156&partnerID=40&md5=527c67b16a4903503e3d8396576d24d3
AB  - Large language models (LLMs) have gained popularity in various fields for their exceptional capability of generating human-like text. Their potential misuse has raised social concerns about plagiarism in academic contexts. However, effective artificial scientific text detection is a non-trivial task due to several challenges, including (1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, (2) the poor generalization performance of existing methods caused by out-of-distribution issues, and (3) the limited support for human-machine collaboration with sufficient interpretability during the detection process. In this paper, we first identify the critical distinctions between machine-generated and human-written scientific text through a quantitative experiment. Then, we propose a mixed-initiative workflow that combines human experts’ prior knowledge with machine intelligence, along with a visual analytics system to facilitate efficient and trustworthy scientific text detection. Finally, we demonstrate the effectiveness of our approach through two case studies and a controlled user study. We also provide design implications for interactive artificial text detection tools in high-stakes decision-making scenarios. © The Author(s) 2024.
KW  - explainable artificial intelligence
KW  - Large language models
KW  - mixed-initiative
KW  - visual analytics
KW  - Artificial intelligence
KW  - Computational linguistics
KW  - Visualization
KW  - Explainable artificial intelligence
KW  - Human like
KW  - Language model
KW  - Large language model
KW  - Mixed-initiative
KW  - Non-trivial tasks
KW  - Scientific texts
KW  - Social concerns
KW  - Text detection
KW  - Visual analytics
KW  - Decision making
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CHAP
AU  - Mittal, D.
AU  - Parashar, A.R.
AU  - Thawkar, S.
AU  - Katta, V.S.
TI  - Human-machine interaction for knowledge discovery and management
PY  - 2024
T2  - Modern Technology in Healthcare and Medical Education: Blockchain, IoT, AR, and VR
SP  - 88
EP  - 105
DO  - 10.4018/979-8-3693-5493-3.ch006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194997246&doi=10.4018%2f979-8-3693-5493-3.ch006&partnerID=40&md5=d729a4f3cde06a3cfd80291c0f5cf4c9
AB  - The ongoing data explosion driven by digital information growth has led to the emergence of human-machine interaction (HMI) as a vital tool in knowledge discovery and management across various disciplines. The exponential growth of knowledge discovery and management due to the influx of digital data and investigates into HMI applications, encompassing data visualization, natural language processing, machine learning, and augmented reality, with real-world examples demonstrating their ability in deciphering complex data and extracting insights. Ethical concerns and future directions, including issues of bias, privacy, security, and research prospects like explainable AI and personalized interfaces, are also discussed. The societal and workforce implications of HMI are explored, highlighting its potential benefits and challenges while advocating for responsible development and policies to ensure a harmonious interaction between humans and machines in the realm of knowledge discovery and management. © 2024, IGI Global.
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Schrank, A.
AU  - Kettwich, C.
AU  - Oehl, M.
TI  - Aiding Automated Shuttles with Their Driving Tasks as an On-Board Operator: A Case Study on Different Automated Driving Systems in Three Living Labs
PY  - 2024
T2  - Applied Sciences (Switzerland)
VL  - 14
IS  - 8
C7  - 3336
DO  - 10.3390/app14083336
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192506801&doi=10.3390%2fapp14083336&partnerID=40&md5=4cf0fc2eece67f77ec9f611789d6a236
AB  - Highly automated shuttle vehicles (SAE Level 4) have the potential to enhance public transport services by decreasing the demand for drivers, enabling more frequent and flexible ride options. However, at least in a transitionary phase, safety operators that supervise and support the shuttles with their driving tasks may be required on board the vehicle from a technical or legal point of view. A crucial component for executing supervisory and intervening tasks is the human–machine interface between an automated vehicle and its on-board operator. This research presents in-depth case studies from three heterogenous living laboratories in Germany that deployed highly automated shuttle vehicles with on-board operators on public roads. The living labs differed significantly regarding the on-board operators’ tasks and the design of the human–machine interfaces. Originally considered a provisional solution until the vehicle automation is fully capable of running without human support, these interfaces were, in general, not designed in a user-centered way. However, since technological progress has been slower than expected, on-board operator interfaces are likely to persist in the mid-term at least. Hence, this research aims to assess the aptitude of interfaces that are in practical use for the on-board operators’ tasks, in order to determine the user-centered design of future interfaces. Completing questionnaires and undergoing comprehensive, semi-structured interviews, nine on-board operators evaluated their human–machine interfaces in light of the respective tasks they complete regarding user variables such as work context, acceptance, system transparency, and trust. The results were highly diverse across laboratories and underlined that the concrete system setup, encompassing task and interface design, has a considerable impact on these variables. Ergonomics, physical demand, and system transparency were identified as the most significant deficits. These findings and derived recommendations may inform the design of on-board operator workspaces, and bear implications for remote operation workstations as well. © 2024 by the authors.
KW  - automated driving
KW  - highly automated vehicles
KW  - human–machine interface
KW  - living lab
KW  - on-board operators
KW  - public transport
KW  - shuttles
KW  - workplace analysis
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Yu, H.
AU  - Kamat, V.R.
AU  - Menassa, C.C.
TI  - Cloud-Based Hierarchical Imitation Learning for Scalable Transfer of Construction Skills from Human Workers to Assisting Robots
PY  - 2024
T2  - Journal of Computing in Civil Engineering
VL  - 38
IS  - 4
C7  - 04024019
DO  - 10.1061/JCCEE5.CPENG-5731
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191248891&doi=10.1061%2fJCCEE5.CPENG-5731&partnerID=40&md5=6248de17957b46907db2f26a59033263
AB  - Assigning repetitive and physically demanding construction tasks to robots can alleviate human workers’ exposure to occupational injuries, which often result in significant downtime or premature retirement. However, the successful delegation of construction tasks and the achievement of high-quality robot-constructed work requires transferring necessary dexterous and adaptive construction craft skills from workers to robots. Predefined motion planning scripts tend to generate rigid and collision-prone robotic behaviors in unstructured construction site environments. In contrast, imitation learning (IL) offers a more robust and flexible skill transfer scheme. However, the majority of IL algorithms rely on human workers repeatedly demonstrating task performance at full scale, which can be counterproductive and infeasible in the case of construction work. To address this concern, in this paper, we propose an immersive and Cloud Robotics-based virtual demonstration framework that serves two primary purposes. First, it digitalizes the demonstration process, eliminating the need for repetitive physical manipulation of heavy construction objects. Second, it employs a federated collection of reusable demonstrations that are transferable for similar tasks in the future and can, consequently, reduce the requirement for repetitive illustration of tasks by human agents. In addition, to enhance the trustworthiness, explainability, and ethical soundness of the robot training, this framework utilizes a hierarchical imitation learning (HIL) model to decompose human manipulation skills into sequential and reactive subskills. These two layers of skills are represented by deep generative models; these models enable adaptive control of robot action. The proposed framework has the potential to mitigate technical adoption barriers and facilitate the practical deployment of full-scale construction robots to perform a variety of tasks with human supervision. By delegating the physical strains of construction work to human-trained robots, this framework promotes the inclusion of workers with diverse physical capabilities and educational backgrounds within the construction industry. © 2024 American Society of Civil Engineers.
KW  - Demonstrations
KW  - Motion planning
KW  - Occupational risks
KW  - Personnel training
KW  - Robot programming
KW  - Virtual addresses
KW  - Cloud-based
KW  - Construction skills
KW  - Construction works
KW  - Craft skills
KW  - High quality
KW  - Imitation learning
KW  - Motion-planning
KW  - Occupational injury
KW  - Workers'
KW  - Workers' exposures
KW  - Construction industry
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Li, P.
AU  - Hosseinzadeh, P.
AU  - Bahri, O.
AU  - Boubrahimi, S.F.
AU  - Hamdi, S.M.
TI  - Reliable Time Series Counterfactual Explanations Guided by ShapeDBA
PY  - 2024
T2  - Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024
SP  - 1574
EP  - 1579
DO  - 10.1109/BigData62323.2024.10825447
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217991038&doi=10.1109%2fBigData62323.2024.10825447&partnerID=40&md5=642de01887135cdd0c8f23704350c9bc
AB  - Artificial intelligence (AI) and algorithmic decision-making are profoundly shaping various aspects of society, with applications in healthcare, business, education, etc. As these systems become more integral to high-stakes decisions, concerns about their transparency and interpretability are growing. To address these concerns, explainable AI (XAI) methods have been developed, with counterfactual explanations emerging as a powerful tool. Counterfactuals help users understand AI decisions by demonstrating how small changes in input could alter the outcome, providing a clear and intuitive way to interpret AI behavior. Despite their potential, generating valid, interpretable, and efficient counterfactual explanations is particularly challenging in time series domains, where data points are interdependent. In this paper, we introduce a novel approach to counterfactual explanations guided by ShapeDTW Barycenter Averaging (ShapeDBA). By integrating ShapeDBA into the counterfactual generation process, we ensure that the produced explanations are not only valid and interpretable but also efficient to generate. Our approach provides counterfactuals that align closely with human intuition while maintaining the computational efficiency required for practical deployment. This work represents a significant step forward in the development of interpretable AI systems, particularly in the complex domain of time series analysis. © 2024 IEEE.
KW  - counterfactual explanations
KW  - DTW barycenter averaging
KW  - Explainable Artificial Intelligence (XAI)
KW  - time series classification
KW  - Economic and social effects
KW  - Algorithmics
KW  - Barycenters
KW  - Business educations
KW  - Counterfactual explanation
KW  - Counterfactuals
KW  - Decisions makings
KW  - DTW barycenter averaging
KW  - Explainable artificial intelligence (XAI)
KW  - Time series classifications
KW  - Times series
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - BOOK
AU  - Sarker, I.H.
TI  - AI-Driven Cybersecurity and Threat Intelligence: Cyber Automation, Intelligent Decision-Making and Explainability
PY  - 2024
T2  - AI-Driven Cybersecurity and Threat Intelligence: Cyber Automation, Intelligent Decision-Making and Explainability
SP  - 1
EP  - 200
DO  - 10.1007/978-3-031-54497-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200166453&doi=10.1007%2f978-3-031-54497-2&partnerID=40&md5=e1a1546ee712d46205910fb07a262e45
AB  - This book explores the dynamics of how AI (Artificial Intelligence) technology intersects with cybersecurity challenges and threat intelligence as they evolve. Integrating AI into cybersecurity not only offers enhanced defense mechanisms, but this book introduces a paradigm shift illustrating how one conceptualize, detect and mitigate cyber threats. An in-depth exploration of AI-driven solutions is presented, including machine learning algorithms, data science modeling, generative AI modeling, threat intelligence frameworks and Explainable AI (XAI) models. As a roadmap or comprehensive guide to leveraging AI/XAI to defend digital ecosystems against evolving cyber threats, this book provides insights, modeling, real-world applications and research issues. Throughout this journey, the authors discover innovation, challenges, and opportunities. It provides a holistic perspective on the transformative role of AI in securing the digital world. Overall, the useof AI can transform the way one detects, responds and defends against threats, by enabling proactive threat detection, rapid response and adaptive defense mechanisms. AI-driven cybersecurity systems excel at analyzing vast datasets rapidly, identifying patterns that indicate malicious activities, detecting threats in real time as well as conducting predictive analytics for proactive solution. Moreover, AI enhances the ability to detect anomalies, predict potential threats, and respond swiftly, preventing risks from escalated. As cyber threats become increasingly diverse and relentless, incorporating AI/XAI into cybersecurity is not just a choice, but a necessity for improving resilience and staying ahead of ever-changing threats. This book targets advanced-level students in computer science as a secondary textbook. Researchers and industry professionals working in various areas, such as Cyber AI, Explainable and Responsible AI, Human-AI Collaboration, Automation and Intelligent Systems, Adaptive and Robust Security Systems, Cybersecurity Data Science and Data-Driven Decision Making will also find this book useful as reference book. © All rights reserved.
KW  - Artificial intelligence
KW  - Automation
KW  - Cyber data analytics
KW  - Cyber threat intelligence
KW  - Cybersecurity
KW  - Data science
KW  - Deep learning
KW  - Explainable AI
KW  - Generative AI
KW  - Intelligent decision-making
KW  - Large language modeling
KW  - Machine learning,
KW  - Next-generation cybersecurity applications
KW  - Responsible AI
M3  - Book
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 32
ER  -

TY  - JOUR
AU  - Giavina Bianchi, M.
AU  - D'adario, A.
AU  - Giavina Bianchi, P.
AU  - Machado, B.S.
TI  - Three versions of an atopic dermatitis case report written by humans, artificial intelligence, or both: Identification of authorship and preferences
PY  - 2025
T2  - Journal of Allergy and Clinical Immunology: Global
VL  - 4
IS  - 1
C7  - 100373
DO  - 10.1016/j.jacig.2024.100373
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211716448&doi=10.1016%2fj.jacig.2024.100373&partnerID=40&md5=d73e655c4a2472ecaddf29913b3be5ed
AB  - Background: The use of artificial intelligence (AI) in scientific writing is rapidly increasing, raising concerns about authorship identification, content quality, and writing efficiency. Objectives: This study investigates the real-world impact of ChatGPT, a large language model, on those aspects in a simulated publication scenario. Methods: Forty-eight individuals representing 3 medical expertise levels (medical students, residents, and experts in allergy or dermatology) evaluated 3 blinded versions of an atopic dermatitis case report: one each human written (HUM), AI generated (AI), and combined written (COM). The survey assessed authorship, ranked their preference, and graded 13 quality criteria for each text. Time taken to generate each manuscript was also recorded. Results: Authorship identification accuracy mirrored the odds at 33%. Expert participants (50.9%) demonstrated significantly higher accuracy compared to residents (27.7%) and students (19.6%, P < .001). Participants favored AI-assisted versions (AI and COM) over HUM (P < .001), with COM receiving the highest quality scores. COM and AI achieved 83.8% and 84.3% reduction in writing time, respectively, compared to HUM, while showing 13.9% (P < .001) and 11.1% improvement in quality (P < .001), respectively. However, experts assigned the lowest score for the references of the AI manuscript, potentially hindering its publication. Conclusion: AI can deceptively mimic human writing, particularly for less experienced readers. Although AI-assisted writing is appealing and offers significant time savings, human oversight remains crucial to ensure accuracy, ethical considerations, and optimal quality. These findings underscore the need for transparency in AI use and highlight the potential of human-AI collaboration in the future of scientific writing. © 2024 The Author(s)
KW  - artificial intelligence
KW  - ChatGPT
KW  - Generative Pre-training Transformer (GPT)
KW  - large language model (LLM)
KW  - medical survey
KW  - scientific writing
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Din, M.
AU  - Daga, K.
AU  - Saoud, J.
AU  - Wood, D.
AU  - Kierkegaard, P.
AU  - Brex, P.
AU  - Booth, T.C.
TI  - Clinicians’ perspectives on the use of artificial intelligence to triage MRI brain scans
PY  - 2025
T2  - European Journal of Radiology
VL  - 183
C7  - 111921
DO  - 10.1016/j.ejrad.2025.111921
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214584451&doi=10.1016%2fj.ejrad.2025.111921&partnerID=40&md5=fa8e8324e642d6746863e38140c54325
AB  - Artificial intelligence (AI) tools can triage radiology scans to streamline the patient pathway and also relieve clinician workload. Validated AI tools can mitigate the delays in reporting scans by flagging time-sensitive and actionable findings. In this study, we aim to investigate current stakeholder perspectives and identify obstacles to integrating AI in clinical pathways. We created a survey to ascertain the perspectives of 133 clinicians across the United Kingdom regarding the acceptability of an AI tool that triages MRI brain scans into ‘normal’ and ‘abnormal’. As part of this survey, we supplied clinicians with information on training and validation case numbers, model performance, validation using unseen data, and explainability saliency maps. With regards to the specific use case of AI in MRI brain scans, 71% of respondents preferred the use of an AI-assisted triage compared to the current system without triage, typically chronologically. Notably, information that explained and helped visualise the AI model's decision making was found to improve clinician confidence. When shown a heatmap, 60% of participants felt more confident in the AI's decision. The results of this short communication demonstrate a positive support for the implementation of AI-assistive tools in triage. © 2025 The Authors
KW  - Artificial Intelligence
KW  - Brain
KW  - MRI
KW  - Triage
KW  - Artificial Intelligence
KW  - Attitude of Health Personnel
KW  - Brain
KW  - Humans
KW  - Magnetic Resonance Imaging
KW  - Surveys and Questionnaires
KW  - Triage
KW  - United Kingdom
KW  - adult
KW  - Article
KW  - artificial intelligence
KW  - brain scintiscanning
KW  - clinical pathway
KW  - clinical practice
KW  - clinician
KW  - decision making
KW  - demographics
KW  - female
KW  - follow up
KW  - health care
KW  - human
KW  - Likert scale
KW  - male
KW  - neuroimaging
KW  - neuroradiologist
KW  - normal human
KW  - nuclear magnetic resonance imaging
KW  - patient triage
KW  - questionnaire
KW  - training
KW  - brain
KW  - diagnostic imaging
KW  - health personnel attitude
KW  - patient triage
KW  - procedures
KW  - United Kingdom
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CHAP
AU  - Widjaja, G.
AU  - Wahid, N.
AU  - Raha, S.
AU  - Pande, S.D.
AU  - Manerkar, S.G.V.
TI  - An exhaustive exploration of explainable AI-driven applications in healthcare, enhancing diagnostic accuracy, treatment efficacy, and patient trust
PY  - 2024
T2  - Explainable Artificial Intelligence for Biomedical and Healthcare Applications
SP  - 230
EP  - 248
DO  - 10.1201/9781003220107-14
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210641547&doi=10.1201%2f9781003220107-14&partnerID=40&md5=a1abf906a23b751d84dd65448f009e23
AB  - The purpose of this abstract is to offer a comprehensive assessment of the methodologies. Explainable artificial intelligence (XAI) is transforming medical treatment and patient care. It focuses on the several ways that XAI-powered apps are transforming the healthcare environment by enhancing treatment effectiveness, diagnostic accuracy, and patient confidence in AI-assisted healthcare systems. The goal of this research is to look at the use of XAI in diagnosis, therapeutic planning, and continuous patient monitoring. The capacity of XAI to provide a clear rationale for AI judgments is critical in sensitive healthcare situations where trust, openness, and lives are at risk. According to various research studies, artificial intelligence may enhance medical results by leveraging real-world case studies and settings. It also investigates the ethical implications of XAI in healthcare. Patient privacy, algorithm fairness, and legal compliance are all critical considerations here. © 2025 selection and editorial matter, Aditya Khamparia and Deepak Gupta; individual chapters, the contributors.
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Sobrie, L.
AU  - Verschelde, M.
TI  - Real-time decision support for human–machine interaction in digital railway control rooms
PY  - 2024
T2  - Decision Support Systems
VL  - 181
C7  - 114216
DO  - 10.1016/j.dss.2024.114216
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189566946&doi=10.1016%2fj.dss.2024.114216&partnerID=40&md5=05fe7c51cf9aedbe645d0f84978cfec9
AB  - This study proposes a real-time Decision Support System (DSS) using machine learning to enhance proactive management of Human–Machine Interaction (HMI) in safety–critical digital control rooms. The DSS provides explainable predictions and recommendations regarding near-future automation usage, customized for the railway control room management, who supervise the operations of traffic controllers (TCs). In this setting, TCs decide on the spot whether to manually or automatically open signals to regulate railway traffic, a critical aspect of ensuring punctuality and safety. This time-setting specific HMI differs across TCs and is not yet supported by a data-driven tool. The proposed DSS includes agreement levels for predictions among different modeling paradigms: linear models, tree-based models, and deep neural networks. SHAP (SHapley Additive exPlanations) values are deployed to assess the agreement level in explainability between these different modeling paradigms. The prescriptions are based on the HMI of well-performing peers. We implement the DSS as proof of concept at the Belgian railway infrastructure company and report end-user feedback on the perception, the operational impact, and the inclusion of agreement levels. © 2024 Elsevier B.V.
KW  - Behavioral prescription
KW  - Decision support systems
KW  - End-user feedback
KW  - Explainable prediction
KW  - Human–machine interaction
KW  - Real-time railway traffic management
KW  - Decision making
KW  - Deep neural networks
KW  - Digital control systems
KW  - Feedback
KW  - Forecasting
KW  - Railroad transportation
KW  - Railroads
KW  - Rails
KW  - Real time systems
KW  - Behavioral prescription
KW  - End-user feedback
KW  - Explainable prediction
KW  - Human machine interaction
KW  - Modeling paradigms
KW  - Railway control
KW  - Railway traffic management
KW  - Real- time
KW  - Real-time railway traffic management
KW  - Traffic controllers
KW  - Decision support systems
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Sloane, M.
AU  - Wüllhorst, E.
TI  - A systematic review of regulatory strategies and transparency mandates in AI regulation in Europe, the United States, and Canada
PY  - 2025
T2  - Data and Policy
VL  - 7
C7  - e11
DO  - 10.1017/dap.2024.54
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000818351&doi=10.1017%2fdap.2024.54&partnerID=40&md5=fed8a8e010afc89161af793627c2ea17
AB  - In this paper, we provide a systematic review of existing artificial intelligence (AI) regulations in Europe, the United States, and Canada. We build on the qualitative analysis of 129 AI regulations (enacted and not enacted) to identify patterns in regulatory strategies and in AI transparency requirements. Based on the analysis of this sample, we suggest that there are three main regulatory strategies for AI: AI-focused overhauls of existing regulation, the introduction of novel AI regulation, and the omnibus approach. We argue that although these types emerge as distinct strategies, their boundaries are porous as the AI regulation landscape is rapidly evolving. We find that across our sample, AI transparency is effectively treated as a central mechanism for meaningful mitigation of potential AI harms. We therefore focus on AI transparency mandates in our analysis and identify six AI transparency patterns: human in the loop, assessments, audits, disclosures, inventories, and red teaming. We contend that this qualitative analysis of AI regulations and AI transparency patterns provides a much needed bridge between the policy discourse on AI, which is all too often bound up in very detailed legal discussions and applied sociotechnical research on AI fairness, accountability, and transparency. © The Author(s), 2025.
KW  - AI regulation
KW  - compliance
KW  - sociotechnical research
KW  - transparency
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Lee, C.P.
TI  - Design, Development, and Deployment of Context-Adaptive AI Systems for Enhanced User Adoption
PY  - 2024
T2  - Conference on Human Factors in Computing Systems - Proceedings
C7  - 429
DO  - 10.1145/3613905.3638195
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194167021&doi=10.1145%2f3613905.3638195&partnerID=40&md5=1b3f94bf12fa6b7cadae2fc07928095b
AB  - My research centers on the development of context-adaptive AI systems to improve end-user adoption through the integration of technical methods. I deploy these AI systems across various interaction modalities, including user interfaces and embodied agents like robots, to expand their practical applicability. My research unfolds in three key stages: design, development, and deployment. In the design phase, user-centered approaches were used to understand user experiences with AI systems and create design tools for user participation in crafting AI explanations. In the ongoing development stage, a safety-guaranteed AI system for a robot agent was created to automatically provide adaptive solutions and explanations for unforeseen scenarios. The next steps will involve the implementation and evaluation of context-adaptive AI systems in various interaction forms. I seek to prioritize human needs in technology development, creating AI systems that tangibly benefit end-users in real-world applications and enhance interaction experiences. © 2024 Association for Computing Machinery. All rights reserved.
KW  - Human-AI interaction
KW  - human-robot interaction
KW  - user-centered design
KW  - Human robot interaction
KW  - Machine design
KW  - Software design
KW  - User interfaces
KW  - AI systems
KW  - Design development
KW  - Design phase
KW  - Embodied agent
KW  - End-users
KW  - Human-AI interaction
KW  - Humans-robot interactions
KW  - Research center
KW  - User adoptions
KW  - User interface agents
KW  - User centered design
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Finkelstein, J.
AU  - Gabriel, A.
AU  - Schmer, S.
AU  - Truong, T.-T.
AU  - Dunn, A.
TI  - Identifying Facilitators and Barriers to Implementation of AI-Assisted Clinical Decision Support in an Electronic Health Record System
PY  - 2024
T2  - Journal of Medical Systems
VL  - 48
IS  - 1
C7  - 89
DO  - 10.1007/s10916-024-02104-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204309975&doi=10.1007%2fs10916-024-02104-9&partnerID=40&md5=57cb4d04e4773e5eaf3837faf7eb4b87
AB  - Recent advancements in computing have led to the development of artificial intelligence (AI) enabled healthcare technologies. AI-assisted clinical decision support (CDS) integrated into electronic health records (EHR) was demonstrated to have a significant potential to improve clinical care. With the rapid proliferation of AI-assisted CDS, came the realization that a lack of careful consideration of socio-technical issues surrounding the implementation and maintenance of these tools can result in unanticipated consequences, missed opportunities, and suboptimal uptake of these potentially useful technologies. The 48-h Discharge Prediction Tool (48DPT) is a new AI-assisted EHR CDS to facilitate discharge planning. This study aimed to methodologically assess the implementation of 48DPT and identify the barriers and facilitators of adoption and maintenance using the validated implementation science frameworks. The major dimensions of RE-AIM (Reach, Effectiveness, Adoption, Implementation, Maintenance) and the constructs of the Consolidated Framework for Implementation Research (CFIR) frameworks have been used to analyze interviews of 24 key stakeholders using 48DPT. The systematic assessment of the 48DPT implementation allowed us to describe facilitators and barriers to implementation such as lack of awareness, lack of accuracy and trust, limited accessibility, and transparency. Based on our evaluation, the factors that are crucial for the successful implementation of AI-assisted EHR CDS were identified. Future implementation efforts of AI-assisted EHR CDS should engage the key clinical stakeholders in the AI tool development from the very inception of the project, support transparency and explainability of the AI models, provide ongoing education and onboarding of the clinical users, and obtain continuous input from clinical staff on the CDS performance. © The Author(s) 2024.
KW  - Artificial Intelligence
KW  - Clinical Decision Support
KW  - Electronic Health Record
KW  - Implementation Science
KW  - Socio-Technical Factors
KW  - Artificial Intelligence
KW  - Decision Support Systems, Clinical
KW  - Electronic Health Records
KW  - Humans
KW  - Patient Discharge
KW  - article
KW  - artificial intelligence
KW  - awareness
KW  - clinical decision support system
KW  - drug therapy
KW  - electronic health record
KW  - hospital discharge
KW  - human
KW  - implementation science
KW  - interview
KW  - major clinical study
KW  - pilot study
KW  - prediction
KW  - organization and management
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Schemmer, M.
AU  - Kuehl, N.
AU  - Benz, C.
AU  - Bartos, A.
AU  - Satzger, G.
TI  - Appropriate Reliance on AI Advice: Conceptualization and the Effect of Explanations
PY  - 2023
T2  - International Conference on Intelligent User Interfaces, Proceedings IUI
SP  - 410
EP  - 422
DO  - 10.1145/3581641.3584066
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152119120&doi=10.1145%2f3581641.3584066&partnerID=40&md5=ae468e21ff729bc86d2af226e0301e17
AB  - AI advice is becoming increasingly popular, e.g., in investment and medical treatment decisions. As this advice is typically imperfect, decision-makers have to exert discretion as to whether actually follow that advice: they have to "appropriately"rely on correct and turn down incorrect advice. However, current research on appropriate reliance still lacks a common definition as well as an operational measurement concept. Additionally, no in-depth behavioral experiments have been conducted that help understand the factors influencing this behavior. In this paper, we propose Appropriateness of Reliance (AoR) as an underlying, quantifiable two-dimensional measurement concept. We develop a research model that analyzes the effect of providing explanations for AI advice. In an experiment with 200 participants, we demonstrate how these explanations influence the AoR, and, thus, the effectiveness of AI advice. Our work contributes fundamental concepts for the analysis of reliance behavior and the purposeful design of AI advisors.  © 2023 ACM.
KW  - Appropriate Reliance
KW  - Explainable AI
KW  - Human-AI Collaboration
KW  - Human-AI Complementarity
KW  - Decision making
KW  - 'current
KW  - Appropriate reliance
KW  - Behavioral experiment
KW  - Decision makers
KW  - Explainable AI
KW  - Human-AI collaboration
KW  - Human-AI complementarity
KW  - Measurement concepts
KW  - Medical treatment
KW  - Two-dimensional measurement
KW  - Human engineering
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 74
ER  -

TY  - CHAP
AU  - Victoria, A.H.
AU  - Tiwari, R.S.
AU  - Ghulam, A.K.
TI  - Libraries for Explainable Artificial Intelligence (EXAI) Python
PY  - 2024
T2  - Explainable AI (XAI) for Sustainable Development: Trends and Applications
SP  - 211
EP  - 232
DO  - 10.1201/9781003457176-13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196457231&doi=10.1201%2f9781003457176-13&partnerID=40&md5=ebfd141ed84779d7bd0192debb39e9c2
AB  - Artificial Intelligence (AI) was first coined by Sir John McCarthy in the 1960s, and many of the algorithms that are used today were invented by researchers several decades ago. But due to restrictions on storage and computing power, they were not able to be implemented in real-world applications. Around 2010, the implementation pace increased drastically because of the breakthroughs in computing as well as storage. Since then, AI has been implemented in almost every field. Healthcare is one of the few fields where AI is taking its baby steps. AI is dependent on the dataset though healthcare has existed for several centuries. But we have a dataset shortage when it comes to train the AI model, especially deep learning (DL) models. Somehow, researchers are able to solve the dataset shortage problem by augmenting the original dataset or by generating synthetic dataset – considering several parameters from the original dataset such as mean, dispersion, mode range, and a variety of parameters to train/test and deploy the model. Machine learning (ML) and DL are subsets of AI, which is dependent on quality data for training. In recent years, researchers have succeeded in training a state-of-the-art model that can predict fatigue, wear-tear, life cycle, and other numerous properties of materials. But with recent breakthroughs, one problem of bias is persistent, and it has made a catastrophic, i.e., biased model. Quality of model output is clearly dependent on the quality of the dataset, and if the dataset is biased, then the model will become biased about certain features that can lead to incorrect results. We have several epitomes of biased models; for instance, in the USA, black skin colored people were categorized as criminals, women were considered as homemakers, and men were considered as breadwinners by the NLP word-embedding model and several other models which became biased on certain features after training. So, there is a need to explain the model behavior with respect to the dataset by considering each specific feature as well as by considering the full dataset. So Explainable Artificial Intelligence (XAI) comes into existence which provides the reason why the model predicted this output. Healthcare is a very sensitive area where small error can cost numerous human lives and early detection of diseases can save numerous lives. ML is holding its ground but the reason why the respective ML model comes to certain decisions is still unknown; hence, it is known as black-box models. ML products and algorithms have been utilized in humongous amounts by our environment, and a single error or wrong prediction can cost numerous human lives. So, there is a need to explain why the model predicted this specific value of class so that the human supervisor can be satisfied as well as the chance of error can be minimized. Recently, XAI has explained the reason behind the model prediction satisfactorily by employing algorithms such as Feature Importance, Permutation of Features, SHAP Value, and Activation at each layer which are used by various libraries to visually represent the reason behind the prediction. Nowadays, there are a variety of frameworks and libraries to justify the prediction of the model such as What-If Tool by TensorFlow, ELI 5, Shapley, Lime, IBM 360 Explainability, Deep LIFT, Skater, etc. © 2024 selection and editorial matter, Lakshmi D., Ravi Shekhar Tiwari, Rajesh Kumar Dhanaraj and Seifedine Kadry; individual chapters, the contributors.
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Kottinger, J.
AU  - Almagor, S.
AU  - Lahijanian, M.
TI  - Conflict-Based Search for Explainable Multi-Agent Path Finding
PY  - 2022
T2  - Proceedings International Conference on Automated Planning and Scheduling, ICAPS
VL  - 32
SP  - 692
EP  - 700
DO  - 10.1609/icaps.v32i1.19859
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142631834&doi=10.1609%2ficaps.v32i1.19859&partnerID=40&md5=91baf553137034fe489d840701c5b3db
AB  - The goal of the Multi-Agent Path Finding (MAPF) problem is to find non-colliding paths for agents in an environment, such that each agent reaches its goal from its initial location. In safety-critical applications, a human supervisor may want to verify that the plan is indeed collision-free. To this end, a recent work introduces a notion of explainability for MAPF based on a visualization of the plan as a short sequence of images representing time segments, where in each time segment the trajectories of the agents are disjoint. Then, the problem of Explainable MAPF via Segmentation asks for a set of non-colliding paths that admit a short-enough explanation. Explainable MAPF adds a new difficulty to MAPF, in that it is NP-hard with respect to the size of the environment, and not just the number of agents. Thus, traditional MAPF algorithms are not equipped to directly handle Explainable MAPF. In this work, we adapt Conflict Based Search (CBS), a well-studied algorithm for MAPF, to handle Explainable MAPF. We show how to add explainability constraints on top of the standard CBS tree and its underlying A∗ search. We examine the usefulness of this approach and, in particular, the trade-off between planning time and explainability. © 2022, Association for the Advancement of Artificial Intelligence.
KW  - Electric circuit breakers
KW  - Image segmentation
KW  - Multi agent systems
KW  - Safety engineering
KW  - Collision-free
KW  - Human supervisors
KW  - Multi agent
KW  - NP-hard
KW  - Path finding
KW  - Path finding problems
KW  - Safety critical applications
KW  - Sequence of images
KW  - Short sequences
KW  - Time segments
KW  - Economic and social effects
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 11
ER  -

TY  - JOUR
AU  - Mualla, Y.
AU  - Tchappi, I.
AU  - Kampik, T.
AU  - Najjar, A.
AU  - Calvaresi, D.
AU  - Abbas-Turki, A.
AU  - Galland, S.
AU  - Nicolle, C.
TI  - The quest of parsimonious XAI: A human-agent architecture for explanation formulation
PY  - 2022
T2  - Artificial Intelligence
VL  - 302
C7  - 103573
DO  - 10.1016/j.artint.2021.103573
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113998530&doi=10.1016%2fj.artint.2021.103573&partnerID=40&md5=709fb1f7eec11394a7b634e3f9e1519b
AB  - With the widespread use of Artificial Intelligence (AI), understanding the behavior of intelligent agents and robots is crucial to guarantee successful human-agent collaboration since it is not straightforward for humans to understand an agent's state of mind. Recent empirical studies have confirmed that explaining a system's behavior to human users fosters the latter's acceptance of the system. However, providing overwhelming or unnecessary information may also confuse the users and cause failure. For these reasons, parsimony has been outlined as one of the key features allowing successful human-agent interaction with parsimonious explanation defined as the simplest explanation (i.e. least complex) that describes the situation adequately (i.e. descriptive adequacy). While parsimony is receiving growing attention in the literature, most of the works are carried out on the conceptual front. This paper proposes a mechanism for parsimonious eXplainable AI (XAI). In particular, it introduces the process of explanation formulation and proposes HAExA, a human-agent explainability architecture allowing to make it operational for remote robots. To provide parsimonious explanations, HAExA relies on both contrastive explanations and explanation filtering. To evaluate the proposed architecture, several research hypotheses are investigated in an empirical user study that relies on well-established XAI metrics to estimate how trustworthy and satisfactory the explanations provided by HAExA are. The results are analyzed using parametric and non-parametric statistical testing. © 2021 Elsevier B.V.
KW  - Empirical user studies
KW  - Explainable artificial intelligence
KW  - Human-computer interaction
KW  - Multi-agent systems
KW  - Statistical testing
KW  - Artificial intelligence
KW  - Behavioral research
KW  - Intelligent agents
KW  - Empirical studies
KW  - Human agent interactions
KW  - Human users
KW  - Key feature
KW  - Non-parametric
KW  - Proposed architectures
KW  - Remote robot
KW  - Statistical testing
KW  - Social robots
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 32
ER  -

TY  - CONF
AU  - Sipos, L.
AU  - Schäfer, U.
AU  - Glinka, K.
AU  - Müller-Birn, C.
TI  - Identifying Explanation Needs of End-users: Applying and Extending the XAI Question Bank
PY  - 2023
T2  - ACM International Conference Proceeding Series
SP  - 492
EP  - 497
DO  - 10.1145/3603555.3608551
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171176071&doi=10.1145%2f3603555.3608551&partnerID=40&md5=f872b7700f8a37b3fc3d3e8a71beec5c
AB  - Explainable Artificial Intelligence (XAI) is concerned with making the decisions of AI systems interpretable to humans. Explanations are typically developed by AI experts and focus on algorithmic transparency and the inner workings of AI systems. Research has shown that such explanations do not meet the needs of users who do not have AI expertise. As a result, explanations are often ineffective in making system decisions interpretable and understandable. We aim to strengthen a socio-technical view of AI by following a Human-Centered Explainable Artificial Intelligence (HC-XAI) approach, which investigates the explanation needs of end-users (i.e., subject matter experts and lay users) in specific usage contexts. One of the most influential works in this area is the XAI Question Bank (XAIQB) by Liao et al. The authors propose a set of questions that end-users might ask when using an AI system, which in turn is intended to help developers and designers identify and address explanation needs. Although the XAIQB is widely referenced, there are few reports of its use in practice. In particular, it is unclear to what extent the XAIQB sufficiently captures the explanation needs of end-users and what potential problems exist in the practical application of the XAIQB. To explore these open questions, we used the XAIQB as the basis for analyzing 12 think-aloud software explorations with subject matter experts, i.e., art historians. We investigated the suitability of the XAIQB as a tool for identifying explanation needs in a specific usage context. Our analysis revealed a number of explanation needs that were missing from the question bank, but that emerged repeatedly as our study participants interacted with an AI system. We also found that some of the XAIQB questions were difficult to distinguish and required interpretation during use. Our contribution is an extension of the XAIQB with 11 new questions. In addition, we have expanded the descriptions of all new and existing questions to facilitate their use. We hope that this extension will enable HCI researchers and practitioners to use the XAIQB in practice and may provide a basis for future studies on the identification of explanation needs in different contexts.  © 2023 Owner/Author.
KW  - Explainable AI
KW  - Explanation needs
KW  - Human-AI collaboration
KW  - User study
KW  - User interfaces
KW  - AI systems
KW  - Algorithmics
KW  - End-users
KW  - Explainable AI
KW  - Explanation need
KW  - Human-AI collaboration
KW  - Question banks
KW  - Subject matter experts
KW  - Usage context
KW  - User study
KW  - Artificial intelligence
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Naiseh, M.
AU  - Al-Thani, D.
AU  - Jiang, N.
AU  - Ali, R.
TI  - How the different explanation classes impact trust calibration: The case of clinical decision support systems
PY  - 2023
T2  - International Journal of Human Computer Studies
VL  - 169
C7  - 102941
DO  - 10.1016/j.ijhcs.2022.102941
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139859152&doi=10.1016%2fj.ijhcs.2022.102941&partnerID=40&md5=87e55eeea60ef783e02b78186514493b
AB  - Machine learning has made rapid advances in safety-critical applications, such as traffic control, finance, and healthcare. With the criticality of decisions they support and the potential consequences of following their recommendations, it also became critical to provide users with explanations to interpret machine learning models in general, and black-box models in particular. However, despite the agreement on explainability as a necessity, there is little evidence on how recent advances in eXplainable Artificial Intelligence literature (XAI) can be applied in collaborative decision-making tasks, i.e., human decision-maker and an AI system working together, to contribute to the process of trust calibration effectively. This research conducts an empirical study to evaluate four XAI classes for their impact on trust calibration. We take clinical decision support systems as a case study and adopt a within-subject design followed by semi-structured interviews. We gave participants clinical scenarios and XAI interfaces as a basis for decision-making and rating tasks. Our study involved 41 medical practitioners who use clinical decision support systems frequently. We found that users perceive the contribution of explanations to trust calibration differently according to the XAI class and to whether XAI interface design fits their job constraints and scope. We revealed additional requirements on how explanations shall be instantiated and designed to help a better trust calibration. Finally, we build on our findings and present guidelines for designing XAI interfaces. © 2022
KW  - Clinical decision support systems
KW  - Explainable AI
KW  - Human-AI Interaction
KW  - Trust Calibration
KW  - Decision making
KW  - Decision support systems
KW  - Machine learning
KW  - Safety engineering
KW  - Traffic control
KW  - Black box modelling
KW  - Clinical decision support systems
KW  - Collaborative decision making
KW  - Explainable AI
KW  - Human decisions
KW  - Human-AI interaction
KW  - Machine learning models
KW  - Machine-learning
KW  - Safety critical applications
KW  - Trust calibration
KW  - Calibration
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 65
ER  -

TY  - CONF
AU  - Ellenrieder, S.
AU  - Kallina, E.
AU  - Pumplun, L.
AU  - Gawlitza, J.F.
AU  - Ziegelmayer, S.
AU  - Buxmann, P.
TI  - Promoting Learning Through Explainable Artificial Intelligence: An Experimental Study in Radiology
PY  - 2023
T2  - International Conference on Information Systems, ICIS 2023: "Rising like a Phoenix: Emerging from the Pandemic and Reshaping Human Endeavors with Digital Technologies"
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192570517&partnerID=40&md5=b5a1efccf1a48359852d4f953934d5bf
AB  - The deployment of machine learning (ML)-based decision support systems (DSSs) in high-risk environments such as radiology is increasing. Despite having achieved high decision accuracy, they are prone to errors. Thus, they are primarily used to assist radiologists in their decision making. However, collaborative decision making poses risks to the decision maker, e.g. automation bias and long-term performance degradation. To address these issues, we propose combining findings of the research streams of explainable artificial intelligence and education to promote human learning through interaction with ML-based DSSs. We provided radiologists with explainable vs non-explainable decision support that was high- vs low-performing in a between-subject experimental study to support manual segmentation of 690 brain tumor scans. Our results show that explainable ML-based DSSs improved human learning outcomes and prevented false learning triggered by incorrect decision support. In fact, radiologists were able to learn from errors made by the low-performing explainable ML-based DSS. © 2023 International Conference on Information Systems, ICIS 2023: "Rising like a Phoenix: Emerging from the Pandemic and Reshaping Hu. All Rights Reserved.
KW  - experimental study
KW  - Explainable artificial intelligence
KW  - human learning
KW  - human-AI collaboration
KW  - machine learning
KW  - radiology
KW  - Decision making
KW  - Information systems
KW  - Information use
KW  - Machine learning
KW  - Radiology
KW  - Collaborative decision making
KW  - Decision accuracies
KW  - Decision supports
KW  - Decisions makings
KW  - Experimental study
KW  - Explainable artificial intelligence
KW  - High risk environment
KW  - Human learning
KW  - Human-AI collaboration
KW  - Machine-learning
KW  - Decision support systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Retzlaff, C.O.
AU  - Das, S.
AU  - Wayllace, C.
AU  - Mousavi, P.
AU  - Afshari, M.
AU  - Yang, T.
AU  - Saranti, A.
AU  - Angerschmid, A.
AU  - Taylor, M.E.
AU  - Holzinger, A.
TI  - Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities
PY  - 2024
T2  - Journal of Artificial Intelligence Research
VL  - 79
SP  - 359
EP  - 415
DO  - 10.1613/jair.1.15348
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184656775&doi=10.1613%2fjair.1.15348&partnerID=40&md5=7d43b0336a7075eb678e93ab7d56a7bf
AB  - Artificial intelligence (AI) and especially reinforcement learning (RL) have the potential to enable agents to learn and perform tasks autonomously with superhuman performance. However, we consider RL as fundamentally a Human-in-the-Loop (HITL) paradigm, even when an agent eventually performs its task autonomously. In cases where the reward function is challenging or impossible to define, HITL approaches are considered particularly advantageous. The application of Reinforcement Learning from Human Feedback (RLHF) in systems such as ChatGPT demonstrates the effectiveness of optimizing for user experience and integrating their feedback into the training loop. In HITL RL, human input is integrated during the agent’s learning process, allowing iterative updates and fine-tuning based on human feedback, thus enhancing the agent’s performance. Since the human is an essential part of this process, we argue that human-centric approaches are the key to successful RL, a fact that has not been adequately considered in the existing literature. This paper aims to inform readers about current explainability methods in HITL RL. It also shows how the application of explainable AI (xAI) and specific improvements to existing explainability approaches can enable a better human-agent interaction in HITL RL for all types of users, whether for lay people, domain experts, or machine learning specialists. Accounting for the workflow in HITL RL and based on software and machine learning methodologies, this article identifies four phases for human involvement for creating HITL RL systems: (1) Agent Development, (2) Agent Learning, (3) Agent Evaluation, and (4) Agent Deployment. We highlight human involvement, explanation requirements, new challenges, and goals for each phase. We furthermore identify low-risk, high-return opportunities for explainability research in HITL RL and present long-term research goals to advance the field. Finally, we propose a vision of human-robot collaboration that allows both parties to reach their full potential and cooperate effectively. ©2024 The Authors. Published by AI Access Foundation under Creative Commons Attribution License CC BY 4.0.
KW  - Autonomous agents
KW  - Iterative methods
KW  - Learning systems
KW  - Robots
KW  - Fine tuning
KW  - Human-in-the-loop
KW  - Iterative update
KW  - Learn+
KW  - Learning process
KW  - Machine-learning
KW  - Performance
KW  - Reinforcement learnings
KW  - Reward function
KW  - Users' experiences
KW  - Reinforcement learning
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 42
ER  -

TY  - JOUR
AU  - Linja, A.
AU  - Mamun, T.I.
AU  - Mueller, S.T.
TI  - When Self-Driving Fails: Evaluating Social Media Posts Regarding Problems and Misconceptions about Tesla’s FSD Mode
PY  - 2022
T2  - Multimodal Technologies and Interaction
VL  - 6
IS  - 10
C7  - 86
DO  - 10.3390/mti6100086
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140474124&doi=10.3390%2fmti6100086&partnerID=40&md5=b7eac6332adcd381cb0ea9b12d2c8c3e
AB  - With the recent deployment of the latest generation of Tesla’s Full Self-Driving (FSD) mode, consumers are using semi-autonomous vehicles in both highway and residential driving for the first time. As a result, drivers are facing complex and unanticipated situations with an unproven technology, which is a central challenge for cooperative cognition. One way to support cooperative cognition in such situations is to inform and educate the user about potential limitations. Because these limitations are not always easily discovered, users have turned to the internet and social media to document their experiences, seek answers to questions they have, provide advice on features to others, and assist other drivers with less FSD experience. In this paper, we explore a novel approach to supporting cooperative cognition: Using social media posts can help characterize the limitations of the automation in order to get information about the limitations of the system and explanations and workarounds for how to deal with these limitations. Ultimately, our goal is to determine the kinds of problems being reported via social media that might be useful in helping users anticipate and develop a better mental model of an AI system that they rely on. To do so, we examine a corpus of social media posts about FSD problems to identify (1) the typical problems reported, (2) the kinds of explanations or answers provided by users, and (3) the feasibility of using such user-generated information to provide training and assistance for new drivers. The results reveal a number of limitations of the FSD system (e.g., lane-keeping and phantom braking) that may be anticipated by drivers, enabling them to predict and avoid the problems, thus allowing better mental models of the system and supporting cooperative cognition of the human-AI system in more situations. © 2022 by the authors.
KW  - cooperative cognition
KW  - Explainable AI
KW  - Tesla FSD
KW  - user-centered AI
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 5
ER  -

TY  - CONF
AU  - Banf, M.
AU  - Steinhagen, G.
TI  - Supervising The Supervisor – Model Monitoring In Production Using Deep Feature Embeddings With Applications To Workpiece Inspection
PY  - 2022
T2  - Proceedings of the World Congress on Electrical Engineering and Computer Systems and Science
C7  - MVML 104
DO  - 10.11159/mhci22.104
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141827266&doi=10.11159%2fmhci22.104&partnerID=40&md5=b828c923ce57fcfbf157903a0dc62119
AB  - The automation of condition monitoring and workpiece inspection plays an essential role in maintaining high quality as well as high throughput of the manufacturing process. To this end, the recent rise of developments in machine learning has lead to vast improvements in the area of autonomous process supervision. However, the more complex and powerful these models become, the less transparent and explainable they generally are as well. One of the main challenges is the monitoring of live deployments of these machine learning systems and raising alerts when encountering events that might impact model performance. In particular, supervised classifiers are typically build under the assumption of stationarity in the underlying data distribution. For example, a visual inspection system trained on a set of material surface defects generally does not adapt or even recognize gradual changes in the data distribution-an issue known as "data drift"-such as the emergence of new types of surface defects. This, in turn, may lead to detrimental mispredictions, e.g. samples from new defect classes being classified as non-defective. To this end, it is desirable to provide real-time tracking of a classifier’s performance to inform about the putative onset of additional error classes and the necessity for manual intervention with respect to classifier re-training. Here, we propose an unsupervised framework that acts on top of a supervised classification system, thereby harnessing its internal deep feature representations as a proxy to track changes in the data distribution during deployment and, hence, to anticipate classifier performance degradation. © 2022, Avestia Publishing. All rights reserved.
KW  - condition monitoring
KW  - data drift detection
KW  - deep feature learning
KW  - explainable artificial intelligence
KW  - model performance monitoring
KW  - transfer learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Morris, J.
AU  - Liu, Z.
AU  - Liang, H.
AU  - Nagala, S.
AU  - Hong, X.
TI  - ThyExp: An explainable AI-assisted Decision Making Toolkit for Thyroid Nodule Diagnosis based on Ultra-sound Images
PY  - 2023
T2  - International Conference on Information and Knowledge Management, Proceedings
SP  - 5371
EP  - 5375
DO  - 10.1145/3583780.3615131
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178133932&doi=10.1145%2f3583780.3615131&partnerID=40&md5=a98a65bf67321db23c092309eecefc1b
AB  - Radiologists have an important task of diagnosing thyroid nodules present in ultra sound images. Although reporting systems exist to aid in the diagnosis process, these systems do not provide explanations about the diagnosis results. We present ThyExp - a web based toolkit for it use by medical professionals, allowing for accurate diagnosis with explanations of thyroid nodules present in ultrasound images utilising artificial intelligence models. The proposed web-based toolkit can be easily incorporated into current medical workflows, and allows medical professionals to have the confidence of a highly accurate machine learning model with explanations to provide supplementary diagnosis data. The solution provides classification results with their probability accuracy, as well as the explanations in the form of presenting the key features or characteristics that contribute to the classification results. The experiments conducted on a real-world UK NHS hospital patient dataset demonstrate the effectiveness of the proposed approach. This toolkit can improve the trust of medical professional to understand the confidence of the model in its predictions. This toolkit can improve the trust of medical professionals in understanding the models reasoning behind its predictions. © 2023 Copyright held by the owner/author(s).
KW  - Artificial Intelligence Assisted disease diagnosis
KW  - Thyroid Nodule
KW  - Artificial intelligence
KW  - Computer aided diagnosis
KW  - Decision making
KW  - Medical imaging
KW  - Artificial intelligence assisted disease diagnose
KW  - Classification results
KW  - Decisions makings
KW  - Disease diagnosis
KW  - Medical professionals
KW  - Reporting systems
KW  - Sound image
KW  - Thyroid nodule
KW  - Ultrasound images
KW  - Web based
KW  - Websites
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Ding, Y.
AU  - Jia, L.
AU  - Du, N.
TI  - Designing for Trust and Situational Awareness in Automated Vehicles: Effects of Information Type and Error Type
PY  - 2023
T2  - Proceedings of the Human Factors and Ergonomics Society
VL  - 67
IS  - 1
SP  - 1176
EP  - 1177
DO  - 10.1177/21695067231192406
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191010449&doi=10.1177%2f21695067231192406&partnerID=40&md5=e084cecdc3d7a406bb25e90332a3875e
AB  - Trust and situational awareness (SA) are crucial to the adoption and safety of automated vehicles (AVs). Appropriate design of AV explanations could promote drivers’ acceptance, trust, and SA, enabling drivers to get more benefits from the technology. This study investigated the effects of error type and information type of AV explanations on drivers’ trust and SA. We recruited 300 participants for an online video study with a 3 (information type) × 2 (error type) mixed design. Linear mixed model analyses showed that compared with false alarms, misses were associated with more trust decrease after the error and more trust decrease after the post-error recovery. Compared with why information, how information was associated with lower SA generally and risked potential over-trust in false alarms. Therefore, we recommend deploying AV decision systems that are less miss-prone and including why information in AV explanations. © 2023 Human Factors and Ergonomics Society.
KW  - Automated Vehicles
KW  - Explainable Artificial Intelligence
KW  - Human Factors
KW  - Human-Machine Interface
KW  - Situational Awareness
KW  - Trust
KW  - Automation
KW  - Human engineering
KW  - Vehicles
KW  - Appropriate designs
KW  - Automated vehicles
KW  - Error types
KW  - Explainable artificial intelligence
KW  - Falsealarms
KW  - Human Machine Interface
KW  - Information types
KW  - Situational awareness
KW  - Trust
KW  - Trust-awareness
KW  - Errors
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Manca, G.
AU  - Bhattacharya, N.
AU  - MacZey, S.
AU  - Ziobro, D.
AU  - Brorsson, E.
AU  - Bång, M.
TI  - XAIProcessLens: A Counterfactual-Based Dashboard for Explainable AI in Process Industries
PY  - 2023
T2  - Frontiers in Artificial Intelligence and Applications
VL  - 368
SP  - 401
EP  - 403
DO  - 10.3233/FAIA230110
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171476418&doi=10.3233%2fFAIA230110&partnerID=40&md5=bbe7f575c5be745013383e081f0ba748
AB  - We present a novel counterfactual-based dashboard for explainable artificial intelligence (XAI) in process industries, aimed at enhancing the understanding and adoption of machine learning (ML) models by providing transparency, explainability, and performance evaluation. Our dashboard comprises two modules: a statistical analysis module for data visualization and model performance assessment, and an XAI module for exploring counterfactual explanations at varying levels of abstraction. Through a case study of an industrial batch process, we demonstrate the dashboard's applicability and potential to increase trust in ML models among stakeholders, paving the way for confident deployment in process industries. © 2023 The Authors.
KW  - Counterfactual explanations
KW  - dashboard
KW  - explainable AI
KW  - human-in-the-loop
KW  - machine learning
KW  - process industries
KW  - Batch data processing
KW  - Data visualization
KW  - Counterfactual explanation
KW  - Counterfactuals
KW  - Dashboard
KW  - Explainable AI
KW  - Human-in-the-loop
KW  - In-process
KW  - Machine learning models
KW  - Machine-learning
KW  - Performances evaluation
KW  - Process industries
KW  - Machine learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Berretta, S.
AU  - Tausch, A.
AU  - Ontrup, G.
AU  - Gilles, B.
AU  - Peifer, C.
AU  - Kluge, A.
TI  - Defining human-AI teaming the human-centered way: a scoping review and network analysis
PY  - 2023
T2  - Frontiers in Artificial Intelligence
VL  - 6
C7  - 1250725
DO  - 10.3389/frai.2023.1250725
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174188953&doi=10.3389%2ffrai.2023.1250725&partnerID=40&md5=c68c9632825b83d85fb4119b5f789226
AB  - Introduction: With the advancement of technology and the increasing utilization of AI, the nature of human work is evolving, requiring individuals to collaborate not only with other humans but also with AI technologies to accomplish complex goals. This requires a shift in perspective from technology-driven questions to a human-centered research and design agenda putting people and evolving teams in the center of attention. A socio-technical approach is needed to view AI as more than just a technological tool, but as a team member, leading to the emergence of human-AI teaming (HAIT). In this new form of work, humans and AI synergistically combine their respective capabilities to accomplish shared goals. Methods: The aim of our work is to uncover current research streams on HAIT and derive a unified understanding of the construct through a bibliometric network analysis, a scoping review and synthetization of a definition from a socio-technical point of view. In addition, antecedents and outcomes examined in the literature are extracted to guide future research in this field. Results: Through network analysis, five clusters with different research focuses on HAIT were identified. These clusters revolve around (1) human and (2) task-dependent variables, (3) AI explainability, (4) AI-driven robotic systems, and (5) the effects of AI performance on human perception. Despite these diverse research focuses, the current body of literature is predominantly driven by a technology-centric and engineering perspective, with no consistent definition or terminology of HAIT emerging to date. Discussion: We propose a unifying definition combining a human-centered and team-oriented perspective as well as summarize what is still needed in future research regarding HAIT. Thus, this work contributes to support the idea of the Frontiers Research Topic of a theoretical and conceptual basis for human work with AI systems. Copyright © 2023 Berretta, Tausch, Ontrup, Gilles, Peifer and Kluge.
KW  - artificial intelligence
KW  - bibliometric analysis
KW  - bibliometric coupling
KW  - human-AI teaming
KW  - human-centered AI
KW  - humane work
KW  - network analysis
KW  - work psychology
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 24
ER  -

TY  - JOUR
AU  - Krinkin, K.
AU  - Shichkina, Y.
AU  - Ignatyev, A.
TI  - Co-evolutionary hybrid intelligence is a key concept for the world intellectualization
PY  - 2023
T2  - Kybernetes
VL  - 52
IS  - 9
SP  - 2907
EP  - 2923
DO  - 10.1108/K-03-2022-0472
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139970979&doi=10.1108%2fK-03-2022-0472&partnerID=40&md5=6e177ab56f4f344fb19961df94488f91
AB  - Purpose: This study aims to show the inconsistency of the approach to the development of artificial intelligence as an independent tool (just one more tool that humans have developed); to describe the logic and concept of intelligence development regardless of its substrate: a human or a machine and to prove that the co-evolutionary hybridization of the machine and human intelligence will make it possible to reach a solution for the problems inaccessible to humanity so far (global climate monitoring and control, pandemics, etc.). Design/methodology/approach: The global trend for artificial intelligence development (has been) was set during the Dartmouth seminar in 1956. The main goal was to define characteristics and research directions for artificial intelligence comparable to or even outperforming human intelligence. It should be able to acquire and create new knowledge in a highly uncertain dynamic environment (the real-world environment is an example) and apply that knowledge to solving practical problems. Nowadays artificial intelligence overperforms human abilities (playing games, speech recognition, search, art generation, extracting patterns from data etc.), but all these examples show that developers have come to a dead end. Narrow artificial intelligence has no connection to real human intelligence and even cannot be successfully used in many cases due to lack of transparency, explainability, computational ineffectiveness and many other limits. A strong artificial intelligence development model can be discussed unrelated to the substrate development of intelligence and its general properties that are inherent in this development. Only then it is to be clarified which part of cognitive functions can be transferred to an artificial medium. The process of development of intelligence (as mutual development (co-development) of human and artificial intelligence) should correspond to the property of increasing cognitive interoperability. The degree of cognitive interoperability is arranged in the same way as the method of measuring the strength of intelligence. It is stronger if knowledge can be transferred between different domains on a higher level of abstraction (Chollet, 2018). Findings: The key factors behind the development of hybrid intelligence are interoperability – the ability to create a common ontology in the context of the problem being solved, plan and carry out joint activities; co-evolution – ensuring the growth of aggregate intellectual ability without the loss of subjectness by each of the substrates (human, machine). The rate of co-evolution depends on the rate of knowledge interchange and the manufacturability of this process. Research limitations/implications: Resistance to the idea of developing co-evolutionary hybrid intelligence can be expected from agents and developers who have bet on and invested in data-driven artificial intelligence and machine learning. Practical implications: Revision of the approach to intellectualization through the development of hybrid intelligence methods will help bridge the gap between the developers of specific solutions and those who apply them. Co-evolution of machine intelligence and human intelligence will ensure seamless integration of smart new solutions into the global division of labor and social institutions. Originality/value: The novelty of the research is connected with a new look at the principles of the development of machine and human intelligence in the co-evolution style. Also new is the statement that the development of intelligence should take place within the framework of integration of the following four domains: global challenges and tasks, concepts (general hybrid intelligence), technologies and products (specific applications that satisfy the needs of the market). © 2022, Emerald Publishing Limited.
KW  - AI ethics
KW  - Artificial intelligence
KW  - Co-evolution
KW  - Cognitive functions
KW  - Division of labor
KW  - Human–machine hybridization
KW  - Hybrid intelligence
KW  - Artificial intelligence
KW  - Brain
KW  - Cognitive systems
KW  - Computation theory
KW  - Interoperability
KW  - Speech recognition
KW  - AI ethic
KW  - Co-evolution
KW  - Co-evolutionary
KW  - Cognitive functions
KW  - Division of labor
KW  - Human intelligence
KW  - Human-machine
KW  - Human–machine hybridization
KW  - Hybrid intelligence
KW  - Hybridisation
KW  - Substrates
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Turner, C.J.
AU  - Garn, W.
TI  - Next generation DES simulation: A research agenda for human centric manufacturing systems
PY  - 2022
T2  - Journal of Industrial Information Integration
VL  - 28
C7  - 100354
DO  - 10.1016/j.jii.2022.100354
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134201179&doi=10.1016%2fj.jii.2022.100354&partnerID=40&md5=6e8eab20ae2d49b51bfac2bb3a9ee9a1
AB  - In this paper we introduce a research agenda to guide the development of the next generation of Discrete Event Simulation (DES) systems. Interfaces to digital twins are projected to go beyond physical representations to become blueprints for the actual “objects” and an active dashboard for their control. The role and importance of real-time interactive animations presented in an Extended Reality (XR) format will be explored. The need for using game engines, particularly their physics engines and AI within interactive simulated Extended Reality is expanded on. Importing and scanning real-world environments is assumed to become more efficient when using AR. Exporting to VR and AR is recommended to be a default feature. A technology framework for the next generation simulators is presented along with a proposed set of implementation guidelines. The need for more human centric technology approaches, nascent in Industry 4.0, are now central to the emerging Industry 5.0 paradigm; an agenda that is discussed in this research as part of a human in the loop future, supported by DES. The potential role of Explainable Artificial Intelligence is also explored along with an audit trail approach to provide a justification of complex and automated decision-making systems with relation to DES. A technology framework is proposed, which brings the above together and can serve as a guide for the next generation of holistic simulators for manufacturing. © 2022
KW  - Agent based simulation
KW  - Discrete event simulation (DES)
KW  - Explainable artificial intelligence (XAI)
KW  - Extended reality (XR)
KW  - Human centric manufacturing
KW  - Human in the loop
KW  - Industry 4.0
KW  - Industry 5.0
KW  - Animation
KW  - Artificial intelligence
KW  - Decision making
KW  - Discrete event simulation
KW  - Industrial research
KW  - Agent based simulation
KW  - Discrete event simulation
KW  - Discrete-event simulations
KW  - Explainable artificial intelligence (XAI)
KW  - Extended reality (XR)
KW  - Human centric manufacturing
KW  - Human-centric
KW  - Human-in-the-loop
KW  - Industry 5.0
KW  - Research agenda
KW  - Industry 4.0
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 49
ER  -

TY  - JOUR
AU  - Cabour, G.
AU  - Morales-Forero, A.
AU  - Ledoux, É.
AU  - Bassetto, S.
TI  - An explanation space to align user studies with the technical development of Explainable AI
PY  - 2023
T2  - AI and Society
VL  - 38
IS  - 2
SP  - 869
EP  - 887
DO  - 10.1007/s00146-022-01536-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134830553&doi=10.1007%2fs00146-022-01536-6&partnerID=40&md5=4fd352ddc41443ef17eccf3f893f65aa
AB  - Providing meaningful and actionable explanations for end-users is a situated problem requiring the intersection of multiple disciplines to address social, operational, and technical challenges. However, the explainable artificial intelligence community has not commonly adopted or created tangible design tools that allow interdisciplinary work to develop reliable AI-powered solutions. This paper proposes a formative architecture that defines the explanation space from a user-inspired perspective. The architecture comprises five intertwined components to outline explanation requirements for a task: (1) the end-users’ mental models, (2) the end-users’ cognitive process, (3) the user interface, (4) the Human-Explainer Agent, and (5) the agent process. We first define each component of the architecture. Then, we present the Abstracted Explanation Space, a modeling tool that aggregates the architecture’s components to support designers in systematically aligning explanations with end-users’ work practices, needs, and goals. It guides the specifications of what needs to be explained (content: end-users’ mental model), why this explanation is necessary (context: end-users’ cognitive process), to delimit how to explain it (format: Human-Explainer Agent and user interface), and when the explanations should be given. We then exemplify the tool’s use in an ongoing case study in the aircraft maintenance domain. Finally, we discuss possible contributions of the tool, known limitations or areas for improvement, and future work to be done. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.
KW  - Explainable AI
KW  - Human–AI teaming
KW  - Interdisciplinary study
KW  - User-centered design
KW  - Architecture
KW  - Cognitive systems
KW  - Software design
KW  - User interfaces
KW  - Cognitive process
KW  - End-users
KW  - Explainable AI
KW  - Human–AI teaming
KW  - Inter-disciplinary studies
KW  - Mental model
KW  - Multiple disciplines
KW  - Social challenges
KW  - Technical development
KW  - User study
KW  - User centered design
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Wilchek, M.
AU  - Hanley, W.
AU  - Lim, J.
AU  - Luther, K.
AU  - Batarseh, F.A.
TI  - Human-in-the-loop for computer vision assurance: A survey
PY  - 2023
T2  - Engineering Applications of Artificial Intelligence
VL  - 123
C7  - 106376
DO  - 10.1016/j.engappai.2023.106376
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159075824&doi=10.1016%2fj.engappai.2023.106376&partnerID=40&md5=eacdedfc2fced6869d3efeece7ecb080
AB  - Human-in-the-loop (HITL), a key branch of Human–Computer Interaction (HCI), is increasingly proposed in the research literature as a key assurance method for automated analyses and predictive application designs. As the need increases to improve methods in Artificial Intelligence (AI) model training, optimize systems performance, provide AI explainability, and monitor AI system operations, the concept of HITL is gaining traction due to its value in solving these challenges. This survey of existing works on HITL from a computer vision system design perspective focuses on the following AI assurance principles: (1) improved data assurance, such as data preparation or automated data labeling; (2) algorithmic assurance, such as managing uncertainty and AI trustworthiness; and (3) critical limitations and capabilities introduced by HITL into a system's operational efficiency. We survey prior work within these foci, including technical strengths and weaknesses of novel approaches and ongoing research. This review of the state of the art in HITL computer vision research supports an informed discussion of considerations and future opportunities in this critical space. © 2023 Elsevier Ltd
KW  - AI assurance
KW  - Computer vision
KW  - Human-in-the-loop
KW  - Human–computer interaction
KW  - Learning systems
KW  - Object detection
KW  - Computer vision
KW  - Human computer interaction
KW  - Learning systems
KW  - Application design
KW  - Artificial intelligence assurance
KW  - Artificial intelligence systems
KW  - Automated analysis
KW  - Human-in-the-loop
KW  - Intelligence models
KW  - Model training
KW  - Objects detection
KW  - Systems operation
KW  - Systems performance
KW  - Object detection
M3  - Short survey
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 10
ER  -

TY  - CONF
AU  - Patil, M.S.
AU  - Främling, K.
TI  - Do Intermediate Feature Coalitions Aid Explainability of Black-Box Models?
PY  - 2023
T2  - Communications in Computer and Information Science
VL  - 1901 CCIS
SP  - 115
EP  - 130
DO  - 10.1007/978-3-031-44064-9_7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176954534&doi=10.1007%2f978-3-031-44064-9_7&partnerID=40&md5=ccc61257ca776322622814d0f2589bde
AB  - This work introduces the notion of intermediate concepts based on levels structure to aid explainability for black-box models. The levels structure is a hierarchical structure in which each level corresponds to features of a dataset (i.e., a player-set partition). The level of coarseness increases from the trivial set, which only comprises singletons, to the set, which only contains the grand coalition. In addition, it is possible to establish meronomies, i.e., part-whole relationships, via a domain expert that can be utilised to generate explanations at an abstract level. We illustrate the usability of this approach in a real-world car model example and the Titanic dataset, where intermediate concepts aid in explainability at different levels of abstraction. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
KW  - Coalition Formation
KW  - Explainability
KW  - Trust in Human-Agent Systems
KW  - Agent systems
KW  - Black box modelling
KW  - Coalition formations
KW  - Concept-based
KW  - Explainability
KW  - Hierarchical structures
KW  - Human agent
KW  - Intermediate concept
KW  - Level structure
KW  - Trust in human-agent system
KW  - Model automobiles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CHAP
AU  - Sikdar, S.
AU  - Bhattacharya, P.
TI  - Interpretability of Deep Neural Models
PY  - 2023
T2  - Studies in Computational Intelligence
VL  - 1123
SP  - 131
EP  - 143
DO  - 10.1007/978-981-99-7184-8_8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182477687&doi=10.1007%2f978-981-99-7184-8_8&partnerID=40&md5=ce4f8759ea81e1b14dc0b68497bbd14e
AB  - The rise of deep neural networks in machine learning has been remarkable, leading to their deployment in algorithmic decision-making. However, this has raised questions about the explainability and interpretability of these models, given their growing importance in society. To address this, the field of interpretability in machine learning has been developed, with the goal of creating frameworks that can explain the decisions of a machine learning model in a way that is comprehensible to humans. This could be essential in building trust in the system, as well as debugging models for potential errors and meeting legal requirements (e.g., GDPR). Even though the success of deep neural network is attributed to its ability to capture higher level feature interactions, most of existing frameworks still focus on highlighting important individual features (e.g., words in text or pixels in images). Hence, to further improve interpretability, we propose to quantify the importance of feature interactions in addition to individual features. In this work, we introduce integrated directional gradients (IDG), a game-theory inspired method for assigning importance scores to higher level feature interactions. Our experiments with DNN-based text classifiers on the task of sentiment classification demonstrate that IDG is able to effectively capture the importance of feature interactions. © The Institution of Engineers (India) 2023.
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Veale, M.
AU  - Silberman, M.‘.
AU  - Binns, R.
TI  - Fortifying the algorithmic management provisions in the proposed Platform Work Directive
PY  - 2023
T2  - European Labour Law Journal
VL  - 14
IS  - 2
SP  - 308
EP  - 332
DO  - 10.1177/20319525231167983
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175214916&doi=10.1177%2f20319525231167983&partnerID=40&md5=6021eea71f98f4235fbcb05f3c226ff2
AB  - The European Commission proposed a Directive on Platform Work at the end of 2021. While much attention has been placed on its effort to address misclassification of the employed as self-employed, it also contains ambitious provisions for the regulation of the algorithmic management prevalent on these platforms. Overall, these provisions are well-drafted, yet they require extra scrutiny in light of the fierce lobbying and resistance they will likely encounter in the legislative process, in implementation and in enforcement. In this article, we place the proposal in its sociotechnical context, drawing upon wide cross-disciplinary scholarship to identify a range of tensions, potential misinterpretations, and perversions that should be pre-empted and guarded against at the earliest possible stage. These include improvements to ex ante and ex post algorithmic transparency; identifying and strengthening the standard against which human reviewers of algorithmic decisions review; anticipating challenges of representation and organising in complex platform contexts; creating realistic ambitions for digital worker communication channels; and accountably monitoring and evaluating impacts on workers while limiting data collection. We encourage legislators and regulators at both European and national levels to act to fortify these provisions in the negotiation of the Directive, its potential transposition, and in its enforcement. © The Author(s) 2023.
KW  - algorithmic decisions
KW  - algorithmic explanations
KW  - algorithmic management
KW  - platform work
KW  - workplace surveillance
M3  - Note
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 6
ER  -

TY  - CONF
AU  - Natali, C.
AU  - Famiglini, L.
AU  - Campagner, A.
AU  - La Maida, G.A.
AU  - Gallazzi, E.
AU  - Cabitza, F.
TI  - Color Shadows 2: Assessing the Impact of XAI on Diagnostic Decision-Making
PY  - 2023
T2  - Communications in Computer and Information Science
VL  - 1901 CCIS
SP  - 618
EP  - 629
DO  - 10.1007/978-3-031-44064-9_33
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176926415&doi=10.1007%2f978-3-031-44064-9_33&partnerID=40&md5=98420ae9dd7a9054f6fe45351a164f80
AB  - A comprehensive assessment of the impact of eXplainable AI (XAI) on diagnostic decision-making should adopt a socio-technical perspective. Our study focuses on Decision Support Systems (DSS) that provide explanations in the form of Activation Maps, assessing their impact in terms of automation bias and algorithmic aversion. Specifically, we focus on the XAI-assisted task of detecting thoraco-lumbar fractures from X-rays by radiologists, taking into account the complexity of the cases and the experience level of users. Our results show how XAI support has a clear and positive impact on diagnostic performance. By introducing the concepts of technology impact, reliance patterns, and the white box paradox, we highlight the importance of designing Human-AI Collaboration Protocols (HAI-CP) that are specific to the task at hand to optimize the integration of XAI into diagnostic decision-making. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
KW  - Decision Support Systems (DSS)
KW  - eXplainable AI (XAI)
KW  - Human-AI Collaboration Protocol (HAI-CP)
KW  - Artificial intelligence
KW  - Decision making
KW  - Activation maps
KW  - Algorithmics
KW  - Automation bias
KW  - Collaboration protocols
KW  - Comprehensive assessment
KW  - Decision support system
KW  - Diagnostic decision makings
KW  - Explainable AI (XAI)
KW  - Human-AI collaboration protocol
KW  - Socio-technical perspective
KW  - Decision support systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Liang, P.P.
AU  - Lyu, Y.
AU  - Chhablani, G.
AU  - Jain, N.
AU  - Deng, Z.
AU  - Wang, X.
AU  - Morency, L.-P.
AU  - Salakhutdinov, R.
TI  - MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models
PY  - 2023
T2  - Conference on Human Factors in Computing Systems - Proceedings
C7  - 214
DO  - 10.1145/3544549.3585604
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158076483&doi=10.1145%2f3544549.3585604&partnerID=40&md5=55392ca22e88f5084b899e0779c0b185
AB  - The nature of human and computer interactions are inherently multimodal, which has led to substantial interest in building interpretable, interactive, and reliable multimodal interfaces. However, modern multimodal models and interfaces are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize their internal workings in order to empower stakeholders to visualize model behavior, perform model debugging, and promote trust in these models? Our paper proposes MultiViz, a method for analyzing the behavior of multimodal models via 4 stages: (1) unimodal importance, (2) cross-modal interactions, (3) multimodal representations and (4) multimodal prediction. MultiViz includes modular visualization tools for each stage before combining outputs from all stages through an interactive and human-in-the-loop API. Through user studies with 21 participants on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available at https://github.com/pliang279/MultiViz, will be regularly updated with new visualization tools and metrics, and welcomes input from the community1. © 2023 Owner/Author.
KW  - explainable AI
KW  - human-in-the-loop
KW  - interpretability
KW  - model analysis and debugging
KW  - multimodal machine learning
KW  - visualization
KW  - Error analysis
KW  - Learning systems
KW  - Machine learning
KW  - Neural networks
KW  - Program debugging
KW  - User interfaces
KW  - Explainable AI
KW  - Human-in-the-loop
KW  - Interpretability
KW  - Machine-learning
KW  - Model analyse and debugging
KW  - Modeling analyzes
KW  - Multi-modal
KW  - Multi-modal interfaces
KW  - Multimodal machine learning
KW  - Multimodal models
KW  - Visualization
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Ehsan, U.
AU  - Wintersberger, P.
AU  - Watkins, E.A.
AU  - Manger, C.
AU  - Ramos, G.
AU  - Weisz, J.D.
AU  - Daumé, H.
AU  - Riener, A.
AU  - Riedl, M.O.
TI  - Human-Centered Explainable AI (HCXAI): Coming of Age
PY  - 2023
T2  - Conference on Human Factors in Computing Systems - Proceedings
C7  - 353
DO  - 10.1145/3544549.3573832
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158123516&doi=10.1145%2f3544549.3573832&partnerID=40&md5=f45781f736c33aca8cdccddc3e641b64
AB  - Explainability is an essential pillar of Responsible AI that calls for equitable and ethical Human-AI interaction. Explanations are essential to hold AI systems and their producers accountable, and can serve as a means to ensure humans' right to understand and contest AI decisions. Human-centered XAI (HCXAI) argues that there is more to making AI explainable than algorithmic transparency. Explainability of AI is more than just "opening"the black box - who opens it matters just as much, if not more, as the ways of opening it. In this third CHI workshop on Human-centered XAI (HCXAI), we build on the maturation through the first two installments to craft the coming-of-age story of HCXAI, which embodies a deeper discourse around operationalizing human-centered perspectives in XAI. We aim towards actionable interventions that recognize both affordances and potential pitfalls of XAI. The goal of the third installment is to go beyond the black box and examine how human-centered perspectives in XAI can be operationalized at the conceptual, methodological, and technical levels. Encouraging holistic (historical, sociological, and technical) approaches, we emphasize "operationalizing."Within our research agenda for XAI, we seek actionable analysis frameworks, concrete design guidelines, transferable evaluation methods, and principles for accountability. © 2023 Owner/Author.
KW  - Affordances
KW  - AI systems
KW  - Algorithmics
KW  - Analysis frameworks
KW  - Black boxes
KW  - Conceptual levels
KW  - Concrete design
KW  - Human rights
KW  - Research agenda
KW  - Technical levels
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 14
ER  -

TY  - JOUR
AU  - Estivill-Castro, V.
AU  - Gilmore, E.
AU  - Hexel, R.
TI  - Constructing Explainable Classifiers from the Start—Enabling Human-in-the Loop Machine Learning
PY  - 2022
T2  - Information (Switzerland)
VL  - 13
IS  - 10
C7  - 464
DO  - 10.3390/info13100464
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140467816&doi=10.3390%2finfo13100464&partnerID=40&md5=22c05ee2e98766db109295f11c54f70a
AB  - Interactive machine learning (IML) enables the incorporation of human expertise because the human participates in the construction of the learned model. Moreover, with human-in-the-loop machine learning (HITL-ML), the human experts drive the learning, and they can steer the learning objective not only for accuracy but perhaps for characterisation and discrimination rules, where separating one class from others is the primary objective. Moreover, this interaction enables humans to explore and gain insights into the dataset as well as validate the learned models. Validation requires transparency and interpretable classifiers. The huge relevance of understandable classification has been recently emphasised for many applications under the banner of explainable artificial intelligence (XAI). We use parallel coordinates to deploy an IML system that enables the visualisation of decision tree classifiers but also the generation of interpretable splits beyond parallel axis splits. Moreover, we show that characterisation and discrimination rules are also well communicated using parallel coordinates. In particular, we report results from the largest usability study of a IML system, confirming the merits of our approach. © 2022 by the authors.
KW  - decision tree classifiers
KW  - interactive machine learning
KW  - parallel coordinates
KW  - transparent-by-design
KW  - Decision trees
KW  - Decision tree classifiers
KW  - Human expert
KW  - Human expertise
KW  - Human-in-the-loop
KW  - Interactive machine learning
KW  - Learning objectives
KW  - Machine learning systems
KW  - Machine-learning
KW  - Parallel coordinates
KW  - Transparent-by-design
KW  - Machine learning
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Friedrich, M.
AU  - Richards, D.
AU  - Huttner, J.-P.
TI  - Evaluation of Icons to Support Safety Risk Monitoring of Autonomous Small Unmanned Aircraft Systems
PY  - 2024
T2  - 2024 IEEE 4th International Conference on Human-Machine Systems, ICHMS 2024
DO  - 10.1109/ICHMS59971.2024.10555752
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197435232&doi=10.1109%2fICHMS59971.2024.10555752&partnerID=40&md5=6fd49eeee7a42ff06d5dc132bc073410
AB  - The integration of small Unmanned Aircraft Systems (sUAS) into low-altitude urban airspace is gaining momentum. Safety challenges arise, necessitating automated (and in some cases autonomous) technical solutions. In order for the operator to monitor and engage with the sUAS an effective human-machine interface (HMI) is a vital component. Equally, the manner in how information is presented on this HMI requires careful consideration - more so when the operator may very well have more than one aerial platform under their control and/or supervision. This study explores the role of icon design for autonomous sUAS supervisory control to contribute to the growing body of research on explainable artificial intelligence. Altogether, 14 icons are proposed as representations of safety-critical functions related to autonomous sUAS operation in low-altitude urban airspace. In an online questionnaire study, 46 participants with experience in operating sUAS rated the icons on established icon-function fit metrics. The analysis of agreement scores indicates that the icons related to battery health, geofence conformance, and meteorological constraints were well-recognized, while those representing casualty risk, positional accuracy, airspace conformance, and sensor health performed poorly. These findings emphasize the importance of concreteness, familiarity, and semantic distance in icon design, where higher values positively influence icon recognition and thus icon-function fit. The integration of empirically derived icon design principles is proposed to enhance transparency in safety-critical autonomous systems. This study underscores the significance of targeted usage of unambiguous icons to facilitate deeper user understanding through making system-side decision-making processes transparent to the user, enabling more effective interaction between humans and autonomous systems.  © 2024 IEEE.
KW  - autonomous systems
KW  - human machine interface
KW  - icon design
KW  - unmanned aircraft systems
KW  - Decision making
KW  - Health risks
KW  - Integration
KW  - Man machine systems
KW  - Risk assessment
KW  - Safety engineering
KW  - Semantics
KW  - Unmanned aerial vehicles (UAV)
KW  - Autonomous system
KW  - Gaining momentum
KW  - Human Machine Interface
KW  - Icon designs
KW  - Low altitudes
KW  - Risk monitoring
KW  - Safety risks
KW  - Small unmanned aircrafts
KW  - Technical solutions
KW  - Unmanned aircraft system
KW  - Antennas
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Prabhudesai, S.
AU  - Yang, L.
AU  - Asthana, S.
AU  - Huan, X.
AU  - Vera Liao, Q.
AU  - Banovic, N.
TI  - Understanding Uncertainty: How Lay Decision-makers Perceive and Interpret Uncertainty in Human-AI Decision Making
PY  - 2023
T2  - International Conference on Intelligent User Interfaces, Proceedings IUI
SP  - 379
EP  - 396
DO  - 10.1145/3581641.3584033
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152132042&doi=10.1145%2f3581641.3584033&partnerID=40&md5=9574365dc41056e90595baab33e766f9
AB  - Decision Support Systems (DSS) based on Machine Learning (ML) often aim to assist lay decision-makers, who are not math-savvy, in making high-stakes decisions. However, existing ML-based DSS are not always transparent about the probabilistic nature of ML predictions and how uncertain each prediction is. This lack of transparency could give lay decision-makers a false sense of reliability. Growing calls for AI transparency have led to increasing efforts to quantify and communicate model uncertainty. However, there are still gaps in knowledge regarding how and why the decision-makers utilize ML uncertainty information in their decision process. Here, we conducted a qualitative, think-aloud user study with 17 lay decision-makers who interacted with three different DSS: 1) interactive visualization, 2) DSS based on an ML model that provides predictions without uncertainty information, and 3) the same DSS with uncertainty information. Our qualitative analysis found that communicating uncertainty about ML predictions forced participants to slow down and think analytically about their decisions. This in turn made participants more vigilant, resulting in reduction in over-reliance on ML-based DSS. Our work contributes empirical knowledge on how lay decision-makers perceive, interpret, and make use of uncertainty information when interacting with DSS. Such foundational knowledge informs the design of future ML-based DSS that embrace transparent uncertainty communication.  © 2023 ACM.
KW  - Decision-making
KW  - Machine Learning
KW  - Uncertainty
KW  - Decision support systems
KW  - Forecasting
KW  - Machine learning
KW  - Transparency
KW  - Uncertainty analysis
KW  - Visualization
KW  - Decision makers
KW  - Decision process
KW  - Decisions makings
KW  - Machine-learning
KW  - Modeling uncertainties
KW  - On-machines
KW  - Probabilistics
KW  - Think aloud
KW  - Uncertainty
KW  - Uncertainty informations
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 35
ER  -

TY  - CONF
AU  - Nauta, M.
AU  - Hegeman, J.H.
AU  - Geerdink, J.
AU  - Schlötterer, J.
AU  - Keulen, M.
AU  - Seifert, C.
TI  - Interpreting and Correcting Medical Image Classification with PIP-Net
PY  - 2024
T2  - Communications in Computer and Information Science
VL  - 1947
SP  - 198
EP  - 215
DO  - 10.1007/978-3-031-50396-2_11
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184120156&doi=10.1007%2f978-3-031-50396-2_11&partnerID=40&md5=79fd00affdfcec892203436d6ac5a122
AB  - Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net’s decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net’s unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
KW  - Explainable AI
KW  - hybrid intelligence
KW  - interpretable machine learning
KW  - medical imaging
KW  - prototypes
KW  - Computer aided diagnosis
KW  - Decision making
KW  - Image classification
KW  - Medical applications
KW  - Medical imaging
KW  - Black boxes
KW  - Explainable AI
KW  - Hybrid intelligence
KW  - Image Classifiers
KW  - Interpretability
KW  - Interpretable machine learning
KW  - Machine-learning
KW  - Medical image classification
KW  - Prototype
KW  - Prototype models
KW  - Machine learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 9
ER  -

TY  - CHAP
AU  - Plass, M.
AU  - Kargl, M.
AU  - Evans, T.
AU  - Brcic, L.
AU  - Regitnig, P.
AU  - Geißler, C.
AU  - Carvalho, R.
AU  - Jansen, C.
AU  - Zerbe, N.
AU  - Holzinger, A.
AU  - Müller, H.
TI  - Human-AI Interfaces are a Central Component of Trustworthy AI
PY  - 2023
T2  - Intelligent Systems Reference Library
VL  - 232
SP  - 225
EP  - 256
DO  - 10.1007/978-3-031-12807-3_11
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140745775&doi=10.1007%2f978-3-031-12807-3_11&partnerID=40&md5=f3f15e3bd1c9c24e20420cd2658abb37
AB  - This chapter demonstrates the crucial role that human-AI interfaces play in conveying the trustworthiness of AI solutions to their users. Explainability is a central component of such interfaces, particularly in high-stake domains where human oversight is essential: justice, finance, security, and medicine. To successfully build and communicate trustworthiness, a user-centered approach to the design and development of AI solutions and their human interfaces is essential. In this chapter, we explain how proven methods for stakeholder analysis and user testing from human-computer interaction (HCI) research can be adapted to human-AI interaction (HAII) in support of this goal. The practical implementation of a user-centric approach is described within the context of AI applications in computational pathology. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Anderegg, A.H.A.
AU  - Balakrishnan, K.
AU  - Mulcare, S.P.
TI  - Predicting New Risks: Crew Resource Management in a Human-Machine Team
PY  - 2022
T2  - AIAA AVIATION 2022 Forum
C7  - AIAA 2022-3833
DO  - 10.2514/6.2022-3833
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135087011&doi=10.2514%2f6.2022-3833&partnerID=40&md5=4ae26ed5013642eda786ef7cd4c1deb1
AB  - Increased operational complexity often adds new hazards with procedural controls that increase the crew task loads. The duties shared among crew members must constantly be coordinated and rebalanced. Crew resource management (CRM) reduces the likelihood of errors by building and maintaining flight crew cohesion. CRM clarifies the authorities, communications process, decision making, and coordination to manage distractions and uncertainties that might impede sound aeronautical decision making. Simplified vehicle operations are redefining the nature of a flight crew, bringing humans and machines into peer relationships with complementary cognitive roles. As operations continue to evolve toward remotely piloted and uncrewed vehicles, more of the functions will move to automated crew members. In prior work we developed cognitive decision models to reflect different peer and supervisory roles in decision-making. This paper examines the information sharing, shared accountability, and other CRM principles as they would apply to a crew comprised of people and machines in various roles. Automated crew members are allocated roles that can be renegotiated between the parties with a human pilot ultimately having supervisory controls. In CRM, control procedures assume the crew can communicate in real-time to create situational awareness, shared understanding and reasoning for informed decisions and actions. In developing the application of CRM to a human-machine team, this paper extends the artificial intelligence concept of explainability to a bidirectional exchange between human and automated crew members. The paper shows how interfaces must enable crew member engagement to verify situations and perceptions, confirm mental models, and develop a coordinated course of action. © 2022, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.
KW  - AI
KW  - AI Explainability
KW  - Autonomous Systems
KW  - Crew Resource Management
KW  - Distributed Decision Making
KW  - Human-Machine Teaming
KW  - Predictive analytics
KW  - Run-time Assurance
KW  - Automation
KW  - Aviation
KW  - Behavioral research
KW  - Human resource management
KW  - Natural resources management
KW  - Predictive analytics
KW  - Resource allocation
KW  - AI explainability
KW  - Autonomous system
KW  - Crew members
KW  - Crew resource managements
KW  - Distributed decision making
KW  - Distributed-decision makings
KW  - Human-machine
KW  - Human-machine teaming
KW  - Run-time assurance
KW  - Runtimes
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Panigutti, C.
AU  - Perotti, A.
AU  - Panisson, A.
AU  - Bajardi, P.
AU  - Pedreschi, D.
TI  - FairLens: Auditing black-box clinical decision support systems
PY  - 2021
T2  - Information Processing and Management
VL  - 58
IS  - 5
C7  - 102657
DO  - 10.1016/j.ipm.2021.102657
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108332873&doi=10.1016%2fj.ipm.2021.102657&partnerID=40&md5=5baef6cbf23a8b6c54abb95fffcbc14d
AB  - The pervasive application of algorithmic decision-making is raising concerns on the risk of unintended bias in AI systems deployed in critical settings such as healthcare. The detection and mitigation of model bias is a very delicate task that should be tackled with care and involving domain experts in the loop. In this paper we introduce FairLens, a methodology for discovering and explaining biases. We show how this tool can audit a fictional commercial black-box model acting as a clinical decision support system (DSS). In this scenario, the healthcare facility experts can use FairLens on their historical data to discover the biases of the model before incorporating it into the clinical decision flow. FairLens first stratifies the available patient data according to demographic attributes such as age, ethnicity, gender and healthcare insurance; it then assesses the model performance on such groups highlighting the most common misclassifications. Finally, FairLens allows the expert to examine one misclassification of interest by explaining which elements of the affected patients’ clinical history drive the model error in the problematic group. We validate FairLens’ ability to highlight bias in multilabel clinical DSSs introducing a multilabel-appropriate metric of disparity and proving its efficacy against other standard metrics. © 2021
KW  - Clinical decision support systems
KW  - eXplainable artificial intelligence
KW  - Fairness and bias in machine learning systems
KW  - Decision making
KW  - Digital storage
KW  - Health care
KW  - Hospital data processing
KW  - Machine learning
KW  - Algorithmics
KW  - Black boxes
KW  - Clinical decision support systems
KW  - Decisions makings
KW  - Explainable artificial intelligence
KW  - Fairness and bias in machine learning system
KW  - Misclassifications
KW  - Multi-labels
KW  - Multilabel
KW  - Pervasive applications
KW  - Decision support systems
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 55
ER  -

TY  - CONF
AU  - He, Y.
AU  - Gu, B.
AU  - Li, C.
AU  - Yan, L.
TI  - Black-box Models’ Explainability: A Theoretical and Practical Perspective
PY  - 2023
T2  - 29th Annual Americas Conference on Information Systems, AMCIS 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192879269&partnerID=40&md5=f13ae0e3a9e97b6db517fa7aa2eabe0f
AB  - The lack of explainability remains a critical challenge to the widespread adoption of artificial intelligence (AI) in many fields. “Understanding brings in trust”, while machine-learning models offer superior prediction accuracy, understanding the underlying logic is equally important to foster trust in these models. In this paper, we present eXplainable AI (XAI) as a solution to this challenge. Our research focuses on three key aspects of XAI: mathematics, humanities and social sciences, and practical applications. We demonstrated the feasibility of XAI through the use of artificially-constructed and model-derived ground truth, and verified performances of different XAIs. We also explored three dimensions of explainable consistency and emphasized the significance of human-machine consistency. Finally, we applied our research to a real-world scenario by cooperating with a national bank in China. Our findings highlight that XAI is both mathematically and practically meaningful, but more efforts need to be dedicated to this human-machine communication field. © 2023 29th Annual Americas Conference on Information Systems, AMCIS 2023. All rights reserved.
KW  - explainability-accuracy frontiers
KW  - eXplainable Artificial Intelligence
KW  - human-machine consistency
KW  - Artificial intelligence
KW  - Information use
KW  - Black box modelling
KW  - Critical challenges
KW  - Explainability-accuracy frontier
KW  - Explainable artificial intelligence
KW  - Human-machine
KW  - Human-machine consistency
KW  - Humanities and social science
KW  - Machine learning models
KW  - Prediction accuracy
KW  - Research focus
KW  - Information systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - de Boer, M.H.T.
AU  - Vethman, S.
AU  - Bakker, R.M.
AU  - Adhikari, A.
AU  - Marcus, M.
AU  - de Greeff, J.
AU  - van der Waa, J.
AU  - Schoonderwoerd, T.A.J.
AU  - Tolios, I.
AU  - van Zoelen, E.M.
AU  - Hillerström, F.
AU  - Kamphorst, B.
TI  - The FATE System Iterated: Fair, Transparent and Explainable Decision Making in a Juridical Case
PY  - 2022
T2  - CEUR Workshop Proceedings
VL  - 3121
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128741731&partnerID=40&md5=6f80ed6f95e368a246fd8487f49bd14a
AB  - The goal of the FATE system is decision support with use of state-of-the-art human-AI co-learning, explainable AI and fair, secure and privacy-preserving usage of data. This AI-based support system is a general system, in which the modules can be tuned to specific use cases. The FATE system is designed to address different user roles, such as a researcher, domain expert/consultant and subject/patient, each with their own requirements. Having examined a Diabetes Type 2 use case before, in this paper we slightly iterate the FATE system and focus on a juridical use case. For a given new juridical case the relevant older court cases are suggested by the system. The relevant older cases can be explained using the eXplainable AI (XAI) module, and the system can be improved based on feedback about the relevant cases using the Co-learning module through interaction with a user. In the Bias module, the use of the system is investigated for potential bias by inspecting the properties of suggested cases. Secure Learning offers privacy-by-design alternatives for functionality found in the aforementioned modules. These results show how the generic FATE system can be implemented in a number of real-world use cases. In future work we plan to explore more use cases within this system. © 2022 Copyright for this paper by its authors
KW  - Bias
KW  - Co-Learning
KW  - Explainable AI
KW  - FAIR AI
KW  - Hybrid AI
KW  - Knowledge Engineering
KW  - Secure Learning
KW  - Decision support systems
KW  - Knowledge engineering
KW  - Learning systems
KW  - Bias
KW  - Co-learning
KW  - Decision supports
KW  - Decisions makings
KW  - Explainable AI
KW  - FAIR AI
KW  - Hybrid AI
KW  - Privacy preserving
KW  - Secure learning
KW  - State of the art
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Sheu, R.-K.
AU  - Pardeshi, M.S.
AU  - Pai, K.-C.
AU  - Chen, L.-C.
AU  - Wu, C.-L.
AU  - Chen, W.-C.
TI  - Interpretable Classification of Pneumonia Infection Using eXplainable AI (XAI-ICP)
PY  - 2023
T2  - IEEE Access
VL  - 11
SP  - 28896
EP  - 28919
DO  - 10.1109/ACCESS.2023.3255403
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149878571&doi=10.1109%2fACCESS.2023.3255403&partnerID=40&md5=11b5fba4d101e70dc49d11f9f4c4b151
AB  - Open-box models in the medical domain have high acceptance and demand by many medical examiners. Even though the accuracy predicted by most of convolutional neural network (CNN) models is high, it is still not convincing as the detailed discussion regarding the outcome is semi-transparent in the functioning process. Pneumonia is known as one of the top contagious infections that makes most of the population affected due to low immunity. Therefore, the goal of this paper is to implement an interpretable classification of pneumonia infection using eXplainable AI (XAI-ICP). Thus, XAI-ICP is the highly efficient system designed to solve this challenge by adapting to the recent population health conditions. The aim is to design an interpretable deep classification and transfer learning based evaluation for pneumonia infection classification. The model is primarily pre-trained using the open Chest X-Ray (CXR) dataset from National Institutes of Health (NIH). Whereas, the training input and testing given to this system is Taichung Veterans General Hospital (TCVGH) for independent learning, Taiwan + VinDr open dataset for transfer learning of pneumonia affected patients with labeled CXR images possessing three features of infiltrate, cardiomegaly and effusion. The data labeling is performed by the medical examiners with the XAI human-in-the-loop approach. XAI-ICP demonstrates the XAI based reconfigurable DCNN with human-in-the-loop as a novel approach. The interpretable deep classification provides detailed transparency analysis and transfer learning for competitive accuracy. The purpose of this work, to design a re-configurable model that can continuously improve itself by using a feedback system and provide feasibility for the model deployment across multiple countries to provide an efficient system for the pneumonia infection classification. The designed model then provides detailed decisions taken at each step as transparency and features used within the algorithm for the pneumonia classification during the hospitalization. Thus, the scope can be given as explainable AI usage for the diagnosis classification using data preprocessing and interpretable deep convolutional neural network by the CXR evaluation. The accuracy achieved by using independent learning classification is 92.14% and is further improved based on successive transfer learning based evaluation is 93.29%. The XAI-ICP model adapts to the different populations by using transfer learning, while providing competitive results to the affected conditions. © 2013 IEEE.
KW  - Medical XAI
KW  - pneumonia infection
KW  - transfer learning
KW  - XAI pnuemonia
KW  - Classification (of information)
KW  - Computer aided diagnosis
KW  - Deep neural networks
KW  - Hospitals
KW  - Medical computing
KW  - Medical imaging
KW  - Pulmonary diseases
KW  - Statistical tests
KW  - Transparency
KW  - Convolutional neural network
KW  - Deep classifications
KW  - Independent learning
KW  - Medical diagnostic imaging
KW  - Medical XAI
KW  - Pneumonia infection
KW  - Solid modelling
KW  - Transfer learning
KW  - XAI pnuemonia
KW  - Convolution
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 24
ER  -

TY  - CONF
AU  - Gowtham Reddy, A.
TI  - Causality in Neural Networks - An Extended Abstract
PY  - 2021
T2  - AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society
SP  - 271
EP  - 272
DO  - 10.1145/3461702.3462467
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112440996&doi=10.1145%2f3461702.3462467&partnerID=40&md5=58a202fcc178996266d4303788e64d67
AB  - Causal reasoning is the main learning and explanation tool used by humans. AI systems should possess causal reasoning capabilities to be deployed in the real world with trust and reliability. Introducing the ideas of causality to machine learning helps in providing better learning and explainable models. Explainability, causal disentanglement are some important aspects of any machine learning model. Causal explanations are required to believe in a model's decision and causal disentanglement learning is important for transfer learning applications. We exploit the ideas of causality to be used in deep learning models to achieve better and causally explainable models that are useful in fairness, disentangled representation, etc. © 2021 Owner/Author.
KW  - causality
KW  - counterfactuals
KW  - disentanglement
KW  - explainability
KW  - machine learning
KW  - neural networks
KW  - Deep learning
KW  - Neural networks
KW  - Philosophical aspects
KW  - Transfer learning
KW  - AI systems
KW  - Causal explanations
KW  - Causal reasoning
KW  - Extended abstracts
KW  - Learning models
KW  - Machine learning models
KW  - Real-world
KW  - Learning systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Chen, X.
AU  - Tang, T.
AU  - Ren, J.
AU  - Lee, I.
AU  - Chen, H.
AU  - Xia, F.
TI  - Heterogeneous Graph Learning for Explainable Recommendation over Academic Networks
PY  - 2021
T2  - ACM International Conference Proceeding Series
SP  - 29
EP  - 36
DO  - 10.1145/3498851.3498926
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128549969&doi=10.1145%2f3498851.3498926&partnerID=40&md5=7c5ce130773150cab4ac9d4127bd9104
AB  - With the explosive growth of new graduates with research degrees every year, unprecedented challenges arise for early-career researchers to find a job at a suitable institution. This study aims to understand the behavior of academic job transition and hence recommend suitable institutions for PhD graduates. Specifically, we design a deep learning model to predict the career move of early-career researchers and provide suggestions. The design is built on top of scholarly/academic networks, which contains abundant information about scientific collaboration among scholars and institutions. We construct a heterogeneous scholarly network to facilitate the exploring of the behavior of career moves and the recommendation of institutions for scholars. We devise an unsupervised learning model called HAI (Heterogeneous graph Attention InfoMax) which aggregates attention mechanism and mutual information for institution recommendation. Moreover, we propose scholar attention and meta-path attention to discover the hidden relationships between several meta-paths. With these mechanisms, HAI provides ordered recommendations with explainability. We evaluate HAI upon a real-world dataset against baseline methods. Experimental results verify the effectiveness and efficiency of our approach.  © 2021 ACM.
KW  - academic social networks
KW  - explainability
KW  - graph learning
KW  - heterogeneous networks
KW  - recommender systems
KW  - Deep learning
KW  - Academic jobs
KW  - Academic social network
KW  - Attention mechanisms
KW  - Explainability
KW  - Explosive growth
KW  - Graph learning
KW  - Heterogeneous graph
KW  - Infomax
KW  - Learning models
KW  - Scientific collaboration
KW  - Heterogeneous networks
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 6
ER  -

TY  - CONF
AU  - Schloer, N.
AU  - Boos, S.
AU  - Harst, F.
AU  - Lanquillon, C.
AU  - Ohrnberger, M.
AU  - Schoch, F.
AU  - Stache, N.C.
AU  - Wittenberg, C.
TI  - Optimizing the Industrial Human-Machine Interface: Holographic Surfaces and Their Role in Anomaly Detection
PY  - 2024
T2  - Communications in Computer and Information Science
VL  - 2120 CCIS
SP  - 430
EP  - 435
DO  - 10.1007/978-3-031-62110-9_47
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196738230&doi=10.1007%2f978-3-031-62110-9_47&partnerID=40&md5=7ede322fe909bb16618cce3bef574cdb
AB  - This abstract explores the integration of holographic surfaces into industrial human-machine interfaces (HMIs) to optimize anomaly detection processes. By utilizing this cutting-edge technology, industrial environments can achieve more efficient and effective anomaly detection, ultimately leading to increased safety, productivity, and operational integrity. Holographic user interfaces provide a novel approach by visualizing complex data in an immersive, intuitive, and interactive way. By projecting three-dimensional representations of equipment, processes, and real-time data onto user interfaces, operators can quickly recognize anomalies and deviations from normal operating conditions. The holographic interfaces provide contextual awareness, improving operators’ understanding of the industrial environment and enabling them to make informed decisions with confidence. Holographic interfaces have the potential to revolutionize anomaly detection in industrial HMIs, resulting in improved efficiency and safety in industrial operations. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
KW  - Anomaly detection
KW  - Artificial Intelligence
KW  - Explainable AI
KW  - Human-Computer Interaction
KW  - Mixed Reality
KW  - Predictive Maintenance
KW  - Predictive Quality
KW  - process parameter optimization
KW  - Accident prevention
KW  - Anomaly detection
KW  - Holography
KW  - Mixed reality
KW  - User interfaces
KW  - Anomaly detection
KW  - Cutting edge technology
KW  - Detection process
KW  - Explainable AI
KW  - Human Machine Interface
KW  - Industrial environments
KW  - Mixed reality
KW  - Predictive maintenance
KW  - Predictive quality
KW  - Process parameters optimizations
KW  - Human computer interaction
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Ehsan, U.
AU  - Saha, K.
AU  - De Choudhury, M.
AU  - Riedl, M.O.
TI  - Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI
PY  - 2023
T2  - Proceedings of the ACM on Human-Computer Interaction
VL  - 7
IS  - 1 CSCW
C7  - 34
DO  - 10.1145/3579467
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153945382&doi=10.1145%2f3579467&partnerID=40&md5=ff3b1929d8f8abc61e96c5327d931c2b
AB  - Explainable AI (XAI) systems are sociotechnical in nature; thus, they are subject to the sociotechnical gap-divide between the technical affordances and the social needs. However, charting this gap is challenging. In the context of XAI, we argue that charting the gap improves our problem understanding, which can reflexively provide actionable insights to improve explainability. Utilizing two case studies in distinct domains, we empirically derive a framework that facilitates systematic charting of the sociotechnical gap by connecting AI guidelines in the context of XAI and elucidating how to use them to address the gap. We apply the framework to a third case in a new domain, showcasing its affordances. Finally, we discuss conceptual implications of the framework, share practical considerations in its operationalization, and offer guidance on transferring it to new contexts. By making conceptual and practical contributions to understanding the sociotechnical gap in XAI, the framework expands the XAI design space.  © 2023 Owner/Author.
KW  - AI ethics
KW  - AI governance
KW  - explainable ai
KW  - fate
KW  - framework
KW  - human-AI interaction
KW  - human-centered explainable ai
KW  - organizational dynamics
KW  - participatory design
KW  - responsible ai
KW  - sociotechnical gap
KW  - user study
KW  - AI ethic
KW  - AI governance
KW  - Explainable ai
KW  - Fate
KW  - Framework
KW  - Human-AI interaction
KW  - Human-centered explainable ai
KW  - Organizational dynamics
KW  - Participatory design
KW  - Responsible ai
KW  - Sociotechnical
KW  - Sociotechnical gap
KW  - User study
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 40
ER  -

TY  - CONF
AU  - Wang, X.
AU  - Liang, C.
AU  - Yin, M.
TI  - The Effects of AI Biases and Explanations on Human Decision Fairness: A Case Study of Bidding in Rental Housing Markets
PY  - 2023
T2  - IJCAI International Joint Conference on Artificial Intelligence
VL  - 2023-August
SP  - 3076
EP  - 3084
DO  - 10.24963/ijcai.2023/343
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170401323&doi=10.24963%2fijcai.2023%2f343&partnerID=40&md5=bef8466f9afaa8212fcf894ec08858f9
AB  - The use of AI-based decision aids in diverse domains has inspired many empirical investigations into how AI models' decision recommendations impact humans' decision accuracy in AI-assisted decision making, while explorations on the impacts on humans' decision fairness are largely lacking despite their clear importance. In this paper, using a real-world business decision making scenario-bidding in rental housing markets-as our testbed, we present an experimental study on understanding how the bias level of the AI-based decision aid as well as the provision of AI explanations affect the fairness level of humans' decisions, both during and after their usage of the decision aid. Our results suggest that when people are assisted by an AI-based decision aid, both the higher level of racial biases the decision aid exhibits and surprisingly, the presence of AI explanations, result in more unfair human decisions across racial groups. Moreover, these impacts are partly made through triggering humans' “disparate interactions” with AI. However, regardless of the AI bias level and the presence of AI explanations, when people return to make independent decisions after their usage of the AI-based decision aid, their decisions no longer exhibit significant unfairness across racial groups. © 2023 International Joint Conferences on Artificial Intelligence. All rights reserved.
KW  - Apartment houses
KW  - Artificial intelligence
KW  - Behavioral research
KW  - Commerce
KW  - Decision support systems
KW  - Bias levels
KW  - Case-studies
KW  - Decision accuracies
KW  - Decision aids
KW  - Decisions makings
KW  - Diverse domains
KW  - Empirical investigation
KW  - Housing markets
KW  - Human decisions
KW  - Modeling decisions
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Engelmann, D.C.
AU  - Ferrando, A.
AU  - Panisson, A.R.
AU  - Ancona, D.
AU  - Bordini, R.H.
AU  - Mascardi, V.
TI  - RV4JaCa—Towards Runtime Verification of Multi-Agent Systems and Robotic Applications
PY  - 2023
T2  - Robotics
VL  - 12
IS  - 2
C7  - 49
DO  - 10.3390/robotics12020049
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153763040&doi=10.3390%2frobotics12020049&partnerID=40&md5=ee7e9c42cd6eadde971edfbbdf08012b
AB  - This paper presents a Runtime Verification (RV) approach for Multi-Agent Systems (MAS) using the JaCaMo framework. Our objective is to bring a layer of security to the MAS. This is achieved keeping in mind possible safety-critical uses of the MAS, such as robotic applications. This layer is capable of controlling events during the execution of the system without needing a specific implementation in the behaviour of each agent to recognise the events. In this paper, we mainly focus on MAS when used in the context of hybrid intelligence. This use requires communication between software agents and human beings. In some cases, communication takes place via natural language dialogues. However, this kind of communication brings us to a concern related to controlling the flow of dialogue so that agents can prevent any change in the topic of discussion that could impair their reasoning. The latter may be a problem and undermine the development of the software agents. In this paper, we tackle this problem by proposing and demonstrating the implementation of a framework that aims to control the dialogue flow in a MAS; especially when the MAS communicates with the user through natural language to aid decision-making in a hospital bed allocation scenario. © 2023 by the authors.
KW  - dialogue systems
KW  - explainable artificial intelligence
KW  - JaCaMo framework
KW  - multi-agent systems
KW  - robotic applications
KW  - runtime verification
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Lyu, Y.
AU  - Liang, P.P.
AU  - Deng, Z.
AU  - Salakhutdinov, R.
AU  - Morency, L.-P.
TI  - DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations
PY  - 2022
T2  - AIES 2022 - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society
SP  - 455
EP  - 467
DO  - 10.1145/3514094.3534148
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137165724&doi=10.1145%2f3514094.3534148&partnerID=40&md5=6a30f3d27f98a2740ac7cbf72130d948
AB  - The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-The-Art in interpreting multimodal models-a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment.  © 2022 Owner/Author.
KW  - explainability
KW  - interpretability
KW  - multimodal machine learning
KW  - visualization
KW  - Decision making
KW  - Human computer interaction
KW  - Human robot interaction
KW  - Interactive computer systems
KW  - Learning systems
KW  - Explainability
KW  - Fine grained
KW  - Intelligence models
KW  - Interpretability
KW  - Machine-learning
KW  - Modeling behaviour
KW  - Multi-modal
KW  - Multimodal machine learning
KW  - Multimodal models
KW  - Real-world
KW  - Machine learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 13
ER  -

TY  - JOUR
AU  - Aloisi, C.
TI  - The future of standardised assessment: Validity and trust in algorithms for assessment and scoring
PY  - 2023
T2  - European Journal of Education
VL  - 58
IS  - 1
SP  - 98
EP  - 110
DO  - 10.1111/ejed.12542
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145032494&doi=10.1111%2fejed.12542&partnerID=40&md5=ab05a508a302c6d69dea7181ede319f7
AB  - This article considers the challenges of using artificial intelligence (AI) and machine learning (ML) to assist high-stakes standardised assessment. It focuses on the detrimental effect that even state-of-the-art AI and ML systems could have on the validity of national exams of secondary education, and how lower validity would negatively affect trust in the system. To reach this conclusion, three unresolved issues in AI (unreliability, low explainability and bias) are addressed, to show how each of them would compromise the interpretations and uses of exam results (i.e., exam validity). Furthermore, the article relates validity to trust, and specifically to the ABI+ model of trust. Evidence gathered as part of exam validation supports each of the four trust-enabling components of the ABI+ model (ability, benevolence, integrity and predictability). It is argued, therefore, that the three AI barriers to exam validity limit the extent to which an AI-assisted exam system could be trusted. The article suggests that addressing the issues of AI unreliability, low explainability and bias should be sufficient to put AI-assisted exams on par with traditional ones, but might not go as far as fully reassure the public. To achieve this, it is argued that changes to the quality assurance mechanisms of the exam system will be required. This may involve, for example, integrating principled AI frameworks in assessment policy and regulation. © 2023 John Wiley & Sons Ltd.
KW  - artificial intelligence
KW  - automated assessment
KW  - bias
KW  - education
KW  - trust
KW  - United Kingdom
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 19
ER  -

TY  - JOUR
AU  - Englisch, J.
AU  - Schuh, M.
TI  - Algorithm-supported administrative procedures - Fields of application, risks and the need for additional controls
ST  - ALGORITHMENGESTüTZTE VERWALTUNGSVERFAHREN – EINSATZFELDER, RISIKEN UND NOTWENDIGKEIT ERGäNZENDER KONTROLLEN
PY  - 2022
T2  - Verwaltung
VL  - 55
IS  - 2
SP  - 155
EP  - 190
DO  - 10.3790/verw.55.2.155
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145276587&doi=10.3790%2fverw.55.2.155&partnerID=40&md5=12caea383e27bd9f41dd898ac2fced26
AB  - The digitization of the German public sector is gaining momentum and it now also extends to the use of algorithms and artificial intelligence in administrative procedures. On the basis of recently published official data, a broad spectrum of fields and variants of application can be identified. However, this increasing reliance on algorithms also creates particular challenges to the rule of law. In particular, the reliance on algorithms can increase the risk of systematic unlawfulness and bias in administrative decision-making, whether fully automated or still accounted for by a human official. Recent studies suggest that also in the latter case, those risks are often not properly managed – and are sometimes even exacerbated – by the humans in the loop. The traditional instruments of judicial protection and internal administrative supervision are not well suited to address the aforementioned inherent and operational risks either; moreover, they offer only ex post remedies and have no preventive effect. Therefore, additional technical precautions and institutional arrangements are required to uphold the rule of law. They should include investments in explainable artificial intelligence (“XAI”) and adequate staff training programs, as well as the creation of public registers and independent standard-setting and auditing bodies for public sector algorithms. In particularly sensitive areas of administration, the use of technical or proprietary black box algorithms should be entirely banned until further technological progress on transparency and risk control has been made. © 2022 Duncker und Humblot GmbH. All rights reserved.
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Dandolo, D.
AU  - Masiero, C.
AU  - Carletti, M.
AU  - Dalle Pezze, D.
AU  - Susto, G.A.
TI  - AcME—Accelerated model-agnostic explanations: Fast whitening of the machine-learning black box
PY  - 2023
T2  - Expert Systems with Applications
VL  - 214
C7  - 119115
DO  - 10.1016/j.eswa.2022.119115
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141498082&doi=10.1016%2fj.eswa.2022.119115&partnerID=40&md5=b00e95b5e01c4312c44f571487f7a584
AB  - In the context of human-in-the-loop Machine Learning applications, like Decision Support Systems, interpretability approaches should provide actionable insights without making the users wait. In this paper, we propose Accelerated Model-agnostic Explanations (AcME), an interpretability approach that quickly provides feature importance scores both at the global and the local level. AcME can be applied a posteriori to each regression or classification model based on tabular data. Not only AcME computes feature ranking, but it also provides a what-if analysis tool to assess how changes in features values would affect model predictions. We evaluated the proposed approach on synthetic and real-world datasets, also in comparison with SHapley Additive exPlanations (SHAP), the approach we drew inspiration from, which is currently one of the state-of-the-art model-agnostic interpretability approaches. We achieved comparable results in terms of quality of produced explanations while reducing dramatically the computational time and providing consistent visualization for global and local interpretations. To foster research in this field, and for the sake of reproducibility, we also provide a repository with the code used for the experiments. © 2022 Elsevier Ltd
KW  - Decision Support Systems
KW  - Explainable artificial intelligence
KW  - Machine learning
KW  - Machine learning interpretability
KW  - Machine learning
KW  - Accelerated models
KW  - Black boxes
KW  - Explainable artificial intelligence
KW  - Human-in-the-loop
KW  - Interpretability
KW  - Machine learning applications
KW  - Machine learning interpretability
KW  - Machine-learning
KW  - Posteriori
KW  - Decision support systems
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 20
ER  -

TY  - CONF
AU  - Protopapadakis, G.
AU  - Apostolidis, A.
AU  - Kalfas, A.I.
TI  - EXPLAINABLE AND INTERPRETABLE AI-ASSISTED REMAINING USEFUL LIFE ESTIMATION FOR AEROENGINES
PY  - 2022
T2  - Proceedings of the ASME Turbo Expo
VL  - 2
C7  - V002T05A002
DO  - 10.1115/GT2022-80777
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141396871&doi=10.1115%2fGT2022-80777&partnerID=40&md5=259f42bd3450931c909d7d0dc9a49a38
AB  - Remaining Useful Life (RUL) estimation is directly related with the application of predictive maintenance. When RUL estimation is performed via data-driven methods and Artificial Intelligence algorithms, explainability and interpretability of the model are necessary for trusted predictions. This is especially important when predictive maintenance is applied to gas turbines or aeroengines, as they have high operational and maintenance costs, while their safety standards are strict and highly regulated. The objective of this work is to study the explainability of a Deep Neural Network (DNN) RUL prediction model. An open-source database is used, which is composed by computed measurements through a thermodynamic model for a given turbofan engine, considering non-linear degradation and data points for every second of a full flight cycle. First, the necessary data pre-processing is performed, and a DNN is used for the regression model. The selection of its hyper-parameters is done using random search and Bayesian optimisation. Tests considering the feature selection and the requirements of additional virtual sensors are discussed. The generalisability of the model is performed, showing that the type of faults as well as the dominant degradation has an important effect on the overall accuracy of the model. The explainability and interpretability aspects are studied, following the Local Interpretable Model-agnostic Explanations (LIME) method. The outcomes are showing that for simple data sets, the model can better understand physics, and LIME can give a good explanation. However, as the complexity of the data increases, both the accuracy of the model drops but also LIME seems to have difficulties in giving satisfactory explanations. Copyright © 2022 by ASME.
KW  - Aircraft engines
KW  - Data handling
KW  - Lime
KW  - Maintenance
KW  - Regression analysis
KW  - Turbofan engines
KW  - Aero-engine
KW  - Artificial intelligence algorithms
KW  - Data-driven methods
KW  - Interpretability
KW  - Life estimation
KW  - Life prediction models
KW  - Maintenance cost
KW  - Predictive maintenance
KW  - Remaining useful lives
KW  - Safety standard
KW  - Deep neural networks
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 14
ER  -

TY  - JOUR
AU  - Schraagen, J.M.
TI  - Responsible use of AI in military systems: prospects and challenges
PY  - 2023
T2  - Ergonomics
VL  - 66
IS  - 11
SP  - 1719
EP  - 1729
DO  - 10.1080/00140139.2023.2278394
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176251711&doi=10.1080%2f00140139.2023.2278394&partnerID=40&md5=5196c95b4c66a8d412fe4ec44f592e78
AB  - Artificial Intelligence (AI) holds great potential for the military domain but is also seen as prone to data bias and lacking transparency and explainability. In order to advance the trustworthiness of AI-enabled systems, a dynamic approach to the development, deployment and use of AI systems is required. This approach, when incorporating ethical principles such as lawfulness, traceability, reliability and bias mitigation, is called ‘Responsible AI’. This article describes the challenges of using AI responsibly in the military domain from a human factors and ergonomics perspective. Many of the ironies of automation originally described by Bainbridge still apply in the field of AI, but there are also some unique challenges and requirements that need to be considered, such as a larger emphasis on ethical risk analyses and validation and verification up-front, as well as moral situation awareness during deployment and use of AI in military systems. © 2023 Informa UK Limited, trading as Taylor & Francis Group.
KW  - Artificial Intelligence
KW  - ethics
KW  - explainability
KW  - human-machine teaming
KW  - military systems
KW  - testing and evaluation
KW  - transparency
KW  - validation and verification
KW  - Artificial Intelligence
KW  - Automation
KW  - Awareness
KW  - Humans
KW  - Military Personnel
KW  - Reproducibility of Results
KW  - Artificial intelligence
KW  - Ergonomics
KW  - Ethical technology
KW  - Risk analysis
KW  - Artificial intelligence systems
KW  - Dynamic approaches
KW  - Ethical principles
KW  - Explainability
KW  - Human-machine
KW  - Human-machine teaming
KW  - Military domains
KW  - Military systems
KW  - Testing and evaluation
KW  - Validation and verification
KW  - army
KW  - article
KW  - artificial intelligence
KW  - automation
KW  - awareness
KW  - ergonomics
KW  - ethics
KW  - human
KW  - mitigation
KW  - morality
KW  - reliability
KW  - risk assessment
KW  - military personnel
KW  - reproducibility
KW  - Transparency
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Kaur, D.
AU  - Uslu, S.
AU  - Rittichier, K.J.
AU  - Durresi, A.
TI  - Trustworthy Artificial Intelligence: A Review
PY  - 2023
T2  - ACM Computing Surveys
VL  - 55
IS  - 2
C7  - 3491209
DO  - 10.1145/3491209
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128190943&doi=10.1145%2f3491209&partnerID=40&md5=622d2c23d91ae624a63a320c7d89453b
AB  - Artificial intelligence (AI) and algorithmic decision making are having a profound impact on our daily lives. These systems are vastly used in different high-stakes applications like healthcare, business, government, education, and justice, moving us toward a more algorithmic society. However, despite so many advantages of these systems, they sometimes directly or indirectly cause harm to the users and society. Therefore, it has become essential to make these systems safe, reliable, and trustworthy. Several requirements, such as fairness, explainability, accountability, reliability, and acceptance, have been proposed in this direction to make these systems trustworthy. This survey analyzes all of these different requirements through the lens of the literature. It provides an overview of different approaches that can help mitigate AI risks and increase trust and acceptance of the systems by utilizing the users and society. It also discusses existing strategies for validating and verifying these systems and the current standardization efforts for trustworthy AI. Finally, we present a holistic view of the recent advancements in trustworthy AI to help the interested researchers grasp the crucial facets of the topic efficiently and offer possible future research directions.  © 2022 Association for Computing Machinery.
KW  - acceptance
KW  - accountability
KW  - Artificial intelligence
KW  - black-box problem
KW  - explainability
KW  - explainable AI
KW  - fairness
KW  - machine learning
KW  - privacy
KW  - trustworthy AI
KW  - Data privacy
KW  - Decision making
KW  - Acceptance
KW  - Accountability
KW  - Black boxes
KW  - Black-box problem
KW  - Explainability
KW  - Explainable artificial intelligence
KW  - Fairness
KW  - Machine-learning
KW  - Privacy
KW  - Trustworthy artificial intelligence
KW  - Machine learning
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 375
ER  -

TY  - CONF
AU  - Lindner, F.
AU  - Reiner, G.
TI  - Industry 5.0 and Operations Management - the Importance of Human Factors
PY  - 2023
T2  - Proceedings of IEEE/IFIP Network Operations and Management Symposium 2023, NOMS 2023
DO  - 10.1109/NOMS56928.2023.10154282
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164735233&doi=10.1109%2fNOMS56928.2023.10154282&partnerID=40&md5=6da4fede7636f538213e2c75912432f7
AB  - In this position paper, we highlight the importance of human factors, especially cognition, for operations management during the transition from Industry 4.0 to 5.0 and within. We argue that the increasing prevalence of (digital) technology and data for manufacturing operations urges human-centered approaches and solutions, as well - to enable efficient and effective operations that benefit from both humans' and technologies' strengths. To stress our point, we give examples from behavioral operations management where technology may both foster or mitigate deviations from rational decision-making. In addition, we show prospects of human-AI interaction and explainable AI, specifically by using visualizations, to improve operational performance. © 2023 IEEE.
KW  - behavioral manufacturing operations management
KW  - cognitive biases
KW  - human factors
KW  - Industry 5.0
KW  - visualizations
KW  - Decision making
KW  - Human engineering
KW  - Industry 4.0
KW  - Behavioral manufacturing operation management
KW  - Behavioral operations
KW  - Cognitive bias
KW  - Digital datas
KW  - Digital technologies
KW  - Industry 5.0
KW  - Manufacturing operations
KW  - Operation management
KW  - Position papers
KW  - Rational decision making
KW  - Visualization
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Hoffman, R.R.
AU  - Mueller, S.T.
AU  - Klein, G.
AU  - Litman, J.
TI  - Measures for explainable AI: Explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance
PY  - 2023
T2  - Frontiers in Computer Science
VL  - 5
C7  - 1096257
DO  - 10.3389/fcomp.2023.1096257
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148513751&doi=10.3389%2ffcomp.2023.1096257&partnerID=40&md5=4f90e0925d5a793fcb658604a55708c4
AB  - If a user is presented an AI system that portends to explain how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? This question entails some key concepts of measurement such as explanation goodness and trust. We present methods for enabling developers and researchers to: (1) Assess the a priori goodness of explanations, (2) Assess users' satisfaction with explanations, (3) Reveal user's mental model of an AI system, (4) Assess user's curiosity or need for explanations, (5) Assess whether the user's trust and reliance on the AI are appropriate, and finally, (6) Assess how the human-XAI work system performs. The methods we present derive from our integration of extensive research literatures and our own psychometric evaluations. We point to the previous research that led to the measurement scales which we aggregated and tailored specifically for the XAI context. Scales are presented in sufficient detail to enable their use by XAI researchers. For Mental Model assessment and Work System Performance, XAI researchers have choices. We point to a number of methods, expressed in terms of methods' strengths and weaknesses, and pertinent measurement issues. Copyright © 2023 Hoffman, Mueller, Klein and Litman.
KW  - explanation goodness
KW  - explanatory reasoning
KW  - machine-generated explanations
KW  - measurement
KW  - mental models
KW  - performance
KW  - trust
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 88
ER  -

TY  - CONF
AU  - Oza, S.
AU  - Zou, Y.
TI  - Transparency in Algorithmic Management: A Psychological Ownership Perspective
PY  - 2022
T2  - 28th Americas Conference on Information Systems, AMCIS 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192551807&partnerID=40&md5=15fb0960ab41573f7207731300c7643c
AB  - Decision-making and forecasting capabilities of algorithmic systems have helped organizations improve work productivity and business performance. Specifically, AI-enabled information systems (IS) are increasingly being used to track employee's work hours and automate their work shifts in retail and service industries including hospitality, leisure, and health services. For example, companies such Kronos, Zoho and Deputy specialize in workforce management software programs that utilize AI technologies to match employer's staffing needs for labor to at-the-moment customer demand. Software programs do not only fine-tune and optimize scheduling decisions but also send automatic updates to employees about their shift changes (Loggins, 2020). According to a report from the Reportlinker.com (2022), the global market of cloud-based work scheduling software is estimated to grow by over 4 billion dollars during the forecast period of 2022 to 2026. With the increasing relevance of algorithmic systems in workforce scheduling and management, it is critical to understand their impact on employees' work experiences and effectiveness. Specifically, past research has indicated that the use of algorithmic systems in the workplace can lead to several ramifications including discrimination, surveillance, manipulation, disempowerment of employees, precarity, and stress (e.g., Kellogg et al., 2020). Nevertheless, there remains an equivocal understanding of why employees would have those negative experiences with the deployment of algorithmic systems and what organizations could do to mitigate those negative experiences effectively. In this research, we center on investigating the effects of employees' perceptions of transparency about work scheduling AI software on their job satisfaction and affective organizational commitment. According to a theory of psychological ownership in organizations (Pierce et al., 2001), individuals have an innate motive to be in control and to be efficient and effectant (Pierce et al., 2003). Based on this core premise, the present study suggests that when the inner workings of work management AI software are unclear to employees, the compliance to automated work schedules can negatively affect employees' perceptions of job autonomy and job-based psychological ownership, that could further decrease employees' job satisfaction and affective organizational commitment. In contrast, when employees are provided with an explanation about why and how work management AI software programs are deployed to manage their work shifts, they are likely to perceive such programs as more transparent and less opaque. As a result, employees are likely to experience freedom and flexibility in controlling their own work schedules with the use of those programs, and such work experiences can enhance job satisfaction and organizational commitment. The present research is intended to extend prior research on AI-related work design. A close examination of algorithmic transparency from a psychological ownership lens can help to shed light onto both positive and negative effects of AI-related work management on employees' work outcomes and psychological experiences. Study results from this research can also help to inform HR managers, supervisors, and stakeholders at organizations of the importance of building and using work management AI software in ways that can facilitate transparency and ensure worker well-being and a committed workforce. © 2022 28th Americas Conference on Information Systems, AMCIS 2022. All Rights Reserved.
KW  - Computer software
KW  - Human resource management
KW  - Information systems
KW  - Information use
KW  - International trade
KW  - Job satisfaction
KW  - Sales
KW  - Transparency
KW  - Algorithmics
KW  - Negative experiences
KW  - Organizational Commitment
KW  - Psychological ownership
KW  - Software project
KW  - Work experience
KW  - Work management
KW  - Work scheduling
KW  - Work shifts
KW  - Workforce management
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Bell, A.
AU  - Solano-Kamaiko, I.
AU  - Nov, O.
AU  - Stoyanovich, J.
TI  - It's Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy
PY  - 2022
T2  - ACM International Conference Proceeding Series
SP  - 248
EP  - 266
DO  - 10.1145/3531146.3533090
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133005363&doi=10.1145%2f3531146.3533090&partnerID=40&md5=3d2768d325d769b5a5461f63cf21fc7d
AB  - To achieve high accuracy in machine learning (ML) systems, practitioners often use complex "black-box"models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature - and even the existence - of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque. Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human's ability to anticipate a model's output or identify the most important feature of a model, and subjective measures, such as a human's perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It's just not that simple! © 2022 ACM.
KW  - explainability
KW  - machine learning
KW  - public policy
KW  - responsible AI
KW  - Decision trees
KW  - Economic and social effects
KW  - Machine learning
KW  - Random forests
KW  - Black box modelling
KW  - Empirical studies
KW  - Explainability
KW  - High-accuracy
KW  - Human-in-the-loop
KW  - Machine-learning
KW  - Modeling accuracy
KW  - Responsible AI
KW  - Simple++
KW  - Trade off
KW  - Public policy
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 61
ER  -

TY  - CONF
AU  - Cohen, M.C.
AU  - Mancenido, M.V.
AU  - Chiou, E.K.
AU  - Cooke, N.J.
TI  - Teamness and Trust in AI-Enabled Decision Support Systems: Current Challenges and Future Directions
PY  - 2023
T2  - CEUR Workshop Proceedings
VL  - 3456
SP  - 175
EP  - 187
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171150711&partnerID=40&md5=7a1f3e6aeeac544ddfe4d7e398b0bb1d
AB  - Artificial intelligence-enabled decision support systems (AI-DSSs) can process highly complex information to recommend or execute decisions autonomously, but often at the cost of lacking transparency and explainability. The existence of inherent human limitations in understanding increasingly inexplicable AI-DSSs, however, raise the question of people’s roles in the high-stakes, rapid decision-making domains for which AI-DSSs are being developed. In this paper, we summarize the current state of human-AI teaming research in light of how emergent cognitive properties arise from human interactions with AI-DSSs. We also identify important open research questions in accounting for the teamness of AI-DSSs in light of current directions in trust research. Finally, we outline some anticipated challenges in methodological approaches and generalizability when attempting to design studies to answer these questions. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
KW  - AI-DSS
KW  - Decision support systems
KW  - Human-AI teaming
KW  - Teamness
KW  - Trust
KW  - Artificial intelligence
KW  - Behavioral research
KW  - Decision making
KW  - 'current
KW  - Artificial intelligence-enabled decision support system
KW  - Cognitive properties
KW  - Complex information
KW  - Decisions makings
KW  - Human limitations
KW  - Human-AI teaming
KW  - Humaninteraction
KW  - Teamness
KW  - Trust
KW  - Decision support systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Jin, X.
AU  - Lv, S.
AU  - Kong, Z.
AU  - Yang, H.
AU  - Zhang, Y.
AU  - Guo, Y.
AU  - Xu, Z.
TI  - Graph Spatio-Temporal Networks for Condition Monitoring of Wind Turbine
PY  - 2024
T2  - IEEE Transactions on Sustainable Energy
VL  - 15
IS  - 4
SP  - 2276
EP  - 2286
DO  - 10.1109/TSTE.2024.3411884
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196084737&doi=10.1109%2fTSTE.2024.3411884&partnerID=40&md5=26cb9d0ddf4d3d6cc840b81246d1e831
AB  - Condition monitoring of wind turbines (WTs) is essential for advancing wind energy. Existing data-driven methods heavily rely on deep learning and big data, leading to challenges in distinguishing true faults from false alarms, impacting operational decisions negatively. Thus, this paper proposes a spatio-temporal graph neural network framework that incorporates prior knowledge. Prior WT knowledge is utilized by establishing a spatially structured directed graph embedded in a graph attention network (GAT). The features in WTs' supervisory control and data acquisition system are indicated by the nodes in GAT. Then, the global and local attention embedding layers as well as long short-term memory layers are employed to combine spatio-temporal information from each node. Finally, the condition monitoring in WTs' graph and node-level are established, and a fault propagation chain at node-level is constructed for explaining condition monitoring results. To demonstrate the explainability, robustness and sensitivity of the proposed approach, a comparative analysis between a true fault case and a false alarm case are given, and anomaly detection results are also reported.  © 2010-2012 IEEE.
KW  - condition monitoring
KW  - graph spatio-temporal neural networks
KW  - prior knowledge
KW  - Wind turbine (WT)
KW  - Anomaly detection
KW  - Brain
KW  - Condition monitoring
KW  - Directed graphs
KW  - Errors
KW  - SCADA systems
KW  - Wind power
KW  - Wind turbines
KW  - Data-driven methods
KW  - Falsealarms
KW  - Features extraction
KW  - Graph spatio-temporal neural network
KW  - Operational decisions
KW  - Prior-knowledge
KW  - Spatio-temporal
KW  - Temporal networks
KW  - Temporal neural networks
KW  - Wind turbine
KW  - Long short-term memory
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Graham, T.
AU  - Thangavel, K.
TI  - Artificial Intelligence in Space: An Analysis of Responsible AI Principles for the Space Domain
PY  - 2023
T2  - Proceedings of the International Astronautical Congress, IAC
VL  - 2023-October
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188016281&partnerID=40&md5=1cca0a1acc839aefde90bec4b730419c
AB  - Advances in Artificial Intelligence (AI) technologies are enabling a plethora of new applications across many industries. There are already a multitude of applications for AI systems in the space industry, but as the AI and space industries continue to grow in size and value rapidly, further use-cases will become apparent and proliferate to all corners of space operations and data analytics. Such space-based AI systems will bring many economic, scientific, and environmental benefits; however, they could also enable harm to individuals, organisations, and the environment if they are not developed and managed properly. Potential breaches of privacy through AI-assisted analysis of Earth observation imagery and collisions between objects in orbit due to malfunctioning automated maneuvering systems are examples of the harms that could eventuate if poorly designed AI systems are deployed in the space-sector. Responsible AI practices are needed to ensure such risks do not eventuate. 'Responsible' (or 'ethical') AI has emerged as a discipline designed to guide responsible AI development wherein the goal is to maximise the benefits of AI systems for individuals and society while mitigating against any potential harm that they may cause. Commonly accepted Responsible AI principles include accountability, contestability, fairness, security, privacy, transparency, explainability, and reliability. At times notions of 'do-no-harm' and generating 'net benefits' for society and the environment are also included. These principles of Responsible AI are generalizable and industry agnostic; however, they should be carefully considered in the context of the unique physical, economic, political, and technological characteristics of the space domain before being adopted wholesale by the space industry. While concepts such as security and reliability can be readily applied to applications of AI systems in the space domain, other ideals such as contestability, fairness, and explainability may not be as relevant to the use cases found within the space industry. This paper introduces the concept of Responsible AI and Responsible AI principles and then examines the applicability and appropriateness of widely accepted Responsible AI principles in the context of existing and emerging regulatory instruments relevant to the space industry. This serves as a first step towards creating a standardized regulatory framework for the responsible development of space-based AI systems and preventing harms associated with such systems occurring. Copyright © 2023 by SmartSat CRC. Published by the IAF, with permission and released to the IAF to publish in all forms.
KW  - Artificial Intelligence
KW  - Guidelines and Technical Standards
KW  - International Law
KW  - Liability
KW  - National Law and Regulation
KW  - Responsible AI
KW  - Space Law
KW  - Artificial intelligence
KW  - Data Analytics
KW  - Orbits
KW  - Space flight
KW  - Artificial intelligence systems
KW  - Guideline and technical standard
KW  - Laws and regulations
KW  - Liability
KW  - National law and regulation
KW  - National laws
KW  - Responsible artificial intelligence
KW  - Space industry
KW  - Space laws
KW  - Technical standards
KW  - Earth (planet)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Handelman, D.A.
AU  - Rivera, C.G.
AU  - St. Amant, R.
AU  - Holmes, E.A.
AU  - Badger, A.R.
AU  - Yeh, B.Y.
TI  - Adaptive human-robot teaming through integrated symbolic and subsymbolic artificial intelligence: preliminary results
PY  - 2022
T2  - Proceedings of SPIE - The International Society for Optical Engineering
VL  - 12113
C7  - 121130I
DO  - 10.1117/12.2618686
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134162538&doi=10.1117%2f12.2618686&partnerID=40&md5=bd62567b771d0348e26092dff29a67a1
AB  - As the autonomy of intelligent systems continues to increase, the ability of humans to maintain control over machine behavior, work effectively in concert with them, and trust them, becomes paramount. Ideally, a machine’s plan of action would be accessible to and understandable by human team members, and machine behavior would be modifiable in real time, in the field, to accommodate unanticipated situations. The ability of machines to adapt to new situations quickly and reliably based on both human input and autonomous learning has the potential to enhance numerous human-machine teaming scenarios. Our research focuses on the question, “Can robots become competent and adaptive teammates by emulating human skill acquisition strategies?” In this paper we describe the Robotic Skill Acquisition (RSA) cognitive architecture and show preliminary results of teaming experiments involving a human wearing an augmented reality headset and a quadruped robot performing tasks related to reconnaissance. The goal is to combine instruction and discovery by integrating declarative symbolic AI and reflexive neural network learning to produce robust, explainable and trusted robot behavior, adjustable autonomy, and adaptive human-robot teaming. Humans and robots start with a playbook of modifiable hierarchical task descriptions that encode explicit task knowledge. Neural network based feedback error learning enables human-directed behavior shaping, and reinforcement learning enables discovery of novel subtask control strategies. It is anticipated that modifications to and transitions between symbolic and subsymbolic processing will enable highly adaptive behavior in support of enhanced situational awareness and operational effectiveness of human-robot teams. © 2022 SPIE.
KW  - adaptive teaming
KW  - adjustable autonomy
KW  - cognitive architecture
KW  - explainable artificial intelligence
KW  - Human-machine teaming
KW  - human-robot collaboration
KW  - machine learning
KW  - shared mental model
KW  - Augmented reality
KW  - Behavioral research
KW  - Intelligent robots
KW  - Learning systems
KW  - Network architecture
KW  - Reinforcement learning
KW  - Adaptive teaming
KW  - Adjustable autonomy
KW  - Cognitive architectures
KW  - Explainable artificial intelligence
KW  - Human robots
KW  - Human-machine
KW  - Human-machine teaming
KW  - Human-robot collaboration
KW  - Machine-learning
KW  - Shared mental model
KW  - Intelligent systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Lee, M.H.
AU  - Chew, C.J.
TI  - Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making
PY  - 2023
T2  - Proceedings of the ACM on Human-Computer Interaction
VL  - 7
IS  - CSCW2
C7  - 369
DO  - 10.1145/3610218
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174692019&doi=10.1145%2f3610218&partnerID=40&md5=782cb00cf7b3bcc9cbf0df6fa15968cc
AB  - Artificial intelligence (AI) is increasingly being considered to assist human decision-making in high-stake domains (e.g. health). However, researchers have discussed an issue that humans can over-rely on wrong suggestions of the AI model instead of achieving human AI complementary performance. In this work, we utilized salient feature explanations along with what-if, counterfactual explanations to make humans review AI suggestions more analytically to reduce overreliance on AI and explored the effect of these explanations on trust and reliance on AI during clinical decision-making. We conducted an experiment with seven therapists and ten laypersons on the task of assessing post-stroke survivors' quality of motion, and analyzed their performance, agreement level on the task, and reliance on AI without and with two types of AI explanations. Our results showed that the AI model with both salient features and counterfactual explanations assisted therapists and laypersons to improve their performance and agreement level on the task when 'right' AI outputs are presented. While both therapists and laypersons over-relied on 'wrong' AI outputs, counterfactual explanations assisted both therapists and laypersons to reduce their over-reliance on 'wrong' AI outputs by 21% compared to salient feature explanations. Specifically, laypersons had higher performance degrades by 18.0 f1-score with salient feature explanations and 14.0 f1-score with counterfactual explanations than therapists with performance degrades of 8.6 and 2.8 f1-scores respectively. Our work discusses the potential of counterfactual explanations to better estimate the accuracy of an AI model and reduce over-reliance on 'wrong' AI outputs and implications for improving human-AI collaborative decision-making. © 2023 Owner/Author.
KW  - clinical decision support systems
KW  - explainable AI
KW  - human centered AI
KW  - human-AI collaboration
KW  - physical stroke rehabilitation assessment
KW  - reliance
KW  - trust
KW  - Decision making
KW  - Decision support systems
KW  - Patient rehabilitation
KW  - Clinical decision support systems
KW  - Counterfactuals
KW  - Explainable artificial intelligence
KW  - Human centered artificial intelligence
KW  - Human-artificial intelligence collaboration
KW  - Performance
KW  - Physical stroke rehabilitation assessment
KW  - Reliance
KW  - Stroke rehabilitation
KW  - Trust
KW  - Artificial intelligence
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 23
ER  -

TY  - JOUR
AU  - Verhagen, R.S.
AU  - Neerincx, M.A.
AU  - Tielman, M.L.
TI  - The influence of interdependence and a transparent or explainable communication style on human-robot teamwork
PY  - 2022
T2  - Frontiers in Robotics and AI
VL  - 9
C7  - 993997
DO  - 10.3389/frobt.2022.993997
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138570664&doi=10.3389%2ffrobt.2022.993997&partnerID=40&md5=6dd464459338c56fa260930dff1220c4
AB  - Humans and robots are increasingly working together in human-robot teams. Teamwork requires communication, especially when interdependence between team members is high. In previous work, we identified a conceptual difference between sharing what you are doing (i.e., being transparent) and why you are doing it (i.e., being explainable). Although the second might sound better, it is important to avoid information overload. Therefore, an online experiment (n = 72) was conducted to study the effect of communication style of a robot (silent, transparent, explainable, or adaptive based on time pressure and relevancy) on human-robot teamwork. We examined the effects of these communication styles on trust in the robot, workload during the task, situation awareness, reliance on the robot, human contribution during the task, human communication frequency, and team performance. Moreover, we included two levels of interdependence between human and robot (high vs. low), since mutual dependency might influence which communication style is best. Participants collaborated with a virtual robot during two simulated search and rescue tasks varying in their level of interdependence. Results confirm that in general robot communication results in more trust in and understanding of the robot, while showing no evidence of a higher workload when the robot communicates or adds explanations to being transparent. Providing explanations, however, did result in more reliance on RescueBot. Furthermore, compared to being silent, only being explainable results a higher situation awareness when interdependence is high. Results further show that being highly interdependent decreases trust, reliance, and team performance while increasing workload and situation awareness. High interdependence also increases human communication if the robot is not silent, human rescue contribution if the robot does not provide explanations, and the strength of the positive association between situation awareness and team performance. From these results, we can conclude that robot communication is crucial for human-robot teamwork, and that important differences exist between being transparent, explainable, or adaptive. Our findings also highlight the fundamental importance of interdependence in studies on explainability in robots. Copyright © 2022 Verhagen, Neerincx and Tielman.
KW  - communication
KW  - explainability
KW  - explainable AI
KW  - human-agent teaming
KW  - human-robot teamwork
KW  - interdependence
KW  - transparency
KW  - user study
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 15
ER  -

TY  - CONF
AU  - Barnhoorn, J.S.
AU  - Rakhorst-Oudendijk, M.
AU  - Veltman, K.
AU  - Holleman, B.
AU  - Haije, T.
AU  - Wolbers, J.
AU  - Janssen, J.
TI  - IMMENS: Integrated Multi-Manager Environment for Naval Ships
PY  - 2022
T2  - Proceedings of the International Ship Control Systems Symposium
DO  - 10.24868/10732
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187463474&doi=10.24868%2f10732&partnerID=40&md5=5c515c263bd4bd19411b0a5650853a43
AB  - The Royal Netherlands Navy is moving towards more autonomous ships to cope with the increasingly complex naval operations while sailing with a reduced manning. The strong expansion of automation and autonomous systems on board requires new solutions for optimal collaboration between the crew and the extensive set of ship systems. We are developing a concept for an Integrated Multi-Manager Environment for Naval Ships: IMMENS. IMMENS aims to be an intelligent ship system that supports the crews of future ship classes, while enabling these crews to exercise and maintain meaningful human control. The ship’s subsystems are integrated in a multi-agent system that formulates and proposes plans to the command team based on an internal representation of the ship’s goals and the current situation. These plans consist of actions that are to be executed in order to achieve the mission goals. The system’s computational design, combined with specific human-machine teaming functionality facilitates the crew and IMMENS to work as a joint cognitive system. This paper shortly introduces IMMENS but reports mainly on the development and application of humanmachine teaming concepts that support cooperation between the crew and IMMENS. The first concept offers interaction based on a human-machine language using Goals, Tasks, Constraints and Resources as shared knowledge elements. Since both human and machine understand these elements, human comprehension of the machine's inner workings is supported. The second concept provides the explication of relationships between goals, tasks, constraints, resources and/or external world as a means of intuitively explaining the machine's reasoning by providing transparency regarding, for instance, to which goals certain tasks contribute. Thirdly, we applied and extended on the concept of play-based delegation. A play is a plan-template that allows a user to quickly delegate a complex task to the systems, without the need to drill down, while leaving room for system intelligence to fill in the details. These concepts have been applied in an integrated interactive demonstrator of IMMENS. An evaluation with subject matter experts from the Royal Netherlands Navy and senior human factors researchers was conducted based on a scenario of a frigate escorting a high-value asset. During the design, implementation and evaluation of the demonstrator, we learned that the first concept provided a workable shared knowledge model and indeed fitted with the IMMENS architecture on the one hand, and was intuitive to the users on the other hand. Explicating relationships provided the users with relevant insights, allowing them to understand, for instance, by means of which tasks and resources IMMENS expects to achieve the mission goals. Play-based delegation was found this a clear and valuable concept. Many opportunities for future research were identified, the most important of which concerns the feasibility of constructing a valid, complete and generic goal representation for IMMENS that takes into account the complexity of future missions and operational contexts, and includes the necessary world model. © 2022, Institute of Marine Engineering Science & Technology. All rights reserved.
KW  - Autonomous Systems
KW  - Explainable AI
KW  - Human-machine Teaming
KW  - Multi-Agent System
KW  - Transparency
KW  - Utility-based Reasoning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Field, J.R.
TI  - Exploring Trust and Explainability of Unmanned Systems
PY  - 2023
T2  - OCEANS 2023 - Limerick, OCEANS Limerick 2023
DO  - 10.1109/OCEANSLimerick52467.2023.10244349
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173671622&doi=10.1109%2fOCEANSLimerick52467.2023.10244349&partnerID=40&md5=3f2dfff8b663be96db2cd2de055e6a6a
AB  - Explainability (often referred to as interpretability) refers to the concept of providing context to an AI/ML model and its output, thereby assisting a human user in understanding the system's decision-making process. The concept of explainability is especially helpful when we consider the high cognitive load and intense data management strategies that are required for current human-in-the-loop operations in use today. The work presented here aims to provide an explainability framework for autonomous systems, to provide system transparency, and enhance operator awareness. This work served to develop a novel method of sorting and evaluating data streams taken from an operational system, to filter and transmit data packages based on mission conditions. Post mission analysis yielded apparent trends in messaging hierarchy, indicating that certain health and status data streams were consistently prioritized, regardless of the pre-defined metrics. Additional data analysis was performed to evaluate sensor outputs with respect to health and status messaging. This process included conducting data correlation and data characterization, to evaluate relationships between data streams, identify data associated with nominal behavior, and perform anomaly detection. Key functional categories were developed, in which the system's behavior is mapped to a corresponding component (and its respective data stream). Monitoring subsystem performance assists with cross-referencing sensor outputs, to confirm data projections and/or aid in identifying faulty readings. Furthermore, the application of anomaly detection algorithms is coupled with data correlation and/or pattern recognition to extract the most important and salient information. © 2023 IEEE.
KW  - Autonomy
KW  - Explainability
KW  - UUV
KW  - Anomaly detection
KW  - Information management
KW  - Pattern recognition
KW  - Autonomy
KW  - Data correlations
KW  - Data stream
KW  - Decision-making process
KW  - Explainability
KW  - Human users
KW  - Interpretability
KW  - Sensor output
KW  - Unmanned system
KW  - UUV
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Gregory, J.M.
AU  - Sanchez, F.
AU  - Lancaster, E.
AU  - Agha-Mohammadi, A.-A.
AU  - Gupta, S.K.
TI  - Using Decision Support in Human-in-the-Loop Experimental Design Toward Building Trustworthy Autonomous Systems
PY  - 2023
T2  - IEEE International Workshop on Robot and Human Communication, RO-MAN
SP  - 205
EP  - 212
DO  - 10.1109/RO-MAN57019.2023.10309571
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187021369&doi=10.1109%2fRO-MAN57019.2023.10309571&partnerID=40&md5=5b1d46dc3a4139956e0792c179218878
AB  - Experimental design of autonomous systems involves defining experimental inputs to maximize the experimenter's information gained, minimize costs, and balance risk. This effectively leads to improved understanding and trustworthiness, which are necessary for deployment in realworld settings. Since experimental design is inherently a human-in-the-loop, sequential decision making problem, and decisions are being made about complex systems, an investigation into decision-making quality and decision-supporting methods is warranted. In this work, we investigate a decision support system (DSS) to augment the human's experimental design decision making abilities, and conduct an exploratory user study to investigate the potential for decision support. Our findings show that experimenters, including experienced field roboticists, make suboptimal decisions and mistakes during the experimental design process, which suggests robotics research could benefit from DSSs. Our proposed DSS shows promise in some select aspects of experimental design, including helping to reduce suboptimal decisions, and participants in the user study reported favorable opinions of using such a system, including a sense of usefulness and lack of burden. The broader implication of this work is the identification of decision support in experimental design as one way to help bridge the gap between academia and industry by way of accelerated, informative experimentation and increased system explainability. © 2023 IEEE.
KW  - Artificial intelligence
KW  - Bridges
KW  - Decision making
KW  - Design
KW  - Statistics
KW  - Decision supporting
KW  - Decision supports
KW  - Decision-making problem
KW  - Decisions makings
KW  - Design decision-making
KW  - Design-process
KW  - Human-in-the-loop
KW  - Real-world
KW  - Sequential decision making
KW  - User study
KW  - Decision support systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Würfel, J.
AU  - Papenfuß, A.
AU  - Wies, M.
TI  - Operationalizing AI Explainability Using Interpretability Cues in the Cockpit: Insights from User-Centered Development of the Intelligent Pilot Advisory System (IPAS)
PY  - 2024
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 14734 LNAI
SP  - 297
EP  - 315
DO  - 10.1007/978-3-031-60606-9_17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196158745&doi=10.1007%2f978-3-031-60606-9_17&partnerID=40&md5=7f8ef7e5b6070033b713562ae405ff88
AB  - This paper presents a concept for operationalizing Artificial Intelligence (AI) explainability for the Intelligent Pilot Advisory System (IPAS) as requested in the European Aviation Safety Agency’s AI Roadmap 2.0 in order to meet the requirement of Trustworthy AI. The IPAS is currently being developed to provide AI-based decision support in commercial aircraft to assist the flight crew, especially in emergency situations. The development of the IPAS is following a user-centred and exploratory design approach, with the active involvement of airline pilots in the early stages of development to iteratively tailor the system to their requirements. The concept presented in this paper aims to provide interpretability cues to achieve “operational explainability of AI”, which should enable commercial aircraft pilots to understand and adequately trust the recommendations generated by AI when making decisions in emergencies. Focus of the research was to identify initial interpretability requirements and to answer the question of what interpretation cues pilots need from the AI-based system. Based on a user study with airline pilots, four requirements for interpretation cues were formulated. These results will form the basis for the next iteration of the IPAS, where the requirements will be implemented. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
KW  - Ethical and Trustworthy AI
KW  - Explainable AI
KW  - Human-AI Teaming
KW  - Human-Centered AI
KW  - Interpretable AI
KW  - Cockpits (aircraft)
KW  - User centered design
KW  - Advisory systems
KW  - Airline pilots
KW  - Commercial aircraft
KW  - Ethical and trustworthy artificial intelligence
KW  - Explainable artificial intelligence
KW  - Human-artificial intelligence teaming
KW  - Human-centered artificial intelligence
KW  - Interpretability
KW  - Interpretable artificial intelligence
KW  - User-centered development
KW  - Decision support systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Yamin, P.A.R.
AU  - Park, J.
AU  - Kim, H.K.
TI  - Towards a human-machine interface guidance for in-vehicle augmented reality head-up displays
PY  - 2021
T2  - ICIC Express Letters
VL  - 15
IS  - 12
SP  - 1313
EP  - 1318
DO  - 10.24507/icicel.15.12.1313
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118996430&doi=10.24507%2ficicel.15.12.1313&partnerID=40&md5=76bd281ea0710ca16b6b2e1d15a03a1f
AB  - Recently Augmented-Reality Head-Up Displays (AR-HUDs) has emerged as a next evolution of in-vehicle display technologies. Augmented image should be overlayed onto real-world objects providing alerts that can be viewed in the driver’s line of sight. However, there are currently no guidelines that apply specifically to the Human-Machine Interface (HMI) for AR-HUDs. A review on existing literature on AR-HUD and applicable human factors and human-computer interaction guidelines was conducted, using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) approach. Based on the literature, an initial set of guidelines for developing AR-HUD was derived. The guidelines are grouped into virtual image distance and field of view, brightness, projected information, and legibility. Research gaps are also discussed for future experiments. Taken together, this study is a starting point for developing the interface of AR-HUDs. © 2021 ICIC International. All rights reserved.
KW  - AR-HUD
KW  - Guideline
KW  - HMI
KW  - In-vehicle
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Alzubaidi, L.
AU  - Al-Sabaawi, A.
AU  - Bai, J.
AU  - Dukhan, A.
AU  - Alkenani, A.H.
AU  - Al-Asadi, A.
AU  - Alwzwazy, H.A.
AU  - Manoufali, M.
AU  - Fadhel, M.A.
AU  - Albahri, A.S.
AU  - Moreira, C.
AU  - Ouyang, C.
AU  - Zhang, J.
AU  - Santamaría, J.
AU  - Salhi, A.
AU  - Hollman, F.
AU  - Gupta, A.
AU  - Duan, Y.
AU  - Rabczuk, T.
AU  - Abbosh, A.
AU  - Gu, Y.
TI  - Towards Risk-Free Trustworthy Artificial Intelligence: Significance and Requirements
PY  - 2023
T2  - International Journal of Intelligent Systems
VL  - 2023
C7  - 4459198
DO  - 10.1155/2023/4459198
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176242263&doi=10.1155%2f2023%2f4459198&partnerID=40&md5=47549faf5f791af77e087a395a997f8b
AB  - Given the tremendous potential and influence of artificial intelligence (AI) and algorithmic decision-making (DM), these systems have found wide-ranging applications across diverse fields, including education, business, healthcare industries, government, and justice sectors. While AI and DM offer significant benefits, they also carry the risk of unfavourable outcomes for users and society. As a result, ensuring the safety, reliability, and trustworthiness of these systems becomes crucial. This article aims to provide a comprehensive review of the synergy between AI and DM, focussing on the importance of trustworthiness. The review addresses the following four key questions, guiding readers towards a deeper understanding of this topic: (i) why do we need trustworthy AI? (ii) what are the requirements for trustworthy AI? In line with this second question, the key requirements that establish the trustworthiness of these systems have been explained, including explainability, accountability, robustness, fairness, acceptance of AI, privacy, accuracy, reproducibility, and human agency, and oversight. (iii) how can we have trustworthy data? and (iv) what are the priorities in terms of trustworthy requirements for challenging applications? Regarding this last question, six different applications have been discussed, including trustworthy AI in education, environmental science, 5G-based IoT networks, robotics for architecture, engineering and construction, financial technology, and healthcare. The review emphasises the need to address trustworthiness in AI systems before their deployment in order to achieve the AI goal for good. An example is provided that demonstrates how trustworthy AI can be employed to eliminate bias in human resources management systems. The insights and recommendations presented in this paper will serve as a valuable guide for AI researchers seeking to achieve trustworthiness in their applications.  © 2023 Laith Alzubaidi et al.
KW  - 5G mobile communication systems
KW  - Decision making
KW  - Engineering education
KW  - Environmental technology
KW  - Health care
KW  - Algorithmics
KW  - Decisions makings
KW  - Diverse fields
KW  - Healthcare industry
KW  - Human oversight
KW  - Industry government
KW  - Intelligence agencies
KW  - Reproducibilities
KW  - Risk free
KW  - Wide-ranging applications
KW  - Artificial intelligence
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 43
ER  -

TY  - CONF
AU  - Ibrahim, L.
AU  - Ghassemi, M.M.
AU  - Alhanai, T.
TI  - Do Explanations Improve the Quality of AI-assisted Human Decisions? An Algorithm-in-the-Loop Analysis of Factual & Counterfactual Explanations
PY  - 2023
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
VL  - 2023-May
SP  - 326
EP  - 334
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171266753&partnerID=40&md5=f890b122821d48e52559780201d1ab3e
AB  - The increased use of AI algorithmic aids in high-stakes decision making has prompted interest in explainable AI (xAI), and the role of counterfactual explanations to increase trust in human-algorithm collaborations and to mitigate unfair outcomes. However, research is limited in understanding how explainable AI improves human decision-making. We conduct an online experiment with 559 participants, utilizing an “algorithm-in-the-loop" framework and real-world pre-trial data to investigate how explanations of algorithmic pretrial risk assessments generated from state-of-the-art machine learning explanation methods (counterfactual explanations via DiCE & factual explanations via SHAP) influences the quality of decision-makers' assessment of recidivism. Our results show that counterfactual and factual explanations achieve different desirable goals (separately improve human assessment of model accuracy, fairness, and calibration), yet still fall short of improving the combined accuracy, fairness, and reliability of human predictions - reinstating the need for sociotechnical, empirical evaluations in xAI. We conclude with user feedback on DiCE counterfactual explanations, as well as a discussion of the broader implications of our results to AI-assisted decision-making and xAI. © 2023 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.
KW  - counterfactuals
KW  - explanations
KW  - fairness
KW  - risk assessments
KW  - sociotechnical systems
KW  - trust
KW  - user studies
KW  - Autonomous agents
KW  - Decision making
KW  - Learning algorithms
KW  - Machine learning
KW  - Multi agent systems
KW  - Algorithmics
KW  - Counterfactuals
KW  - Decisions makings
KW  - Explanation
KW  - Fairness
KW  - Human decisions
KW  - Risks assessments
KW  - Sociotechnical systems
KW  - Trust
KW  - User study
KW  - Risk assessment
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Gerogiannis, D.
AU  - Arsenos, A.
AU  - Kollias, D.
AU  - Nikitopoulos, D.
AU  - Kollias, S.
TI  - Covid-19 Computer-Aided Diagnosis through AI-Assisted CT Imaging Analysis: Deploying a Medical AI System
PY  - 2024
T2  - Proceedings - International Symposium on Biomedical Imaging
DO  - 10.1109/ISBI56570.2024.10635484
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197560143&doi=10.1109%2fISBI56570.2024.10635484&partnerID=40&md5=40f239ee71d30ab580c5b01f1d0cf50c
AB  - Computer-aided diagnosis (CAD) systems stand out as potent aids for physicians in identifying the novel Coronavirus Disease 2019 (COVID-19) through medical imaging modalities. In this paper, we showcase the integration and reliable and fast deployment of a state-of-the-art AI system designed to automatically analyze CT images, offering infection probability for the swift detection of COVID-19. The suggested system, comprising both classification and segmentation components, is anticipated to reduce physicians' detection time and enhance the overall efficiency of COVID-19 detection. We successfully surmounted various challenges, such as data discrepancy and anonymisation, testing the time-effectiveness of the model, and data security, enabling reliable and scalable deployment of the system on both cloud and edge environments. Additionally, our AI system assigns a probability of infection to each 3D CT scan and enhances explainability through anchor set similarity, facilitating timely confirmation and segregation of infected patients by physicians. © 2024 IEEE.
KW  - Cloud
KW  - COVID-19 Diagnosis
KW  - Deep Learning
KW  - Edge
KW  - Medical Imaging
KW  - Microservices
KW  - Sandbox
KW  - Computerized tomography
KW  - Coronavirus
KW  - Image segmentation
KW  - AI systems
KW  - Computer-aided
KW  - Coronavirus disease 2019 diagnose
KW  - Coronaviruses
KW  - CT imaging
KW  - Deep learning
KW  - Edge
KW  - Imaging analysis
KW  - Microservice
KW  - Sandbox
KW  - COVID-19
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Angerschmid, A.
AU  - Zhou, J.
AU  - Theuermann, K.
AU  - Chen, F.
AU  - Holzinger, A.
TI  - Fairness and Explanation in AI-Informed Decision Making
PY  - 2022
T2  - Machine Learning and Knowledge Extraction
VL  - 4
IS  - 2
SP  - 556
EP  - 579
DO  - 10.3390/make4020026
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137274188&doi=10.3390%2fmake4020026&partnerID=40&md5=57f74bc2e09b676fabeace5fd46299ce
AB  - AI-assisted decision-making that impacts individuals raises critical questions about transparency and fairness in artificial intelligence (AI). Much research has highlighted the reciprocal relationships between the transparency/explanation and fairness in AI-assisted decision-making. Thus, considering their impact on user trust or perceived fairness simultaneously benefits responsible use of socio-technical AI systems, but currently receives little attention. In this paper, we investigate the effects of AI explanations and fairness on human-AI trust and perceived fairness, respectively, in specific AI-based decision-making scenarios. A user study simulating AI-assisted decision-making in two health insurance and medical treatment decision-making scenarios provided important insights. Due to the global pandemic and restrictions thereof, the user studies were conducted as online surveys. From the participant’s trust perspective, fairness was found to affect user trust only under the condition of a low fairness level, with the low fairness level reducing user trust. However, adding explanations helped users increase their trust in AI-assisted decision-making. From the perspective of perceived fairness, our work found that low levels of introduced fairness decreased users’ perceptions of fairness, while high levels of introduced fairness increased users’ perceptions of fairness. The addition of explanations definitely increased the perception of fairness. Furthermore, we found that application scenarios influenced trust and perceptions of fairness. The results show that the use of AI explanations and fairness statements in AI applications is complex: we need to consider not only the type of explanations and the degree of fairness introduced, but also the scenarios in which AI-assisted decision-making is used. © 2022 by the authors.
KW  - AI ethics
KW  - AI explanation
KW  - AI fairness
KW  - perception of fairness
KW  - trust
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 95
ER  -

TY  - JOUR
AU  - Cameron, S.
AU  - Franks, P.
AU  - Hamidzadeh, B.
TI  - Positioning Paradata: A Conceptual Frame for AI Processual Documentation in Archives and Recordkeeping Contexts
PY  - 2023
T2  - Journal on Computing and Cultural Heritage
VL  - 16
IS  - 4
C7  - 75
DO  - 10.1145/3594728
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183042379&doi=10.1145%2f3594728&partnerID=40&md5=a6bcd8795bf5d18eed1f04d6e9ed72b2
AB  - The emergence of sophisticated Artificial Intelligence (AI) and machine learning tools poses a challenge to archives and records professionals, who are accustomed to understanding and documenting the activities of human agents rather than the often-opaque processes of sophisticated AI functioning. Preliminary work has proposed the term paradata to describe the unique documentation needs that emerge for archivists using AI tools to process records in their collections. For the purposes of archivists working with AI, paradata is conceptualized here as information recorded and preserved about records’ processing with AI tools; it is a category of data that is defined both by its relationship with other datasets and by the documentary purpose it serves. This article surveys relevant literature across three contexts to scope the relevant scholarship that archivists may draw upon to develop appropriate AI documentation practices. From the statistical social sciences and the visual heritage fields, the article discusses existing definitions of paradata and its ambiguous, often contextually dependent relationship with existing metadata categories. Approaching the problem from a sociotechnical perspective, literature on Explainable Artificial Intelligence (XAI) insists pointedly that explainability be attuned to specific users’ stated needs—needs that archivists may better articulate using the framework of paradata. Most importantly, the article situates AI as a challenge to accountability, transparency, and impartiality in archives by introducing an unfamiliar non-human agency, one that pushes the limits of existing archival practice and demands the development of new concepts and vocabularies to shape future technological and methodological developments in archives. © 2023 Copyright held by the owner/author(s).
KW  - accountability
KW  - archives
KW  - Explainable Artificial Intelligence
KW  - metadata
KW  - Paradata
KW  - processual documentation
KW  - records
KW  - records management
KW  - XAI
KW  - Artificial intelligence
KW  - Records management
KW  - Accountability
KW  - Archive
KW  - Artificial intelligence tools
KW  - Explainable artificial intelligence
KW  - Paradata
KW  - Processual documentation
KW  - Record
KW  - Record keeping
KW  - Record management
KW  - XAI
KW  - Metadata
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 12
ER  -

TY  - CONF
AU  - Graefe, J.
AU  - Rittger, L.
AU  - Carollo, G.
AU  - Engelhardt, D.
AU  - Bengler, K.
TI  - Evaluating the Potential of Interactivity in Explanations for User-Adaptive In-Vehicle Systems – Insights from a Real-World Driving Study
PY  - 2023
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 14057 LNCS
SP  - 294
EP  - 312
DO  - 10.1007/978-3-031-48047-8_19
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178505297&doi=10.1007%2f978-3-031-48047-8_19&partnerID=40&md5=eec3473667ae22a3fe70b132f24dd90e
AB  - Due to advances in artificial intelligence (AI), humans are increasingly facing algorithm-generated content in everyday applications. To avoid threads to the system’s transparency and trustworthiness, the approach of explainable AI (XAI) will play an important role when designing these systems tied to the needs and characteristics of their end-users. Our work investigates explanation strategies for AI-based adaptive in-vehicle systems from a human-centered point of view. We present two explanation concepts: one interactive and one text-based approach. The concepts were evaluated and compared in a real-world driving study with 36 participants. The aim is to assess whether interactive engagement with explanations fosters the system’s understandability and the user’s mental model. Our results did not show significant differences between the concepts. Both groups performed well when assessing their mental model after experiencing the explanation concept. However, we found significant decreases in the mental model when measuring it again after participants experienced the prototypical adaptations of the system during the test drive. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
KW  - Automotive User Interfaces
KW  - Human-AI Interaction
KW  - Human-Centered Explainable AI
KW  - Real-World Driving Study
KW  - User-Adaptive Systems
KW  - Adaptive systems
KW  - Cognitive systems
KW  - Automotive user interface
KW  - Automotives
KW  - Human-artificial intelligence interaction
KW  - Human-centered explainable artificial intelligence
KW  - In-vehicle systems
KW  - Interactivity
KW  - Mental model
KW  - Real-world driving study
KW  - Real-world drivings
KW  - User-adaptive systems
KW  - User interfaces
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Razavi, S.
TI  - Deep learning, explained: Fundamentals, explainability, and bridgeability to process-based modelling
PY  - 2021
T2  - Environmental Modelling and Software
VL  - 144
C7  - 105159
DO  - 10.1016/j.envsoft.2021.105159
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114319353&doi=10.1016%2fj.envsoft.2021.105159&partnerID=40&md5=3d45ea302786d3802f3dfbc27e8e22b8
AB  - Recent breakthroughs in artificial intelligence (AI), and particularly in deep learning (DL), have created tremendous excitement and opportunities in the earth and environmental sciences communities. To leverage these new ‘data-driven’ technologies, however, one needs to understand the fundamental concepts that give rise to DL and how they differ from ‘process-based’, mechanistic modelling. This paper revisits those fundamentals and addresses 10 questions that might be posed by earth and environmental scientists, and with the aid of a real-world modelling experiment, it explains some critical, but often ignored, issues DL may face in practice. The overarching objective is to contribute to a future of AI-assisted earth and environmental sciences where AI models can (1) embrace the typically ignored knowledge base available, (2) function credibly in ‘true’ out-of-sample prediction, and (3) handle non-stationarity in earth and environmental systems. Comparing and contrasting earth and environmental problems with prominent AI applications, such as playing chess and trading in stock markets, provides critical insights for better directing future research in this field. © 2021 The Author(s)
KW  - Artificial intelligence
KW  - Artificial neural networks
KW  - Deep learning
KW  - Earth systems
KW  - Hydrology
KW  - Machine learning
KW  - Process-based modelling
KW  - Commerce
KW  - Deep neural networks
KW  - Data driven
KW  - Deep learning
KW  - Earth systems
KW  - Environmental science
KW  - Environmental scientists
KW  - Fundamental concepts
KW  - Mechanistic models
KW  - Process-based
KW  - Process-based modeling
KW  - Science community
KW  - artificial intelligence
KW  - environmental issue
KW  - numerical model
KW  - stock market
KW  - Knowledge based systems
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 116
ER  -

TY  - JOUR
AU  - Le Guillou, M.
AU  - Prévot, L.
AU  - Berberian, B.
TI  - Bringing Together Ergonomic Concepts and Cognitive Mechanisms for Human—AI Agents Cooperation
PY  - 2023
T2  - International Journal of Human-Computer Interaction
VL  - 39
IS  - 9
SP  - 1827
EP  - 1840
DO  - 10.1080/10447318.2022.2129741
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139789673&doi=10.1080%2f10447318.2022.2129741&partnerID=40&md5=cdc3aeac5db43885c96ebc8adc9ad32d
AB  - The deployment of artificial intelligence from experimental settings to concrete applications implies to consider the social aspects of the environment and consequently to conceive the interaction between humans and computers endowed with the aim of being partners in action. This article proposes a review of the research initiatives regarding human-artificial agents interaction, including eXplainable Artificial Intelligence (XAI) and HRI/HCI. We argue that even if vocabulary and approaches are different, the concepts converge on the necessity for the artificial agents to provide an accurate mental model of their behavior to the humans they are interacting with. This has different implications depending on whether we consider a tool/user interaction or a cooperation interaction—which is far less documented despite being at the heart of the future concepts of autonomous vehicles. From this observation, the article uses the cognitive science corpus on joint-action to raise finer cognitive mechanisms proved to be essential for human joint-action which could be considered as cognitive requirements for future artificial agents, including shared task representation and mentalization. Finally, interactions content hypotheses are arisen to satisfy the identified mechanisms, including the ability for the artificial agent to elicit its intentions and to trigger mentalization toward them from the human cooperators. © 2022 Taylor & Francis Group, LLC.
KW  - Artificial intelligence
KW  - Agent cooperation
KW  - Agent interaction
KW  - Artificial agents
KW  - Autonomous Vehicles
KW  - Cognitive mechanisms
KW  - Concrete applications
KW  - Joint actions
KW  - Mental model
KW  - Research initiatives
KW  - User interaction
KW  - Social aspects
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Omeiza, D.
AU  - Webb, H.
AU  - Jirotka, M.
AU  - Kunze, L.
TI  - Explanations in Autonomous Driving: A Survey
PY  - 2022
T2  - IEEE Transactions on Intelligent Transportation Systems
VL  - 23
IS  - 8
SP  - 10142
EP  - 10162
DO  - 10.1109/TITS.2021.3122865
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136123594&doi=10.1109%2fTITS.2021.3122865&partnerID=40&md5=67567b47dbe26b44bd98b02c7dd55c1d
AB  - The automotive industry has witnessed an increasing level of development in the past decades; from manufacturing manually operated vehicles to manufacturing vehicles with a high level of automation. With the recent developments in Artificial Intelligence (AI), automotive companies now employ blackbox AI models to enable vehicles to perceive their environment and make driving decisions with little or no input from a human. With the hope to deploy autonomous vehicles (AV) on a commercial scale, the acceptance of AV by society becomes paramount and may largely depend on their degree of transparency, trustworthiness, and compliance with regulations. The assessment of the compliance of AVs to these acceptance requirements can be facilitated through the provision of explanations for AVs' behaviour. Explainability is therefore seen as an important requirement for AVs. AVs should be able to explain what they have 'seen', done, and might do in environments in which they operate. In this paper, we provide a comprehensive survey of the existing work in explainable autonomous driving. First, we open by providing a motivation for explanations by highlighting the importance of transparency, accountability, and trust in AVs; and examining existing regulations and standards related to AVs. Second, we identify and categorise the different stakeholders involved in the development, use, and regulation of AVs and elicit their AV explanation requirements. Third, we provide a rigorous review of previous work on explanations for the different AV operations (i.e., perception, localisation, planning, vehicle control, and system management). Finally, we discuss pertinent challenges and provide recommendations including a conceptual framework for AV explainability. This survey aims to provide the fundamental knowledge required of researchers who are interested in explanation provisions in autonomous driving.  © 2000-2011 IEEE.
KW  - accountability
KW  - autonomous vehicles
KW  - explainable AI
KW  - Explanations
KW  - human-machine interaction
KW  - intelligent vehicles
KW  - regulations
KW  - robotics
KW  - standards
KW  - trust
KW  - Automotive industry
KW  - Commercial vehicles
KW  - Control system synthesis
KW  - Human computer interaction
KW  - Human robot interaction
KW  - Manufacture
KW  - Surveys
KW  - Transparency
KW  - Accountability
KW  - Automotive companies
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Explainable artificial intelligence
KW  - Explanation
KW  - Human machine interaction
KW  - Levels of automation
KW  - Regulation
KW  - Trust
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 176
ER  -

TY  - JOUR
AU  - Chen, C.-L.
AU  - Golubchik, L.
AU  - Pal, R.
TI  - Achieving Transparency Report Privacy in Linear Time
PY  - 2022
T2  - Journal of Data and Information Quality
VL  - 14
IS  - 2
C7  - 8
DO  - 10.1145/3460001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129254825&doi=10.1145%2f3460001&partnerID=40&md5=51ef40acd74172e80f64f4af11290d4c
AB  - An accountable algorithmic transparency report (ATR) should ideally investigate (a) transparency of the underlying algorithm, and (b) fairness of the algorithmic decisions, and at the same time preserve data subjects' privacy. However, a provably formal study of the impact to data subjects' privacy caused by the utility of releasing an ATR (that investigates transparency and fairness), has yet to be addressed in the literature. The far-fetched benefit of such a study lies in the methodical characterization of privacy-utility trade-offs for release of ATRs in public, and their consequential application-specific impact on the dimensions of society, politics, and economics. In this paper, we first investigate and demonstrate potential privacy hazards brought on by the deployment of transparency and fairness measures in released ATRs. To preserve data subjects' privacy, we then propose a linear-time optimal-privacy scheme, built upon standard linear fractional programming (LFP) theory, for announcing ATRs, subject to constraints controlling the tolerance of privacy perturbation on the utility of transparency schemes. Subsequently, we quantify the privacy-utility trade-offs induced by our scheme, and analyze the impact of privacy perturbation on fairness measures in ATRs. To the best of our knowledge, this is the first analytical work that simultaneously addresses trade-offs between the triad of privacy, utility, and fairness, applicable to algorithmic transparency reports.  © 2022 Association for Computing Machinery.
KW  - algorithmic transparency
KW  - fairness
KW  - linear fractional programming
KW  - Privacy
KW  - Commerce
KW  - Data privacy
KW  - Economic and social effects
KW  - Linear programming
KW  - Algorithmic transparency
KW  - Algorithmics
KW  - Data subjects
KW  - Fairness
KW  - Fairness measures
KW  - Formal studies
KW  - Linear fractional programming
KW  - Linear time
KW  - Privacy
KW  - Trade off
KW  - Transparency
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Jarrahi, M.H.
AU  - Davoudi, V.
AU  - Haeri, M.
TI  - The key to an effective AI-powered digital pathology: Establishing a symbiotic workflow between pathologists and machine
PY  - 2022
T2  - Journal of Pathology Informatics
VL  - 13
C7  - 100156
DO  - 10.1016/j.jpi.2022.100156
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142832102&doi=10.1016%2fj.jpi.2022.100156&partnerID=40&md5=9e155f5b9efec4f80a300725da4a4920
AB  - Pathology is a fundamental element of modern medicine that determines the final diagnosis of medical conditions, leads medical decisions, and portrays the prognosis. Due to continuous improvements in AI capabilities (e.g., object recognition and image processing), intelligent systems are bound to play a key role in augmenting pathology research and clinical practices. Despite the pervasive deployment of computational approaches in similar fields such as radiology, there has been less success in integrating AI in clinical practices and histopathological diagnosis. This is partly due to the opacity of end-to-end AI systems, which raises issues of interoperability and accountability of medical practices. In this article, we draw on interactive machine learning to take advantage of AI in digital pathology to open the black box of AI and generate a more effective partnership between pathologists and AI systems based on the metaphors of parameterization and implicitization. © 2022 The Authors
KW  - Artificial intelligence
KW  - Computational pathology
KW  - Digital pathology
KW  - Explainable AI
KW  - Human-in-the-loop
KW  - Image analysis
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Tinguely, P.N.
AU  - Lee, J.
AU  - He, V.F.
TI  - Designing human resource management systems in the age of AI
PY  - 2023
T2  - Journal of Organization Design
VL  - 12
IS  - 4
SP  - 263
EP  - 269
DO  - 10.1007/s41469-023-00153-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172083461&doi=10.1007%2fs41469-023-00153-x&partnerID=40&md5=ebce8ffe6b6bcb432ef71089218bf273
AB  - The increasing adoption of artificial intelligence (AI) is reshaping the practices of human resource management (HRM). We propose a typology of HR–AI collaboration systems across the dimensions of task characteristics (routine vs. non-routine; low vs. high cognitive complexity) and social acceptability of such systems among organizational members. We discuss how organizations should design HR–AI collaboration systems in light of issues of AI explainability, high stakes contexts, and threat to employees’ professional identities. We point out important design considerations that may affect employees' perceptions of organizational fairness and emphasize HR professionals' role in the design process. We conclude by discussing how our Point of View article contributes to literatures on organization design and human–AI collaboration and suggesting potential avenues for future research. © 2023, The Author(s).
KW  - Artificial intelligence
KW  - Human resource management
KW  - Organization design
KW  - Organizational fairness
KW  - Social acceptability
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 6
ER  -

TY  - CONF
AU  - Lai, V.
AU  - Carton, S.
AU  - Bhatnagar, R.
AU  - Liao, Q.V.
AU  - Zhang, Y.
AU  - Tan, C.
TI  - Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation
PY  - 2022
T2  - Conference on Human Factors in Computing Systems - Proceedings
C7  - 54
DO  - 10.1145/3491102.3501999
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130569803&doi=10.1145%2f3491102.3501999&partnerID=40&md5=3b52304ff699f31c806124cc5df96f7b
AB  - Despite impressive performance in many benchmark datasets, AI models can still make mistakes, especially among out-of-distribution examples. It remains an open question how such imperfect models can be used effectively in collaboration with humans. Prior work has focused on AI assistance that helps people make individual high-stakes decisions, which is not scalable for a large amount of relatively low-stakes decisions, e.g., moderating social media comments. Instead, we propose conditional delegation as an alternative paradigm for human-AI collaboration where humans create rules to indicate trustworthy regions of a model. Using content moderation as a testbed, we develop novel interfaces to assist humans in creating conditional delegation rules and conduct a randomized experiment with two datasets to simulate in-distribution and out-of-distribution scenarios. Our study demonstrates the promise of conditional delegation in improving model performance and provides insights into design for this novel paradigm, including the effect of AI explanations. © 2022 Owner/Author.
KW  - Artificial intelligence
KW  - Benchmark datasets
KW  - Case-studies
KW  - Imperfect modeling
KW  - Large amounts
KW  - Modeling performance
KW  - Performance
KW  - Randomized experiments
KW  - Social media
KW  - Benchmarking
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 95
ER  -

TY  - JOUR
AU  - Beltrán, S.
AU  - Castro, A.
AU  - Irizar, I.
AU  - Naveran, G.
AU  - Yeregui, I.
TI  - Framework for collaborative intelligence in forecasting day-ahead electricity price
PY  - 2022
T2  - Applied Energy
VL  - 306
C7  - 118049
DO  - 10.1016/j.apenergy.2021.118049
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118701086&doi=10.1016%2fj.apenergy.2021.118049&partnerID=40&md5=e30a237410f3d5de91592efe2cb9ba96
AB  - Electricity price forecasting in wholesale markets is an essential asset for deciding bidding strategies and operational schedules. The decision making process is limited if no understanding is given on how and why such electricity price points have been forecast. The present article proposes a novel framework that promotes human–machine collaboration in forecasting day-ahead electricity price in wholesale markets. The framework is based on a new model architecture that uses a plethora of statistical and machine learning models, a wide range of exogenous features, a combination of several time series decomposition methods and a collection of time series characteristics based on signal processing and time series analysis methods. The model architecture is supported by open-source automated machine learning platforms that provide a baseline reference used for comparison purposes. The objective of the framework is not only to provide forecasts, but to promote a human-in-the-loop approach by providing a data story based on a collection of model-agnostic methods aimed at interpreting the mechanisms and behavior of the new model architecture and its predictions. The framework has been applied to the Spanish wholesale market. The forecasting results show good accuracy on mean absolute error (1.859, 95% HDI [0.575, 3.924] EUR(MWh)−1) and mean absolute scaled error (0.378, 95% HDI [0.091, 0.934]). Moreover, the framework demonstrates its human-centric capabilities by providing graphical and numeric explanations that augments understanding on the model and its electricity price point forecasts. © 2021 The Authors
KW  - Augmented analytics
KW  - Automated machine learning
KW  - Ensemble models
KW  - Explainable artificial intelligence
KW  - Time series decomposition
KW  - Time series hybrid models
KW  - Decision making
KW  - Machine learning
KW  - Power markets
KW  - Signal processing
KW  - Time series analysis
KW  - Augmented analytic
KW  - Automated machine learning
KW  - Electricity prices
KW  - Ensemble models
KW  - Explainable artificial intelligence
KW  - Hybrid model
KW  - Series hybrids
KW  - Time series decomposition
KW  - Time series hybrid model
KW  - Times series
KW  - artificial intelligence
KW  - decision making
KW  - electricity
KW  - forecasting method
KW  - machine learning
KW  - price dynamics
KW  - signal processing
KW  - strategic approach
KW  - Forecasting
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 28
ER  -

TY  - CONF
AU  - Jha, S.
AU  - Velasquez, A.
AU  - Ewetz, R.
AU  - Pullum, L.
AU  - Jha, S.
TI  - ExplainIt! A Tool for Computing Robust Attributions of DNNs
PY  - 2022
T2  - IJCAI International Joint Conference on Artificial Intelligence
SP  - 5916
EP  - 5919
DO  - 10.24963/ijcai.2022/853
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137926703&doi=10.24963%2fijcai.2022%2f853&partnerID=40&md5=eec5ef0855aa714734a1f92ab60322ba
AB  - Responsible integration of deep neural networks into the design of trustworthy systems requires the ability to explain decisions made by these models. Explainability and transparency are critical for system analysis, certification, and human-machine teaming. We have recently demonstrated that neural stochastic differential equations (SDEs) present an explanation-friendly DNN architecture. In this paper, we present ExplainIt, an online tool for explaining AI decisions that uses neural SDEs to create visually sharper and more robust attributions than traditional residual neural networks. Our tool shows that the injection of noise in every layer of a residual network often leads to less noisy and less fragile integrated gradient attributions. The discrete neural stochastic differential equation model is trained on the ImageNet data set with a million images, and the demonstration produces robust attributions on images in the ImageNet validation library and on a variety of images in the wild. Our online tool is hosted publicly for educational purposes. © 2022 International Joint Conferences on Artificial Intelligence. All rights reserved.
KW  - Deep neural networks
KW  - Differential equations
KW  - Stochastic models
KW  - Data set
KW  - Human-machine
KW  - Less noisy
KW  - Neural-networks
KW  - On-line tools
KW  - Stochastic differential equation models
KW  - Stochastic differential equations
KW  - Trustworthy systems
KW  - Stochastic systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Naiseh, M.
AU  - Soorati, M.D.
AU  - Ramchurn, S.
TI  - Outlining the Design Space of eXplainable Swarm (xSwarm): Experts’ Perspective
PY  - 2024
T2  - Springer Proceedings in Advanced Robotics
VL  - 28
SP  - 28
EP  - 41
DO  - 10.1007/978-3-031-51497-5_3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190140280&doi=10.1007%2f978-3-031-51497-5_3&partnerID=40&md5=b0af7b8aad8c616c0a4279a289e3cae1
AB  - In swarm robotics, agents interact through local roles to solve complex tasks beyond an individual’s ability. Even though swarms are capable of carrying out some operations without the need for human intervention, many safety-critical applications still call for human operators to control and monitor the swarm. There are novel challenges to effective Human-Swarm Interaction (HSI) that are only beginning to be addressed. Explainability is one factor that can facilitate effective and trustworthy HSI and improves the overall performance of Human-Swarm team. Explainability was studied across various Human-AI domains, such as Human-Robot Interaction and Human-Centered ML. However, it is still ambiguous whether explanations studied in Human-AI literature would be beneficial in Human-Swarm research and development. Furthermore, the literature lacks foundational research on the prerequisites for explainability requirements in swarm robotics, i.e., what kind of questions an explainable swarm is expected to answer, and what types of explanations a swarm is expected to generate. By surveying 26 swarm experts, we seek to answer these questions and identify challenges experts faced to generate explanations in Human-Swarm environments. Our work contributes insights into defining a new area of research of eXplainable Swarm (xSwarm) which looks at how explainability can be implemented and developed in swarm systems. This paper opens discussion on xSwarm and paves the way for more research in the field. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
KW  - Explainable AI
KW  - Human-Swarm Interaction
KW  - Swarm Robotics
KW  - Man machine systems
KW  - Safety engineering
KW  - Swarm intelligence
KW  - Complex task
KW  - Control and monitor
KW  - Design spaces
KW  - Explainable AI
KW  - Human intervention
KW  - Human operator
KW  - Human-swarm interaction
KW  - Robotic agents
KW  - Safety critical applications
KW  - Swarm robotics
KW  - Human robot interaction
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Zhang, N.
AU  - Bahsoon, R.
AU  - Tziritas, N.
AU  - Theodoropoulos, G.
TI  - Explainable Human-in-the-Loop Dynamic Data-Driven Digital Twins
PY  - 2024
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 13984 LNCS
SP  - 233
EP  - 243
DO  - 10.1007/978-3-031-52670-1_23
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187794106&doi=10.1007%2f978-3-031-52670-1_23&partnerID=40&md5=27dd299be060d3b1ce083e1681d2022f
AB  - Digital Twins (DT) are essentially dynamic data-driven models that serve as real-time symbiotic “virtual replicas” of real-world systems. DT can leverage fundamentals of Dynamic Data-Driven Applications Systems (DDDAS) bidirectional symbiotic sensing feedback loops for its continuous updates. Sensing loops can consequently steer measurement, analysis and reconfiguration aimed at more accurate modelling and analysis in DT. The reconfiguration decisions can be autonomous or interactive, keeping human-in-the-loop. The trustworthiness of these decisions can be hindered by inadequate explainability of the rationale, and utility gained in implementing the decision for the given situation among alternatives. Additionally, different decision-making algorithms and models have varying complexity, quality and can result in different utility gained for the model. The inadequacy of explainability can limit the extent to which humans can evaluate the decisions, often leading to updates which are unfit for the given situation, erroneous, compromising the overall accuracy of the model. The novel contribution of this paper is an approach to harnessing explainability in human-in-the-loop DDDAS and DT systems, leveraging bidirectional symbiotic sensing feedback. The approach utilises interpretable machine learning and goal modelling to explainability, and considers trade-off analysis of utility gained. We use examples from smart warehousing to demonstrate the approach. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
KW  - DDDAS
KW  - Digital Twins
KW  - Explainability
KW  - Human-in-the-loop
KW  - Continuous time systems
KW  - Economic and social effects
KW  - Feedback
KW  - Real time systems
KW  - Data driven
KW  - Data-driven model
KW  - Dynamic data
KW  - Dynamic Data Driven Application Systems
KW  - Explainability
KW  - Human-in-the-loop
KW  - Loop dynamics
KW  - Real- time
KW  - Real-world system
KW  - Symbiotics
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Vouros, G.A.
TI  - Explainable Deep Reinforcement Learning: State of the Art and Challenges
PY  - 2023
T2  - ACM Computing Surveys
VL  - 55
IS  - 5
C7  - 92
DO  - 10.1145/3527448
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146307412&doi=10.1145%2f3527448&partnerID=40&md5=ffb5edf983cae1174db3f221f90141ef
AB  - Interpretability, explainability, and transparency are key issues to introducing artificial intelligence methods in many critical domains. This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability, and fairness, and has important consequences toward keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. Although the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article, we aim to provide a review of state-of-the-art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators - that is, of those who make the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state-of-the-art methods, categorizing them into classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes by identifying open questions and important challenges.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
KW  - Deep learning
KW  - deep reinforcement learning
KW  - explainability
KW  - interpretability
KW  - transparency
KW  - Autonomous agents
KW  - Decision making
KW  - Deep learning
KW  - Ethical technology
KW  - Learning systems
KW  - Reinforcement learning
KW  - Artificial intelligence methods
KW  - Deep learning
KW  - Deep reinforcement learning
KW  - Explainability
KW  - Interpretability
KW  - Key Issues
KW  - Reinforcement learning method
KW  - Reinforcement learnings
KW  - State of the art
KW  - State-of-the-art methods
KW  - Transparency
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 66
ER  -

TY  - JOUR
AU  - Yablonsky, S.
TI  - AI-driven platform enterprise maturity: from human led to machine governed
PY  - 2021
T2  - Kybernetes
VL  - 50
IS  - 10
SP  - 2753
EP  - 2789
DO  - 10.1108/K-06-2020-0384
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107867806&doi=10.1108%2fK-06-2020-0384&partnerID=40&md5=89d0cff440ed979f7a8e1078184c4c9a
AB  - Purpose: To be more effective, artificial intelligence (AI) requires a broad overall view of the design and transformation of enterprise architecture and capabilities. Maturity models (MMs) are the recognized tools to identify strengths and weaknesses of certain domains of an organization. They consist of multiple, archetypal levels of maturity of a certain domain and can be used for organizational assessment and development. In the case of AI, quite a few numbers of MMs have been proposed. Generally, the links between AI technology, AI usage and organizational performance stay unclear. To address these gaps, this paper aims to introduce the complete details of the AI maturity model (AIMM) for AI-driven platform companies. The associated AI-Driven Platform Enterprise Maturity framework proposed here can help to achieve most of the AI-driven platform companies' objectives. Design/methodology/approach: Qualitative research is performed in two stages. In the first stage, a review of the existing literature is performed to identify the types, barriers, drivers, challenges and opportunities of MMs in AI, Advanced Analytics and Big Data domains. In the second stage, a research framework is proposed to align company value chain with AI technologies and levels of the platform enterprise maturity. Findings: The paper proposes a new five level AI-Driven Platform Enterprise Maturity framework by constructing a formal organizational value chain taxonomy model that explains a vast group of MM phenomena related with the AI-Driven Platform Enterprises. In addition, this study proposes a clear and precise description and structuring of the information in the multidimensional Platform, AI, Advanced Analytics and Big Data domains. The AI-Driven Platform Enterprise Maturity framework assists in identification, creation, assessment and disclosure research of AI-driven platform business organizations. Research limitations/implications: This research is focused on the basic dimensions of AI value chain. The full reference model of AI consists of much more concepts. In the last few years, AI has achieved a notable drive that, if connected appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in machine learning, especially in deep neural networks, the entire community stands in front of the barrier of explainability. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models in industry. Our prospects lead toward the concept of a methodology for the large-scale implementation of AI methods in platform organizations with fairness, model explainability and accountability at its core. Practical implications: AI-driven platform enterprise maturity framework can be used for better communicate to clients the value of AI capabilities through the lens of changing human-machine interactions and in the context of legal, ethical and societal norms. Social implications: The authors discuss AI in the enterprise platform stack including talent platform, human capital management and recruiting. Originality/value: The AI value chain and AI-Driven Platform Enterprise Maturity framework are original and represent an effective tools for assessing AI-driven platform enterprises. © 2021, Emerald Publishing Limited.
KW  - Advance analytics
KW  - AI maturity models
KW  - AI-Driven value chains
KW  - Artificial intelligence
KW  - Big data
KW  - Business platform
KW  - Machine learning
KW  - Platform stack
KW  - Technological platform
KW  - Advanced Analytics
KW  - Big data
KW  - Deep neural networks
KW  - Digital storage
KW  - Design/methodology/approach
KW  - Enterprise Architecture
KW  - Human machine interaction
KW  - Multidimensional platforms
KW  - Organizational assessment
KW  - Organizational performance
KW  - Organizational value chain
KW  - Qualitative research
KW  - Artificial intelligence
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 21
ER  -

TY  - JOUR
AU  - Kisten, M.
AU  - Ezugwu, A.E.-S.
AU  - Olusanya, M.O.
TI  - Explainable Artificial Intelligence Model for Predictive Maintenance in Smart Agricultural Facilities
PY  - 2024
T2  - IEEE Access
VL  - 12
SP  - 24348
EP  - 24367
DO  - 10.1109/ACCESS.2024.3365586
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185546988&doi=10.1109%2fACCESS.2024.3365586&partnerID=40&md5=7fed6e1bb57f87dd9c0d019c60b917fe
AB  - Artificial Intelligence (AI) in Smart Agricultural Facilities (SAF) often lacks explainability, hindering farmers from taking full advantage of their capabilities. This study tackles this gap by introducing a model that combines eXplainable Artificial Intelligence (XAI), with Predictive Maintenance (PdM). The model aims to provide both predictive insights and explanations across four key dimensions, namely data, model, outcome, and end-user. This approach marks a shift in agricultural AI, reshaping how these technologies are understood and applied. The model outperforms related studies, showing quantifiable improvements. Specifically, the Long-Short-Term Memory (LSTM) classifier shows a 5.81% rise in accuracy. The eXtreme Gradient Boosting (XGBoost) classifier exhibits a 7.09% higher F1 score, 10.66% increased accuracy, and a 4.29% increase in Receiver Operating Characteristic-Area Under the Curve (ROC-AUC). These results could lead to more precise maintenance predictions in real-world settings. This study also provides insights into data purity, global and local explanations, and counterfactual scenarios for PdM in SAF. It advances AI by emphasising the importance of explainability beyond traditional accuracy metrics. The results confirm the superiority of the proposed model, marking a significant contribution to PdM in SAF. Moreover, this study promotes the understanding of AI in agriculture, emphasising explainability dimensions. Future research directions are advocated, including multi-modal data integration and implementing Human-in-the-Loop (HITL) systems aimed at improving the effectiveness of AI and addressing ethical concerns such as Fairness, Accountability, and Transparency (FAT) in agricultural AI applications.  © 2013 IEEE.
KW  - Agriculture
KW  - deep learning
KW  - explainable artificial intelligence
KW  - machine learning
KW  - predictive maintenance
KW  - smart agricultural facilities
KW  - Data integration
KW  - Learning systems
KW  - Long short-term memory
KW  - Maintenance
KW  - Modal analysis
KW  - Boosting
KW  - Deep learning
KW  - Explainable artificial intelligence
KW  - Machine-learning
KW  - Predictive maintenance
KW  - Predictive models
KW  - Smart agricultural facility
KW  - Smart agricultures
KW  - Agriculture
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 15
ER  -

TY  - CONF
AU  - Cavagnetto, N.
AU  - Venditti, R.
AU  - Cocchioni, M.
AU  - Bonelli, S.
TI  - Demo: SectorX, an en-route ATC simulator for AI-based decision support to Air Traffic Controllers: A case study in the MAHALO project
PY  - 2023
T2  - ACM International Conference Proceeding Series
C7  - 56
DO  - 10.1145/3605390.3610822
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173705907&doi=10.1145%2f3605390.3610822&partnerID=40&md5=d966dc41a2f19933e95d006b1df03ba7
AB  - The EU funded MAHALO Project investigated the effects of transparent and conformal Machine Learning models in support of the safety-critical time-pressured task of Conflict Detection and Resolution for en-route Air Traffic Controllers. The experimental phase was conducted in Italy and Sweden, involving 36 Air Traffic Controllers playing simulated en-route Air Traffic Control scenarios. These scenarios were presented through Sector X, an en-route Air Traffic Control simulator with an ecological User Interface based on the Maastricht Upper Area Control's Controller Working Position. The researchers investigated transparency by introducing three different explainability levels through three visual explanations, and conformance by training with three different conflict resolution styles in the Machine Learning model. The researchers will present the UI designed to achieve transparency to the participants of the demo sessions, who will be able to play Air Traffic Control scenarios.  © 2023 Owner/Author.
KW  - Conformance
KW  - Explainable AI
KW  - Human-AI Teaming
KW  - Human-Computer Interaction
KW  - Transparency
KW  - Air navigation
KW  - Air traffic control
KW  - Controllers
KW  - Decision support systems
KW  - Human computer interaction
KW  - Machine learning
KW  - Safety engineering
KW  - User interfaces
KW  - Air traffic controller
KW  - Case-studies
KW  - Conflict detection and resolution
KW  - Conformance
KW  - Critical time
KW  - Decision supports
KW  - En-route
KW  - Explainable AI
KW  - Human-AI teaming
KW  - Machine learning models
KW  - Transparency
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Saranti, A.
AU  - Hudec, M.
AU  - Mináriková, E.
AU  - Takáč, Z.
AU  - Großschedl, U.
AU  - Koch, C.
AU  - Pfeifer, B.
AU  - Angerschmid, A.
AU  - Holzinger, A.
TI  - Actionable Explainable AI (AxAI): A Practical Example with Aggregation Functions for Adaptive Classification and Textual Explanations for Interpretable Machine Learning
PY  - 2022
T2  - Machine Learning and Knowledge Extraction
VL  - 4
IS  - 4
SP  - 924
EP  - 953
DO  - 10.3390/make4040047
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144729975&doi=10.3390%2fmake4040047&partnerID=40&md5=fa969a4713666675ab9e8c756de97854
AB  - In many domains of our daily life (e.g., agriculture, forestry, health, etc.), both laymen and experts need to classify entities into two binary classes (yes/no, good/bad, sufficient/insufficient, benign/malign, etc.). For many entities, this decision is difficult and we need another class called “maybe”, which contains a corresponding quantifiable tendency toward one of these two opposites. Human domain experts are often able to mark any entity, place it in a different class and adjust the position of the slope in the class. Moreover, they can often explain the classification space linguistically—depending on their individual domain experience and previous knowledge. We consider this human-in-the-loop extremely important and call our approach actionable explainable AI. Consequently, the parameters of the functions are adapted to these requirements and the solution is explained to the domain experts accordingly. Specifically, this paper contains three novelties going beyond the state-of-the-art: (1) A novel method for detecting the appropriate parameter range for the averaging function to treat the slope in the “maybe” class, along with a proposal for a better generalisation than the existing solution. (2) the insight that for a given problem, the family of t-norms and t-conorms covering the whole range of nilpotency is suitable because we need a clear “no” or “yes” not only for the borderline cases. Consequently, we adopted the Schweizer–Sklar family of t-norms or t-conorms in ordinal sums. (3) A new fuzzy quasi-dissimilarity function for classification into three classes: Main difference, irrelevant difference and partial difference. We conducted all of our experiments with real-world datasets. © 2022 by the authors.
KW  - actionable explainable AI
KW  - aggregation functions
KW  - classification
KW  - continuous XOR-problem
KW  - interpretable machine learning
KW  - ordinal sums
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 21
ER  -

TY  - JOUR
AU  - Kiani, M.
AU  - Andreu-Perez, J.
AU  - Hagras, H.
TI  - A Temporal Type-2 Fuzzy System for Time-Dependent Explainable Artificial Intelligence
PY  - 2023
T2  - IEEE Transactions on Artificial Intelligence
VL  - 4
IS  - 3
SP  - 573
EP  - 586
DO  - 10.1109/TAI.2022.3210895
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139437865&doi=10.1109%2fTAI.2022.3210895&partnerID=40&md5=a39be0001b94e5b84ac03f18ecf179bb
AB  - Explainable artificial intelligence (XAI) focuses on transparent AI models and decisions, which are easy to understand, analyze, and augment by a nontechnical audience. Fuzzy logic systems (FLS)-based XAI provides an explainable framework while also modeling uncertainties in real-world environments. However, most real-life processes are not characterized by high uncertainty alone; they are also inherently time dependent, i.e., the processes are time variant. In this work, we present a novel temporal type-2 FLS-based approach for time-dependent XAI (TXAI) systems, which can account for the likelihood of a sample occurrence in the time domain by its the frequency. In the proposed temporal type-2 fuzzy sets (TT2FSs), a 4-D time-dependent membership function integrates the universe of discourse, its membership, and its frequency of occurrence across time. The TXAI system manifested better classification prowess in cross-validation tests, with a mean recall of 95.40% than a standard XAI system (based on nontemporal general type-2 fuzzy sets) that had a mean recall of 87.04%. TXAI also performed significantly better than most nonexplainable AI systems, with between 3.95% and 19.04% improvement gain in mean recall. In addition, TXAI can also outline the most likely time-dependent trajectories using the frequency and time dimensions embedded in the TXAI model; viz. given a rule at a determined time interval, what will be the next most likely rule at a subsequent time interval. In this regard, the proposed TXAI system can have profound implications for delineating the evolution of real-life time-dependent processes, such as behavioral or biological processes.  © 2020 IEEE.
KW  - Artificial intelligence
KW  - explainable artificial intelligence
KW  - fuzzy systems
KW  - human computer interaction
KW  - human in the loop
KW  - time-varying
KW  - trusted computing
KW  - Artificial intelligence
KW  - Classification (of information)
KW  - Computer circuits
KW  - Fuzzy sets
KW  - Membership functions
KW  - Temperature measurement
KW  - Time domain analysis
KW  - Uncertainty analysis
KW  - Computational modelling
KW  - Fuzzy logic system
KW  - Fuzzy-Logic
KW  - Modeling uncertainties
KW  - Most likely
KW  - Time dependent
KW  - Time interval
KW  - Type-2 fuzzy set
KW  - Type-2 fuzzy systems
KW  - Uncertainty
KW  - Fuzzy logic
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Henin, C.
AU  - Le Métayer, D.
TI  - Beyond explainability: justifiability and contestability of algorithmic decision systems
PY  - 2022
T2  - AI and Society
VL  - 37
IS  - 4
SP  - 1397
EP  - 1410
DO  - 10.1007/s00146-021-01251-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111488372&doi=10.1007%2fs00146-021-01251-8&partnerID=40&md5=2b2ed463315c011db6d04c0cc91ae48b
AB  - In this paper, we point out that explainability is useful but not sufficient to ensure the legitimacy of algorithmic decision systems. We argue that the key requirements for high-stakes decision systems should be justifiability and contestability. We highlight the conceptual differences between explanations and justifications, provide dual definitions of justifications and contestations, and suggest different ways to operationalize justifiability and contestability. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.
KW  - Algorithmic decision system
KW  - Challenge
KW  - Contestation
KW  - Evidence
KW  - Explanation
KW  - Justification
KW  - Machine learning
KW  - Algorithmic decision system
KW  - Algorithmics
KW  - Challenge
KW  - Contestation
KW  - Decision systems
KW  - Evidence
KW  - Explanation
KW  - Justification
KW  - Machine-learning
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 27
ER  -

TY  - CONF
AU  - Qian, P.
AU  - Unhelkar, V.
TI  - Evaluating the Role of Interactivity on Improving Transparency in Autonomous Agents
PY  - 2022
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
VL  - 2
SP  - 1083
EP  - 1091
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134321454&partnerID=40&md5=0663b43de829093145a69430de431914
AB  - Autonomous agents are increasingly being deployed amongst human end-users. Yet, human users often have little knowledge of how these agents work or what they will do next. This lack of transparency has already resulted in unintended consequences during AI use: a concerning trend which is projected to increase with the proliferation of autonomous agents. To curb this trend and ensure safe use of AI, assisting users in establishing an accurate understanding of agents that they work with is essential. In this work, we present AI Teacher, a user-centered Explainable AI framework to address this need for autonomous agents that follow a Markovian policy. Our framework first computes salient instructions of agent behavior by estimating a user's mental model and utilizing algorithms for sequential decision-making. Next, in contrast to existing solutions, these instructions are presented interactively to the end-users, thereby enabling a personalized approach to improving AI transparency. We evaluate our framework, with emphasis on its interactive features, through experiments with human participants. The experiment results suggest that, relative to non-interactive approaches, interactive teaching can both reduce the amount of time it takes for humans to create accurate mental models of these agents and is subjectively preferred by human users. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved
KW  - Explainable AI
KW  - Human-AI Collaboration
KW  - Machine Teaching
KW  - Monte-Carlo Tree Search
KW  - Shared Mental Models
KW  - Autonomous agents
KW  - Behavioral research
KW  - Cognitive systems
KW  - Decision making
KW  - Multi agent systems
KW  - End-users
KW  - Explainable AI
KW  - Human users
KW  - Human-AI collaboration
KW  - Interactivity
KW  - Mental model
KW  - Monte-carlo tree search
KW  - Shared mental model
KW  - Tree-search
KW  - Unintended consequences
KW  - Transparency
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 12
ER  -

TY  - JOUR
AU  - Ball, B.
AU  - Koliousis, A.
TI  - Training philosopher engineers for better AI
PY  - 2023
T2  - AI and Society
VL  - 38
IS  - 2
SP  - 861
EP  - 868
DO  - 10.1007/s00146-022-01535-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134644710&doi=10.1007%2fs00146-022-01535-7&partnerID=40&md5=39cf732c772a57756254d976686b509d
AB  - There is a deluge of AI-assisted decision-making systems, where our data serve as proxy to our actions, suggested by AI. The closer we investigate our data (raw input, or their learned representations, or the suggested actions), we begin to discover “bugs”. Outside of their test, controlled environments, AI systems may encounter situations investigated primarily by those in other disciplines, but experts in those fields are typically excluded from the design process and are only invited to attest to the ethical features of the resulting system or to comment on demonstrations of intelligence and aspects of craftmanship after the fact. This communicative impasse must be overcome. Our idea is that philosophical and engineering considerations interact and can be fruitfully combined in the AI design process from the very beginning. We embody this idea in the role of a philosopher engineer. We discuss the role of philosopher engineers in the three main design stages of an AI system: deployment management (what is the system’s intended use, in what environment?); objective setting (what should the system be trained to do, and how?); and training (what model should be used, and why?). We then exemplify the need for philosopher engineers with an illustrative example, investigating how the future decisions of an AI-based hiring system can be fairer than those contained in the biased input data on which it is trained; and we briefly sketch the kind of interdisciplinary education that we envision will help to bring about better AI. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.
KW  - Explainability
KW  - Fairness
KW  - Inductive bias
KW  - Interdisciplinary training
KW  - Societal bias
KW  - Decision making
KW  - Personnel training
KW  - Philosophical aspects
KW  - AI systems
KW  - Controlled environment
KW  - Craftmanship
KW  - Decision-making systems
KW  - Design-process
KW  - Explainability
KW  - Fairness
KW  - Inductive bias
KW  - Interdisciplinary training
KW  - Societal bias
KW  - Engineers
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Ramesh, D.
AU  - Kameswaran, V.
AU  - Wang, D.
AU  - Sambasivan, N.
TI  - How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India
PY  - 2022
T2  - ACM International Conference Proceeding Series
SP  - 1917
EP  - 1928
DO  - 10.1145/3531146.3533237
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132969228&doi=10.1145%2f3531146.3533237&partnerID=40&md5=fba658cd91ef2c69eec8e03761ebc93e
AB  - Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a g high-risk' AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the g boon' of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond. © 2022 Owner/Author.
KW  - algorithmic accountability
KW  - algorithmic fairness
KW  - human-ai interaction
KW  - instant loans
KW  - socio-technical systems
KW  - Transparency
KW  - Algorithmic accountability
KW  - Algorithmic fairness
KW  - Algorithmics
KW  - Human-ai interaction
KW  - Instant loan
KW  - Policy makers
KW  - Power relations
KW  - Shape algorithmics
KW  - Sociotechnical systems
KW  - Sensitive data
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 26
ER  -

TY  - JOUR
AU  - Bacula, A.
AU  - Mercer, J.
AU  - Berger, J.
AU  - Adams, J.
AU  - Knight, H.
TI  - Integrating Robot Manufacturer Perspectives into Legible Factory Robot Light Communications
PY  - 2023
T2  - ACM Transactions on Human-Robot Interaction
VL  - 12
IS  - 1
C7  - 13
DO  - 10.1145/3570732
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149017305&doi=10.1145%2f3570732&partnerID=40&md5=3566cc2090ce5c6e3617511585a5c254
AB  - In a world with increasing numbers of robots operating in everyday human spaces, the employees at this robotics company are pioneers, with intelligent point-to-point path planning and autonomous transport operations in 150+ factory and warehouse locations in North America. At the time of research, this robotics company consisted of 250 employees. Unlike other industry models, their robots are designed to operate with people in mixed human-machine spaces, yet no HRI style evaluations had previously been run with their robots. As early observers of how factory workers and transport robot interact, across varied job roles ranging from technology design to customer relations, this work sought to leverage employee knowledge and experiences to identify opportunities for improving the communication capabilities of the robots, resulting in the addition of several robot state communications to their initial software set leveraging both employee- and social robotics literature- sourced ideas for communicating with lights. To achieve this a social robotics researcher spent a summer onsite at the robotics company, getting to know their software stack and culture. Her research activities included: (1) a company-wide survey relative to the robot's light, sound, and motion communications was sent out and analyzed, (2) the development of three new light sets (car-like, sweeping, heartbeat) and five overall states (blocked, at goal, turning, idle), and (3) a user study evaluating the developed light sets relative to the current robot default light patterns, all significantly improving the overall legibility of the targeted robot state communications: at goal, blocked, turning, and idle. Our initial findings advance knowledge in which style of light patterns is best for different communication states, showing that eye-catching lights are best for high urgency states, such as blocked, and subtle lights are best for low urgency states, such as idle. Finally, the latest software release for this robot has deployed a subset of these light patterns to all of their currently operating client sites, i.e., anyone who updates their robots to the latest release will benefit from these research results. This deployment sets the ground for future researchers exploring how end-users at different sites have responded to the new, more communicative light patterns.  © 2023 Association for Computing Machinery.
KW  - expressive lights
KW  - Factory robotics
KW  - social robotics
KW  - Human robot interaction
KW  - Intelligent robots
KW  - Knowledge management
KW  - Machine design
KW  - Personnel
KW  - Public relations
KW  - Robot programming
KW  - Expressive light
KW  - Factory robotic
KW  - Human-machine
KW  - Light patterns
KW  - Robot manufacturers
KW  - Social robotics
KW  - Technology designs
KW  - Transport operations
KW  - Warehouse location
KW  - Workers'
KW  - Motion planning
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - CHAP
AU  - Majumdar, S.
TI  - Fairness, explainability, privacy, and robustness for trustworthy algorithmic decision-making
PY  - 2022
T2  - Big Data Analytics in Chemoinformatics and Bioinformatics: with Applications to Computer-Aided Drug Design, Cancer Biology, Emerging Pathogens and Computational Toxicology
SP  - 61
EP  - 95
DO  - 10.1016/B978-0-323-85713-0.00017-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144531723&doi=10.1016%2fB978-0-323-85713-0.00017-7&partnerID=40&md5=e8914a7b48833c4bb88f82bd1bda15ca
AB  - With the rapid increase in the use and deployment of machine learning (ML) systems in the world, concomitant concerns on the ethical implications of their downstream effect have surfaced in recent years. Responding to this challenge, the field of trustworthy ML has grown rapidly and resulted in a large body of methods and algorithms that embody desirable qualities such as fairness, transparency, privacy, and robustness. In this chapter, we survey the current landscape of trustworthy ML methods, introduce fundamental concepts, and summarize research directions. To bridge the gap between theory and practice, we provide implementation details of each category of methods that are currently available publicly. © 2023 Elsevier Inc. All rights reserved.
KW  - adversarial robustness
KW  - algorithmic fairness
KW  - differential privacy
KW  - explainable AI
KW  - ML bias
KW  - Trustworthy machine learning
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Dargaud, L.
AU  - Ibsen, M.
AU  - Tapia, J.
AU  - Busch, C.
TI  - A Principal Component Analysis-Based Approach for Single Morphing Attack Detection
PY  - 2023
T2  - Proceedings - 2023 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops, WACVW 2023
SP  - 683
EP  - 692
DO  - 10.1109/WACVW58289.2023.00075
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148323034&doi=10.1109%2fWACVW58289.2023.00075&partnerID=40&md5=43f4b01dcc900bfa9686aa8cd94f1913
AB  - This paper proposes an explicit method for single face image morphing attack detection, using an RGB decomposition based on Principal Component Analysis from texture patterns. Handcrafted detection algorithms can be advantageous over deep learning-based methods as they constitute increased explainability, showcased in this work by visualizing relevant face areas for morphing attack detection. Such information can be relevant for deployed systems in real-world scenarios with humans in the loop. The morphing detection capability of the proposed method is evaluated extensively across three datasets and six morphing algorithms in single, cross-dataset and cross-morphed scenarios and compared to a fine-tuned MobileNetV2 architecture. The results show how single image morphing attack detection remains challenging, especially in cross-domain scenarios involving realistic diversity of morphing algorithms, including StyleGAN-based approaches. In such conditions, the proposed method can be as good or even better than the evaluated MobileNetV2 approach.  © 2023 IEEE.
KW  - Computer vision
KW  - Deep learning
KW  - Textures
KW  - Analysis-based approaches
KW  - Attack detection
KW  - Detection algorithm
KW  - Explicit method
KW  - Face images
KW  - Image morphing
KW  - Morphing
KW  - Morphing algorithms
KW  - Principal-component analysis
KW  - Texture patterns
KW  - Principal component analysis
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Vasu, B.
AU  - Hu, B.
AU  - Dong, B.
AU  - Collins, R.
AU  - Hoogs, A.
TI  - Explainable, interactive content-based image retrieval
PY  - 2021
T2  - Applied AI Letters
VL  - 2
IS  - 4
C7  - e41
DO  - 10.1002/ail2.41
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159929554&doi=10.1002%2fail2.41&partnerID=40&md5=44f221818a7127d4a54e434497c8d0b4
AB  - Quantifying the value of explanations in a human-in-the-loop (HITL) system is difficult. Previous methods either measure explanation-specific values that do not correspond to user tasks and needs or poll users on how useful they find the explanations to be. In this work, we quantify how much explanations help the user through a utility-based paradigm that measures change in task performance when using explanations vs not. Our chosen task is content-based image retrieval (CBIR), which has well-established baselines and performance metrics independent of explainability. We extend an existing HITL image retrieval system that incorporates user feedback with similarity-based saliency maps (SBSM) that indicate to the user which parts of the retrieved images are most similar to the query image. The system helps the user understand what it is paying attention to through saliency maps, and the user helps the system understand their goal through saliency-guided relevance feedback. Using the MS-COCO dataset, a standard object detection and segmentation dataset, we conducted extensive, crowd-sourced experiments validating that SBSM improves interactive image retrieval. Although the performance increase is modest in the general case, in more difficult cases such as cluttered scenes, using explanations yields an 6.5% increase in accuracy. To the best of our knowledge, this is the first large-scale user study showing that visual saliency map explanations improve performance on a real-world, interactive task. Our utility-based evaluation paradigm is general and potentially applicable to any task for which explainability can be incorporated. © 2021 Kitware, Inc. Applied AI Letters published by John Wiley & Sons Ltd.
KW  - explainable AI
KW  - image retrieval
KW  - saliency
KW  - user study
M3  - Letter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Ott, T.
AU  - Dabrock, P.
TI  - Transparent human – (non-) transparent technology? The Janus-faced call for transparency in AI-based health care technologies
PY  - 2022
T2  - Frontiers in Genetics
VL  - 13
C7  - 902960
DO  - 10.3389/fgene.2022.902960
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137847247&doi=10.3389%2ffgene.2022.902960&partnerID=40&md5=234618c3906f48b6ba7e9687ffc82e51
AB  - The use of Artificial Intelligence and Big Data in health care opens up new opportunities for the measurement of the human. Their application aims not only at gathering more and better data points but also at doing it less invasive. With this change in health care towards its extension to almost all areas of life and its increasing invisibility and opacity, new questions of transparency arise. While the complex human-machine interactions involved in deploying and using AI tend to become non-transparent, the use of these technologies makes the patient seemingly transparent. Papers on the ethical implementation of AI plead for transparency but neglect the factor of the “transparent patient” as intertwined with AI. Transparency in this regard appears to be Janus-faced: The precondition for receiving help - e.g., treatment advice regarding the own health - is to become transparent for the digitized health care system. That is, for instance, to donate data and become visible to the AI and its operators. The paper reflects on this entanglement of transparent patients and (non-) transparent technology. It argues that transparency regarding both AI and humans is not an ethical principle per se but an infraethical concept. Further, it is no sufficient basis for avoiding harm and human dignity violations. Rather, transparency must be enriched by intelligibility following Judith Butler’s use of the term. Intelligibility is understood as an epistemological presupposition for recognition and the ensuing humane treatment. Finally, the paper highlights ways to testify intelligibility in dealing with AI in health care ex ante, ex post, and continuously. Copyright © 2022 Ott and Dabrock.
KW  - AI
KW  - Data
KW  - Ethics
KW  - Health Care
KW  - Infraethics
KW  - Intelligibility
KW  - Learning Systems
KW  - Transparency
KW  - adult
KW  - article
KW  - artificial intelligence
KW  - big data
KW  - ethics
KW  - health care system
KW  - human
KW  - human dignity
KW  - learning
KW  - neglect
KW  - speech intelligibility
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Keppel, J.
AU  - Liebers, J.
AU  - Auda, J.
AU  - Gruenefeld, U.
AU  - Schneegass, S.
TI  - ExplAInable Pixels: Investigating One-Pixel Attacks on Deep Learning Models with Explainable Visualizations
PY  - 2022
T2  - ACM International Conference Proceeding Series
SP  - 231
EP  - 242
DO  - 10.1145/3568444.3568469
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145875220&doi=10.1145%2f3568444.3568469&partnerID=40&md5=99a50f6b53efc10555c615af7e7f2a1e
AB  - Nowadays, deep learning models enable numerous safety-critical applications, such as biometric authentication, medical diagnosis support, and self-driving cars. However, previous studies have frequently demonstrated that these models are attackable through slight modifications of their inputs, so-called adversarial attacks. Hence, researchers proposed investigating examples of these attacks with explainable artificial intelligence to understand them better. In this line, we developed an expert tool to explore adversarial attacks and defenses against them. To demonstrate the capabilities of our visualization tool, we worked with the publicly available CIFAR-10 dataset and generated one-pixel attacks. After that, we conducted an online evaluation with 16 experts. We found that our tool is usable and practical, providing evidence that it can support understanding, explaining, and preventing adversarial examples.  © 2022 ACM.
KW  - adversarial examples
KW  - explainability
KW  - human-in-The-loop
KW  - one-pixel attacks
KW  - Deep learning
KW  - Diagnosis
KW  - Learning systems
KW  - Safety engineering
KW  - Visualization
KW  - Adversarial example
KW  - Biometric authentication
KW  - Explainability
KW  - Human-in-the-loop
KW  - Learning models
KW  - Medical diagnosis support
KW  - On-line evaluation
KW  - One-pixel attack
KW  - Safety critical applications
KW  - Visualization tools
KW  - Pixels
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Robertson, S.
AU  - Díaz, M.
TI  - Understanding and Being Understood: User Strategies for Identifying and Recovering From Mistranslations in Machine Translation-Mediated Chat
PY  - 2022
T2  - ACM International Conference Proceeding Series
SP  - 2223
EP  - 2238
DO  - 10.1145/3531146.3534638
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130777908&doi=10.1145%2f3531146.3534638&partnerID=40&md5=d830539398d328a34550a97006814500
AB  - Machine translation (MT) is now widely and freely available, and has the potential to greatly improve cross-lingual communication. In order to use MT reliably and safely, end users must be able to assess the quality of system outputs and determine how much they can rely on them to guide their decisions and actions. However, it can be difficult for users to detect and recover from mistranslations due to limited language skills. In this work we collected 19 MT-mediated role-play conversations in housing and employment scenarios, and conducted in-depth interviews to understand how users identify and recover from translation errors. Participants communicated using four language pairs: English, and one of Spanish, Farsi, Igbo, or Tagalog. We conducted qualitative analysis to understand user challenges in light of limited system transparency, strategies for recovery, and the kinds of translation errors that proved more or less difficult for users to overcome. We found that users broadly lacked relevant and helpful information to guide their assessments of translation quality. Instances where a user erroneously thought they had understood a translation correctly were rare but held the potential for serious consequences in the real world. Finally, inaccurate and disfluent translations had social consequences for participants, because it was difficult to discern when a disfluent message was reflective of the other person's intentions, or an artifact of imperfect MT. We draw on theories of grounding and repair in communication to contextualize these findings, and propose design implications for explainable AI (XAI) researchers, MT researchers, as well as collaboration among them to support transparency and explainability in MT. These directions include handling typos and non-standard grammar common in interpersonal communication, making MT in interfaces more visible to help users evaluate errors, supporting collaborative repair of conversation breakdowns, and communicating model strengths and weaknesses to users. © 2022 Owner/Author.
KW  - computer-mediated communication
KW  - explainable machine learning
KW  - human-AI interaction
KW  - machine translation
KW  - Computational linguistics
KW  - Computer aided language translation
KW  - Errors
KW  - Human computer interaction
KW  - Machine learning
KW  - Machine translation
KW  - Transparency
KW  - User interfaces
KW  - Computer-mediated communication
KW  - Cross-lingual communication
KW  - End-users
KW  - Explainable machine learning
KW  - Human-AI interaction
KW  - Machine translations
KW  - Machine-learning
KW  - Role-plays
KW  - System output
KW  - User strategies
KW  - Recovery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Nelekar, S.
AU  - Abdulrahman, A.
AU  - Gupta, M.
AU  - Richards, D.
TI  - Effectiveness of embodied conversational agents for managing academic stress at an Indian University (ARU) during COVID-19
PY  - 2022
T2  - British Journal of Educational Technology
VL  - 53
IS  - 3
SP  - 491
EP  - 511
DO  - 10.1111/bjet.13174
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121470125&doi=10.1111%2fbjet.13174&partnerID=40&md5=5ae6f5ea07f2647a05e6454e7808e598
AB  - Stress has become one of the major reasons for many mental health related issues among students of all age groups, which has resulted in devastating personal losses including suicide. Societal and familial pressure to succeed is high, particularly in developing countries where education is highly valued as a key enabler. As part of stress management during the COVID-19 pandemic, demand for online intelligent virtual advisors has risen and, consequently, the need for personalised explanation that is culturally sensitive to the user's context is essential to improve the user's understanding of and trust in the recommendations provided by the virtual advisor. This paper presents the mAnaging stRess at University embodied conversational agent (ECA) that has been adapted for Indian university students from an explainable agent that was found to help Western students reduce their stress by providing study tips with explanations based on the student's beliefs and/or goals. We conducted a research study with sixty students which measured the impact of providing three different patterns of tailored explanations (belief-based, goal-based, and belief and goal-based explanation) on the students' intentions to change the recommended behaviours and the relationship built with the ECA. The experimental results indicate that there was stress reduction across all student groups provided with different types of explanations. Further, the students showed trust and a good working alliance with the conversational agent, along with an intention to change behaviour across all types of explanations. However, it was observed that the user context played an important role in behaviour change intention and hence explanations could be tailored further, making them culturally more relevant to Indian students. Practitioner notes What is already known about this topic Embodied conversational agents (ECAs) have been mostly developed, applied and shown to be effective in developed countries. Hence, their design and development are mostly guided by the intended user's needs and preferences. In a Western context, ECAs have been found to be beneficial for reducing study stress in university students. There is a pertinent need for use of low cost, effective technology that can aid academic stress reduction in higher educational institutions in developing countries owing to their high youth populations, lack of adequate mental healthcare facilities and associated social stigma. What this paper adds The adaptation and use of ECAs to reduce study stress in higher education students in a developing country is evaluated. The ECA technology is adapted for an Indian context in terms of its physical appearance, colour, speech dialect and dialog content so that it is culturally more aligned to the target population. The ECA engages in an empathic conversation tailored for the Indian students and their COVID-19 context providing them with explanation-backed behaviour recommendations that take their beliefs and goals into account. The ECA provides three types of explanation: belief-only; goal-only; and both belief and goal. Results of a study carried out in an Indian university with 61 students, randomly assigned to one of the explanation types, to capture their demographics, study stress statistics, behaviour change intentions and trust/working alliance with the conversational agent. The major findings include stress reduction across all explanation groups, development of a positive relationship between the ECA and the students regardless of its explanation pattern, and changes in behaviour intentions across all types of explanations for all recommended behaviours. However, differences in change intentions for certain behaviours indicate further tailoring of explanations is required based on the user context. Implications for practice and/or policy The ECA technology has shown promise in terms of stress reduction amongst Indian students. Higher Education Institutions in developing countries could utilise low-cost and widely accessible ECAs to overcome lack of access to human-based support and reluctance to use available services due to stigmatized attitudes to mental health issues. This technology can be further improved and deployed into a larger number of Indian educational institutions leading to a widespread impact on overall student health and wellbeing. Digital technologies to support mental health have become more prominent during the COVID-19 pandemic, at least in Western countries. The ECA technology evaluated in our study demonstrates its viability and potential value for use in developing countries, with appropriate tailoring. © 2021 British Educational Research Association.
KW  - artificial intelligence
KW  - behavior change
KW  - embodied conversational agents
KW  - human-machine interface
KW  - learner attitudes/perceptions
KW  - stress management
KW  - undergraduate education
KW  - virtual assistant
KW  - Artificial intelligence
KW  - Developing countries
KW  - E-learning
KW  - Human computer interaction
KW  - Man machine systems
KW  - Risk management
KW  - Virtual reality
KW  - Behaviour changes
KW  - Embodied conversational agent
KW  - Human Machine Interface
KW  - Learner attitude/perception
KW  - Learner's attitudes
KW  - Mental health
KW  - Stress management
KW  - Stress reduction
KW  - Undergraduate education
KW  - Virtual assistants
KW  - Students
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 19
ER  -

TY  - JOUR
AU  - Lebovitz, S.
AU  - Lifshitz-Assaf, H.
AU  - Levina, N.
TI  - To Engage or Not to Engage with AI for Critical Judgments: How Professionals Deal with Opacity When Using AI for Medical Diagnosis
PY  - 2022
T2  - Organization Science
VL  - 33
IS  - 1
SP  - 126
EP  - 148
DO  - 10.1287/ORSC.2021.1549
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125569707&doi=10.1287%2fORSC.2021.1549&partnerID=40&md5=23c2dfe4c60b500c31012b4000cf5177
AB  - Artificial intelligence (AI) technologies promise to transform how professionals conduct knowledge work by augmenting their capabilities for making professional judgments. We know little, however, about how human-AI augmentation takes place in practice. Yet, gaining this understanding is particularly important when professionals use AI tools to form judgments on critical decisions. We conducted an in-depth field study in a major U.S. hospital where AI tools were used in three departments by diagnostic radiologists making breast cancer, lung cancer, and bone age determinations. The study illustrates the hindering effects of opacity that professionals experienced when using AI tools and explores how these professionals grappled with it in practice. In all three departments, this opacity resulted in professionals experiencing increased uncertainty because AI tool results often diverged from their initial judgment without providing underlying reasoning. Only in one department (of the three) did professionals consistently incorporate AI results into their final judgments, achieving what we call engaged augmentation. These professionals invested in AI interrogation practices-practices enacted by human experts to relate their own knowledge claims to AI knowledge claims. Professionals in the other two departments did not enact such practices and did not incorporate AI inputs into their final decisions, which we call unengaged “augmentation.” Our study unpacks the challenges involved in augmenting professional judgment with powerful, yet opaque, technologies and contributes to literature on AI adoption in knowledge work. © 2022 INFORMS
KW  - Artificial intelligence
KW  - Augmentation
KW  - Decision making
KW  - Expertise
KW  - Explainability
KW  - Innovation
KW  - Medical diagnosis
KW  - Opacity
KW  - Professional judgment
KW  - Technology adoption and use
KW  - Transparency
KW  - Uncertainty
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 238
ER  -

TY  - CONF
AU  - Recki, L.
AU  - Esau-Held, M.
AU  - Lawo, D.
AU  - Stevens, G.
TI  - AI said, She said - How Users Perceive Consumer Scoring in Practice
PY  - 2023
T2  - ACM International Conference Proceeding Series
SP  - 149
EP  - 160
DO  - 10.1145/3603555.3603562
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171163858&doi=10.1145%2f3603555.3603562&partnerID=40&md5=7d55de9e887f24804f31f389ef2e11a2
AB  - As digitization continues, consumers are increasingly exposed to AI scoring decisions. However, currently lacking is a thorough understanding of how users' misjudgments of an AI-supported system lead to it being rejected. Therefore, investigations are needed into the appropriation of such socio-technical systems in practice and how users describe their experience with algorithm-based scoring. To address this issue, we evaluated 1,003 user reviews of an app on car insurance that calculates premiums based on the consumers' individual driving behavior. We find evidence that users develop their own folk theories to explain the algorithms with the help of situation-related experiences and that insufficient explanations lead to power asymmetries between consumers, the system, and the company. In particular, as a result of the different needs of the stakeholders, we uncover a fundamental conflict between computational risk assessment and the perceived agency to influence the score.  © 2023 ACM.
KW  - Algorithmic Decision Making
KW  - Empirical study
KW  - Explainable AI
KW  - Fairness
KW  - Perception
KW  - Computation theory
KW  - Consumer behavior
KW  - Risk assessment
KW  - Algorithmic decision making
KW  - Algorithmics
KW  - Decisions makings
KW  - Digitisation
KW  - Empirical studies
KW  - Explainable AI
KW  - Exposed to
KW  - Fairness
KW  - Sociotechnical systems
KW  - User reviews
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Kim, S.S.Y.
AU  - Watkins, E.A.
AU  - Russakovsky, O.
AU  - Fong, R.
AU  - Monroy-Hernández, A.
TI  - "Help Me Help the AI": Understanding How Explainability Can Support Human-AI Interaction
PY  - 2023
T2  - Conference on Human Factors in Computing Systems - Proceedings
C7  - 250
DO  - 10.1145/3544548.3581001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152299586&doi=10.1145%2f3544548.3581001&partnerID=40&md5=357a3d0006ebef8761dbd422c3f9dee9
AB  - Despite the proliferation of explainable AI (XAI) methods, little is understood about end-users' explainability needs and behaviors around XAI explanations. To address this gap and contribute to understanding how explainability can support human-AI interaction, we conducted a mixed-methods study with 20 end-users of a real-world AI application, the Merlin bird identification app, and inquired about their XAI needs, uses, and perceptions. We found that participants desire practically useful information that can improve their collaboration with the AI, more so than technical system details. Relatedly, participants intended to use XAI explanations for various purposes beyond understanding the AI's outputs: calibrating trust, improving their task skills, changing their behavior to supply better inputs to the AI, and giving constructive feedback to developers. Finally, among existing XAI approaches, participants preferred part-based explanations that resemble human reasoning and explanations. We discuss the implications of our findings and provide recommendations for future XAI design. © 2023 Owner/Author.
KW  - Explainable AI (XAI)
KW  - Human-AI Collaboration
KW  - Human-AI Interaction
KW  - Human-Centered XAI
KW  - Interpretability
KW  - Local Explanations
KW  - XAI for Computer Vision
KW  - Human computer interaction
KW  - User interfaces
KW  - End-users
KW  - Explainable AI (XAI)
KW  - Human-AI collaboration
KW  - Human-AI interaction
KW  - Human-centered XAI
KW  - Interpretability
KW  - Local explanation
KW  - Mixed method
KW  - Real-world
KW  - XAI for computer vision
KW  - Computer vision
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 104
ER  -

TY  - CONF
AU  - Hassan, A.
AU  - Abdulhak, M.A.A.
AU  - Sulaiman, R.B.
AU  - Kahtan, H.
TI  - User centric explanations: A breakthrough for explainable models
PY  - 2021
T2  - 2021 International Conference on Information Technology, ICIT 2021 - Proceedings
C7  - 9491641
SP  - 702
EP  - 707
DO  - 10.1109/ICIT52682.2021.9491641
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112177882&doi=10.1109%2fICIT52682.2021.9491641&partnerID=40&md5=7b9459571ec0379673b4765a55181b63
AB  - Thanks to recent developments in explainable Deep Learning models, researchers have shown that these models can be incredibly successful and provide encouraging results. However, a lack of model interpretability can hinder the efficient implementation of Deep Learning models in real-world applications. This has encouraged researchers to develop and design a large number of algorithms to support transparency. Although studies have raised awareness of the importance of explainable artificial intelligence, the question of how to solve the needs of real users to understand artificial intelligence remains unanswered. In this paper, we provide an overview of the current state of the research field at Human-Centered Machine Learning and new methods for user-centric explanations for deep learning models. Furthermore, we outline future directions for interpretable machine learning and discuss the challenges facing this research field, as well as the importance and motivation behind developing user-centric explanations for Deep Learning models. © 2021 IEEE.
KW  - explainable artificial intelligence
KW  - human-AI interaction
KW  - machine learning
KW  - Deep learning
KW  - Efficient implementation
KW  - Interpretability
KW  - Learning models
KW  - Real-world
KW  - Research fields
KW  - User-centric
KW  - Learning systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Alm, C.O.
AU  - Alvarez, A.
AU  - Font, J.
AU  - Liapis, A.
AU  - Pederson, T.
AU  - Salo, J.
TI  - Invisible AI-driven HCI Systems - When, Why and How
PY  - 2020
T2  - ACM International Conference Proceeding Series
DO  - 10.1145/3419249.3420099
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123040047&doi=10.1145%2f3419249.3420099&partnerID=40&md5=cc5ca128c605ef629c8da5fc6c7b3939
AB  - The InvisibleAI (InvAI'20) workshop aims to systematically discuss a growing class of interactive systems that invisibly remove some decision-making tasks away from humans to machines, based on recent advances in artificial intelligence (AI), data science, and sensor or actuation technology. While the interest in the affordances as well as the risks of hidden pervasive AI are high on the agenda in public debate, discussion on the topic is needed within the human-computer interaction (HCI) community. In particular, we want to gather insights, ideas, and models for approaching the use of barely noticeable AI decision-making in systems design from a human-centered perspective, so as to make the most out of the automated systems and algorithms that support human activity both as designers and users. Concurrently, these systems should safeguard that humans remain in charge when it counts (high stakes decisions, privacy, monitoring lack of explainability and fairness, etc.). What to automate and what not to automate is often a system designer's choice [8]. By taking the established concept of explicit interaction between a system and its user as a point of departure, and inviting authors to provide examples from their own research, we aim to stimulate dynamic discussion while keeping the workshop concrete and system design-focused. The workshop especially directs itself to participants from the interaction design, AI, and HCI communities. The targeted scientific outcome of the workshop is an up-to-date ontology of invisible AI-HCI systems and hybrid human-AI collaboration mechanisms, and approaches. Additionally, we expect that the workgroups and the roundtables will provide starting points shaping continued discussions, new collaborations, and innovative scientific contributions that springboard from the workgroups' findings. The focus of the proposed workshop involves the bridging of two spaces of computational research that impact user experiences and societal domains (HCI and AI). Thus, the proposed workshop topic aligns well with the theme of this year's NordiCHI conference which is Shaping Experiences, Shaping Society. © 2020 Owner/Author.
KW  - artificial intelligence
KW  - Human-computer interaction
KW  - human-machine collaboration
KW  - Automation
KW  - Data Science
KW  - Decision making
KW  - Human computer interaction
KW  - Privacy by design
KW  - Public risks
KW  - Systems analysis
KW  - User experience
KW  - User interfaces
KW  - Actuation technologies
KW  - Collaboration mechanisms
KW  - Computational researches
KW  - Human computer interaction (HCI)
KW  - Interaction design
KW  - Interactive system
KW  - Point of departures
KW  - Scientific contributions
KW  - Artificial intelligence
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Gee, A.H.
AU  - Garcia-Olano, D.
AU  - Ghosh, J.
AU  - Paydarfar, D.
TI  - Explaining deep classification of time-series data with learned prototypes
PY  - 2019
T2  - CEUR Workshop Proceedings
VL  - 2429
SP  - 15
EP  - 22
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071664014&partnerID=40&md5=7ac5451acedcfebb59b81e2e19358207
AB  - The emergence of deep learning networks raises a need for explainable AI so that users and domain experts can be confident applying them to high-risk decisions. In this paper, we leverage data from the latent space induced by deep learning models to learn stereotypical representations or "prototypes" during training to elucidate the algorithmic decision-making process. We study how leveraging prototypes effect classification decisions of two dimensional time-series data in a few different settings: (1) electrocardiogram (ECG) waveforms to detect clinical bradycardia, a slowing of heart rate, in preterm infants, (2) respiration waveforms to detect apnea of prematurity, and (3) audio waveforms to classify spoken digits. We improve upon existing models by optimizing for increased prototype diversity and robustness, visualize how these prototypes in the latent space are used by the model to distinguish classes, and show that prototypes are capable of learning features on two dimensional time-series data to produce explainable insights during classification tasks. We show that the prototypes are capable of learning real-world features - bradycardia in ECG, apnea in respiration, and articulation in speech - as well as features within sub-classes. Our novel work leverages learned prototypical framework on two dimensional time-series data to produce explainable insights during classification tasks. © 2019 for this paper by its authors.
KW  - Decision making
KW  - Deep learning
KW  - Electrocardiography
KW  - Health care
KW  - Time series
KW  - Classification decision
KW  - Classification tasks
KW  - Decision making process
KW  - Deep classifications
KW  - Learning models
KW  - Learning network
KW  - Respiration waveform
KW  - Time-series data
KW  - Classification (of information)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 32
ER  -

TY  - JOUR
AU  - Neves, I.
AU  - Folgado, D.
AU  - Santos, S.
AU  - Barandas, M.
AU  - Campagner, A.
AU  - Ronzio, L.
AU  - Cabitza, F.
AU  - Gamboa, H.
TI  - Interpretable heartbeat classification using local model-agnostic explanations on ECGs
PY  - 2021
T2  - Computers in Biology and Medicine
VL  - 133
C7  - 104393
DO  - 10.1016/j.compbiomed.2021.104393
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105692908&doi=10.1016%2fj.compbiomed.2021.104393&partnerID=40&md5=af03380d665e7ac5f319cbacd40fa127
AB  - Treatment and prevention of cardiovascular diseases often rely on Electrocardiogram (ECG) interpretation. Dependent on the physician's variability, ECG interpretation is subjective and prone to errors. Machine learning models are often developed and used to support doctors; however, their lack of interpretability stands as one of the main drawbacks of their widespread operation. This paper focuses on an Explainable Artificial Intelligence (XAI) solution to make heartbeat classification more explainable using several state-of-the-art model-agnostic methods. We introduce a high-level conceptual framework for explainable time series and propose an original method that adds temporal dependency between time samples using the time series' derivative. The results were validated in the MIT-BIH arrhythmia dataset: we performed a performance's analysis to evaluate whether the explanations fit the model's behaviour; and employed the 1-D Jaccard's index to compare the subsequences extracted from an interpretable model and the XAI methods used. Our results show that the use of the raw signal and its derivative includes temporal dependency between samples to promote classification explanation. A small but informative user study concludes this study to evaluate the potential of the visual explanations produced by our original method for being adopted in real-world clinical settings, either as diagnostic aids or training resource. © 2021 Elsevier Ltd
KW  - Electrocardiogram
KW  - Explainable artificial intelligence
KW  - Heartbeat classification
KW  - Human–AI interfaces
KW  - Machine learning
KW  - Model-agnostic method
KW  - Time series
KW  - Usability
KW  - Visual explanations
KW  - Arrhythmias, Cardiac
KW  - Artificial Intelligence
KW  - Electrocardiography
KW  - Heart Rate
KW  - Humans
KW  - Machine Learning
KW  - Diseases
KW  - Electrocardiography
KW  - Machine learning
KW  - Cardiovascular disease
KW  - Explainable artificial intelligence
KW  - Heartbeat classifications
KW  - Human–AI interface
KW  - Local model
KW  - Machine learning models
KW  - Machine-learning
KW  - Model-agnostic method
KW  - Times series
KW  - Visual explanation
KW  - Article
KW  - artificial intelligence
KW  - binary classification
KW  - clinical article
KW  - conceptual framework
KW  - controlled study
KW  - data classification
KW  - detection algorithm
KW  - electrocardiogram
KW  - evaluation study
KW  - explainable artificial intelligence
KW  - heart arrhythmia
KW  - heart beat
KW  - heart ventricle extrasystole
KW  - human
KW  - image segmentation
KW  - Jaccard index
KW  - machine learning
KW  - model agnostic method
KW  - multiclass classification
KW  - priority journal
KW  - receiver operating characteristic
KW  - statistical model
KW  - taxonomy
KW  - time series analysis
KW  - usability
KW  - electrocardiography
KW  - heart arrhythmia
KW  - heart rate
KW  - Time series
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 70
ER  -

TY  - CONF
AU  - Lin, J.
AU  - Pan, S.
AU  - Lee, C.S.
AU  - Oviatt, S.
TI  - An explainable deep fusion network for affect recognition using physiological signals
PY  - 2019
T2  - International Conference on Information and Knowledge Management, Proceedings
SP  - 2069
EP  - 2072
DO  - 10.1145/3357384.3358160
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075478890&doi=10.1145%2f3357384.3358160&partnerID=40&md5=6132d67a7408138e61bf339ab82d2746
AB  - Affective computing is an emerging research area which provides insights on human's mental state through human-machine interaction. During the interaction process, bio-signal analysis is essential to detect human affective changes. Currently, machine learning methods to analyse bio-signals are the state of the art to detect the affective states, but most empirical works mainly deploy traditional machine learning methods rather than deep learning models due to the need for explainability. In this paper, we propose a deep learning model to process multimodal-multisensory bio-signals for affect recognition. It supports batch training for different sampling rate signals at the same time, and our results show significant improvement compared to the state of the art. Furthermore, the results are interpreted at the sensor- and signal- level to improve the explainaibility of our deep learning model. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
KW  - Affect recognition
KW  - Deep learning
KW  - Explainability
KW  - Multimodal fusion
KW  - Biomedical signal processing
KW  - Knowledge management
KW  - Machine learning
KW  - Affect recognition
KW  - Affective Computing
KW  - Explainability
KW  - Human machine interaction
KW  - Interaction process
KW  - Machine learning methods
KW  - Multi-modal fusion
KW  - Physiological signals
KW  - Deep learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 73
ER  -

TY  - CONF
AU  - Subramonyam, H.
AU  - Seifert, C.
AU  - Adar, E.
TI  - ProtoAI: Model-Informed Prototyping for AI-Powered Interfaces
PY  - 2021
T2  - International Conference on Intelligent User Interfaces, Proceedings IUI
SP  - 48
EP  - 58
DO  - 10.1145/3397481.3450640
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104556035&doi=10.1145%2f3397481.3450640&partnerID=40&md5=8fba24c315ca423713d673744baa3b9c
AB  - When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. However, AI poses challenges to design due to its dynamic behavior in response to training data, end-user data, and feedback. Designers must consider AI's uncertainties and offer adaptations such as explainability, error recovery, and automation vs. human task control. Unfortunately, current prototyping tools assume a black-box view of AI, forcing designers to work with separate tools to explore machine learning models, understand model performance, and align interface choices with model behavior. This introduces friction to rapid and iterative prototyping. We propose Model-Informed Prototyping (MIP), a workflow for AIX design that combines model exploration with UI prototyping tasks. Our system, ProtoAI, allows designers to directly incorporate model outputs into interface designs, evaluate design choices across different inputs, and iteratively revise designs by analyzing model breakdowns. We demonstrate how ProtoAI can readily operationalize human-AI design guidelines. Our user study finds that designers can effectively engage in MIP to create and evaluate AI-powered interfaces during AIX design.  © 2021 ACM.
KW  - AI-Powered Interfaces
KW  - Design-by-Instance
KW  - Human-Centered AI
KW  - Artificial intelligence
KW  - Dynamic behaviors
KW  - Interface designers
KW  - Interface designs
KW  - Iterative prototyping
KW  - Machine learning models
KW  - Model performance
KW  - Modeling behavior
KW  - Prototyping tools
KW  - User interfaces
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 34
ER  -

TY  - CONF
AU  - Rožanec, J.M.
AU  - Zajec, P.
AU  - Kenda, K.
AU  - Novalija, I.
AU  - Fortuna, B.
AU  - Mladenić, D.
AU  - Veliou, E.
AU  - Papamartzivanos, D.
AU  - Giannetsos, T.
AU  - Menesidou, S.A.
AU  - Alonso, R.
AU  - Cauli, N.
AU  - Recupero, D.R.
AU  - Kyriazis, D.
AU  - Sofianidis, G.
AU  - Theodoropoulos, S.
AU  - Soldatos, J.
TI  - STARdom: An Architecture for Trusted and Secure Human-Centered Manufacturing Systems
PY  - 2021
T2  - IFIP Advances in Information and Communication Technology
VL  - 633 IFIP
SP  - 199
EP  - 207
DO  - 10.1007/978-3-030-85910-7_21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115251296&doi=10.1007%2f978-3-030-85910-7_21&partnerID=40&md5=c71fecc8144710df8de74258e20292a5
AB  - There is a lack of a single architecture specification that addresses the needs of trusted and secure Artificial Intelligence systems with humans in the loop, such as human-centered manufacturing systems at the core of the evolution towards Industry 5.0. To realize this, we propose an architecture that integrates forecasts, Explainable Artificial Intelligence, supports collecting users’ feedback and uses Active Learning and Simulated Reality to enhance forecasts and provide decision-making recommendations. The architecture security is addressed at all levels. We align the proposed architecture with the Big Data Value Association Reference Architecture Model. We tailor it for the domain of demand forecasting and validate it on a real-world case study. © 2021, IFIP International Federation for Information Processing.
KW  - Active learning
KW  - Demand forecasting
KW  - Explainable Artificial Intelligence (XAI)
KW  - Industry 4.0
KW  - Smart manufacturing
KW  - Architecture
KW  - Decision making
KW  - Forecasting
KW  - Industrial management
KW  - Manufacture
KW  - Architecture specification
KW  - Artificial intelligence systems
KW  - Decision making recommendations
KW  - Demand forecasting
KW  - Human-centered manufacturing
KW  - Proposed architectures
KW  - Reference architecture
KW  - Simulated reality
KW  - Artificial intelligence
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 11
ER  -

TY  - JOUR
AU  - van de Poel, I.
TI  - Embedding Values in Artificial Intelligence (AI) Systems
PY  - 2020
T2  - Minds and Machines
VL  - 30
IS  - 3
SP  - 385
EP  - 409
DO  - 10.1007/s11023-020-09537-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090214633&doi=10.1007%2fs11023-020-09537-4&partnerID=40&md5=e6dfeba44f1e6bf7a4d43b13321bc393
AB  - Organizations such as the EU High-Level Expert Group on AI and the IEEE have recently formulated ethical principles and (moral) values that should be adhered to in the design and deployment of artificial intelligence (AI). These include respect for autonomy, non-maleficence, fairness, transparency, explainability, and accountability. But how can we ensure and verify that an AI system actually respects these values? To help answer this question, I propose an account for determining when an AI system can be said to embody certain values. This account understands embodied values as the result of design activities intended to embed those values in such systems. AI systems are here understood as a special kind of sociotechnical system that, like traditional sociotechnical systems, are composed of technical artifacts, human agents, and institutions but—in addition—contain artificial agents and certain technical norms that regulate interactions between artificial agents and other elements of the system. The specific challenges and opportunities of embedding values in AI systems are discussed, and some lessons for better embedding values in AI systems are drawn. © 2020, The Author(s).
KW  - Artificial agent
KW  - Artificial intelligence
KW  - Ethics
KW  - Institution
KW  - Multi-agent system
KW  - Norms
KW  - Sociotechnical system
KW  - Value embedding
KW  - Values
KW  - Embeddings
KW  - AI systems
KW  - Artificial agents
KW  - Design activity
KW  - Ethical principles
KW  - Human agent
KW  - Sociotechnical systems
KW  - Technical artifacts
KW  - Technical norms
KW  - Artificial intelligence
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 149
ER  -

TY  - JOUR
TI  - 20th IFIP WG 6.11 Conference on e-Business, e-Services and e-Society, I3E 2021
PY  - 2021
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 12896 LNCS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115191822&partnerID=40&md5=dd33c6f1dc80f55f3f9baa7b5865bb76
AB  - The proceedings contain 65 papers. The special focus in this conference is on e-Business, e-Services and e-Society. The topics include: Data-Driven Collaborative Human-AI Decision Making; Always Trust the Advice of AI in Difficulties? Perceptions Around AI in Decision Making; big Data Analytics Affordances for Social Innovation: A Theoretical Framework; COVID-19 Discrepancies Rising from Population Density Political Polarization Exacerbates Policy Gap; Ethics and AI Issues: Old Container with New Wine?; governing Artificial Intelligence and Algorithmic Decision Making: Human Rights and Beyond; Analysing AI via Husserl and Kuhn How a Phenomenological Approach to Artificial Intelligence Imposes a Paradigm Shift; the Ethical Implications of Lawtech; Deploying AI Governance Practices: A Revelatory Case Study; AI in the Workplace: Exploring Chatbot Use and Users’ Emotions; Towards Ecosystems for Responsible AI: Expectations on Sociotechnical Systems, Agendas, and Networks in EU Documents; Ethics in AI: A Software Developmental and Philosophical Perspective; stop Ordering Machine Learning Algorithms by Their Explainability! An Empirical Investigation of the Tradeoff Between Performance and Explainability; Gender Bias in AI: Implications for Managerial Practices; a Systematic Review of Fairness in Artificial Intelligence Algorithms; is Downloading This App Consistent with My Values?: Conceptualizing a Value-Centered Privacy Assistant; operationalization of a Glass Box Through Visualization: Applied to a Data Driven Profiling Approach; artificial Intelligence and the Evolution of Managerial Skills: An Exploratory Study; the Diffusion of Innovation Experience: Leveraging the Human Factor to Improve Technological Adoption Within an Organisation; exploring the Link Between Digitalization and Sustainable Development: Research Agendas; industry 4.0 and Organisations: Key Organisational Capabilities.
M3  - Conference review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - van der Waa, J.
AU  - Verdult, S.
AU  - van den Bosch, K.
AU  - van Diggelen, J.
AU  - Haije, T.
AU  - van der Stigchel, B.
AU  - Cocu, I.
TI  - Moral Decision Making in Human-Agent Teams: Human Control and the Role of Explanations
PY  - 2021
T2  - Frontiers in Robotics and AI
VL  - 8
C7  - 640647
DO  - 10.3389/frobt.2021.640647
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107645105&doi=10.3389%2ffrobt.2021.640647&partnerID=40&md5=9e403f9aa57a6d99eef76a6c7b85f5f0
AB  - With the progress of Artificial Intelligence, intelligent agents are increasingly being deployed in tasks for which ethical guidelines and moral values apply. As artificial agents do not have a legal position, humans should be held accountable if actions do not comply, implying humans need to exercise control. This is often labeled as Meaningful Human Control (MHC). In this paper, achieving MHC is addressed as a design problem, defining the collaboration between humans and agents. We propose three possible team designs (Team Design Patterns), varying in the level of autonomy on the agent’s part. The team designs include explanations given by the agent to clarify its reasoning and decision-making. The designs were implemented in a simulation of a medical triage task, to be executed by a domain expert and an artificial agent. The triage task simulates making decisions under time pressure, with too few resources available to comply with all medical guidelines all the time, hence involving moral choices. Domain experts (i.e., health care professionals) participated in the present study. One goal was to assess the ecological relevance of the simulation. Secondly, to explore the control that the human has over the agent to warrant moral compliant behavior in each proposed team design. Thirdly, to evaluate the role of agent explanations on the human’s understanding in the agent’s reasoning. Results showed that the experts overall found the task a believable simulation of what might occur in reality. Domain experts experienced control over the team’s moral compliance when consequences were quickly noticeable. When instead the consequences emerged much later, the experts experienced less control and felt less responsible. Possibly due to the experienced time pressure implemented in the task or over trust in the agent, the experts did not use explanations much during the task; when asked afterwards they however considered these to be useful. It is concluded that a team design should emphasize and support the human to develop a sense of responsibility for the agent’s behavior and for the team’s decisions. The design should include explanations that fit with the assigned team roles as well as the human cognitive state. © Copyright © 2021 van der Waa, Verdult, van den Bosch, van Diggelen, Haije, van der Stigchel and Cocu.
KW  - artificial intelligence
KW  - ethical AI
KW  - explainable AI
KW  - human study
KW  - human-agent teaming
KW  - meaningful human control
KW  - moral AI
KW  - team design patterns
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 19
ER  -

TY  - CONF
AU  - Qian, K.
AU  - Popa, L.
AU  - Sen, P.
TI  - SystemER: A humanintheloop system for explainable entity resolution
PY  - 2018
T2  - Proceedings of the VLDB Endowment
VL  - 12
IS  - 12
SP  - 1794
EP  - 1797
DO  - 10.14778/3352063.3352068
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074532449&doi=10.14778%2f3352063.3352068&partnerID=40&md5=a88ca915f6f348a4b095ff38f5a9fc97
AB  - Entity Resolution (ER) is the task of identifying different representations of the same real-world object. To achieve scalability and the desired level of quality, the typical ER pipeline includes multiple steps that may involve low-level coding and extensive human labor. We present SystemER, a tool for learning explainable ER models that reduces the human labor all throughout the stages of the ER pipeline. SystemER achieves explainability by learning rules that not only perform a given ER task but are human-comprehensible; this provides transparency into the learning process, and further enables verification and customization of the learned model by the domain experts. By leveraging a human in the loop and active learning, SystemER also ensures that a small number of labeled examples is sufficient to learn high-quality ER models. SystemER is a fulledged tool that includes an easy to use interface, support for both flat files and semi-structured data, and scale-out capabilities by distributing computation via Apache Spark. © 2019 VLDB Endowment.
KW  - Pipelines
KW  - Active Learning
KW  - Domain experts
KW  - Entity resolutions
KW  - Human-in-the-loop
KW  - Learning process
KW  - Learning rules
KW  - Real-world objects
KW  - Semi structured data
KW  - Learning systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 16
ER  -

TY  - CONF
AU  - Zhang, Z.T.
AU  - HuÃ mann, H.
TI  - How to Manage Output Uncertainty: Targeting the Actual End User Problem in Interactions with AI
PY  - 2021
T2  - CEUR Workshop Proceedings
VL  - 2903
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110534161&partnerID=40&md5=45ef43fd6796cf3a8c88c4f6295d9464
AB  - Given the opaqueness and complexity of modern AI algorithms, there is currently a strong focus on developing transparent and explainable AI, especially in high-stakes domains. We claim that opaqueness and complexity are not the core issues for end users when interacting with AI. Instead, we propose that the output uncertainty inherent to AI systems is the actual problem, with opaqueness and complexity as contributing factors. Transparency and explainability should therefore not be the end goals, as such a focus tends to place the human into a passive supervisory role in what is in reality an algorithm-centered system design. To enable effective management of output uncertainty, we believe it is necessary to focus on truly human-centered AI designs that keep the human in an active role of control. We discuss the conceptual implications of such a shift in focus and give examples from literature to illustrate the more holistic, interactive designs that we envision. © 2021 Copyright for this paper by its authors.
KW  - Explainability
KW  - Human-ai interaction
KW  - Intelligent systems
KW  - Output uncertainty
KW  - Transparency
KW  - User control
KW  - User interfaces
KW  - AI algorithms
KW  - AI systems
KW  - Contributing factor
KW  - Effective management
KW  - End users
KW  - Interactive design
KW  - Artificial intelligence
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Lage, I.
AU  - Doshi-Velez, F.
AU  - Lifschitz, D.
AU  - Amir, O.
TI  - Toward robust policy summarization
PY  - 2019
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
VL  - 4
SP  - 2081
EP  - 2083
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069868681&partnerID=40&md5=aa27f20c7d8b7522f484beff1ce8af29
AB  - AI agents are being developed to help people with high stakes decision-making processes from driving cars to prescribing drugs. It is therefore becoming increasingly important to develop "explainable AI" methods that help people understand the behavior of such agents. Summaries of agent policies can help human users anticipate agent behavior and facilitate more effective collaboration. Prior work has framed agent summarization as a machine teaching problem where examples of agent behavior are chosen to maximize reconstruction quality under the assumption that people do inverse reinforcement learning to infer an agent's policy from demonstrations. We compare summaries generated under this assumption to summaries generated under the assumption that people use imitation learning. We show through simulations that in some domains, there exist summaries that produce high-quality reconstructions under different models, but in other domains, only matching the summary extraction model to the reconstruction model produces high-quality reconstructions. These results highlight the importance of assuming correct computational models for how humans extrapolate from a summary, suggesting human-in-the-loop approaches to summary extraction. © 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.
KW  - Explainable AI
KW  - Policy summarization
KW  - Behavioral research
KW  - Decision making
KW  - Extraction
KW  - Inverse problems
KW  - Multi agent systems
KW  - Reinforcement learning
KW  - Repair
KW  - Computational model
KW  - Decision making process
KW  - High quality reconstruction
KW  - Human-in-the-loop
KW  - Imitation learning
KW  - Inverse reinforcement learning
KW  - Reconstruction quality
KW  - Teaching problems
KW  - Autonomous agents
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 11
ER  -

TY  - CONF
AU  - Hoffman, R.R.
AU  - Klein, G.
AU  - Mueller, S.T.
TI  - Explaining explanation for "explainable AI
PY  - 2018
T2  - Proceedings of the Human Factors and Ergonomics Society
VL  - 1
SP  - 197
EP  - 201
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072724450&partnerID=40&md5=ddd1ae7deed5ea1f065ddf2818765134
AB  - What makes for an explanation of "black box" AI systems such as Deep Nets? We reviewed the pertinent literatures on explanation and derived key ideas. This set the stage for our empirical inquiries, which include conceptual cognitive modeling, the analysis of a corpus of cases of "naturalistic explanation" of computational systems, computational cognitive modeling, and the development of measures for performance evaluation. The purpose of our work is to contribute to the program of research on "Explainable AI." In this report we focus on our initial synthetic modeling activities and the development of measures for the evaluation of explainability in human-machine work systems. © 2018 Human Factors an Ergonomics Society Inc.. All rights reserved.
KW  - Ergonomics
KW  - AI systems
KW  - Black boxes
KW  - Cognitive model
KW  - Computational cognitive modeling
KW  - Computational system
KW  - Human-machine
KW  - Synthetic models
KW  - Work system
KW  - Cognitive systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 74
ER  -

TY  - CONF
AU  - Shen, H.
AU  - Liao, K.
AU  - Liao, Z.
AU  - Doornberg, J.
AU  - Qiao, M.
AU  - Van Den Hengel, A.
AU  - Verjans, J.W.
TI  - Human-AI Interactive and Continuous Sensemaking: A Case Study of Image Classification using Scribble Attention Maps
PY  - 2021
T2  - Conference on Human Factors in Computing Systems - Proceedings
DO  - 10.1145/3411763.3451798
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105775455&doi=10.1145%2f3411763.3451798&partnerID=40&md5=90e11ee29f0c40ac8e3a7da8fc2bff24
AB  - Advances in Artificial Intelligence (AI), especially the stunning achievements of Deep Learning (DL) in recent years, have shown AI/DL models possess remarkable understanding towards the logic reasoning behind the solved tasks. However, human understanding towards what knowledge is captured by deep neural networks is still elementary and this has a detrimental effect on human's trust in the decisions made by AI systems. Explainable AI (XAI) is a hot topic in both AI and HCI communities in order to open up the blackbox to elucidate the reasoning processes of AI algorithms in such a way that makes sense to humans. However, XAI is only half of human-AI interaction and research on the other half - human's feedback on AI explanations together with AI making sense of the feedback - is generally lacking. Human cognition is also a blackbox to AI and effective human-AI interaction requires unveiling both blackboxes to each other for mutual sensemaking. The main contribution of this paper is a conceptual framework for supporting effective human-AI interaction, referred to as interactive and continuous sensemaking (HAICS). We further implement this framework in an image classification application using deep Convolutional Neural Network (CNN) classifiers as a browser-based tool that displays network attention maps to the human for explainability and collects human's feedback in the form of scribble annotations overlaid onto the maps. Experimental results using a real-world dataset has shown significant improvement of classification accuracy (the AI performance) with the HAICS framework. © 2021 ACM.
KW  - attention map
KW  - explainable AI
KW  - image classification
KW  - interactive sensemaking
KW  - scribble interaction
KW  - Classification (of information)
KW  - Computation theory
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Deep neural networks
KW  - Human engineering
KW  - AI algorithms
KW  - Classification accuracy
KW  - Conceptual frameworks
KW  - Human cognition
KW  - Human understanding
KW  - Logic reasoning
KW  - Reasoning process
KW  - Sensemaking
KW  - Image classification
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 10
ER  -

TY  - CONF
AU  - Krishnan, J.
AU  - Coronado, P.
AU  - Reed, T.
TI  - Seva: A systems engineer’s virtual assistant
PY  - 2019
T2  - CEUR Workshop Proceedings
VL  - 2350
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064968994&partnerID=40&md5=d6af8f4e57864861809ba87ceb2d2063
AB  - A Systems Engineer’s Virtual Assistant (SEVA) is a novel attempt to bridge the gap between Natural Language Processing (NLP), Knowledge Base (KB) Construction research, and NASA’s Systems Engineering domain. In this work, we propose the design of an explainable, human-in-the-loop, and interactive personal assistant system. The assistant will help a Systems Engineer in their daily work environment through complex information management and high-level question-answering to augment their problem-solving abilities. We describe the fundamental characteristics of the assistant by understanding operational, functional, and system requirements from Systems Engineers and NASA’s Systems Engineering Handbook. The assistant is designed to act as a workbench to manage dynamic information about projects and analyze hypothetical scenarios. It is also designed to make logical inferences and perform temporal reasoning by handling domain information and information related to schedule and resources. In addition, the system learns new information over time by interacting with its user and can perform case-based reasoning from previous experiences. The knowledge base design describes a novel hybrid approach to build a domain-independent common-sense framework with which domain-specific engineers can attune it and build their projects. Using these specific objectives and constraints, the architecture of a personal assistant is proposed. Main contributions of this design paper are Systems Engineering (SE) domain analysis, a survey of existing research, preliminary experiments using the state-of-the-art systems to explore the feasibility, a proposal of a complete architecture with component level detail, and identification of areas that require further research and development. Copyright held by the author(s).
KW  - Explainable AI
KW  - Intelligent Agents
KW  - Knowledge Base Construction
KW  - Natural Language Processing
KW  - Ontology
KW  - Question-Answering
KW  - Systems Engineering
KW  - Bridges
KW  - Case based reasoning
KW  - Engineering education
KW  - Engineers
KW  - Information management
KW  - Intelligent agents
KW  - Knowledge based systems
KW  - Knowledge engineering
KW  - Learning algorithms
KW  - Machine learning
KW  - NASA
KW  - Ontology
KW  - Springs (components)
KW  - Systems engineering
KW  - Fundamental characteristics
KW  - Knowledge base designs
KW  - Knowledge-base construction
KW  - NAtural language processing
KW  - Problem-solving abilities
KW  - Question Answering
KW  - Research and development
KW  - State-of-the-art system
KW  - Natural language processing systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Alexandrov, N.M.
TI  - Explainable AI decisions for human-autonomy interactions
PY  - 2017
T2  - 17th AIAA Aviation Technology, Integration, and Operations Conference, 2017
DO  - 10.2514/6.2017-3991
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085850591&doi=10.2514%2f6.2017-3991&partnerID=40&md5=cb81b95dbe5686a86b83c2f0c3a3b714
AB  - Autonomous systems governed by a variety of adaptive algorithms are making an appearance in safety-critical and time-critical domains, such as automobile traffic and aviation. Until autonomous systems are proven and perceived to be as or more adaptable than humans, and resilient in the face of unanticipated faults and variable conditions, humans will have to remain in ultimate control of decision-making, while supported by machine-based information and advice. Human-machine interaction in many domains has numerous well-known difficulties, including lack or excess of trust, both of which can lead to serious problems, especially when human decision makers are overwhelmed with information. Interactions between humans and autonomous systems-a subset of general human-machine interactions--will be more problematic still. One complication is that sophisticated machine learning systems produce outputs that may be difficult for a human user to interpret. The development of trust has been a major motivation for the area of explainable AI, or XAI. We offer initial observations about the relation between trust and explicability and propose candidate methods for formalizing these notions in technical domains. © 2017, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.
KW  - Adaptive algorithms
KW  - Decision making
KW  - Man machine systems
KW  - Safety engineering
KW  - Automobile traffic
KW  - Autonomous systems
KW  - Human decisions
KW  - Human machine interaction
KW  - Human users
KW  - Sophisticated machines
KW  - Time-critical domains
KW  - Variable conditions
KW  - Learning systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 10
ER  -

TY  - CONF
AU  - Pan, M.
AU  - Huang, W.
AU  - Li, Y.
AU  - Zhou, X.
AU  - Luo, J.
TI  - XGAIL: Explainable Generative Adversarial Imitation Learning for Explainable Human Decision Analysis
PY  - 2020
T2  - Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
SP  - 1334
EP  - 1343
DO  - 10.1145/3394486.3403186
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090418654&doi=10.1145%2f3394486.3403186&partnerID=40&md5=52047ca2260182747f032111d168524f
AB  - To make daily decisions, human agents devise their own "strategies" governing their mobility dynamics (e.g., taxi drivers have preferred working regions and times, and urban commuters have preferred routes and transit modes). Recent research such as generative adversarial imitation learning (GAIL) demonstrates successes in learning human decision-making strategies from their behavior data using deep neural networks (DNNs), which can accurately mimic how humans behave in various scenarios, e.g., playing video games, etc. However, such DNN-based models are "black box" models in nature, making it hard to explain what knowledge the models have learned from human, and how the models make such decisions, which was not addressed in the literature of imitation learning. This paper addresses this research gap by proposing xGAIL, the first explainable generative adversarial imitation learning framework. The proposed xGAIL framework consists of two novel components, including Spatial Activation Maximization (SpatialAM) and Spatial Randomized Input Sampling Explanation (SpatialRISE), to extract both global and local knowledge from a well-trained GAIL model that explains how a human agent makes decisions. Especially, we take taxi drivers' passenger-seeking strategy as an example to validate the effectiveness of the proposed xGAIL framework. Our analysis on a large-scale real-world taxi trajectory data shows promising results from two aspects: i) global explainable knowledge of what nearby traffic condition impels a taxi driver to choose a particular direction to find the next passenger, and ii) local explainable knowledge of what key (sometimes hidden) factors a taxi driver considers when making a particular decision. © 2020 ACM.
KW  - explainable artificial intelligence
KW  - generative adversarial imitation learning
KW  - human behavior analysis
KW  - Behavioral research
KW  - Decision making
KW  - Deep learning
KW  - Deep neural networks
KW  - Taxicabs
KW  - Human decision making
KW  - Imitation learning
KW  - Local knowledge
KW  - Novel component
KW  - Preferred routes
KW  - Recent researches
KW  - Traffic conditions
KW  - Trajectory data
KW  - Data mining
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 31
ER  -

TY  - CONF
AU  - Knight, H.
AU  - Simmons, R.
TI  - Laban head-motions convey robot state: A call for robot body language
PY  - 2016
T2  - Proceedings - IEEE International Conference on Robotics and Automation
VL  - 2016-June
C7  - 7487451
SP  - 2881
EP  - 2888
DO  - 10.1109/ICRA.2016.7487451
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977520910&doi=10.1109%2fICRA.2016.7487451&partnerID=40&md5=8e19180cfffcbe08d34f4cdf4ba6d7c6
AB  - Functional robots are an increasing presence in shared human-machine environments. Humans efficiently parse motion expressions, gaining an immediate impression of an agent's current action and state. Past work has shown that motion can effectively reveal a robot's current task objective to bystanders and collaborators, however, the layering of expression on pre-existing robot task motions has yet to be explored. Rather than showing us what the robot is doing, these layered motion characteristics leverage the how of the task motions to convey additional robot attitudes, e.g., confidence, adherence to deadline or flexibility of attention. To lay the foundations for this objective, we adapt the Laban Efforts, a system from dance and acting training in use for over 50 years. We operationalize features representing the four Laban Efforts (Time, Space, Weight, and Flow) to the movements of a 2-DOF Nao head and a 4-DOF Keepon robot during simple dance and look-for-someone behaviors. Using online survey, we collect 1028 motion ratings for 72 robot motion videos depicting contrasting Effort motion examples. We achieve statistically significant legibility results for all four Effort implementations. Even without human degrees of freedom, we find that robot motion patterns can convey complex expressions to people. © 2016 IEEE.
KW  - Degrees of freedom (mechanics)
KW  - Robotics
KW  - Head motion
KW  - Human-machine
KW  - Motion characteristics
KW  - Motion expression
KW  - Online surveys
KW  - Robot motion
KW  - Robot tasks
KW  - Task motion
KW  - Robots
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 51
ER  -

TY  - CONF
AU  - Das, H.P.
AU  - Konstantakopoulos, I.C.
AU  - Manasawala, A.B.
AU  - Veeravalli, T.
AU  - Liu, H.
AU  - Spanos, C.J.
TI  - A novel graphical lasso based approach towards segmentation analysis in energy game-theoretic frameworks
PY  - 2019
T2  - Proceedings - 18th IEEE International Conference on Machine Learning and Applications, ICMLA 2019
C7  - 8999336
SP  - 1702
EP  - 1709
DO  - 10.1109/ICMLA.2019.00277
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080916211&doi=10.1109%2fICMLA.2019.00277&partnerID=40&md5=242d72bdc3e2cd7c3557b5e1f7ff9101
AB  - Energy game-theoretic frameworks have emerged to be a successful strategy to encourage energy efficient behavior in large scale by leveraging human-in-the-loop strategy. A number of such frameworks have been introduced over the years which formulate the energy saving process as a competitive game with appropriate incentives for energy efficient players. However, prior works involve an incentive design mechanism which is dependent on knowledge of utility functions for all the players in the game, which is hard to compute especially when the number of players is high, common in energy game-theoretic frameworks. Our research proposes that the utilities of players in such a framework can be grouped together to a relatively small number of clusters, and the clusters can then be targeted with tailored incentives. The key to above segmentation analysis is to learn the features leading to human decision making towards energy usage in competitive environments. We propose a novel graphical lasso based approach to perform such segmentation, by studying the feature correlations in a real-world energy social game dataset. To further improve the explainability of the model, we perform causality study using grangers causality. Proposed segmentation analysis results in characteristic clusters demonstrating different energy usage behaviors. We also present avenues to implement intelligent incentive design using proposed segmentation method. © 2019 IEEE.
KW  - Energy Game-Theoretic Frameworks
KW  - Graphical Lasso
KW  - Segmentation Analysis
KW  - Smart Building
KW  - Decision making
KW  - Energy efficiency
KW  - Intelligent buildings
KW  - Machine learning
KW  - Competitive environment
KW  - Feature correlation
KW  - Game-theoretic
KW  - Graphical lassos
KW  - Human decision making
KW  - Number of clusters
KW  - Segmentation analysis
KW  - Segmentation methods
KW  - Game theory
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 14
ER  -

TY  - CONF
AU  - Wang, D.
AU  - Churchill, E.
AU  - Maes, P.
AU  - Fan, X.
AU  - Shneiderman, B.
AU  - Shi, Y.
AU  - Wang, Q.
TI  - From human-human collaboration to Human-AI collaboration: Designing AI systems that can work together with people
PY  - 2020
T2  - Conference on Human Factors in Computing Systems - Proceedings
C7  - 3381069
DO  - 10.1145/3334480.3381069
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090243725&doi=10.1145%2f3334480.3381069&partnerID=40&md5=b9586a9f9296988f0ff835a02e65a82c
AB  - Artificial Intelligent (AI) and Machine Learning (ML) algorithms are coming out of research labs into the real-world applications, and recent research has focused a lot on Human-AI Interaction (HAI) and Explainable AI (XAI). However, Interaction is not the same as Collaboration. Collaboration involves mutual goal understanding, preemptive task co-management and shared progress tracking. Most of human activities today are done collaboratively, thus, to integrate AI into the already-complicated human workflow, it is critical to bring the Computer-Supported Cooperative Work (CSCW) perspective into the root of the algorithmic research and plan for a Human-AI Collaboration future of work. In this panel we ask: Can this future for trusted human-AI collaboration be realized? If so, what will it take? This panel will bring together HCI experts who work on human collaboration and AI applications in various application contexts, from industry and academia and from both the U.S. and China. Panelists will engage the audience through discussion of their shared and diverging visions, and through suggestions for opportunities and challenges for the future of human-AI collaboration. © 2020 Owner/Author.
KW  - Ai partner
KW  - Ai-powered healthcare
KW  - Computer-supported corporative work
KW  - Explainable ai
KW  - Group collaboration
KW  - Human-ai collaboration
KW  - Trusted ai
KW  - Behavioral research
KW  - Human engineering
KW  - AI applications
KW  - Algorithmic research
KW  - Application contexts
KW  - Artificial intelligent
KW  - Future of works
KW  - Human activities
KW  - Recent researches
KW  - Research labs
KW  - Machine learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 161
ER  -

TY  - CONF
AU  - Cech, F.
TI  - Beyond transparency: Exploring algorithmic accountability
PY  - 2020
T2  - Proceedings of the International ACM SIGGROUP Conference on Supporting Group Work
SP  - 11
EP  - 14
DO  - 10.1145/3323994.3371015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078363671&doi=10.1145%2f3323994.3371015&partnerID=40&md5=3717fe464ef055c4f1cca70125f0b0e6
AB  - Many of the ubiquitous algorithmic systems permeating society have come under scrutiny due to their lack of accountability. As algorithmic decision making increasingly affects our lives, calls to improve the transparency of these systems are met with social, legal and technical limitations that challenge whether transparency alone is the solution to algorithmic accountability. In my dissertation, I explore the role of algorithmic tranparency, algorithmic literacy and related issues as approaches towards holding algorithmic systems more accountable. Bridging HCI and STS communities, my work is grounded in a critical ethnography of algorithmic systems and their impact on its stakeholders. Through this approach, I aim to provide both theoretical insights and material solutions to the problem of accountability. By unpacking the complex socio-technical assemblage that make up these systems and employing both participatory and user-centred design principles, my goal is to co-design measures that support sense-making of algorithmic processes and allow holding these systems accountable. © is held by the owner/author(s).
KW  - Algorithmic Accountability
KW  - Algorithmic Literacy
KW  - Algorithmic Transparency
KW  - Critical Algorithm Studies
KW  - Participatory Design
KW  - Decision making
KW  - Enterprise resource planning
KW  - User centered design
KW  - Algorithm study
KW  - Algorithmic Accountability
KW  - Algorithmic Literacy
KW  - Algorithmic process
KW  - Critical ethnography
KW  - Participatory design
KW  - Sociotechnical
KW  - Technical limitations
KW  - Transparency
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Smith-Renner, A.
AU  - Rua, R.
AU  - Colony, M.
TI  - Towards an explainable threat detection tool
PY  - 2019
T2  - CEUR Workshop Proceedings
VL  - 2327
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063230002&partnerID=40&md5=18f71c59a1a8ce3e82b3f60b1805fced
AB  - In general, threats can be loosely divided into two categories – known threats and unknown threats. Traditional threat detection systems are limited to the identification of known threats that have been previously encountered and labeled by a security expert. These supervised learning systems are able to learn to detect and identify known threats but are unable to react to unknown threats. To this end, we have developed an unsupervised learning anomaly detection system to identify anomalous behavior without training data. Our system’s interactive interface supports human-machine teaming to classify these identified anomalies as threats or benign events; however, system transparency is required to enhance operator trust and improve their feedback into the system. Transparency in this case is particularly challenging as our anomaly detection framework is based on algorithms which are inherently hard to explain (neural networks). In this paper, we introduce a real-world task and system that requires transparency, and we propose explanation methods for increasing the transparency of our threat detection tool alongside a user study for evaluating these explanations. © 2019 for the individual papers by the papers’ authors. Copying permitted for private and academic purposes. This volume is published and copyrighted by its editors.
KW  - Anomaly detection
KW  - Explanations
KW  - Human-machine teaming
KW  - Transparency
KW  - Inspection equipment
KW  - Machine learning
KW  - Transparency
KW  - User interfaces
KW  - Anomalous behavior
KW  - Anomaly detection frameworks
KW  - Anomaly detection systems
KW  - Explanations
KW  - Human-machine
KW  - Interactive interfaces
KW  - Security experts
KW  - Threat detection system
KW  - Anomaly detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 5
ER  -

TY  - CONF
AU  - Cirqueira, D.
AU  - Helfert, M.
AU  - Bezbradica, M.
TI  - Towards Design Principles for User-Centric Explainable AI in Fraud Detection
PY  - 2021
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 12797 LNAI
SP  - 21
EP  - 40
DO  - 10.1007/978-3-030-77772-2_2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112178703&doi=10.1007%2f978-3-030-77772-2_2&partnerID=40&md5=60f3b781993f600dd9942b7bb89535ac
AB  - Experts rely on fraud detection and decision support systems to analyze fraud cases, a growing problem in digital retailing and banking. With the advent of Artificial Intelligence (AI) for decision support, those experts face the black-box problem and lack trust in AI predictions for fraud. Such an issue has been tackled by employing Explainable AI (XAI) to provide experts with explained AI predictions through various explanation methods. However, fraud detection studies supported by XAI lack a user-centric perspective and discussion on how principles are deployed, both important requirements for experts to choose an appropriate explanation method. On the other hand, recent research in Information Systems (IS) and Human-Computer Interaction highlights the need for understanding user requirements to develop tailored design principles for decision support systems. In this research, we adopt a design science research methodology and IS theoretical lens to develop and evaluate design principles, which align fraud expert’s tasks with explanation methods for Explainable AI decision support. We evaluate the utility of these principles using an information quality framework to interview experts in banking fraud, plus a simulation. The results show that the principles are an useful tool for designing decision support systems for fraud detection with embedded user-centric Explainable AI. © 2021, Springer Nature Switzerland AG.
KW  - Artificial intelligence
KW  - Decision support systems
KW  - Design principles
KW  - Explainable AI
KW  - Fraud detection
KW  - HCI
KW  - Human-AI interaction
KW  - Human-centered AI
KW  - Banking
KW  - Crime
KW  - Decision support systems
KW  - Embedded systems
KW  - Human computer interaction
KW  - Quality control
KW  - User centered design
KW  - Decision supports
KW  - Design Principles
KW  - Design-science researches
KW  - Fraud detection
KW  - Information quality framework
KW  - Recent researches
KW  - User requirements
KW  - User-centric
KW  - Artificial intelligence
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 23
ER  -

TY  - CONF
AU  - Kökciyan, N.
AU  - Parsons, S.
AU  - Sassoon, I.
AU  - Sklar, E.
AU  - Modgil, S.
TI  - An Argumentation-Based Approach to Generate Domain-Specific Explanations
PY  - 2020
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 12520 LNAI
SP  - 319
EP  - 337
DO  - 10.1007/978-3-030-66412-1_20
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101334983&doi=10.1007%2f978-3-030-66412-1_20&partnerID=40&md5=b576704b91c9b99a17f71e4cbb60fa8f
AB  - In argumentation theory, argument schemes are constructs to generalise common patterns of reasoning; whereas critical questions (CQs) capture the reasons why argument schemes might not generate arguments. Argument schemes together with CQs are widely used to instantiate arguments; however when it comes to making decisions, much less attention has been paid to the attacks among arguments. This paper provides a high-level description of the key elements necessary for the formalisation of argumentation frameworks such as argument schemes and CQs. Attack schemes are then introduced to represent attacks among arguments, which enable the definition of domain-specific attacks. One algorithm is articulated to operationalise the use of schemes to generate an argumentation framework, and another algorithm to support decision making by generating domain-specific explanations. Such algorithms can then be used by agents to make recommendations and to provide explanations for humans. The applicability of this approach is demonstrated within the context of a medical case study. © 2020, Springer Nature Switzerland AG.
KW  - Computational argumentation
KW  - Explainability
KW  - Human-agent systems
KW  - Decision making
KW  - Argumentation frameworks
KW  - Argumentation theory
KW  - Critical questions
KW  - Domain specific
KW  - Formalisation
KW  - High level description
KW  - Key elements
KW  - Making decision
KW  - Multi agent systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 7
ER  -

TY  - CONF
AU  - Ehsan, U.
AU  - Liao, Q.V.
AU  - Muller, M.
AU  - Riedl, M.O.
AU  - Weisz, J.D.
TI  - Expanding explainability: Towards social transparency in ai systems
PY  - 2021
T2  - Conference on Human Factors in Computing Systems - Proceedings
DO  - 10.1145/3411764.3445188
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106073891&doi=10.1145%2f3411764.3445188&partnerID=40&md5=4f8e76cd0c93dff9717e3cc3c2072b17
AB  - As AI-powered systems increasingly mediate consequential decision-making, their explainability is critical for end-users to take informed and accountable actions. Explanations in human-human interactions are socially-situated. AI systems are often socio-organizationally embedded. However, Explainable AI (XAI) approaches have been predominantly algorithm-centered. We take a developmental step towards socially-situated XAI by introducing and exploring Social Transparency (ST), a sociotechnically informed perspective that incorporates the socio-organizational context into explaining AI-mediated decision-making. To explore ST conceptually, we conducted interviews with 29 AI users and practitioners grounded in a speculative design scenario. We suggested constitutive design elements of ST and developed a conceptual framework to unpack ST's efect and implications at the technical, decision-making, and organizational level. The framework showcases how ST can potentially calibrate trust in AI, improve decision-making, facilitate organizational collective actions, and cultivate holistic explainability. Our work contributes to the discourse of Human-Centered XAI by expanding the design space of XAI. © 2021 ACM.
KW  - Artifcial intelligence
KW  - Explainable ai
KW  - Explanations
KW  - Human-ai interaction
KW  - Social transparency
KW  - socio-organizational context
KW  - Sociotechnical
KW  - Behavioral research
KW  - Embedded systems
KW  - Human engineering
KW  - Transparency
KW  - Collective action
KW  - Conceptual frameworks
KW  - Design elements
KW  - Design spaces
KW  - Human-human interactions
KW  - Organizational context
KW  - Organizational levels
KW  - Social transparencies
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 289
ER  -

TY  - CONF
AU  - Wang, J.
AU  - Liu, C.
AU  - Zhu, M.
AU  - Guo, P.
AU  - Hu, Y.
TI  - Sensor Data Based System-Level Anomaly Prediction for Smart Manufacturing
PY  - 2018
T2  - Proceedings - 2018 IEEE International Congress on Big Data, BigData Congress 2018 - Part of the 2018 IEEE World Congress on Services
C7  - 8457744
SP  - 158
EP  - 165
DO  - 10.1109/BigDataCongress.2018.00028
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057745185&doi=10.1109%2fBigDataCongress.2018.00028&partnerID=40&md5=3025a2f39d5dfa2936165fed56f844d2
AB  - With the popularity of Supervisory Information System (SIS), Supervisory Control and Data Acquisition (SCADA) system and Internet of Things (IoT) sensors, we can easily obtain abundant sensor data in manufacturing. We could save manufacturing maintenance costs and prevent further damages if we can accurately predict system anomalies from the sensor data. Yet learning from individual sensors often cannot directly determine whether the system will have anomaly because each sensor only measures a partial state of a big system. By detecting events across sensors collectively and their temporal dependencies, this paper proposes a new system-level anomaly prediction framework by mining anomaly dependency graph from sensor data. The advantages of the approach include explainability, collective prediction and temporal sensitivity. We applied our approach with a real-world power plant dataset to evaluate its feasibility. © 2018 IEEE.
KW  - Anomaly Prediction
KW  - Data Stream Mining
KW  - Predictive Maintenance
KW  - Sensor Data Driven
KW  - Smart Manufacturing
KW  - Data acquisition
KW  - Flow control
KW  - Forecasting
KW  - Internet of things
KW  - Manufacture
KW  - Anomaly predictions
KW  - Data stream mining
KW  - Predictive maintenance
KW  - Sensor data
KW  - Smart manufacturing
KW  - Big data
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 28
ER  -

TY  - CONF
AU  - Jarrahi, M.H.
AU  - Haeri, M.
TI  - Developing a Symbiotic Workflow between Pathologists and AI through Parameterization and Implicitization
PY  - 2021
T2  - CEUR Workshop Proceedings
VL  - 2903
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110557354&partnerID=40&md5=536b446c69e79fe6ec2a4640ed41e349
AB  - Pathology is a fundamental element of modern medicine that determines the final diagnosis and portrays the prognosis in most medical conditions. Due to continuous improvements in AI capabilities (e.g., object recognition and image processing), intelligent systems are bound to play a key role in augmenting pathology research and clinical practices. Despite the pervasive deployment of computational approaches in similar fields such as radiology, there has been less success in integrating AI in clinical practices and histopathologic diagnosis. This partly has to do with the opacity of end-to-end AI systems, which raises issues of interoperability and accountability of medical practices. In this article, we draw on interactive machine learning to take advantage of AI in digital pathology in an attempt to open the Blackbox of AI and generate a more effective partnership between pathologists and AI systems based on the metaphors of parameterization and implicitization. © 2020 Copyright for this paper by its authors.
KW  - Accountability
KW  - Artificial intelligence
KW  - End-to-end ai
KW  - Explainable ai
KW  - Histopathologic diagnosis
KW  - Human-ai partnership
KW  - Interactive machine learning
KW  - Interpretability
KW  - Medical diagnosis
KW  - Pathology
KW  - Clinical research
KW  - Diagnosis
KW  - Image enhancement
KW  - Intelligent systems
KW  - Interoperability
KW  - Object recognition
KW  - Pathology
KW  - Clinical practices
KW  - Computational approach
KW  - Continuous improvements
KW  - Digital pathologies
KW  - Histopathologic diagnosis
KW  - Interactive machine learning
KW  - Medical conditions
KW  - Medical practice
KW  - User interfaces
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lepri, B.
AU  - Oliver, N.
AU  - Letouzé, E.
AU  - Pentland, A.
AU  - Vinck, P.
TI  - Fair, Transparent, and Accountable Algorithmic Decision-making Processes: The Premise, the Proposed Solutions, and the Open Challenges
PY  - 2018
T2  - Philosophy and Technology
VL  - 31
IS  - 4
SP  - 611
EP  - 627
DO  - 10.1007/s13347-017-0279-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050889982&doi=10.1007%2fs13347-017-0279-x&partnerID=40&md5=abcacb3682c48cee3be18f3007121318
AB  - The combination of increased availability of large amounts of fine-grained human behavioral data and advances in machine learning is presiding over a growing reliance on algorithms to address complex societal problems. Algorithmic decision-making processes might lead to more objective and thus potentially fairer decisions than those made by humans who may be influenced by greed, prejudice, fatigue, or hunger. However, algorithmic decision-making has been criticized for its potential to enhance discrimination, information and power asymmetry, and opacity. In this paper, we provide an overview of available technical solutions to enhance fairness, accountability, and transparency in algorithmic decision-making. We also highlight the criticality and urgency to engage multi-disciplinary teams of researchers, practitioners, policy-makers, and citizens to co-develop, deploy, and evaluate in the real-world algorithmic decision-making processes designed to maximize fairness and transparency. In doing so, we describe the Open Algortihms (OPAL) project as a step towards realizing the vision of a world where data and algorithms are used as lenses and levers in support of democracy and development. © 2017, Springer Science+Business Media B.V.
KW  - Accountability
KW  - Algorithmic decision-making
KW  - Algorithmic transparency
KW  - Fairness
KW  - Social good
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 455
ER  -

TY  - JOUR
AU  - Ogunbiyi, N.
AU  - Basukoski, A.
AU  - Chaussalet, T.
TI  - An exploration of ethical decision making with intelligence augmentation
PY  - 2021
T2  - Social Sciences
VL  - 10
IS  - 2
C7  - 57
SP  - 1
EP  - 14
DO  - 10.3390/socsci10020057
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100794660&doi=10.3390%2fsocsci10020057&partnerID=40&md5=1011d5640733175596455a755af18060
AB  - In recent years, the use of Artificial Intelligence agents to augment and enhance the operational decision making of human agents has increased. This has delivered real benefits in terms of improved service quality, delivery of more personalised services, reduction in processing time, and more efficient allocation of resources, amongst others. However, it has also raised issues which have real-world ethical implications such as recommending different credit outcomes for individuals who have an identical financial profile but different characteristics (e.g., gender, race). The popular press has highlighted several high-profile cases of algorithmic discrimination and the issue has gained traction. While both the fields of ethical decision making and Explainable AI (XAI) have been extensively researched, as yet we are not aware of any studies which have examined the process of ethical decision making with Intelligence augmentation (IA). We aim to address that gap with this study. We amalgamate the literature in both fields of research and propose, but not attempt to validate empirically, propositions and belief statements based on the synthesis of the existing literature, observation, logic, and empirical analogy. We aim to test these propositions in future studies. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - Ethical decision making
KW  - Explainable AI
KW  - Intelligence augmentation
KW  - Values in Design
M3  - Review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Wickramasinghe, C.S.
AU  - Marino, D.L.
AU  - Grandio, J.
AU  - Manic, M.
TI  - Trustworthy AI development guidelines for human system interaction
PY  - 2020
T2  - International Conference on Human System Interaction, HSI
VL  - 2020-June
C7  - 9142644
SP  - 130
EP  - 136
DO  - 10.1109/HSI49210.2020.9142644
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091398123&doi=10.1109%2fHSI49210.2020.9142644&partnerID=40&md5=731def9409838f43d9fe3fb1024ed2c4
AB  - Artificial Intelligence (AI) is influencing almost all areas of human life. Even though these AI-based systems frequently provide state-of-the-art performance, humans still hesitate to develop, deploy, and use AI systems. The main reason for this is the lack of trust in AI systems caused by the deficiency of transparency of existing AI systems. As a solution, 'Trustworthy AI' research area merged with the goal of defining guidelines and frameworks for improving user trust in AI systems, allowing humans to use them without fear. While trust in AI is an active area of research, very little work exists where the focus is to build human trust to improve the interactions between human and AI systems. In this paper, we provide a concise survey on concepts of trustworthy AI. Further, we present trustworthy AI development guidelines for improving the user trust to enhance the interactions between AI systems and humans, that happen during the AI system life cycle.  © 2020 IEEE.
KW  - AI Life Cycle
KW  - Explainable AI
KW  - Human Machine Interactions
KW  - Human System Interactions
KW  - Transparency
KW  - Trustworthy AI
KW  - Life cycle
KW  - Active area
KW  - AI systems
KW  - Human lives
KW  - Human-system interaction
KW  - State-of-the-art performance
KW  - Artificial intelligence
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 46
ER  -

TY  - CHAP
AU  - Devitt, S.K.
TI  - Normative epistemology for lethal autonomous weapons systems
PY  - 2021
T2  - Lethal Autonomous Weapons: Re-Examining the Law and Ethics of Robotic Warfare
SP  - 237
EP  - 258
DO  - 10.1093/oso/9780197546048.003.0016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108299232&doi=10.1093%2foso%2f9780197546048.003.0016&partnerID=40&md5=dc23e00780272abb152bce64ffe38dba
AB  - The rise of human-information systems, cybernetic systems, and increasingly autonomous systems requires the application of epistemic frameworks to machines and human-machine teams. This chapter discusses higher-order design principles to guide the design, evaluation, deployment, and iteration of Lethal Autonomous Weapons Systems (LAWS) based on epistemic models. Epistemology is the study of knowledge. Epistemic models consider the role of accuracy, likelihoods, beliefs, competencies, capabilities, context, and luck in the justification of actions and the attribution of knowledge. The aim is not to provide ethical justification for or against LAWS, but to illustrate how epistemological frameworks can be used in conjunction with moral apparatus to guide the design and deployment of future systems. The models discussed in this chapter aim to make Article 36 reviews of LAWS systematic, expedient, and evaluable. A Bayesian virtue epistemology is proposed to enable justified actions under uncertainty that meet the requirements of the Laws of Armed Conflict and International Humanitarian Law. Epistemic concepts can provide some of the apparatus to meet explainability and transparency requirements in the development, evaluation, deployment, and review of ethical AI. © Oxford University Press 2021.
KW  - Bayesian epistemology
KW  - Epistemology
KW  - Virtue epistemology
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Treiss, A.
AU  - Walk, J.
AU  - Kühl, N.
TI  - An Uncertainty-Based Human-in-the-Loop System for Industrial Tool Wear Analysis
PY  - 2021
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 12461 LNAI
SP  - 85
EP  - 100
DO  - 10.1007/978-3-030-67670-4_6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103247450&doi=10.1007%2f978-3-030-67670-4_6&partnerID=40&md5=c80f72252a28b7c1da219dc9dcab1227
AB  - Convolutional neural networks have shown to achieve superior performance on image segmentation tasks. However, convolutional neural networks, operating as black-box systems, generally do not provide a reliable measure about the confidence of their decisions. This leads to various problems in industrial settings, amongst others, inadequate levels of trust from users in the model’s outputs as well as a non-compliance with current policy guidelines (e.g., EU AI Strategy). To address these issues, we use uncertainty measures based on Monte-Carlo dropout in the context of a human-in-the-loop system to increase the system’s transparency and performance. In particular, we demonstrate the benefits described above on a real-world multi-class image segmentation task of wear analysis in the machining industry. Following previous work, we show that the quality of a prediction correlates with the model’s uncertainty. Additionally, we demonstrate that a multiple linear regression using the model’s uncertainties as independent variables significantly explains the quality of a prediction (R2= 0.718 ). Within the uncertainty-based human-in-the-loop system, the multiple regression aims at identifying failed predictions on an image-level. The system utilizes a human expert to label these failed predictions manually. A simulation study demonstrates that the uncertainty-based human-in-the-loop system increases performance for different levels of human involvement in comparison to a random-based human-in-the-loop system. To ensure generalizability, we show that the presented approach achieves similar results on the publicly available Cityscapes dataset. © 2021, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - Human-in-the-loop
KW  - Image segmentation
KW  - Uncertainty
KW  - Convolution
KW  - Convolutional neural networks
KW  - Cutting tools
KW  - Data mining
KW  - Forecasting
KW  - Image segmentation
KW  - Linear regression
KW  - Machine learning
KW  - Wear of materials
KW  - Human-in-the-loop
KW  - Independent variables
KW  - Industrial settings
KW  - Industrial tools
KW  - Multiple linear regressions
KW  - Multiple regressions
KW  - Simulation studies
KW  - Uncertainty measures
KW  - Uncertainty analysis
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Sokol, K.
AU  - Flach, P.
TI  - One Explanation Does Not Fit All: The Promise of Interactive Explanations for Machine Learning Transparency
PY  - 2020
T2  - KI - Kunstliche Intelligenz
VL  - 34
IS  - 2
SP  - 235
EP  - 250
DO  - 10.1007/s13218-020-00637-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088051328&doi=10.1007%2fs13218-020-00637-y&partnerID=40&md5=cc5636525c456a3c3de2b235cdb415d9
AB  - The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations—a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human–machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms. © 2020, The Author(s).
KW  - Counterfactuals
KW  - Explanations
KW  - Interactive
KW  - Personalised
KW  - Learning algorithms
KW  - Machine learning
KW  - User interfaces
KW  - Black boxes
KW  - Counterfactuals
KW  - Explanation
KW  - Interactive
KW  - Machine learning algorithms
KW  - Machine-learning
KW  - On-machines
KW  - Personalized
KW  - Predictive systems
KW  - Property
KW  - Transparency
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 113
ER  -

TY  - JOUR
AU  - How, M.-L.
TI  - Future-ready strategic oversight of multiple artificial superintelligence-enabled adaptive learning systems via human-centric explainable ai-empowered predictive optimizations of educational outcomes
PY  - 2019
T2  - Big Data and Cognitive Computing
VL  - 3
IS  - 3
C7  - 46
SP  - 1
EP  - 43
DO  - 10.3390/bdcc3030046
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079043753&doi=10.3390%2fbdcc3030046&partnerID=40&md5=2c3e00c1e3a9bb5e385e2e14e5203b54
AB  - Artificial intelligence-enabled adaptive learning systems (AI-ALS) have been increasingly utilized in education. Schools are usually afforded the freedom to deploy the AI-ALS that they prefer. However, even before artificial intelligence autonomously develops into artificial superintelligence in the future, it would be remiss to entirely leave the students to the AI-ALS without any independent oversight of the potential issues. For example, if the students score well in formative assessments within the AI-ALS but subsequently perform badly in paper-based post-tests, or if the relentless algorithm of a particular AI-ALS is suspected of causing undue stress for the students, they should be addressed by educational stakeholders. Policy makers and educational stakeholders should collaborate to analyze the data from multiple AI-ALS deployed in different schools to achieve strategic oversight. The current paper provides exemplars to illustrate how this future-ready strategic oversight could be implemented using an artificial intelligence-based Bayesian network software to analyze the data from five dissimilar AI-ALS, each deployed in a different school. Besides using descriptive analytics to reveal potential issues experienced by students within each AI-ALS, this human-centric AI-empowered approach also enables explainable predictive analytics of the students’ learning outcomes in paper-based summative assessments after training is completed in each AI-ALS. © 2019 by the author. Licensee MDPI, Basel, Switzerland.
KW  - Adaptive learning systems
KW  - AI Thinking
KW  - Artificial intelligence
KW  - Artificial superintelligence
KW  - Bayesian networks
KW  - Explainable AI
KW  - Forecasting AI behavior
KW  - Future-ready
KW  - Human-centric reasoning
KW  - Human-in-the-loop
KW  - Pedagogical motif
KW  - Policy making on AI
KW  - Predictive optimization
KW  - Simulations
KW  - Strategic oversight
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 15
ER  -

TY  - CONF
AU  - Almagor, S.
AU  - Lahijanian, M.
TI  - Explainable multi agent path finding
PY  - 2020
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
VL  - 2020-May
SP  - 34
EP  - 42
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091994842&partnerID=40&md5=139b0b1aff4f314dc082caf025b2642e
AB  - Multi Agent Path Finding (MAPF) is the problem of planning paths for agents to reach their targets from their start locations, such that the agents do not collide while executing the plan. In safety-critical systems, the plan is typically checked by a human supervisor, who decides on whether to allow its execution. In such cases, we wish to convince the human that the plan is indeed collision free. To this end, we propose an explanation scheme for MAPF, which bases explanations on simplicity of visual verification by human's cognitive process. The scheme decomposes a plan into segments such that within each segment, the paths of the agents are disjoint. Then, we can convince the supervisor that the plan is collision free using a small number of images (dubbed an explanation). In addition, we can measure the simplicity of a plan by the number of segments required for the decomposition. We study the complexity of algorithmic problems that arise by the explanation scheme, as well as the tradeoff between the length (makespan) of a plan and its minimal decomposition. We also provide experimental results of our scheme both in a continuous and in a discrete setting. © 2020 International Foundation for Autonomous.
KW  - Explainability
KW  - MAPF
KW  - Motion planning
KW  - Multi-agent systems
KW  - Path finding
KW  - Path planning
KW  - Computational complexity
KW  - Multi agent systems
KW  - Parallel processing systems
KW  - Safety engineering
KW  - Supervisory personnel
KW  - Algorithmic problems
KW  - Cognitive process
KW  - Collision-free
KW  - Discrete settings
KW  - Human supervisors
KW  - Planning paths
KW  - Safety critical systems
KW  - Visual verification
KW  - Autonomous agents
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 21
ER  -

TY  - CONF
AU  - Nunes, T.M.M.
AU  - Borst, C.
AU  - van Kampen, E.-J.
AU  - Westin, C.
AU  - Hilburn, B.
TI  - Human-interpretable Input for Machine Learning in Tactical Air Traffic Control
PY  - 2021
T2  - SESAR Innovation Days
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160711753&partnerID=40&md5=c27a758ce4b9b1f50dc839e421ebf797
AB  - Increasing airspace demand requires an increase in effectiveness and efficiency of the ATC system. Automation, and specifically Machine Learning (ML), may present good prospects for increasing system performance and decreasing workload of ATCOs. AI, however, is typically a “black box” making it hard to include in a socio-technical environment. This exploratory research aims to increase operator trust and acceptance and move towards a more “cooperative” approach to automation in ATC. It focuses on building upon previous efforts by using two different approaches: Strategically Conformal AI and Explainable AI methods to AI-Human interactions. Strategic Conformance aims to increase acceptance by producing individual-sensitive advisories. Explainable AI focuses on producing more optimal solutions and providing a clear explanation for these solutions. In this article, we propose the use of a single visual representation for tactical conflict detection and resolution, called the Solution Space Diagram (SSD), to serve as a common ground for both explainable and conformal AI. Through this research, it has become clear that there needs to be a careful definition given both to optimality and conformance. Likewise, the training of the AI agents comes with requirements for a large amount of data to be available and displaying these solutions in a human-interpretable way, while maintaining optimality, has its own unique challenges to overcome. © 2021 IADIS. All rights reserved.
KW  - Decision Support Systems
KW  - Human Machine Interaction
KW  - Machine Learning
KW  - Air traffic control
KW  - Aviation
KW  - Information systems
KW  - Information use
KW  - Machine learning
KW  - Black boxes
KW  - Box making
KW  - Effectiveness and efficiencies
KW  - Human machine interaction
KW  - Machine-learning
KW  - Optimality
KW  - Sociotechnical
KW  - Systems performance
KW  - Tacticals
KW  - Technical environments
KW  - Decision support systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CHAP
AU  - Ding, A.W.
TI  - A model of fair and explainable artificial intelligence
PY  - 2021
T2  - Artificial Intelligence for Sustainable Value Creation
SP  - 122
EP  - 150
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130176797&partnerID=40&md5=db9605877a3da000dcb521c5bb95ff3d
AB  - Artificial intelligence (AI) is making more and more algorithmic decisions for humans. However, the “intelligence” function in AI relies heavily on learning technologies, which suffer two major flaws leading to legal and technical challenges: potentially discriminative/biased decisions, and unable to explain why and how a machine makes such decisions. Therefore, building a fair and explainable AI model is important and urgent. This article presents a novel theory-based individual-level dynamic learning method that performs learning using data of an individual subject without employing others’ information, and identifies causal mechanism from unobserved data generating process that each subject exhibits. Thus, data selection bias is avoided and a fair and interpretable decision is achieved. We empirically test our method using a real-world dataset on risk assessment for lending decisions. Our results show that the proposed method outperforms conventional learning methods in terms of fairness in treating data subjects, decision accuracy and interpretability. © Margherita Pagani and Renaud Champion 2021.
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Korteling, J.E.
AU  - van de Boer-Visschedijk, G.C.
AU  - Blankendaal, R.A.M.
AU  - Boonekamp, R.C.
AU  - Eikelboom, A.R.
TI  - Human- versus Artificial Intelligence
PY  - 2021
T2  - Frontiers in Artificial Intelligence
VL  - 4
C7  - 622364
DO  - 10.3389/frai.2021.622364
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108600226&doi=10.3389%2ffrai.2021.622364&partnerID=40&md5=581e7b935f20339b75092cc001b708a9
AB  - AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions and, for instance, the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. In order to provide more agreement and to substantiate possible future research objectives, this paper presents three notions on the similarities and differences between human- and artificial intelligence: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple (integrated) forms of narrow-hybrid AI applications. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and “collaborate” with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgment required? How can we capitalize on the specific strengths of human- and artificial intelligence? How to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition (and vice versa)? Should we pursue the development of AI “partners” with human (-level) intelligence or should we focus more at supplementing human limitations? In order to answer these questions, humans working with AI systems in the workplace or in policy making have to develop an adequate mental model of the underlying ‘psychological’ mechanisms of AI. So, in order to obtain well-functioning human-AI systems, Intelligence Awareness in humans should be addressed more vigorously. For this purpose a first framework for educational content is proposed. © Copyright © 2021 Korteling, van de Boer-Visschedijk, Blankendaal, Boonekamp and Eikelboom.
KW  - artificial general intelligence
KW  - artificial intelligence
KW  - cognitive bias
KW  - cognitive complexity
KW  - human intelligence
KW  - human-AI collaboration
KW  - human-level artificial intelligence
KW  - narrow artificial intelligence
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 253
ER  -

TY  - CONF
AU  - Park, S.Y.
AU  - Kuo, P.-Y.
AU  - Barbarin, A.
AU  - Kaziunas, E.
AU  - Chow, A.
AU  - Singh, K.
AU  - Wilcox, L.
AU  - Lasecki, W.S.
TI  - Identifying challenges and opportunities in human-AI collaboration in healthcare
PY  - 2019
T2  - Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW
SP  - 506
EP  - 510
DO  - 10.1145/3311957.3359433
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076091592&doi=10.1145%2f3311957.3359433&partnerID=40&md5=33cd170342790cfd26067b51e6327e53
AB  - The proposed workshop will identify research questions that will enable the field to uncover the types of work, labor relations, and social impacts that should be considered when designing AI-based healthcare technology. The workshop aims to outline key challenges, guidelines, and future agendas for the field, and provide collaboration opportunities for CSCW researchers, social scientists, AI researchers, clinicians, and relevant stakeholders in healthcare, to share their perspectives and co-create sociotechnical approaches to tackle timely issues related to AI and automation in healthcare work. © 2019 Copyright is held by the author/owner(s).
KW  - AI fairness
KW  - AI transparency
KW  - Algorithms
KW  - Artificial intelligence
KW  - Automation
KW  - Explainable AI
KW  - Healthcare
KW  - Machine learning
KW  - Sociotechnical systems
KW  - Algorithms
KW  - Artificial intelligence
KW  - Automation
KW  - Groupware
KW  - Health care
KW  - Interactive computer systems
KW  - Learning systems
KW  - Machine learning
KW  - Healthcare technology
KW  - Labor relations
KW  - Research questions
KW  - Social impact
KW  - Social scientists
KW  - Socio-technical approach
KW  - Sociotechnical systems
KW  - Economic and social effects
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 57
ER  -

TY  - CONF
AU  - de Regt, A.
AU  - Gagnon, E.
TI  - Rethinking how humans and machines make sense together
PY  - 2020
T2  - 26th Americas Conference on Information Systems, AMCIS 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097714493&partnerID=40&md5=46cdf4e93fd21003ab722afa907036ca
AB  - There is a growing realization of Artificial Intelligence (AI)'s importance, including its ability to provide competitive advantage and change work for the better. Indeed, organizations are investing in various AI applications in the hope to automate or augment human judgment. Despite the promise of AI, many organizations' efforts with it are falling short. Therefore, adopting sensemaking theory as a theoretical lens, this study is to investigate under which conditions and how human and machines collaborations should be structured, to enhance each other's capabilities and facilitate optimal strategical decision-making and operational effectiveness. Four types of human-machines interaction are proposed based on the level of complexity of the context and the severity of wrong decisions. Besides providing a new instrument for the analysis and assessment of human-AI interactions, this research aids the development of guidelines and facilitates the move towards explainable AI (XAI) design, development and practices. © 2020 26th Americas Conference on Information Systems, AMCIS 2020. All rights reserved.
KW  - Artificial Intelligence
KW  - Human-computer interactions
KW  - Sensemaking
KW  - XAI
KW  - Competition
KW  - Decision making
KW  - Decision theory
KW  - Information systems
KW  - Information use
KW  - AI applications
KW  - Competitive advantage
KW  - Human judgments
KW  - Human-machine
KW  - Operational effectiveness
KW  - Sense-making theory
KW  - Artificial intelligence
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Duin, A.H.
AU  - Pedersen, I.
TI  - Working Alongside Non-Human Agents
PY  - 2021
T2  - IEEE International Professional Communication Conference
VL  - 2021-October
SP  - 1
EP  - 5
DO  - 10.1109/ProComm52174.2021.00005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133010179&doi=10.1109%2fProComm52174.2021.00005&partnerID=40&md5=804d885d0763b75d2edcce8f422626ea
AB  - We coexist with non-human AI agents, and we now must plan for human and non-human-agent teaming, for cooperation and collaboration, as a means to expand collaborative intelligence in our ongoing quest for user advocacy. For practice and experimentation, we provide links to current non-human agents. We then distinguish automation and autonomy, and discuss humanness design, teaming. A deeper understanding of usability and ethical considerations for working alongside these systems, deploying robots and building bonds and trust with nonhuman agents, begins with differentiation of automation and autonomy, human-autonomy teaming, and a humanness design approach as a means to prevent undesirable autonomy. While TPC scholarship attends to privacy, accountability, safety and security, and transparency and explainability, we need additional vigilance regarding fairness and non-discrimination, human control of technology, TPC professional responsibility, and continued promotion of human values as we work alongside non-human agents.  © 2021 IEEE.
KW  - artificial intelligence
KW  - autonomous agents
KW  - collaboration
KW  - non-human agents
KW  - Autonomous agents
KW  - Ethical technology
KW  - 'current
KW  - Collaboration
KW  - Design approaches
KW  - Ethical considerations
KW  - Human agent
KW  - Human control
KW  - Human-agent teaming
KW  - Non-human agent
KW  - Professional responsibilities
KW  - Safety and securities
KW  - Machine design
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Zhang, Y.
AU  - Vera Liao, Q.
AU  - Bellamy, R.K.E.
TI  - Efect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making
PY  - 2020
T2  - FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency
SP  - 295
EP  - 305
DO  - 10.1145/3351095.3372852
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079608746&doi=10.1145%2f3351095.3372852&partnerID=40&md5=b8faad3cd8a5ff4da0c33e9a645fb466
AB  - Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the signiicance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-speciic model information can calibrate trust and improve the joint performance of the human and AI. Speciically, we study the efect of showing conidence score and local explanation for a particular prediction. Through two human experiments, we show that conidence score can help calibrate people's trust in an AI model, but trust calibration alone is not suicient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI. © 2020 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery.
KW  - Conidence
KW  - Decision support
KW  - Explainable AI
KW  - Trust
KW  - Behavioral research
KW  - Calibration
KW  - Decision support systems
KW  - Transparency
KW  - Conidence
KW  - Decision outcome
KW  - Decision supports
KW  - Individual strength
KW  - Joint performance
KW  - Model informations
KW  - Research communities
KW  - Trust
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 530
ER  -

TY  - CONF
AU  - Zhang, Z.T.
AU  - Liu, Y.
AU  - Hussmann, H.
TI  - Forward reasoning decision support: Toward a more complete view of the human-ai interaction design space
PY  - 2021
T2  - ACM International Conference Proceeding Series
DO  - 10.1145/3464385.3464696
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112717719&doi=10.1145%2f3464385.3464696&partnerID=40&md5=d4472eb8e08f16cbb1e37240146f2b55
AB  - Decision support systems based on AI are usually designed to generate complete outputs entirely automatically and to explain those to users. However, explanations, no matter how well designed, might not adequately address the output uncertainty of such systems in many applications. This is especially the case when the human-out-of-the-loop problem persists, which is a fundamental human limitation. There is no reason to limit decision support systems to such backward reasoning designs, though. We argue how more interactive forward reasoning designs where users are actively involved in the task can be effective in managing output uncertainty. We therefore call for a more complete view of the design space for decision support systems that includes both backward and forward reasoning designs. We argue that such a more complete view is necessary to overcome the barriers that hinder AI deployment especially in high-stakes applications.  © 2021 ACM.
KW  - Backward reasoning
KW  - Decision support
KW  - Explainability
KW  - Forward reasoning
KW  - Human-AI interaction
KW  - Intelligent systems
KW  - Output uncertainty
KW  - Transparency
KW  - User control
KW  - Computer applications
KW  - Computer programming
KW  - Backward reasoning
KW  - Decision supports
KW  - Design spaces
KW  - Forward reasoning
KW  - Human limitations
KW  - Interaction design
KW  - Decision support systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 4
ER  -

TY  - JOUR
TI  - 21st International Conference on Human-Computer Interaction, HCI International 2019
PY  - 2019
T2  - Communications in Computer and Information Science
VL  - 1033
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069729608&partnerID=40&md5=e1a9861928d9585877e250cea8db233b
AB  - The proceedings contain 207 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: Research on Evaluation Model of Social Game Advertising Effect Based on Eye Movement Experiment; towards a Narrative Driven Understanding of Games User Experience; Application of Archery to VR Interface; what Drives Female Players’ Continuance Intention to Play Mobile Games? The Role of Aesthetic and Narrative Design Factors; simultaneous Dialog Robot System; a Robot System Using Mixed Reality to Encourage Driving Review; self-learning Guide for Bioloid Humanoid Robot Assembly with Elements of Augmented Reality to Support Experiential Learning in Sauro Research Seeding; developing a Behavior Converter to Make a Robot Child-Like for Enhancing Human Utterances; can We Recognize Atmosphere as an Agent?: Pilot Study; design Strategies of Corporate Gamification Systems that Evokes Employee Motivation – Creative Process of Gathering Game Design Elements into Working System; GEC-HR: Gamification Exercise Companion for Home Robot with IoT; discussion on the Feasibility of Soft Actuator as an Assistive Tool for Seniors in Minimally Invasive Surgery; Recognition of Listener’s Nodding by LSTM Based on Movement of Facial Keypoints and Speech Intonation; AI-Based Technical Approach for Designing Mobile Decision Aids; human Learning in Data Science; How to Achieve Explainability and Transparency in Human AI Interaction; Data on RAILs: On Interactive Generation of Artificial Linear Correlated Data; adaptation of Machine Learning Frameworks for Use in a Management Environment: Development of a Generic Workflow; software to Support Layout and Data Collection for Machine-Learning-Based Real-World Sensors; phenomenology of Experience in Ambient Intelligence.
M3  - Conference review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Chakraborti, T.
AU  - Fadnis, K.P.
AU  - Talamadupula, K.
AU  - Dholakia, M.
AU  - Srivastava, B.
AU  - Kephart, J.O.
AU  - Bellamy, R.K.E.
TI  - Planning and visualization for a smart meeting room assistant
PY  - 2019
T2  - AI Communications
VL  - 32
IS  - 1
SP  - 91
EP  - 99
DO  - 10.3233/AIC-180609
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063102217&doi=10.3233%2fAIC-180609&partnerID=40&md5=1cc6e6a82284b2bed6ed9fc9e918e9a1
AB  - In this paper, we report on the planning and visualization capabilities of Mr.Jones-a proactive orchestrator and decision-support agent for a collaborative decision making setting embodied by a smart room. The duties of such an agent may range across interactive problem solving with other agents in the environment, developing automated summaries of meetings, visualization of the internal decision-making process, proactive data and resource management, and so on. Specifically, we focus on how the visualization of the planning and plan recognition processes forms a key component of the smart assistant, and establishes transparency in the decision-making process. We also highlight how these processes contribute to the proactive nature of the agent. We demonstrate some of these functionalities in a successful deployment of the system in the CEL-the Cognitive Environments Laboratory at IBM's T.J. Watson Research Center (Yorktown, USA), and report on emerging deployments of the system that have turned into success stories. © 2019-IOS Press and the authors. All rights reserved.
KW  - Explainable AI
KW  - human-in-the-loop
KW  - planning and decision-making
KW  - smart room
KW  - visualization
KW  - Data visualization
KW  - Decision support systems
KW  - Flow visualization
KW  - Information management
KW  - Problem solving
KW  - Visualization
KW  - Automated summaries
KW  - Collaborative decision making
KW  - Decision making process
KW  - Human-in-the-loop
KW  - Interactive problem solving
KW  - Planning and plan recognition
KW  - Resource management
KW  - Smart rooms
KW  - Decision making
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 5
ER  -

TY  - CONF
AU  - Liao, Q.V.
AU  - Gruen, D.
AU  - Miller, S.
TI  - Questioning the AI: Informing Design Practices for Explainable AI User Experiences
PY  - 2020
T2  - Conference on Human Factors in Computing Systems - Proceedings
C7  - 3376590
DO  - 10.1145/3313831.3376590
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082464591&doi=10.1145%2f3313831.3376590&partnerID=40&md5=9a22757e53129d30ce79c7a43eba8e61
AB  - A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI. © 2020 ACM.
KW  - explainable AI
KW  - human-AI interaction
KW  - user experience
KW  - Human engineering
KW  - Product design
KW  - AI systems
KW  - Design practice
KW  - Design practitioners
KW  - Design spaces
KW  - Question banks
KW  - Real-world
KW  - Support design
KW  - User-centered
KW  - User experience
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 571
ER  -

TY  - CONF
AU  - Cai, C.J.
AU  - Winter, S.
AU  - Steiner, D.
AU  - Wilcox, L.
AU  - Terry, M.
TI  - Onboarding Materials as Cross-functional Boundary Objects for Developing AI Assistants
PY  - 2021
T2  - Conference on Human Factors in Computing Systems - Proceedings
DO  - 10.1145/3411763.3443435
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105798197&doi=10.1145%2f3411763.3443435&partnerID=40&md5=be027bbbd6b2989896dbb34901106ac1
AB  - Deep neural networks (DNNs) routinely achieve state-of-the-art performance in a wide range of tasks, but it can often be challenging for them to meet end-user needs in practice. This case study reports on the development of human-AI onboarding materials (i.e., training materials for users prior to using an AI) for a DNN-based medical AI Assistant to aid in the grading of prostate cancer. Specifically, we describe how the process of developing these materials changed the team's understanding of end-user requirements, contributing to modifications in the development and assessment of the underlying machine learning model. Importantly, we discovered that onboarding materials served as a useful boundary object for cross-functional teams, uncovering a new way to assess the ML model and specify its end-user requirements. We also present evidence of the utility of the onboarding materials by describing how it affected user strategies and decision-making with AI in a study deployment to pathologists. © 2021 Owner/Author.
KW  - AI transparency
KW  - boundary objects
KW  - explainability
KW  - machine learning
KW  - Decision making
KW  - Diseases
KW  - Grading
KW  - Human engineering
KW  - Boundary objects
KW  - Cross-functional
KW  - Cross-functional teams
KW  - End user requirements
KW  - Machine learning models
KW  - Prostate cancers
KW  - State-of-the-art performance
KW  - Training material
KW  - Deep neural networks
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 34
ER  -

TY  - CONF
AU  - Lima, G.
AU  - Grgic-Hlaca, N.
AU  - Cha, M.
TI  - Human perceptions on moral responsibility of ai: A case study in ai-assisted bail decision-making
PY  - 2021
T2  - Conference on Human Factors in Computing Systems - Proceedings
DO  - 10.1145/3411764.3445260
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002586046&doi=10.1145%2f3411764.3445260&partnerID=40&md5=35de86b8c3291a43f3f850128144c2a3
AB  - How to attribute responsibility for autonomous artifcial intelligence (AI) systems' actions has been widely debated across the humanities and social science disciplines. This work presents two experiments (N=200 each) that measure people's perceptions of eight diferent notions of moral responsibility concerning AI and human agents in the context of bail decision-making. Using real-life adapted vignettes, our experiments show that AI agents are held causally responsible and blamed similarly to human agents for an identical task. However, there was a meaningful diference in how people perceived these agents' moral responsibility; human agents were ascribed to a higher degree of present-looking and forward-looking notions of responsibility than AI agents. We also found that people expect both AI and human decision-makers and advisors to justify their decisions regardless of their nature. We discuss policy and HCI implications of these fndings, such as the need for explainable AI in high-stakes scenarios. © 2021 ACM.
KW  - Ai
KW  - Bail decision-making
KW  - Blame
KW  - Compas
KW  - Liability
KW  - Moral judgment
KW  - Moral responsibility
KW  - Responsibility
KW  - Behavioral research
KW  - Ai
KW  - Bail decision-making
KW  - Blame
KW  - Compa
KW  - Decisions makings
KW  - Intelligence agents
KW  - Liability
KW  - Moral judgment
KW  - Moral responsibility
KW  - Responsibility
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 55
ER  -

TY  - CONF
AU  - Watkins, E.A.
TI  - The Tension between Information Justice and Security: Perceptions of Facial Recognition Targeting
PY  - 2021
T2  - CEUR Workshop Proceedings
VL  - 2903
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110527290&partnerID=40&md5=4210af23d3b20cc0288d589a61bd2cc0
AB  - In the discourse on human perceptions of algorithmic fairness, researchers have begun to analyze how these perceptions are shaped by sociotechnical context. In thinking through contexts of work, a half-century of research on organizational decision-making tells us that perceptions and interpretations made within these spaces are highly bounded by surrounding contextual constraints. In this paper I report early findings from a survey I conducted to bridge these two conversations, and scrutinize real-world perceptions of algorithmic decision-making in situ in a space of work. I analyze these perceptions through the case of facial recognition (or more accurately, facial verification) as account verification in gig work. In this survey I asked 100 Uber drivers, who all had been actually subjected to Uber's facial verification process known as Real Time Check ID, their fairness perceptions of this process. I designed the survey to elicit their perceptions across five disparate dimensions of justice: Informational, distributive, procedural, reciprocal, and interactional. I also asked them about their strategies for integrating Real Time Check ID into their work flow, including efforts at repair when the system breaks down and their potential preferences for subversive practices. Of those workers who report engaging in subversive tactics to avoid facial recognition, such as taking a picture of their car seat, their hand, or their passenger instead of their own face, one dimension of fairness elicited worse perceptions than any other: Informational justice, a.k.a. transparency, of facial recognition targeting (the process for deciding which workers trigger this extra layer of verification). This research reveals tensions between transparency, security, and workers' perceptions of the "fairness"of an algorithmic system: While "too much"transparency into how workers are targeted for verification may permit bad actors to defraud the system, "too little"explanation, this research shows, is no solution either. Results have crucial implications for the allocation of transparency and the design of explanations in user-facing algorithmic fraud detection, which must address tensions between information justice and security. © 2020 Copyright for this paper by its authors.
KW  - Algorithmic fairness
KW  - Biometric
KW  - Explainability
KW  - Facial recognition
KW  - Facial verification
KW  - Security
KW  - Sociotechnical
KW  - Transparency
KW  - Decision making
KW  - Surveys
KW  - Transparency
KW  - User interfaces
KW  - Contextual constraints
KW  - Facial recognition
KW  - Fraud detection
KW  - Human perception
KW  - One dimension
KW  - Organizational decision making
KW  - Sociotechnical
KW  - Verification process
KW  - Face recognition
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Wang, L.
AU  - Wang, D.
AU  - Tian, F.
AU  - Peng, Z.
AU  - Fan, X.
AU  - Zhang, Z.
AU  - Ma, S.
AU  - Yu, M.
AU  - Ma, X.
AU  - Wang, H.
TI  - CASS: Towards Building a Social-Support Chatbot for Online Health Community
PY  - 2021
T2  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - CSCW1
C7  - 3449083
DO  - 10.1145/3449083
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104946162&doi=10.1145%2f3449083&partnerID=40&md5=c4fbd3194d600290ea4a2a55a177e6e9
AB  - Chatbots systems, despite their popularity in today's HCI and CSCW research, fall short for one of the two reasons: 1) many of the systems use a rule-based dialog flow, thus they can only respond to a limited number of pre-defined inputs with pre-scripted responses; or 2) they are designed with a focus on single-user scenarios, thus it is unclear how these systems may affect other users or the community. In this paper, we develop a generalizable chatbot architecture (CASS) to provide social support for community members in an online health community. The CASS architecture is based on advanced neural network algorithms, thus it can handle new inputs from users and generate a variety of responses to them. CASS is also generalizable as it can be easily migrate to other online communities. With a follow-up field experiment, CASS is proven useful in supporting individual members who seek emotional support. Our work also contributes to fill the research gap on how a chatbot may influence the whole community's engagement.  © 2021 ACM.
KW  - ai deployment
KW  - bot
KW  - chatbot
KW  - conversational agent
KW  - emotional support
KW  - explainable ai
KW  - healthcare
KW  - human ai collaboration
KW  - human ai interaction
KW  - machine learning
KW  - neural network
KW  - online community
KW  - peer support
KW  - pregnancy
KW  - social support
KW  - system building
KW  - trustworthy ai
KW  - Human computer interaction
KW  - User interfaces
KW  - Emotional supports
KW  - Field experiment
KW  - Neural network algorithm
KW  - On-line communities
KW  - Online health communities
KW  - Rule based
KW  - Single users
KW  - Social support
KW  - Network architecture
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 61
ER  -

TY  - JOUR
AU  - Lu, Z.
AU  - Huang, P.
AU  - Dai, P.
AU  - Liu, Z.
AU  - Meng, Z.
TI  - Enhanced transparency dual-user shared control teleoperation architecture with multiple adaptive dominance factors
PY  - 2017
T2  - International Journal of Control, Automation and Systems
VL  - 15
IS  - 5
SP  - 2301
EP  - 2312
DO  - 10.1007/s12555-016-0467-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028854913&doi=10.1007%2fs12555-016-0467-y&partnerID=40&md5=38461922fbb41ea49082dcef8b261624
AB  - In traditional dual-user shared control teleoperation, the users’ operational transparency is influenced by the dominance factorα. Especially when α = 1 and α = 0 the trainer or trainee cannot receive any information form the slave side. For enlarging the control flexibility and enhancing the system transparency, a novel method with multiple adaptive dominance factors is proposed in this paper. An ideal application of this method is on-line operation and supervision. The supervisor and operator can adjust the factors and switch the operation roles and states freely. Once the slave robot handled by the operator diverges from the planned path, the dominance factors will change adaptive to limit the operator’s movement. The adaptive principles are concluded from the dynamic performance measured by the measuring functions. The conclusions suggest that the varying range of the system dynamic performance is wider than the traditional method. In addition, considering the time delays between the slave and master sides, we proved the system stability conditions covering all the range of dominance factors. Finally, we make a discussion of the applying area of the novel shared control architecture. © 2017, Institute of Control, Robotics and Systems and The Korean Institute of Electrical Engineers and Springer-Verlag GmbH Germany.
KW  - Multiple dominance factors
KW  - stability conditions
KW  - teleoperation
KW  - time delay
KW  - transparency
KW  - Delay control systems
KW  - Memory architecture
KW  - Remote control
KW  - System stability
KW  - Time delay
KW  - Transparency
KW  - Adaptive principles
KW  - Dynamic performance
KW  - Multiple dominance factors
KW  - Online operations
KW  - Planned paths
KW  - Shared control
KW  - Stability condition
KW  - System dynamic performance
KW  - Adaptive control systems
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 17
ER  -

TY  - CONF
AU  - Arous, I.
AU  - Dolamic, L.
AU  - Yang, J.
AU  - Bhardwaj, A.
AU  - Cuccu, G.
AU  - Cudré-Mauroux, P.
TI  - MARTA: Leveraging Human Rationales for Explainable Text Classification
PY  - 2021
T2  - 35th AAAI Conference on Artificial Intelligence, AAAI 2021
VL  - 7
SP  - 5868
EP  - 5876
DO  - 10.1609/aaai.v35i7.16734
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129363314&doi=10.1609%2faaai.v35i7.16734&partnerID=40&md5=6178b212289c9d190ac2957aa14aef00
AB  - Explainability is a key requirement for text classification in many application domains ranging from sentiment analysis to medical diagnosis or legal reviews. Existing methods often rely on “attention” mechanisms for explaining classification results by estimating the relative importance of input units. However, recent studies have shown that such mechanisms tend to mis-identify irrelevant input units in their explanation. In this work, we propose a hybrid human-AI approach that incorporates human rationales into attention-based text classification models to improve the explainability of classification results. Specifically, we ask workers to provide rationales for their annotation by selecting relevant pieces of text. We introduce MARTA, a Bayesian framework that jointly learns an attention-based model and the reliability of workers while injecting human rationales into model training. We derive a principled optimization algorithm based on variational inference with efficient updating rules for learning MARTA parameters. Extensive validation on real-world datasets shows that our framework significantly improves the state of the art both in terms of classification explainability and accuracy. Copyright © 2021,
KW  - Classification (of information)
KW  - Computer aided diagnosis
KW  - Inference engines
KW  - Applications domains
KW  - Attention mechanisms
KW  - Bayesian frameworks
KW  - Classification results
KW  - Learn+
KW  - Model training
KW  - Sentiment analysis
KW  - Text classification
KW  - Text classification models
KW  - Workers'
KW  - Sentiment analysis
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 39
ER  -

TY  - CONF
AU  - Waefler, T.
AU  - Schmid, U.
TI  - Explainability is not enough: Requirements for human-AI-partnership in complex socio-technical systems
PY  - 2020
T2  - Proceedings of the European Conference on the Impact of Artificial Intelligence and Robotics, ECIAIR 2020
SP  - 185
EP  - 193
DO  - 10.34190/EAIR.20.007
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097816886&doi=10.34190%2fEAIR.20.007&partnerID=40&md5=aa4d34561f5bd6ae88037960fcafd4cc
AB  - Explainability has been recognized as an important requirement of artificial intelligence (AI) systems. Transparent decision policies and explanations regarding why an AI system comes about a certain decision is a pre-requisite if AI is supposed to support human decision-making or if human-AI collaborative decision-making is envisioned. Human-AI interaction and joint decision-making is required in many real-world domains, where risky decisions have to be made (e.g. medical diagnosis) or complex situations have to be assessed (e.g. states of machines or production processes). However, in this paper we theorize that explainability is necessary but not sufficient. Coming from the point of view of work psychology we argue that for the human part of the human-AI system much more is required than intelligibility. In joint human-AI decision-making a certain role is assigned to the human, which normally encompasses tasks such as (i) verifying AI based decision suggestions, (ii) improving AI systems, (iii) learning from AI systems, and (iv) taking responsibility for the final decision as well as for compliance with legislation and ethical standards. Empowering the human to take this demanding role requires not only human expertise but e.g. also human motivation, which is triggered by a suitable task design. Furthermore, at work humans normally do not take decisions as lonely wolves but in formal and informal cooperation with other humans. Hence, to design effective explainability and to empower for true human-AI collaborative decision-making, embedding human-AI dyads into a socio-technical context is necessary. Coming from theory, this paper presents system design criteria on different levels substantiated by work psychology. The criteria are described and confronted with a use case scenario of AI-supported medical decision making in the context of digital pathology. On this basis, the need for further research is outlined. © ECIAIR 2020.All right reserved.
KW  - Companion technology
KW  - Explainable AI
KW  - Human factors
KW  - Interactive learning
KW  - Motivation
KW  - Socio-technical systems
KW  - Agricultural robots
KW  - Behavioral research
KW  - Diagnosis
KW  - Regulatory compliance
KW  - Robotics
KW  - Collaborative decision making
KW  - Digital pathologies
KW  - Ethical standards
KW  - Human decision making
KW  - Joint decision making
KW  - Medical decision making
KW  - Production process
KW  - Sociotechnical systems
KW  - Decision making
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Streitz, N.
AU  - Charitos, D.
AU  - Kaptein, M.
AU  - Böhlen, M.
TI  - Grand challenges for ambient intelligence and implications for design contexts and smart societies
PY  - 2019
T2  - Journal of Ambient Intelligence and Smart Environments
VL  - 11
IS  - 1
SP  - 87
EP  - 107
DO  - 10.3233/AIS-180507
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060941496&doi=10.3233%2fAIS-180507&partnerID=40&md5=16fa5cb78497d5eb9aad6b3e73c47743
AB  - This paper highlights selected grand challenges that concern especially the social and the design dimensions of research and development in Ambient Intelligence (AmI) and Smart Environments (SmE). Due to the increasing deployment and usage of 'smart' technologies determining a wide range of everyday life activities, there is an urgent need to reconsider their societal implications and how to address these implications with appropriate design methods. The paper presents four perspectives on the subject grounded in different approaches. First, introducing and reflecting on the implications of the 'smart-everything' paradigm, the resulting design trade-offs and their application to smart cities. Second, discussing the potential of non-verbal communication for informing the design of spatial interfaces for AmI design practices. Third, reflecting on the role of new data categories such as 'future data' and the role of uncertainty and their implications for the next generation of AmI environments. Finally, debating the merits and shortfalls of the world's largest professional engineering community effort to craft a global standards body on ethically aligned design for autonomous and intelligent systems. The paper benefits from taking different perspectives on common issues, provides commonalities and relationships between them and provides anchor points for important challenges in the field of ambient intelligence. © 2019 - IOS Press and the authors. All rights reserved.
KW  - Algorithmic transparency
KW  - Ambient intelligence
KW  - Artificial intelligence
KW  - Autonomous intelligent systems
KW  - Citizen-centered design
KW  - Data science
KW  - Design trade-offs
KW  - Ethically aligned design
KW  - Future data
KW  - GDPR
KW  - General artificial intelligence
KW  - General data protection regulations
KW  - Governance of technology
KW  - Human control
KW  - Human in the loop
KW  - Humane and sociable AmI
KW  - Hybrid city
KW  - Machine learning
KW  - Multi-armed bandit problem
KW  - Non-verbal communication
KW  - Opaque AI
KW  - Privacy by design
KW  - Self-aware city
KW  - Smart city
KW  - Smart environments
KW  - Smart-everything
KW  - Spatial communication interfaces
KW  - Traceability of algorithms
KW  - Uncertainty
KW  - Ambient intelligence
KW  - Artificial intelligence
KW  - Commerce
KW  - Data Science
KW  - Design
KW  - Economic and social effects
KW  - Intelligent systems
KW  - Learning algorithms
KW  - Learning systems
KW  - Machine learning
KW  - Samarium compounds
KW  - Smart city
KW  - Autonomous intelligent systems
KW  - Communication interface
KW  - Design tradeoff
KW  - Future data
KW  - GDPR
KW  - General data protection regulations
KW  - Human control
KW  - Human-in-the-loop
KW  - Humane and sociable AmI
KW  - Hybrid city
KW  - Multi-armed bandit problem
KW  - Non-verbal communications
KW  - Self-aware
KW  - Smart environment
KW  - Smart-everything
KW  - Uncertainty
KW  - Iodine compounds
M3  - Article
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 59
ER  -

TY  - CONF
AU  - Scalas, M.
AU  - Giacinto, G.
TI  - On the role of explainable machine learning for secure smart vehicles
PY  - 2020
T2  - 2020 AEIT International Conference of Electrical and Electronic Technologies for Automotive, AEIT AUTOMOTIVE 2020
C7  - 9307431
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099683035&partnerID=40&md5=6551a29c839354268ccc0d53d42344c2
AB  - The concept of mobility is experiencing a serious transformation due to the Mobility-as-a-Service paradigm. Accordingly, vehicles, usually referred to as smart, are seeing their architecture revamped to integrate connection to the outside environment (V2X) and autonomous driving. A significant part of these innovations is enabled by machine learning. However, deploying such systems raises some concerns. First, the complexity of the algorithms often prevents understanding what these models learn, which is relevant in the safety-critical context of mobility. Second, several studies have demonstrated the vulnerability of machine learning-based algorithms to adversarial attacks. For these reasons, research on the explainability of machine learning is raising. In this paper, we then explore the role of interpretable machine learning in the ecosystem of smart vehicles, with the goal of figuring out if and in what terms explanations help to design secure vehicles. We provide an overview of the potential uses of explainable machine learning, along with recent work in the literature that has started to investigate the topic, including from the perspectives of human-agent systems and cyber-physical systems. Our analysis highlights both benefits and criticalities in employing explanations.  © 2020 AEIT.
KW  - Automotive
KW  - Autonomous Driving
KW  - Connected Cars
KW  - Cybersecurity
KW  - Explainability
KW  - Machine learning
KW  - Mobility
KW  - Smart Vehicles
KW  - Computational complexity
KW  - Embedded systems
KW  - Learning algorithms
KW  - Safety engineering
KW  - Vehicles
KW  - Autonomous driving
KW  - Human agent
KW  - Is-enabled
KW  - Service paradigm
KW  - Smart vehicles
KW  - Machine learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 5
ER  -

TY  - CONF
AU  - Wrobel, A.
AU  - Placzek, M.
TI  - Visualization systems for industrial automation systems
PY  - 2018
T2  - IOP Conference Series: Materials Science and Engineering
VL  - 400
IS  - 6
C7  - 062032
DO  - 10.1088/1757-899X/400/6/062032
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055780700&doi=10.1088%2f1757-899X%2f400%2f6%2f062032&partnerID=40&md5=731cbf9196c0cacf4e32b318cd355c26
AB  - In currently designed industrial automation systems, a very important task is fulfilled by appropriate legible and properly designed system visualization. You cannot imagine, for example, a control room in a power station where there are lights, recorders and switches located on the walls. Currently, all power plants or, alternatively designated areas are controlled by automation systems with SCADA systems. These systems allow you to observe variables, change their status, record in real time as well as archiving variables, creating transparent charts, etc. operations [1-4,5,6]. The subject of the article is to present the real object of a high bay warehouse with a SCADA environment as a supervisory control. In the Proficy iFix environment, a visualization was made thanks to which the operator has the possibility to change the process of using the virtual buttons and manually entering setpoints. In addition, the amount of process data obtained is significantly increased, which facilitates and speeds up the operator's decision making. The machine status data is presented in the form of virtual signaling lamps, displayed values of process variables, graphical representation of process variables, and animation of the high bay warehouse execution member reflecting the actual traffic. © Published under licence by IOP Publishing Ltd.
KW  - Automation
KW  - Decision making
KW  - SCADA systems
KW  - Visualization
KW  - Warehouses
KW  - Automation systems
KW  - Graphical representations
KW  - Industrial automation system
KW  - Power station
KW  - Process Variables
KW  - Supervisory control
KW  - System visualization
KW  - Visualization system
KW  - Real time systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 1
ER  -

TY  - JOUR
TI  - 17th International Conference on Engineering Psychology and Cognitive Ergonomics, EPCE 2020, held as part of the 22nd International Conference on Human-Computer Interaction, HCII 2020
PY  - 2020
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 12187 LNAI
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088515428&partnerID=40&md5=bdaac8c609abc3a6bf86d14d5c3654b0
AB  - The proceedings contain 60 papers. The special focus in this conference is on Engineering Psychology and Cognitive Ergonomics. The topics include: Promoting operational readiness through procedures in nuclear domain; research on bim and mobile equipment in substation construction schedule management; modeling distributed situational awareness to improve handling emergency calls in operation centres; multidimensional risk dynamics modeling on operator errors of nuclear power plant; using idheas to analyze incident reports in nuclear power plant commissioning: a case study; cognitive-based severe accident information system development in a human factors project; foreword; measuring situation awareness in control room teams; mixed-initiative human-automated agents teaming: towards a flexible cooperation framework; a framework for human-autonomy team research; safety challenges of ai in autonomous systems design – solutions from human factors perspective emphasizing ai awareness; human-autonomy teaming and explainable ai capabilities in rts games; rationality, cognitive bias, and artificial intelligence: a structural perspective on quantum cognitive science; a concept on the shared use of unmanned assets by multiple users in a manned-unmanned-teaming application; allocation of moral decision-making in human-agent teams: a pattern approach; the cueing effect in retrieval of expertise: designing for future intelligent knowledge management system; the effect of group membership, system reliability and anthropomorphic appearance on user’s trust in intelligent decision support system; assessing professional cultural differences between airline pilots and air traffic controllers; exploring the effects of large screen overview displays in a nuclear control room setting; comparison of pedestrians’ gap acceptance behavior towards automated and human-driven vehicles.
M3  - Conference review
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Sawyer, B.D.
AU  - Dobres, J.
AU  - Chahine, N.
AU  - Reimer, B.
TI  - The cost of cool: Typographic style legibility in reading at a Glance
PY  - 2017
T2  - Proceedings of the Human Factors and Ergonomics Society
VL  - 2017-October
SP  - 833
EP  - 837
DO  - 10.1177/1541931213601698
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042491818&doi=10.1177%2f1541931213601698&partnerID=40&md5=bd5dd034d917e613ef65709ba31c00c6
AB  - When designers typographically tweak fonts to make an interface look 'cool,' they do so amid a rich design tradition, albeit one that is little-studied in regards to the rapid 'at a glance' reading afforded by many modern electronic displays. Such glanceable reading is routinely performed during human-machine interactions where accessing text competes with attention to crucial operational environments. There, adverse events of significant consequence can materialize in milliseconds. As such, the present study set out to test the lower threshold of time needed to read and process text modified with three common typographic manipulations: letter height, width, and case. Results showed significant penalties for the smaller size. Lowercase and condensed width text also decreased performance, especially when presented at a smaller size. These results have important implications for the types of design decisions commonly faced by interface professionals, and underscore the importance of typographic research into the human performance impact of seemingly "aesthetic" design decisions. The cost of "cool" design may be quite steep in high-risk contexts. Copyright 2017 by Human Factors and Ergonomics Society.
KW  - Design
KW  - Human engineering
KW  - Adverse events
KW  - Design decisions
KW  - Electronic display
KW  - Human machine interaction
KW  - Human performance
KW  - Operational environments
KW  - Ergonomics
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 16
ER  -

TY  - CONF
AU  - Cai, C.J.
AU  - Jongejan, J.
AU  - Holbrook, J.
TI  - The effects of example-based explanations in a machine learning interface
PY  - 2019
T2  - International Conference on Intelligent User Interfaces, Proceedings IUI
VL  - Part F147615
SP  - 258
EP  - 262
DO  - 10.1145/3301275.3302289
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065577583&doi=10.1145%2f3301275.3302289&partnerID=40&md5=d598433d5a952339ba1fb903dd733220
AB  - The black-box nature of machine learning algorithms can make their predictions difficult to understand and explain to end-users. In this paper, we propose and evaluate two kinds of example-based explanations in the visual domain, normative explanations and comparative explanations (Figure 1), which automatically surface examples from the training set of a deep neural net sketch-recognition algorithm. To investigate their effects, we deployed these explanations to 1150 users on QuickDraw, an online platform where users draw images and see whether a recognizer has correctly guessed the intended drawing. When the algorithm failed to recognize the drawing, those who received normative explanations felt they had a better understanding of the system, and perceived the system to have higher capability. However, comparative explanations did not always improve perceptions of the algorithm, possibly because they sometimes exposed limitations of the algorithm and may have led to surprise. These findings suggest that examples can serve as a vehicle for explaining algorithmic behavior, but point to relative advantages and disadvantages of using different kinds of examples, depending on the goal. © 2019 Copyright held by the owner/author(s).
KW  - Example-based explanations
KW  - Explainable AI
KW  - Human-AI interaction
KW  - Machine learning
KW  - Deep neural networks
KW  - Learning algorithms
KW  - Learning systems
KW  - User interfaces
KW  - Black boxes
KW  - End users
KW  - Example based
KW  - Human-AI interaction
KW  - Online platforms
KW  - Sketch recognition
KW  - Training sets
KW  - Machine learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 179
ER  -

TY  - CONF
AU  - Viana, J.
AU  - Cohen, K.
TI  - Fault Tolerance Tool for Human and Machine Interaction & Application to Civilian Aircraft
PY  - 2019
T2  - 2019 IEEE Latin American Conference on Computational Intelligence, LA-CCI 2019
C7  - 9037045
DO  - 10.1109/LA-CCI47412.2019.9037045
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083110988&doi=10.1109%2fLA-CCI47412.2019.9037045&partnerID=40&md5=f18638fbc4c24f545cf2455fd452f62e
AB  - Enhancing human-machine interaction is critical to aerospace applications. An essential requirement in safety critical systems is the clear need to guarantee trustworthiness of a system as well as V&V (Verification and Validation). However, the current state of the art concerning decision support systems lacks effective tools in this area. The Coherence Function Package, introduced in this research, is a tool towards providing assurance that the action needed has the approval of both the human and the machine in terms of SAFETY. These algorithms shed light on the future of an Explainable Artificial Intelligence (XAI, [1]), that fosters a synergy between these two factors. This vital requirement that has been further underscored after the tragic events of the Boeing 737 Max 8 crashes [2]. Preliminary results show that the proposed approach is not only able to detect any errors in the system, it also assists in circumventing conflicts leading to incoherence and suggests a preferred solution in real-time. © 2019 IEEE.
KW  - Aircraft Safety
KW  - Equivalences
KW  - Flight Critical Systems
KW  - Human-Machine Interaction
KW  - Implications
KW  - Recursive Functions
KW  - Tautology
KW  - Aerospace applications
KW  - Aircraft
KW  - Aircraft accidents
KW  - Decision support systems
KW  - Intelligent computing
KW  - Civilian aircrafts
KW  - Coherence function
KW  - Effective tool
KW  - Human machine interaction
KW  - Preferred solutions
KW  - Safety critical systems
KW  - State of the art
KW  - Verification-and-validation
KW  - Fault tolerance
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Omeiza, D.
AU  - Anjomshoae, S.
AU  - Kollnig, K.
AU  - Camburu, O.-M.
AU  - Främling, K.
AU  - Kunze, L.
TI  - Towards Explainable and Trustworthy Autonomous Physical Systems
PY  - 2021
T2  - Conference on Human Factors in Computing Systems - Proceedings
DO  - 10.1145/3411763.3441338
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105807078&doi=10.1145%2f3411763.3441338&partnerID=40&md5=aa7f2158265e53a05e358be07999abf3
AB  - The safe deployment of autonomous physical systems in real-world scenarios requires them to be explainable and trustworthy, especially in critical domains. In contrast with g€black-box' systems, explainable and trustworthy autonomous physical systems will lend themselves to easy assessments by system designers and regulators. This promises to pave ways for easy improvements that can lead to enhanced performance, and as well, increased public trust. In this one-day virtual workshop, we aim to gather a globally distributed group of researchers and practitioners to discuss the opportunities and social challenges in the design, implementation, and deployment of explainable and trustworthy autonomous physical systems, especially in a post-pandemic era. Interactions will be fostered through panel discussions and a series of spotlight talks. To ensure lasting impact of the workshop, we will conduct a pre-workshop survey which will examine the public perception of the trustworthiness of autonomous physical systems. Further, we will publish a summary report providing details about the survey as well as the identified challenges resulting from the workshop's panel discussions. © 2021 Owner/Author.
KW  - collaboration
KW  - Explainability
KW  - human-machine interaction
KW  - trust
KW  - Human engineering
KW  - Distributed groups
KW  - Panel discussions
KW  - Physical systems
KW  - Public perception
KW  - Real-world scenario
KW  - Social challenges
KW  - System designers
KW  - Virtual workshops
KW  - Surveys
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 29 May 2025; Cited By: 3
ER  -

