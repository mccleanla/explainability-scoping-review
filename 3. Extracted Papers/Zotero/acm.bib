@article{10.1016/j.jss.2025.112348,
author = {Clemmensen, Torkil and Moghaddam, Mahyar Tourchi and N\o{}rbjerg, Jacob},
title = {Cyber-physical systems with Human-in-the-Loop: A systematic review of socio-technical perspectives},
year = {2025},
issue_date = {Aug 2025},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {226},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2025.112348},
doi = {10.1016/j.jss.2025.112348},
journal = {J. Syst. Softw.},
month = may,
numpages = {22},
keywords = {Cyber-physical systems, Human-in-the-Loop, Socio-technical}
}

@article{10.1287/mnsc.2022.01687,
author = {Benjaafar, Saif and Wang, Zicheng and Yang, Xiaotang},
title = {Human in the Loop Automation: Ride-Hailing with Remote (Tele-)Drivers},
year = {2025},
issue_date = {March 2025},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {71},
number = {3},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2022.01687},
doi = {10.1287/mnsc.2022.01687},
abstract = {Tele-driving refers to a novel concept by which drivers can remotely operate vehicles (without being physically in the vehicle). By putting the human back in the loop, tele-driving has emerged recently as a more viable alternative to fully automated vehicles with ride-hailing (and other on-demand transportation-enabled services) being an important application. Because remote drivers can be operated as a shared resource (any driver can be assigned to any customer regardless of trip origin or destination), it may be possible for such services to deploy fewer drivers than vehicles without significantly reducing service quality. In this paper, we examine the extent to which this is possible. Using a spatial queueing model that captures the dynamics of both pickup and trip times, we show that the impact of reducing the number of drivers depends crucially on system workload relative to the number of vehicles. In particular, when workload is sufficiently high relative to the number of vehicles, we show that, perhaps surprisingly, reducing the number of drivers relative to the number of vehicles can actually improve service level (e.g., as measured by the amount of demand fulfilled in the case of impatient customers). Having fewer drivers than vehicles ensures that there are always idle vehicles; the fewer the drivers, the likelier it is for there to be more idle vehicles. Consequently, the fewer the drivers, the likelier it is for the pickup times to be shorter (making overall shorter service times likelier). The impact of shorter service time is particularly significant when the workload is high, and in this case, it is enough to overcome the loss in driver capacity. When workload is sufficiently low relative to the number of vehicles, we show that it is possible to significantly reduce the number of drivers without significantly reducing service level. In systems in which customers are patient and willing to queue up for the service, we show that reducing the number of drivers can also reduce delay, including stabilizing a system that may otherwise be unstable. In general, relative to a system in which the number of vehicles equals the number of drivers (as in a system with in-vehicle drivers), a system with remote drivers can result in savings in the number of drivers either without significantly degrading performance or actually improving performance. We discuss how these results can, in part, be explained by the interplay of two counteracting forces: (1) having fewer drivers increasing service rate and (2) having fewer drivers reducing the number of servers with the relative strength of these forces depending on system workload.
This paper was accepted by Baris Ata, stochastic models and simulation.

Funding: This work was supported by the US National Science Foundation [Grant SCC-1831140], and the Guangdong (China) Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence [2023B1212010001].
Supplemental Material: The online appendix and data files are available at .},
journal = {Manage. Sci.},
month = mar,
pages = {2527–2543},
numpages = {17},
keywords = {tele-driving, ride hailing, spatial queueing systems, capacity optimization}
}

@inproceedings{10.1007/978-3-031-52670-1_23,
author = {Zhang, Nan and Bahsoon, Rami and Tziritas, Nikos and Theodoropoulos, Georgios},
title = {Explainable Human-in-the-Loop Dynamic Data-Driven Digital Twins},
year = {2022},
isbn = {978-3-031-52669-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-52670-1_23},
doi = {10.1007/978-3-031-52670-1_23},
abstract = {Digital Twins (DT) are essentially dynamic data-driven models that serve as real-time symbiotic “virtual replicas” of real-world systems. DT can leverage fundamentals of Dynamic Data-Driven Applications Systems (DDDAS) bidirectional symbiotic sensing feedback loops for its continuous updates. Sensing loops can consequently steer measurement, analysis and reconfiguration aimed at more accurate modelling and analysis in DT. The reconfiguration decisions can be autonomous or interactive, keeping human-in-the-loop. The trustworthiness of these decisions can be hindered by inadequate explainability of the rationale, and utility gained in implementing the decision for the given situation among alternatives. Additionally, different decision-making algorithms and models have varying complexity, quality and can result in different utility gained for the model. The inadequacy of explainability can limit the extent to which humans can evaluate the decisions, often leading to updates which are unfit for the given situation, erroneous, compromising the overall accuracy of the model. The novel contribution of this paper is an approach to harnessing explainability in human-in-the-loop DDDAS and DT systems, leveraging bidirectional symbiotic sensing feedback. The approach utilises interpretable machine learning and goal modelling to explainability, and considers trade-off analysis of utility gained. We use examples from smart warehousing to demonstrate the approach.},
booktitle = {Dynamic Data Driven Applications Systems: 4th International Conference, DDDAS 2022, Cambridge, MA, USA, October 6–10, 2022, Proceedings},
pages = {233–243},
numpages = {11},
keywords = {Digital Twins, DDDAS, Explainability, Human-in-the-loop},
location = {Cambridge, MA, USA}
}

@article{10.1016/j.dss.2024.114304,
author = {Lash, Michael T.},
title = {HEX: Human-in-the-loop explainability via deep reinforcement learning},
year = {2024},
issue_date = {Dec 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {187},
number = {C},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2024.114304},
doi = {10.1016/j.dss.2024.114304},
journal = {Decis. Support Syst.},
month = dec,
numpages = {12},
keywords = {Explainability, Interpretability, Human-in-the-loop, Deep reinforcement learning, Machine learning, Behavioral machine learning, Decision support}
}

@article{10.1016/j.jsis.2023.101772,
author = {Heyder, Teresa and Passlack, Nina and Posegga, Oliver},
title = {Ethical management of human-AI interaction: Theory development review},
year = {2023},
issue_date = {Sep 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {32},
number = {3},
issn = {0963-8687},
url = {https://doi.org/10.1016/j.jsis.2023.101772},
doi = {10.1016/j.jsis.2023.101772},
journal = {J. Strateg. Inf. Syst.},
month = sep,
numpages = {50},
keywords = {Sociomateriality, Theoretical review, Human-AI interaction, Ethics, Artificial intelligence}
}

@inproceedings{10.1609/aaai.v38i21.30412,
author = {Tabrez, Aaquib},
title = {Autonomous policy explanations for effective human-machine teaming},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i21.30412},
doi = {10.1609/aaai.v38i21.30412},
abstract = {Policy explanation, a process for describing the behavior of an autonomous system, plays a crucial role in effectively conveying an agent's decision-making rationale to human collaborators and is essential for safe real-world deployments. It becomes even more critical in effective human-robot teaming, where good communication allows teams to adapt and improvise successfully during uncertain situations by enabling value alignment within the teams. This thesis proposal focuses on improving human-machine teaming by developing novel human-centered explainable AI (xAI) techniques that empower autonomous agents to communicate their capabilities and limitations via multiple modalities, teach and influence human teammates' behavior as decision-support systems, and effectively build and manage trust in HRI systems.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2689},
numpages = {2},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@article{10.1007/s00146-020-01004-z,
author = {Simmler, Monika and Frischknecht, Ruth},
title = {A taxonomy of human–machine collaboration: capturing automation and technical autonomy},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {1},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-020-01004-z},
doi = {10.1007/s00146-020-01004-z},
abstract = {Due to the ongoing advancements in technology, socio-technical collaboration has become increasingly prevalent. This poses challenges in terms of governance and accountability, as well as issues in various other fields. Therefore, it is crucial to familiarize decision-makers&nbsp;and researchers with the core of human–machine collaboration. This study introduces a taxonomy that enables identification of the very nature of human–machine interaction. A literature review has revealed that automation and technical autonomy are main parameters for describing and understanding such interaction. Both aspects must be carefully evaluated, as their increase has potentially far-reaching consequences. Hence, these two concepts comprise the taxonomy’s axes. Five levels of automation and five levels of technical autonomy are introduced below, based on the assumption that both automation and autonomy are gradual. The levels of automation were developed from existing approaches; those of autonomy were carefully derived from a review of the literature. The taxonomy’s use is also explained, as are its limitations and avenues for further research.},
journal = {AI Soc.},
month = mar,
pages = {239–250},
numpages = {12},
keywords = {Autonomy, Automation, Taxonomy, Human–machine collaboration}
}

@article{10.1016/j.dss.2024.114216,
author = {Sobrie, L\'{e}on and Verschelde, Marijn},
title = {Real-time decision support for human–machine interaction in digital railway control rooms},
year = {2024},
issue_date = {Jun 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {181},
number = {C},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2024.114216},
doi = {10.1016/j.dss.2024.114216},
journal = {Decis. Support Syst.},
month = jun,
numpages = {13},
keywords = {Decision support systems, Human–machine interaction, Explainable prediction, Behavioral prescription, Real-time railway traffic management, End-user feedback}
}

@inproceedings{10.1007/978-3-030-67670-4_6,
author = {Treiss, Alexander and Walk, Jannis and K\"{u}hl, Niklas},
title = {An Uncertainty-Based Human-in-the-Loop System for Industrial Tool Wear Analysis},
year = {2020},
isbn = {978-3-030-67669-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67670-4_6},
doi = {10.1007/978-3-030-67670-4_6},
abstract = {Convolutional neural networks have shown to achieve superior performance on image segmentation tasks. However, convolutional neural networks, operating as black-box systems, generally do not provide a reliable measure about the confidence of their decisions. This leads to various problems in industrial settings, amongst others, inadequate levels of trust from users in the model’s outputs as well as a non-compliance with current policy guidelines (e.g., EU AI Strategy). To address these issues, we use uncertainty measures based on Monte-Carlo dropout in the context of a human-in-the-loop system to increase the system’s transparency and performance. In particular, we demonstrate the benefits described above on a real-world multi-class image segmentation task of wear analysis in the machining industry. Following previous work, we show that the quality of a prediction correlates with the model’s uncertainty. Additionally, we demonstrate that a multiple linear regression using the model’s uncertainties as independent variables significantly explains the quality of a prediction (R2=0.718). Within the uncertainty-based human-in-the-loop system, the multiple regression aims at identifying failed predictions on an image-level. The system utilizes a human expert to label these failed predictions manually. A simulation study demonstrates that the uncertainty-based human-in-the-loop system increases performance for different levels of human involvement in comparison to a random-based human-in-the-loop system. To ensure generalizability, we show that the presented approach achieves similar results on the publicly available Cityscapes dataset.},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science and Demo Track: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part V},
pages = {85–100},
numpages = {16},
keywords = {Human-in-the-loop, Image segmentation, Uncertainty, Deep learning},
location = {Ghent, Belgium}
}

@inproceedings{10.1145/3613904.3641898,
author = {Karusala, Naveena and Upadhyay, Sohini and Veeraraghavan, Rajesh and Gajos, Krzysztof Z.},
title = {Understanding Contestability on the Margins: Implications for the Design of Algorithmic Decision-making in Public Services},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641898},
doi = {10.1145/3613904.3641898},
abstract = {Policymakers have established that the ability to contest decisions made by or with algorithms is core to responsible artificial intelligence (AI). However, there has been a disconnect between research on contestability of algorithms, and what the situated practice of contestation looks like in contexts across the world, especially amongst communities on the margins. We address this gap through a qualitative study of follow-up and contestation in accessing public services for land ownership in rural India and affordable housing in the urban United States. We find there are significant barriers to exercising rights and contesting decisions, which intermediaries like NGO workers or lawyers work with communities to address. We draw on the notion of accompaniment in global health to highlight the open-ended work required to support people in navigating violent social systems. We discuss the implications of our findings for key aspects of contestability, including building capacity for contestation, human review, and the role of explanations. We also discuss how sociotechnical systems of algorithmic decision-making can embody accompaniment by taking on a higher burden of preventing denials and enabling contestation.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {478},
numpages = {16},
keywords = {India, United States, algorithmic decision-making, contestability, public services},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3640543.3645210,
author = {Goyal, Navita and Baumler, Connor and Nguyen, Tin and Daum\'{e} III, Hal},
title = {The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645210},
doi = {10.1145/3640543.3645210},
abstract = {AI systems have been known to amplify biases in real-world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants’ perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments—explanations, model bias disclosure and proxy correlation disclosure—affect fairness perception and parity. We find that explanations help people detect direct but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model biases. Disclosures can help mitigate this effect for indirect biases, improving both unfairness recognition and decision-making fairness. We hope that our findings can help guide further research into advancing explanations in support of fair human-AI decision-making.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {155–180},
numpages = {26},
keywords = {explanations, fairness, human-AI decision-making, indirect biases},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{10.1145/3706599.3706713,
author = {Ehsan, Upol and Watkins, Elizabeth A and Wintersberger, Philipp and Manger, Carina and Hubig, Nina and Savage, Saiph and Weisz, Justin D. and Riener, Andreas},
title = {New Frontiers of Human-centered Explainable AI (HCXAI): Participatory Civic AI, Benchmarking LLMs, XAI Hallucinations, and Responsible AI Audits},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3706713},
doi = {10.1145/3706599.3706713},
abstract = {Explainable AI (XAI) is more than just “opening” the black box — who opens it matters just as much, if not more, as the ways of opening it. Human-centered XAI (HCXAI) advocates that algorithmic transparency alone is not sufficient for making AI explainable. In our fifth CHI workshop on Human-Centered XAI (HCXAI), we shift our focus to new, emerging frontiers of explainability: (1) participatory approaches toward explainability in civic AI applications; (2) addressing hallucinations in LLMs using explainability benchmarks; (3) connecting HCXAI research with Responsible AI practices, algorithmic auditing, and public policy; and (4) improving representation of XAI issues from the Global South. We have built a strong community of HCXAI researchers through our workshop series whose work has made important conceptual, methodological, and technical impact on the field. In this installment, we will push the frontiers of work in HCXAI with an emphasis on operationalizing perspectives sociotechnically.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {793},
numpages = {6},
location = {
},
series = {CHI EA '25}
}

@phdthesis{10.5555/AAI28648408,
author = {Lee, Min Hun and Alexandre, Bernardino, and K., Dey, Anind and Asim, Smailagic, and M., Kitani, Kris and Hugo, Nicolau, and Sergi, Berm\'{u}dez i Badia,},
advisor = {P, Siewiorek, Daniel},
title = {Interactive Hybrid Intelligence Systems for Human-AI/Robot Collaboration: Improving the Practices of Physical Stroke Rehabilitation},
year = {2021},
isbn = {9798538136599},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Rapid advances in machine learning (ML) have made it applicable to healthcare practices. However, the deployment of these ML models remains a challenge due to the lack of user-centered designs and model interpretability and adaptability. This thesis introduces an interactive hybrid approach that combines an ML model with a rule-based model from experts to support transparent interactions with a user, but also iteratively be tuned with user inputs (e.g. therapist's feedback or patient's motions) to improve AI and robotic systems. Through iterative engagements with therapists and post-stroke patients, we explore how humans and artificial intelligence (AI) and robotic systems can collaborate to improve practices of physical stroke rehabilitation therapy: 1) human-AI collaborative decision-making on rehabilitation assessment for therapists and (2) human-robot collaborative rehabilitation therapy for post-stroke survivors. For human-AI collaborative decision-making, we found that our interactive system with transparent, patient-specific analysis significantly reduces therapists' efforts and improves their agreement level on rehabilitation assessment. In addition, therapists can provide feedback on our system for a more personalized prediction and a more efficient development of ML models. For human-robot collaborative rehabilitation therapy, we found that our system can be tuned with post-stroke survivor's data to generate personalized corrective feedback. Both therapists and post-stroke survivors appreciated the potential benefits of our system to achieve more systematic management and improve post-stroke survivors' self-efficacy and motivation in rehabilitation. Overall, this thesis discusses the value of iterative engagement with stakeholders and making a system explainable and interactive to create effective human-AI/robot collaboration on a complex task.},
note = {AAI28648408}
}

@inproceedings{10.1145/3581641.3584033,
author = {Prabhudesai, Snehal and Yang, Leyao and Asthana, Sumit and Huan, Xun and Liao, Q. Vera and Banovic, Nikola},
title = {Understanding Uncertainty: How Lay Decision-makers Perceive and Interpret Uncertainty in Human-AI Decision Making},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584033},
doi = {10.1145/3581641.3584033},
abstract = {Decision Support Systems (DSS) based on Machine Learning (ML) often aim to assist lay decision-makers, who are not math-savvy, in making high-stakes decisions. However, existing ML-based DSS are not always transparent about the probabilistic nature of ML predictions and how uncertain each prediction is. This lack of transparency could give lay decision-makers a false sense of reliability. Growing calls for AI transparency have led to increasing efforts to quantify and communicate model uncertainty. However, there are still gaps in knowledge regarding how and why the decision-makers utilize ML uncertainty information in their decision process. Here, we conducted a qualitative, think-aloud user study with 17 lay decision-makers who interacted with three different DSS: 1) interactive visualization, 2) DSS based on an ML model that provides predictions without uncertainty information, and 3) the same DSS with uncertainty information. Our qualitative analysis found that communicating uncertainty about ML predictions forced participants to slow down and think analytically about their decisions. This in turn made participants more vigilant, resulting in reduction in over-reliance on ML-based DSS. Our work contributes empirical knowledge on how lay decision-makers perceive, interpret, and make use of uncertainty information when interacting with DSS. Such foundational knowledge informs the design of future ML-based DSS that embrace transparent uncertainty communication.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {379–396},
numpages = {18},
keywords = {Decision-making, Machine Learning, Uncertainty.},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@phdthesis{10.5555/AAI30244191,
author = {Roberts, Claudia Veronica and Adji, Bousso Dieng, and Barbara, Engelhardt, and Andr\'{e}s, Monroy-Hern\'{a}ndez, and Matt, Salganik,},
advisor = {Arvind, Narayanan,},
title = {Human-Machine Collaboration in Real-World Machine-Learning Applications},
year = {2023},
isbn = {9798371968654},
publisher = {Princeton University},
address = {USA},
abstract = {Automation tools like machine learning are a necessity in our big data world. Thanks to the Internet and advancements in all facets of computer and storage technology, almost everyone has a voice in the Internet connected world. However, there are still very real physical limits in our physical world. This dichotomy—the seemingly limitless nature of technology enabled data colliding with the physical limits of the real world—has made automation tools a necessity, and predictive models powered by machine learning algorithms are one such tool.The promise of machine learning to accurately predict future human behavior and human preferences has lead practitioners and researchers alike to apply machine learning automation tools to tasks such as product recommendations and speculatory activities such as long term job applicant success. However, due to the mercurial nature of humans, developing mathematical intermediaries to attempt to model and predict human behavior is challenging and not a straight-forward task. One way of harnessing the power of machine-learning backed automation to help reduce the scale of many real-world applications in more challenging domain settings is by having humans and machines collaborating in non-trivial ways. In this dissertation, we delineate the various ways in which humans and machines collaborate in challenging real-world applications. Moreover, we highlight three specific ways in which we can use human-machine collaboration to keep or increase utility and reduce real-world harm when using these systems in the wild: (i) humans enabling computers with domain specific knowledge, (ii) computers providing humans with algorithmic explanations, (iii) humans and computers working together in decision making.},
note = {AAI30244191}
}

@article{10.1016/j.ijinfomgt.2022.102574,
author = {Dennehy, Denis and Griva, Anastasia and Pouloudi, Nancy and M\"{a}ntym\"{a}ki, Matti and Pappas, Ilias},
title = {Artificial intelligence for decision-making and the future of work},
year = {2023},
issue_date = {Apr 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {C},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2022.102574},
doi = {10.1016/j.ijinfomgt.2022.102574},
journal = {Int. J. Inf. Manag.},
month = apr,
numpages = {4},
keywords = {Future of work, Artificial intelligence}
}

@article{10.1145/3632753,
author = {Olsen, Henrik Palmer and Hildebrandt, Thomas Troels and Wiesener, Cornelius and Larsen, Matthias Smed and Fl\"{u}gge, Asbj\o{}rn William Ammitzb\o{}ll},
title = {The Right to Transparency in Public Governance: Freedom of Information and the Use of Artificial Intelligence by Public Agencies},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
url = {https://doi.org/10.1145/3632753},
doi = {10.1145/3632753},
abstract = {What information should and can be transparent for artificial intelligence (AI) algorithms? This article examines the socio-technical and legal perspectives of transparency in relation to algorithmic decision-making in public administration. We show how transparency in AI can be understood in light of the various technologies and the challenges one may encounter. Despite some first steps in that direction, there exists so far no mature standard for documenting AI models. From a legal perspective, this article examined the applicable freedom of information (FOI) regimes across different jurisdictions, with a particular focus on Denmark and other Scandinavian countries. Despite notable differences, our findings show that the FOI regimes generally only grant access to existing documents, and that access can be denied on the basis of the wide proprietary interests and internal documents exemptions. This is why we ultimately conclude that the European data-protection framework and the proposed EU AI Act — with their far-reaching duties to document the functioning of AI systems — provide promising new avenues for research and insights into transparency in AI.},
journal = {Digit. Gov.: Res. Pract.},
month = mar,
articleno = {8},
numpages = {15},
keywords = {Transparency, algorithm, freedom of information, administrative decision-making}
}

@article{10.1007/s00146-024-01941-z,
author = {Soltanzadeh, Sadjad},
title = {A metaphysical account of agency for technology governance},
year = {2024},
issue_date = {Mar 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {40},
number = {3},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-024-01941-z},
doi = {10.1007/s00146-024-01941-z},
abstract = {The way in which agency is conceptualised has implications for understanding human–machine interactions and the governance of technology, especially artificial intelligence (AI)&nbsp;systems. Traditionally, agency is conceptualised as a capacity, defined by intrinsic properties, such as cognitive or volitional facilities. I argue that the capacity-based account of agency is inadequate to explain the dynamics of human–machine interactions and guide technology governance. Instead, I propose to conceptualise agency as impact. Agents as impactful entities can be identified at different levels: from the low level of individual entities to the high level of complex socio-technical systems. Entities can impact their surroundings through different channels, and more influential channels of impact lead to higher degrees of agency. Technology governance must take into account different channels of impact in the contexts of use, design and regulation.},
journal = {AI Soc.},
month = apr,
pages = {1723–1734},
numpages = {12},
keywords = {Capacity-based theories of agency, Agency as impact, Channels of impact, Design and regulation, Human-machine interaction}
}

@inproceedings{10.1145/3411764.3445260,
author = {Lima, Gabriel and Grgi\'{c}-Hla\v{c}a, Nina and Cha, Meeyoung},
title = {Human Perceptions on Moral Responsibility of AI: A Case Study in AI-Assisted Bail Decision-Making},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445260},
doi = {10.1145/3411764.3445260},
abstract = {How to attribute responsibility for autonomous artificial intelligence (AI) systems’ actions has been widely debated across the humanities and social science disciplines. This work presents two experiments (N=200 each) that measure people’s perceptions of eight different notions of moral responsibility concerning AI and human agents in the context of bail decision-making. Using real-life adapted vignettes, our experiments show that AI agents are held causally responsible and blamed similarly to human agents for an identical task. However, there was a meaningful difference in how people perceived these agents’ moral responsibility; human agents were ascribed to a higher degree of present-looking and forward-looking notions of responsibility than AI agents. We also found that people expect both AI and human decision-makers and advisors to justify their decisions regardless of their nature. We discuss policy and HCI implications of these findings, such as the need for explainable AI in high-stakes scenarios.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {235},
numpages = {17},
keywords = {Responsibility, Moral Responsibility, Moral Judgment, Liability, COMPAS, Blame, Bail Decision-Making, AI},
location = {Yokohama, Japan},
series = {CHI '21}
}

@phdthesis{10.5555/AAI29322003,
author = {Lai, Vivian and Chenhao, Tan, and Vera, Liao, Q. and Tamara, Sumner, and Tom, Yeh,},
advisor = {James, Martin,},
title = {Empowering Humans in Human-AI Decision Making},
year = {2022},
isbn = {9798845408556},
publisher = {University of Colorado at Boulder},
address = {USA},
abstract = {Due to recent advances in Artificial Intelligence (AI), AI models are able to surpass human performance in various tasks unprecedentedly and are rapidly integrated into systems that assist humans in making decisions. However, deploying such systems into the real world requires an understanding of the potential risks and challenges we might face. How do we interpret and explain AI models' predictions while being aware of their biases and weaknesses? In this thesis, I discuss my work that empowers humans to make better decisions with AI models through AI-backed interactive systems. I describe (1) how humans make decisions with models (Chapter 2), (2) how explanations differ across models and methods (Chapter 3), (3) how humans learn counterintuitive patterns from models (Chapter 4), and (4) how humans and imperfect models could collaborate effectively (Chapter 5). I conclude by discussing future research perspectives on making human-AI collaborations better and more accessible.},
note = {AAI29322003}
}

@inproceedings{10.1145/3334480.3381069,
author = {Wang, Dakuo and Churchill, Elizabeth and Maes, Pattie and Fan, Xiangmin and Shneiderman, Ben and Shi, Yuanchun and Wang, Qianying},
title = {From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3381069},
doi = {10.1145/3334480.3381069},
abstract = {Artificial Intelligent (AI) and Machine Learning (ML) algorithms are coming out of research labs into the real-world applications, and recent research has focused a lot on Human-AI Interaction (HAI) and Explainable AI (XAI). However, Interaction is not the same as Collaboration. Collaboration involves mutual goal understanding, preemptive task co-management and shared progress tracking. Most of human activities today are done collaboratively, thus, to integrate AI into the already-complicated human workflow, it is critical to bring the Computer-Supported Cooperative Work (CSCW) perspective into the root of the algorithmic research and plan for a Human-AI Collaboration future of work. In this panel we ask: Can this future for trusted human-AI collaboration be realized? If so, what will it take? This panel will bring together HCI experts who work on human collaboration and AI applications in various application contexts, from industry and academia and from both the U.S. and China. Panelists will engage the audience through discussion of their shared and diverging visions, and through suggestions for opportunities and challenges for the future of human-AI collaboration.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {ai partner, ai-powered healthcare, computer-supported corporative work, explainable ai, group collaboration, human-ai collaboration, trusted ai},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{10.1007/978-3-031-35894-4_18,
author = {Herrmann, Thomas},
title = {Collaborative Appropriation of AI in the Context of Interacting with AI},
year = {2023},
isbn = {978-3-031-35893-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-35894-4_18},
doi = {10.1007/978-3-031-35894-4_18},
abstract = {In the context of maintaining technical equipment, AI is used to detect possible problems. Human specialists check whether a real problem is addressed, and, in this case, try to solve it. Furthermore, they go on trying to translate the problem notification into an improvement of other software components into which the AI system is embedded. Thus, every AI result is not only the cause of immediate action but is also a trigger within the process of continuous appropriation of the technical infrastructure that includes AI. The whole socio-technical system is a subject of AI-related improvement as a collaborative task that requires continuous advancement of human competences and skills. This has to be supported by a type of explainable AI by which the process of understanding the reasons driving AI output is not a task for a single end-user but rather the result of combining different specialists’ viewpoints and competences.},
booktitle = {Artificial Intelligence in HCI: 4th International Conference, AI-HCI 2023, Held as Part of the 25th HCI International Conference, HCII 2023, Copenhagen, Denmark, July 23–28, 2023, Proceedings, Part II},
pages = {249–260},
numpages = {12},
keywords = {keeping the organization in the loop, appropriation, rule extraction, socio-technical design, organizational practices, human-centered artificial intelligence},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1145/3603555.3608551,
author = {Sipos, Lars and Sch\"{a}fer, Ulrike and Glinka, Katrin and M\"{u}ller-Birn, Claudia},
title = {Identifying Explanation Needs of End-users: Applying and Extending the XAI Question Bank},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603555.3608551},
doi = {10.1145/3603555.3608551},
abstract = {Explainable Artificial Intelligence (XAI) is concerned with making the decisions of AI systems interpretable to humans. Explanations are typically developed by AI experts and focus on algorithmic transparency and the inner workings of AI systems. Research has shown that such explanations do not meet the needs of users who do not have AI expertise. As a result, explanations are often ineffective in making system decisions interpretable and understandable. We aim to strengthen a socio-technical view of AI by following a Human-Centered Explainable Artificial Intelligence (HC-XAI) approach, which investigates the explanation needs of end-users (i.e., subject matter experts and lay users) in specific usage contexts. One of the most influential works in this area is the XAI Question Bank (XAIQB) by Liao et al. The authors propose a set of questions that end-users might ask when using an AI system, which in turn is intended to help developers and designers identify and address explanation needs. Although the XAIQB is widely referenced, there are few reports of its use in practice. In particular, it is unclear to what extent the XAIQB sufficiently captures the explanation needs of end-users and what potential problems exist in the practical application of the XAIQB. To explore these open questions, we used the XAIQB as the basis for analyzing 12 think-aloud software explorations with subject matter experts, i.e., art historians. We investigated the suitability of the XAIQB as a tool for identifying explanation needs in a specific usage context. Our analysis revealed a number of explanation needs that were missing from the question bank, but that emerged repeatedly as our study participants interacted with an AI system. We also found that some of the XAIQB questions were difficult to distinguish and required interpretation during use. Our contribution is an extension of the XAIQB with 11 new questions. In addition, we have expanded the descriptions of all new and existing questions to facilitate their use. We hope that this extension will enable HCI researchers and practitioners to use the XAIQB in practice and may provide a basis for future studies on the identification of explanation needs in different contexts.},
booktitle = {Proceedings of Mensch Und Computer 2023},
pages = {492–497},
numpages = {6},
keywords = {Explainable AI, Explanation needs, Human-AI collaboration, User study},
location = {Rapperswil, Switzerland},
series = {MuC '23}
}

@inproceedings{10.1145/3603555.3603562,
author = {Recki, Lena and Esau-Held, Margarita and Lawo, Dennis and Stevens, Gunnar},
title = {AI said, She said - How Users Perceive Consumer Scoring in Practice},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603555.3603562},
doi = {10.1145/3603555.3603562},
abstract = {As digitization continues, consumers are increasingly exposed to AI scoring decisions. However, currently lacking is a thorough understanding of how users’ misjudgments of an AI-supported system lead to it being rejected. Therefore, investigations are needed into the appropriation of such socio-technical systems in practice and how users describe their experience with algorithm-based scoring. To address this issue, we evaluated 1,003 user reviews of an app on car insurance that calculates premiums based on the consumers’ individual driving behavior. We find evidence that users develop their own folk theories to explain the algorithms with the help of situation-related experiences and that insufficient explanations lead to power asymmetries between consumers, the system, and the company. In particular, as a result of the different needs of the stakeholders, we uncover a fundamental conflict between computational risk assessment and the perceived agency to influence the score.},
booktitle = {Proceedings of Mensch Und Computer 2023},
pages = {149–160},
numpages = {12},
keywords = {Algorithmic Decision Making, Empirical study, Explainable AI, Fairness, Perception},
location = {Rapperswil, Switzerland},
series = {MuC '23}
}

@inproceedings{10.1145/3394332.3402897,
author = {Middleton, Stuart E. and Lavorgna, Anita and McAlister, Ruth},
title = {STAIDCC20: 1st International Workshop on Socio-technical AI Systems for Defence, Cybercrime and Cybersecurity},
year = {2020},
isbn = {9781450379946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394332.3402897},
doi = {10.1145/3394332.3402897},
abstract = {The purpose of STAIDCC20 workshop is to bring together a mixture of inter-disciplinary researchers and practitioners working in defence, cybercrime and cybersecurity application areas to discuss and explore the challenges and future research directions around socio-technical AI systems. The workshop will showcase where the state of the art is in socio-technical AI, charting a path around issues including transparency, trustworthiness, explaining bias and error, incorporating human judgment and ethical frameworks for deployment of socio-technical AI in the future.},
booktitle = {Companion Publication of the 12th ACM Conference on Web Science},
pages = {78–79},
numpages = {2},
keywords = {Socio-technical, Defence, Cybersecurity, Cybercrime, Criminology, Artificial Intelligence},
location = {Southampton, United Kingdom},
series = {WebSci '20 Companion}
}

@article{10.1145/3512957,
author = {Park, Joon Sung and Karahalios, Karrie and Salehi, Niloufar and Eslami, Motahhare},
title = {Power Dynamics and Value Conflicts in Designing and Maintaining Socio-Technical Algorithmic Processes},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512957},
doi = {10.1145/3512957},
abstract = {How do power dynamics and value conflicts affect our ability to design and maintain socio-technical algorithmic processes? In this paper, we study the SIGCHI student volunteer (SV) selection process that uses a weighted semi-randomized algorithm to recruit a desired pool of volunteers. Our interviews with the community members showed that the process is complex and socio-technical; the algorithm's outputs are interpreted and adjusted by the conference organizers to reflect the community values while ensuring the selection of effective volunteers to help with organizing the conference. This provides a stage in which the power dynamics and value conflicts among the stakeholders play salient roles in determining how the process was perceived and envisioned. For instance, non-organizers of the conference found the algorithm used in the selection process to be a power-balancer that places a check on the organizers who oversee the process. However, even with a participatory process to elicit the algorithm's weights, the power dynamics and value conflicts between the participants made it difficult to reach a consensus on what the SV selection process should consider and prioritize. Our findings highlight the importance of value transparency -- the type of transparency that focuses on explaining why a decision was made rather than how it was made -- as a mechanism for resolving such conflicts. Based on our findings, we lay out design recommendations that can guide communities to better design and maintain algorithmic socio-technical processes over time in the face of power dynamics and value conflicts.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {110},
numpages = {21},
keywords = {participatory algorithm design, fairness, collective participation, algorithmic governance}
}

@inproceedings{10.1145/3613904.3642326,
author = {Bertrand, Astrid and Eagan, James R. and Maxwell, Winston and Brand, Joshua},
title = {AI is Entering Regulated Territory: Understanding the Supervisors' Perspective for Model Justifiability in Financial Crime Detection},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642326},
doi = {10.1145/3613904.3642326},
abstract = {Artificial intelligence (AI) has the potential to bring significant benefits to highly regulated industries such as healthcare or banking. Adoption, however, remains low. AI’s entry into complex socio-techno-legal systems raises issues of transparency, specifically for regulators. However, the perspective of supervisors, regulators who monitor compliance with applicable financial regulations, has rarely been studied. This paper focuses on understanding the needs of supervisors in anti-money laundering (AML) to better inform the design of AI justifications and explanations in highly regulated fields. Through scenario-based workshops with 13 supervisors and 6 banking professionals, we outline the auditing practices and socio-technical context of the supervisor. By combining the workshops’ insights with an analysis of compliance requirements, we identify the AML obligations that conflict with AI opacity. We then formulate seven needs that supervisors have for model justifiability. We discuss the role of explanations as reliable evidence on which to base justifications.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {480},
numpages = {21},
keywords = {AI regulation, anti-money laundering, explainability, highly-regulated environment, justifiability},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3565472.3592959,
author = {Salimzadeh, Sara and He, Gaole and Gadiraju, Ujwal},
title = {A Missing Piece in the Puzzle: Considering the Role of Task Complexity in Human-AI Decision Making},
year = {2023},
isbn = {9781450399326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565472.3592959},
doi = {10.1145/3565472.3592959},
abstract = {Recent advances in the performance of machine learning algorithms have led to the adoption of AI models in decision making contexts across various domains such as healthcare, finance, and education. Different research communities have attempted to optimize and evaluate human-AI team performance through empirical studies by increasing transparency of AI systems, or providing explanations to aid human understanding of such systems. However, the variety in decision making tasks considered and their operationalization in prior empirical work, has led to an opacity around how findings from one task or domain carry forward to another. The lack of a standardized means of considering task attributes prevents straightforward comparisons across decision tasks, thereby limiting the generalizability of findings. We argue that the lens of ‘task complexity’ can be used to tackle this problem of under-specification and facilitate comparison across empirical research in this area. To retrospectively explore how different HCI communities have considered the influence of task complexity in designing experiments in the realm of human-AI decision making, we survey literature and provide an overview of empirical studies on this topic. We found a serious dearth in the consideration of task complexity across various studies in this realm of research. Inspired by Robert Wood’s seminal work on the construct, we operationalized task complexity with respect to three dimensions (component, coordinative, and dynamic) and quantified the complexity of decision tasks in existing work accordingly. We then summarized current trends and proposed research directions for the future. Our study highlights the need to account for task complexity as an important design choice. This is a first step to help the scientific community in drawing meaningful comparisons across empirical studies in human-AI decision making and to provide opportunities to generalize findings across diverse domains and experimental settings.},
booktitle = {Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
pages = {215–227},
numpages = {13},
location = {Limassol, Cyprus},
series = {UMAP '23}
}

@inproceedings{10.1145/3411763.3451798,
author = {Shen, Haifeng and Liao, Kewen and Liao, Zhibin and Doornberg, Job and Qiao, Maoying and van den Hengel, Anton and Verjans, Johan W.},
title = {Human-AI Interactive and Continuous Sensemaking: A Case Study of Image Classification using Scribble Attention Maps},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451798},
doi = {10.1145/3411763.3451798},
abstract = {Advances in Artificial Intelligence (AI), especially the stunning achievements of Deep Learning (DL) in recent years, have shown AI/DL models possess remarkable understanding towards the logic reasoning behind the solved tasks. However, human understanding towards what knowledge is captured by deep neural networks is still elementary and this has a detrimental effect on human’s trust in the decisions made by AI systems. Explainable AI (XAI) is a hot topic in both AI and HCI communities in order to open up the blackbox to elucidate the reasoning processes of AI algorithms in such a way that makes sense to humans. However, XAI is only half of human-AI interaction and research on the other half - human’s feedback on AI explanations together with AI making sense of the feedback - is generally lacking. Human cognition is also a blackbox to AI and effective human-AI interaction requires unveiling both blackboxes to each other for mutual sensemaking. The main contribution of this paper is a conceptual framework for supporting effective human-AI interaction, referred to as interactive and continuous sensemaking (HAICS). We further implement this framework in an image classification application using deep Convolutional Neural Network (CNN) classifiers as a browser-based tool that displays network attention maps to the human for explainability and collects human’s feedback in the form of scribble annotations overlaid onto the maps. Experimental results using a real-world dataset has shown significant improvement of classification accuracy (the AI performance) with the HAICS framework.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {290},
numpages = {8},
keywords = {attention map, explainable AI, image classification, interactive sensemaking, scribble interaction},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3550356.3561538,
author = {Parra-Ullauri, Juan and Garc\'{\i}a-Dom\'{\i}nguez, Antonio and Bencomo, Nelly and Garcia-Paucar, Luis},
title = {History-aware explanations: towards enabling human-in-the-loop in self-adaptive systems},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561538},
doi = {10.1145/3550356.3561538},
abstract = {The complexity of real-world problems requires modern software systems to autonomously adapt and modify their behaviour at run time to deal with internal and external challenges and contexts. Consequently, these self-adaptive systems (SAS) can show unexpected and surprising behaviours to users, who may not understand or agree with them. This is exacerbated due to the ubiquity and complexity of AI-based systems which are often considered as "black-boxes". Users may feel that the decision-making process of SAS is oblivious to the user's own decision-making criteria and priorities. Inevitably, users may mistrust or even avoid using the system. Furthermore, SAS could benefit from the human involvement in satisfying stakeholders' requirements. Accordingly, it is argued that a system should be able to explain its behaviour and how it has reached its current state. A history-aware, human-in-the-loop approach to address these issues is presented in this paper. For this approach, the system should i) offer access and retrieval of historic data about the past behaviour of the system, ii) track over time the reasons for its decisions to show and explain them to the users, and iii) provide capabilities, called effectors, to empower users by allowing them to steer the decision-making based on the information provided by i) and ii). This paper looks into enabling a human-in-the-loop approach into the decision-making of SAS based on the MAPE-K architecture. We present a feedback layer based on temporal graph databases (TGDB) that has been added to the MAPE-K architecture to provide a two-way communication between the human and the SAS. Collaboration, communication and trustworthiness between the human and SAS is promoted by the provision of history-based explanations extracted from the TGDB, and a set of effectors allow human users to influence the system based on the received information. The encouraging results of an application of the approach to a network management case study and a validation from a SAS expert are shown.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {286–295},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@phdthesis{10.5555/AAI29288743,
author = {Lyu, Daoming and Levent, Yilmaz, and Shiwen, Mao, and Anh, Nguyen, and Shiqi, Zhang,},
advisor = {Bo, Liu,},
title = {Towards Trustworthy Decision-Making in Human-Machine Symbiosis},
year = {2022},
isbn = {9798845456618},
publisher = {Auburn University},
address = {USA},
abstract = {As artificial intelligence (AI) evolves, it becomes an integral part of our daily lives. To augment our effectiveness, human-machine symbiosis enables both humans and AI systems to offer different yet complementary capabilities. However, one of the significant concerns in human-machine symbiosis is the lack of human trust due to the potential ramifications, risks, or even dangers caused by AI. The critical question here is no longer whether AI will have an impact but by whom, how, where, and when this positive or negative impact will be felt. Trust is a prerequisite for humans to develop, deploy and use AI. Without AI being demonstrably worthy of trust, its uptake by humans might be hindered, hence undermining the realization of AI's vast economic and social benefits. This dissertation centers on building human trust in AI approaches to sequential decision problems, i.e., trustworthy decision-making. Specifically, there are three significant issues in current approaches.(i.)The first issue regards robustness where the brittleness in the planning indicates its inherent weaknesses. This identifies the potential risk that the AI system is unreliable and may lead to a blind trust that an AI system stays prone to errors even with high performances. To address the issue, I developed a framework to equip planning with the ability to learn so that the representation used for planning can be improved through the learned experience. Experimental results on benchmark domains demonstrate that the proposed approach can adapt to the domain uncertainties and changes and improve reliability.(ii.)The second issue regards interpretability where the learning behavior of deep reinforcement learning based on black-box neural networks is nontransparent and hard to explain and understand. This is identified as one of the main barriers to building human trust in the outcomes produced by the AI system. I developed a framework to address the issue by leveraging task decomposition and causal reasoning. Therefore, the task-level system behaviors can be interpreted in terms of causality -- causal relations among different sub-tasks. Experimental results on the challenging domain with high-dimensional sensory inputs empirically validate the interpretability of sub-tasks, along with improved data efficiency compared with state-of-the-art approaches.(iii.)The third issue regards adaptive autonomy where the concern is to what degree of autonomy should be granted to an AI system. Furthermore, keeping humans in a supervisory role is key to striking a balance between machine-led and human-led decision-making. Therefore, I developed a human-machine collaborative decision-making framework to empower the machine agent to make decisions, with humans maintaining oversights. In addition, the openness supported by this paradigm, i.e., the willingness to give and receive ideas, can also increase human trust. Experiments with human evaluative feedback in different scenarios also demonstrate the effectiveness of the proposed approach.},
note = {AAI29288743}
}

@inbook{10.5555/3716662.3716721,
author = {Kawakami, Anna and Taylor, Jordan and Fox, Sarah and Zhu, Haiyi and Holstein, Kenneth},
title = {AI Failure Loops in Feminized Labor: Understanding the Interplay of Workplace AI and Occupational Devaluation},
year = {2025},
publisher = {AAAI Press},
abstract = {Workplace AI systems often fail in practice. For example, in social services, AI-based decision support tools have been introduced across high-stakes settings, only to be dropped following backlash from workers or the public (Samant et al. 2021). Similarly, in healthcare, researchers have spent decades innovating on AI-based tools to support clinical decision-making, only to find that clinicians ignore them in practice (Yang, Steinfeld, and Zimmerman 2019).Past research has described a range of challenges that help explain these AI failures. For example, human-computer interaction (HCI) and science and technology studies (STS) literature has identified problems of poor contextual fit, where an AI system's design clashes with existing worker practices (e.g., (Suchman 1987; Forsythe 1993; Kawakami et al. 2022)). To address these challenges, prior work has proposed a range of new resources to support more responsible or participatory design. Yet, these resources are rarely used in practice, and AI teams continue to design flawed AI systems for the workplace (Delgado et al. 2023).Existing efforts to improve the design and evaluation of workplace AI tools has overlooked a critical factor: the role of occupational devaluation. How might the devaluation of worker expertise interplay with AI design, evaluation, and deployment practices? In our paper, we examine this through the case of feminized labor, a particularly extreme form of occupational devaluation (Balka and Wagner 2021). In the U.S., feminized labor often involves care-oriented work, like teaching and nursing. Historically misnomered as "women's work," feminized labor is still predominantly performed by women and people of color. While impacts on and of feminized labor have been studied by other disciplines, it remains under-examined in responsible AI research.Drawing together past scholarship on AI deployments in feminized occupations, we argue that workers in societally devalued occupations are particularly vulnerable to flawed AI deployments, rooted in impoverished understandings of workers' tasks and expertise. Moreover, we argue that these AI deployments further obscure the visibility of workers' expertise, triggering a negative feedback loop that further entrenches workers' devaluation.To understand this dynamic, we formalize the concept of AI Failure Loops: a set of interwoven, socio-technical failures across the development lifecycle of workplace AI systems that can amplify the harms from existing occupational devaluation. AI Failure Loops arise at the confluence of overclaims about the capabilities of workplace AI systems and under-recognition of the complexity of the work that workers perform. We ground an understanding of the driving factors, properties, and impacts of AI Failure Loops through three case studies of AI deployments in feminized labor contexts: AI-based risk assessment tools in social work, AI for home healthcare, and AI tutoring systems in K-12 teaching. We conclude with a discussion on future work towards proworker AI practice, policy, and organizing.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {683},
numpages = {1}
}

@article{10.1007/s11023-020-09532-9,
author = {Verdiesen, Ilse and Santoni de Sio, Filippo and Dignum, Virginia},
title = {Accountability and Control Over Autonomous Weapon Systems: A Framework for Comprehensive Human Oversight},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {1},
issn = {0924-6495},
url = {https://doi.org/10.1007/s11023-020-09532-9},
doi = {10.1007/s11023-020-09532-9},
abstract = {Accountability and responsibility are key concepts in the academic and societal debate on Autonomous Weapon Systems, but these notions are often used as high-level overarching constructs and are not operationalised to be useful in practice. “Meaningful Human Control” is often mentioned as a requirement for the deployment of Autonomous Weapon Systems, but a common definition of what this notion means in practice, and a clear understanding of its relation with responsibility and accountability is also lacking. In this paper, we present a definition of these concepts and describe the relations between accountability, responsibility, control and oversight in order to show how these notions are distinct but also connected. We focus on accountability as a particular form of responsibility—the obligation to explain one’s action to a forum—and we present three ways in which the introduction of Autonomous Weapon Systems may create “accountability gaps”. We propose a Framework for Comprehensive Human Oversight based on an engineering, socio-technical and governance perspective on control. Our main claim is that combining the control mechanisms at technical, socio-technical and governance levels will lead to comprehensive human oversight over Autonomous Weapon Systems which may ensure solid controllability and accountability for the behaviour of Autonomous Weapon Systems. Finally, we give an overview of the military control instruments that are currently used in the Netherlands and show the applicability of the comprehensive human oversight Framework to Autonomous Weapon Systems. Our analysis reveals two main gaps in the current control mechanisms as applied to Autonomous Weapon Systems. We have identified three first options as future work for the design of a control mechanism, one in the technological layer, one in the socio-technical layer and one the governance layer, in order to achieve comprehensive human oversight and ensure accountability over Autonomous Weapon Systems.},
journal = {Minds Mach.},
month = mar,
pages = {137–163},
numpages = {27},
keywords = {Comprehensive human oversight framework, Human oversight, Meaningful human control, Accountability gap, Accountability, Responsibility, Autonomous Weapon Systems}
}

@inproceedings{10.1145/3675094.3679001,
author = {Schneider, Jordan and Cheruvalath, Swathy Satheesan and Hassan, Teena},
title = {Time for an Explanation: A Mini-Review of Explainable Physio-Behavioural Time-Series Classification},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3679001},
doi = {10.1145/3675094.3679001},
abstract = {Time-series classification is seeing growing importance as device proliferation has lead to the collection of an abundance of sensor data. Although black-box models, whose internal workings are difficult to understand, are a common choice for this task, their use in safety-critical domains has raised calls for greater transparency. In response, researchers have begun employing explainable artificial intelligence together with physio-behavioural signals in the context of real-world problems. Hence, this paper examines the current literature in this area and contributes principles for future research to overcome the limitations of the reviewed works.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {885–889},
numpages = {5},
keywords = {design principles, explainable artificial intelligence, physio-behavioural signals, time-series classification, ubiquitous computing},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3613905.3638184,
author = {Kim, Sunnie S. Y.},
title = {Establishing Appropriate Trust in AI through Transparency and Explainability},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3638184},
doi = {10.1145/3613905.3638184},
abstract = {As AI systems are increasingly transforming our society, it is critical to support relevant stakeholders to have appropriate understanding and trust in these systems. My dissertation research explores how AI transparency and explainability can help with this goal. I begin with human-centered evaluations of current AI explanation techniques, focusing on their usefulness for people in understanding model behavior and calibrating trust. Next, I identify what explainability needs real AI end-users have and what factors influence their trust through an in-depth case study of a real-world AI application. Finally, I describe two studies, one ongoing and one proposed, that investigate transparency and explainability approaches for Generative AI, such as large language models, to enable safe and successful interactions with this new and powerful technology. My dissertation contributes to both HCI and AI fields by elucidating mechanisms and factors of trust in AI and detailing design considerations for AI transparency and explainability approaches.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {433},
numpages = {6},
keywords = {AI transparency and explainability, Explainable AI, Human-AI collaboration, Trust and reliance},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@phdthesis{10.5555/AAI29210226,
author = {Chong, Leah Myong and Kosa, Goucher-Lambert, and B, Kara, Levent},
advisor = {Jonathan, Cagan, and Kenneth, Kotovsky,},
title = {Role of Human Self-Confidence and Their Confidence in Artificial Intelligence in AI-Assisted Decision-Making in Engineering Design},
year = {2022},
isbn = {9798819338711},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {As artificial intelligence (AI) systems are proving their usefulness in engineering design, they are increasingly deployed to assist human designers' decision-making by providing design suggestions. However, such AI assistance is only effective when human designers properly utilize AI input. Unfortunately, designers often misjudge the AI's and/or their own ability, leading to inappropriate trust and reliance on AI, and therefore bad designs. To avoid such outcomes, it is critical to understand how human designers' self-confidence and their confidence in the AI teammate(s) (two types of human confidence that are related to trust) change and impact AI-assisted decision-making in engineering design. Therefore, this dissertation conducts three cognitive experiments to investigate the influence of trial-by-trial experiences and AI performance during AI-assisted decision-making on human confidences and consequently the decision to accept or reject AI's design suggestions. A dynamic model of human confidence that calculates the change in confidence based on individual experience, accumulated confidence, and bias at any given trial is also developed and used for analysis.The first experiment examines the evolution of human self-confidence and their confidence in AI, as well as the impact of these confidences on their reliance on AI, in a one AI-assisted decision-making context with a non-design task. A non-design task is investigated to understand general problem-solving scenarios as well as to allow identification of design-specific results in the second experiment. The second experiment, then, resembles the first experiment but with an engineering design task. The results of this experiment reveal not only some similarities with the non-design task but also significant discrepancies that lead to a discussion of the differences in the two tasks. Next, the data from the second experiment is utilized to dig deeper into human designers' self-confidence and its role in AI-assisted decision-making. Having discovered in the two initial experiments that human designers' self-confidence significantly impacts their decision-making in individual trials, the dissertation then examines what self-confidence values at various points of collaboration reveal about designers' overall performance as a teammate, specifically in terms of their overall competence, reliance on AI, and team performance. Finally, the last experiment of this dissertation studies an AI-assisted decision-making context with two AIs. This final study offers insights into the effect of an additional AI and its suggestions on human designers' confidence in AI and self-confidence and consequently on the decision-making outcome. Altogether, this dissertation yields valuable information about how human designers' confidences change and impact AI-assisted decision-making in various contexts (i.e., non-design vs. design tasks, one AI vs. two AI teams), enabling detection and explanation of inappropriate confidence levels and presenting opportunities to effectively reduce mis-reliance on AI in engineering design.},
note = {AAI29210226}
}

@article{10.1016/j.artint.2021.103573,
author = {Mualla, Yazan and Tchappi, Igor and Kampik, Timotheus and Najjar, Amro and Calvaresi, Davide and Abbas-Turki, Abdeljalil and Galland, St\'{e}phane and Nicolle, Christophe},
title = {The quest of parsimonious XAI: A human-agent architecture for explanation formulation},
year = {2022},
issue_date = {Jan 2022},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {302},
number = {C},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2021.103573},
doi = {10.1016/j.artint.2021.103573},
journal = {Artif. Intell.},
month = jan,
numpages = {26},
keywords = {Statistical testing, Empirical user studies, Multi-agent systems, Human-computer interaction, Explainable artificial intelligence}
}

@article{10.1016/j.future.2021.07.016,
author = {Sotelo Monge, Marco Antonio and Maestre Vidal, Jorge},
title = {Conceptualization and cases of study on cyber operations against the sustainability of the tactical edge},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2021.07.016},
doi = {10.1016/j.future.2021.07.016},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {869–890},
numpages = {22},
keywords = {Tactical Denial of Sustainability, Situational Awareness, Military operations, Economical Denial of Sustainability, Cyber defense}
}

@article{10.14778/3352063.3352068,
author = {Qian, Kun and Popa, Lucian and Sen, Prithviraj},
title = {SystemER: a human-in-the-loop system for explainable entity resolution},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352068},
doi = {10.14778/3352063.3352068},
abstract = {Entity Resolution (ER) is the task of identifying different representations of the same real-world object. To achieve scalability and the desired level of quality, the typical ER pipeline includes multiple steps that may involve low-level coding and extensive human labor. We present SystemER, a tool for learning explainable ER models that reduces the human labor all throughout the stages of the ER pipeline. SystemER achieves explainability by learning rules that not only perform a given ER task but are human-comprehensible; this provides transparency into the learning process, and further enables verification and customization of the learned model by the domain experts. By leveraging a human in the loop and active learning, SystemER also ensures that a small number of labeled examples is sufficient to learn high-quality ER models. SystemER is a full-fledged tool that includes an easy to use interface, support for both flat files and semi-structured data, and scale-out capabilities by distributing computation via Apache Spark.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1794–1797},
numpages = {4}
}

@article{10.1016/j.artint.2025.104282,
author = {Baron, Sam and Latham, Andrew J. and Varga, Somogy},
title = {Explainable AI and stakes in medicine: A user study},
year = {2025},
issue_date = {Mar 2025},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {340},
number = {C},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2025.104282},
doi = {10.1016/j.artint.2025.104282},
journal = {Artif. Intell.},
month = mar,
numpages = {18},
keywords = {Explainable AI (XAI), Human-centered XAI, Explainable ML, User study, Human-AI interaction, Causation}
}

@inproceedings{10.1145/3623809.3623981,
author = {Ramirez-Amaro, Karinne and Torre, Ilaria and Diehl, Maximilian and Dean, Emmanuel},
title = {The Importance of Human Factors for Trusted Human-Robot Collaborations},
year = {2023},
isbn = {9798400708244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623809.3623981},
doi = {10.1145/3623809.3623981},
abstract = {The next generation of robots is expected to work collaboratively with humans in natural (dynamic) settings. For this, it is important to properly study and model human factors, so that the AI and Robotic models can include them to enable robust Human-Robot Collaborations. This will enable safe and trustworthy hybrid decision-making approaches – Responsible AI – thereby streamlining robust collaborations (as per human-centred expectations). This interdisciplinary workshop will focus on the intersection of Cognitive Human Factors, Interpretable &amp; Explainable AI methods, Social Interaction, and Human-Centred Robotics to stimulate novel long-range avenues for innovative human-centred collaborative methods in real-world contexts.},
booktitle = {Proceedings of the 11th International Conference on Human-Agent Interaction},
pages = {502–503},
numpages = {2},
keywords = {Human factors, Human-Robot Interaction, Interpretable &amp; Explainable AI methods, Safety},
location = {Gothenburg, Sweden},
series = {HAI '23}
}

@inproceedings{10.1145/3708359.3712095,
author = {Upadhyay, Sohini and Lakkaraju, Himabindu and Gajos, Krzysztof Z.},
title = {Counterfactual Explanations May Not Be the Best Algorithmic Recourse Approach},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712095},
doi = {10.1145/3708359.3712095},
abstract = {Algorithmic recourse is a rapidly developing subfield in explainable AI (XAI) concerned with providing individuals subject to adverse high-stakes algorithmic outcomes with explanations indicating how to reverse said outcomes. While XAI research in the machine learning community doesn’t confine itself to counterfactual explanations, its algorithmic recourse subfield does, adopting the assumption that the optimal way to provide recourse is through counterfactual explanations. Though there has been extensive human-AI interaction research on explanations, translating these findings to the algorithmic recourse setting is non-obvious due to meaningful problem setting differences, leaving the question of whether counterfactuals are the most optimal explanation paradigm for recourse unanswered. While intuitively satisfying, the prescriptive nature of counterfactuals makes them vulnerable to poor outcomes when circumstances unknown to the decision-making and explanation generating algorithms affect re-application strategies. With these concerns in mind, we designed a series of experiments comparing different explanation methods in the recourse setting, explicitly incorporating scenarios where circumstances unknown to the decision-making and explanation algorithms affect re-application strategies. In Experiment 1, we compared counterfactuals with reason codes, a simple feature-based explanation, finding that they both yield comparable re-application success, and that reason codes led to better user outcomes when unknown circumstances had a high impact on re-application strategies. In Experiment 2, we sought to improve on reason code outcomes, comparing them to feature attributions, a more informative feature-based explanation, but found no improvements. Finally, in Experiment 3, we aimed to improve on reason code outcomes with a multiple counterfactual explanation condition, finding that multiple counterfactuals led to higher re-application success but still resulted in comparatively worse user outcomes in the face of high impact unknown circumstances. Taken together, these findings call into question whether the standard counterfactual paradigm is the best approach for the algorithmic recourse problem setting.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {446–462},
numpages = {17},
keywords = {algorithmic recourse, counterfactual explanations, AI explanations},
location = {
},
series = {IUI '25}
}

@inproceedings{10.1145/3442188.3445921,
author = {Cobbe, Jennifer and Lee, Michelle Seng Ah and Singh, Jatinder},
title = {Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445921},
doi = {10.1145/3442188.3445921},
abstract = {This paper introduces reviewability as a framework for improving the accountability of automated and algorithmic decisionmaking (ADM) involving machine learning. We draw on an understanding of ADM as a socio-technical process involving both human and technical elements, beginning before a decision is made and extending beyond the decision itself. While explanations and other model-centric mechanisms may assist some accountability concerns, they often provide insufficient information of these broader ADM processes for regulatory oversight and assessments of legal compliance. Reviewability involves breaking down the ADM process into technical and organisational elements to provide a systematic framework for determining the contextually appropriate record-keeping mechanisms to facilitate meaningful review - both of individual decisions and of the process as a whole. We argue that a reviewability framework, drawing on administrative law's approach to reviewing human decision-making, offers a practical way forward towards more a more holistic and legally-relevant form of accountability for ADM.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {598–609},
numpages = {12},
keywords = {Algorithmic systems, accountability, artificial intelligence, audit, automated decision-making, machine learning},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{10.1609/aimag.v40i3.2866,
author = {Lawless, W. F. and Mittu, Ranjeev and Sofge, Donald and Hiatt, Laura},
title = {Artificial Intelligence, Autonomy, and Human‐Machine Teams: Interdependence, Context, and Explainable AI: Editorial Introduction to the Special Articles on Context},
year = {2019},
issue_date = {Fall 2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {40},
number = {3},
issn = {0738-4602},
url = {https://doi.org/10.1609/aimag.v40i3.2866},
doi = {10.1609/aimag.v40i3.2866},
abstract = {Because in military situations, as well as for self‐driving cars, information must be processed faster than humans can achieve, determination of context computationally, also known as situational assessment, is increasingly important. In this article, we introduce the topic of context, and we discuss what is known about the heretofore intractable research problem on the effects of interdependence, present in the best of human teams; we close by proposing that interdependence must be mastered mathematically to operate human‐machine teams efficiently, to advance theory, and to make the machine actions directed by AI explainable to team members and society. The special topic articles in this issue and a subsequent issue of AI Magazine review ongoing mature research and operational programs that address context for human‐machine teams.},
journal = {AI Mag.},
month = sep,
pages = {5–13},
numpages = {9}
}

@article{10.1007/s10676-022-09624-3,
author = {Sartori, Laura and Theodorou, Andreas},
title = {A sociotechnical perspective for the future of AI: narratives, inequalities, and human control},
year = {2022},
issue_date = {Mar 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {1},
issn = {1388-1957},
url = {https://doi.org/10.1007/s10676-022-09624-3},
doi = {10.1007/s10676-022-09624-3},
abstract = {Different people have different perceptions about artificial intelligence (AI). It is extremely important to bring together all the alternative frames of thinking—from the various communities of developers, researchers, business leaders, policymakers, and citizens—to properly start acknowledging AI. This article highlights the ‘fruitful collaboration’ that sociology and AI could develop in both social and technical terms. We discuss how biases and unfairness are among the major challenges to be addressed in such a sociotechnical perspective. First, as intelligent machines reveal their nature of ‘magnifying glasses’ in the automation of existing inequalities, we show how the AI technical community is calling for transparency and explainability, accountability and contestability. Not to be considered as panaceas, they all contribute to ensuring human control in novel practices that include requirement, design and development methodologies for a fairer AI. Second, we elaborate on the mounting attention for technological narratives as technology is recognized as a social practice within a specific institutional context. Not only do narratives reflect organizing visions for society, but they also are a tangible sign of the traditional lines of social, economic, and political inequalities. We conclude with a call for a diverse approach within the AI community and a richer knowledge about narratives as they help in better addressing future technical developments, public debate, and policy. AI practice is interdisciplinary by nature and it will benefit from a socio-technical perspective.},
journal = {Ethics and Inf. Technol.},
month = mar,
numpages = {11},
keywords = {Human control, Inequalities, Narratives, Transparency, Accountability, Sociology, Artificial intelligence}
}

@phdthesis{10.5555/AAI28717215,
author = {Liao, Yuting and Beth, St. Jean, and Amanda, Lazar, and Kyoung, Choe, Eun and Dennis, Kivlighan,},
advisor = {Jessica, Vitak,},
title = {Design and Evaluation of a Conversational Agent for Mental Health Support: Forming Human-Agent Sociotechnical and Therapeutic Relationships},
year = {2021},
isbn = {9798790625947},
publisher = {University of Maryland, College Park},
abstract = {Many people with mental health disorders face significant challenges getting the help they need, including the costs of obtaining psychological counseling or psychiatry services, as well as fear of being stigmatized. As a way of addressing these barriers, text-based conversational agents (chatbots) have gained traction as a new form of e-therapy. Powered by artificial intelligence (AI) and natural language processing techniques, this technology offers more natural interactions and a "judgment-free zone" for clients concerned about stigma. However, literature on psychotherapeutic chatbots is sparse in both the psychology and human computer interaction (HCI) fields. While recent studies indicate that chatbots provide an affordable and effective therapy delivery method, this research has not thoroughly explained the underlying mechanisms for increasing acceptance of chatbots and making them more engaging. Don Norman (1994) has argued the main difficulties of utilizing intelligent agents are social—not technical—and particularly center around people's perceptions of agents. In exploring the use of chatbots in psychotherapy, we must investigate how this technology is conceptually understood, and the thoughts and feelings they evoke when people interact with them. This dissertation focuses on two types of relationships critical to the success of utilizing chatbots for mental health interventions: sociotechnical relationships and therapeutic relationships. A sociotechnical relationship concerns technology adoption, usability, and the compatibility between humans and chatbots. A therapeutic relationship encompasses people's feelings and attitudes toward a chatbot therapist. Therefore, this dissertation asks: What are the optimal design principles for a conversational agent that facilitates the development of both sociotechnical and therapeutic relationships to help people manage their mental health? To investigate this question, I designed an original conversational system with eight gendered and racially heterogeneous personas, and one neutral robot-like persona. Using a mixed-method approach (online experiment and interviews), I evaluated factors related to the adoption and use of conversational agents for psychotherapeutic purposes. I also unpacked the human-agent relational dynamics and evaluated how anthropomorphism and perceived racial similarity impact people's perceptions of and interactions with the chatbot. These findings contributed to the wider understanding of conversational AI application in mental health support and provided actionable design recommendations.},
note = {AAI28717215}
}

@inproceedings{10.1145/3377815.3381375,
author = {Dugdale, Julie and Moghaddam, Mahyar T. and Muccini, Henry},
title = {Human behaviour centered design: developing a software system for cultural heritage},
year = {2020},
isbn = {9781450371254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377815.3381375},
doi = {10.1145/3377815.3381375},
abstract = {This paper introduces an integrated framework for sustainability and urban security socio-technical systems. The focus is to design and develop a hardware/software system based on human expected and real behaviour.The paper explains the steps taken through developing the Uffizi Museum crowd monitoring and queue management system. The goal of implementing such system was to remove queues outside the Museum which is in line with urban security and visitors comfort. We took advantage of a data-driven approach mapped on sustainability framework. Such approach which was fed with both real-time sensory data and prediction models, successfully eliminated long queues to access the museum. We took into consideration performance of software system as well to reduce the response time to a threshold that is compliant with real-time requirements. We started our experiments from fall 2016 and operationalized it in October, 2018. During this experimentation period, we learned a lot of lessons that we report in this paper.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Society},
pages = {85–94},
numpages = {10},
keywords = {sustainability, software engineering, queue management, empirical study, cultural heritage, crowd monitoring},
location = {Seoul, South Korea},
series = {ICSE-SEIS '20}
}

@article{10.1145/3637396,
author = {Ehsan, Upol and Liao, Q. Vera and Passi, Samir and Riedl, Mark O. and Daum\'{e}, Hal},
title = {Seamful XAI: Operationalizing Seamful Design in Explainable AI},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3637396},
doi = {10.1145/3637396},
abstract = {Mistakes in AI systems are inevitable, arising from both technical limitations and sociotechnical gaps. While black-boxing AI systems can make the user experience seamless, hiding the seams risks disempowering users to mitigate fallouts from AI mistakes. Instead of hiding these AI imperfections, can we leverage them to help the user? While Explainable AI (XAI) has predominantly tackled algorithmic opaqueness, we propose that seamful design can foster AI explainability by revealing and leveraging sociotechnical and infrastructural mismatches. We introduce the concept of Seamful XAI by (1) conceptually transferring "seams" to the AI context and (2) developing a design process that helps stakeholders anticipate and design with seams. We explore this process with 43 AI practitioners and real end-users, using a scenario-based co-design activity informed by real-world use cases. We found that the Seamful XAI design process helped users foresee AI harms, identify underlying reasons (seams), locate them in the AI's lifecycle, learn how to leverage seamful information to improve XAI and user agency. We share empirical insights, implications, and reflections on how this process can help practitioners anticipate and craft seams in AI, how seamfulness can improve explainability, empower end-users, and facilitate Responsible AI.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {119},
numpages = {29},
keywords = {explainable ai, human-ai interaction, responsible ai, seamful design}
}

@article{10.4018/JGIM.354062,
author = {Yang, Alan and Talaei, Jason and Takishova, Tolkyn and Masialeti, Masialeti},
title = {How Does Cost Leadership Strategy Suppress the Performance Benefits of Explainability of AI Applications in Organizations?},
year = {2024},
issue_date = {Aug 2024},
publisher = {IGI Global},
address = {USA},
volume = {32},
number = {1},
issn = {1062-7375},
url = {https://doi.org/10.4018/JGIM.354062},
doi = {10.4018/JGIM.354062},
abstract = {This study examines how explainable artificial intelligence-enabled agility, encompassing operational and market agility, influences performance. We examine the potentially negative effect of a cost-leadership strategy on the performance benefits of explainable AI. Data from a survey of IT executives was analyzed. Findings suggest that the explainability of artificial intelligence applications does not directly relate to performance. Instead, its benefits are actualized through operational and market agility. However, our study finds that in organizations that prioritize cost leadership, the positive link between explainability and performance becomes negative and significant. Additionally, the mediating roles of operational and market agility become insignificant in such organizations.},
journal = {J. Glob. Inf. Manage.},
month = sep,
pages = {1–23},
numpages = {23},
keywords = {Explainable Artificial Intelligence, Operational Agility, Market Agility, Cost Leadership, Performance}
}

@inproceedings{10.1145/3582269.3615595,
author = {Kawakami, Anna and Guerdan, Luke and Cheng, Yanghuidi and Glazko, Kate and Lee, Matthew and Carter, Scott and Arechiga, Nikos and Zhu, Haiyi and Holstein, Kenneth},
title = {Training Towards Critical Use: Learning to Situate AI Predictions Relative to Human Knowledge},
year = {2023},
isbn = {9798400701139},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582269.3615595},
doi = {10.1145/3582269.3615595},
abstract = {A growing body of research has explored how to support humans in making better use of AI-based decision support, including via training and onboarding. Existing research has focused on decision-making tasks where it is possible to evaluate “appropriate reliance” by comparing each decision against a ground truth label that cleanly maps to both the AI’s predictive target and the human decision-maker’s goals. However, this assumption does not hold in many real-world settings where AI tools are deployed today (e.g., social work, criminal justice, and healthcare). In this paper, we introduce a process-oriented notion of appropriate reliance called critical use that centers the human’s ability to situate AI predictions against knowledge that is uniquely available to them but unavailable to the AI model. To explore how training can support critical use, we conduct a randomized online experiment in a complex social decision-making setting: child maltreatment screening. We find that, by providing participants with accelerated, low-stakes opportunities to practice AI-assisted decision-making in this setting, novices came to exhibit patterns of disagreement with AI that resemble those of experienced workers. A qualitative examination of participants’ explanations for their AI-assisted decisions revealed that they drew upon qualitative case narratives, to which the AI model did not have access, to learn when (not) to rely on AI predictions. Our findings open new questions for the study and design of training for real-world AI-assisted decision-making.},
booktitle = {Proceedings of The ACM Collective Intelligence Conference},
pages = {63–78},
numpages = {16},
keywords = {AI onboarding and training, algorithm-assisted decision-making, augmented intelligence, human-AI complementarity},
location = {Delft, Netherlands},
series = {CI '23}
}

@inproceedings{10.1145/3708359.3712105,
author = {Abichandani, Harshavardhan Sunil and Zhang, Wencan and Lim, Brian Y},
title = {Robust Relatable Explanations of Machine Learning with Disentangled Cue-specific Saliency},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712105},
doi = {10.1145/3708359.3712105},
abstract = {Concept-based explanations help users understand the relation between model predictions and meaningful cues. However, under noisy real-world conditions, data perturbations lead to distorted and deviated explanations. We hypothesize that these corruptions affect specific cues rather than all, so disentangling them may help reduce model dependency on degraded cues. For the application of explaining emotional speech recognition, we propose RobustRexNet to explain with disentangled and discretized saliency maps for separate speech cues (e.g., loudness, pitch) to improve robustness against noise. Modeling evaluations show that RobustRexNet improved both model performance and explanation faithfulness in noisy and privacy-preserving distortions. User studies further indicate that the robust explanations aligned better with human intuition and improved user emotion labeling under noise. This work contributes toward robust explainable AI to improve user trust under real-world conditions.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {1203–1231},
numpages = {29},
keywords = {Explainable AI, misleading explanations, robust machine learning, vocal emotion},
location = {
},
series = {IUI '25}
}

@inproceedings{10.1145/3643834.3661576,
author = {Lee, Christine P and Lee, Min Kyung and Mutlu, Bilge},
title = {The AI-DEC: A Card-based Design Method for User-centered AI Explanations},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661576},
doi = {10.1145/3643834.3661576},
abstract = {Increasing evidence suggests that many deployed AI systems do not sufficiently support end-user interaction and information needs. Engaging end-users in the design of these systems can reveal user needs and expectations, yet effective ways of engaging end-users in the AI explanation design remain under-explored. To address this gap, we developed a design method, called AI-DEC, that defines four dimensions of AI explanations that are critical for the integration of AI systems—communication content, modality, frequency, and direction—and offers design examples for end-users to design AI explanations that meet their needs. We evaluated this method through co-design sessions with workers in healthcare, finance, and management industries who regularly use AI systems in their daily work. Findings indicate that the AI-DEC effectively supported workers in designing explanations that accommodated diverse levels of performance and autonomy needs, which varied depending on the AI system’s workplace role and worker values. We discuss the implications of using the AI-DEC for the user-centered design of AI explanations in real-world systems.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {1010–1028},
numpages = {19},
keywords = {Design cards, human-AI interaction, user-centered design},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3613905.3637113,
author = {Bell, Andrew and Stoyanovich, Julia},
title = {Making Transparency Influencers: A Case Study of an Educational Approach to Improve Responsible AI Practices in News and Media},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3637113},
doi = {10.1145/3613905.3637113},
abstract = {Concerns about the risks posed by artificial intelligence (AI) have resulted in growing interest in algorithmic transparency. While algorithmic transparency is well-studied, there is evidence that many organizations do not value implementing transparency. In this case study, we test a ground-up approach to ensuring better real-world algorithmic transparency by creating transparency influencers — motivated individuals within organizations who advocate for transparency. We held an interactive online workshop on algorithmic transparency and advocacy for 15 professionals from news, media, and journalism. We reflect on workshop design choices and presents insights from participant interviews. We found positive evidence for our approach: In the days following the workshop, three participants had done pro-transparency advocacy. Notably, one of them advocated for algorithmic transparency at an organization-wide AI strategy meeting. In the words of a participant: “if you are questioning whether or not you need to tell people [about AI], you need to tell people.”},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {523},
numpages = {8},
keywords = {Transparency, artificial intelligence, explainability, machine learning, tempered radicals},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1007/978-3-031-60611-3_28,
author = {Koebler, Alexander and Greisinger, Christian and Paulus, Jan and Thon, Ingo and Buettner, Florian},
title = {Through the&nbsp;Eyes of&nbsp;the&nbsp;Expert: Aligning Human and&nbsp;Machine Attention for&nbsp;Industrial AI},
year = {2024},
isbn = {978-3-031-60613-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-60611-3_28},
doi = {10.1007/978-3-031-60611-3_28},
abstract = {Human expertise and intuition are crucial in solving many tasks in expert-driven domains such as industrial manufacturing or medical diagnosis. In this work, we use the human expert’s gaze information to take a step towards transferring this knowledge to a machine learning model. In this way, we are aligning the attention the machine and the human pay to solve the task. Previous works in the medical field have shown that privileged gaze information during training can increase predictive performance and reduce the label requirement of a machine learning model. We extend on the aim of those works and quantitatively evaluate the benefit of aligning human and machine attention on the quality of the model’s explanations as well as its robustness - thus, its trustworthiness. We demonstrate our approach on a real-world visual quality inspection task in the multi-label setting, which is common in industrial applications. Our work illustrates the importance of incorporating human knowledge more explicitly in training machine learning models and takes a step towards enabling machine learning based systems in high-stakes applications.},
booktitle = {Artificial Intelligence in HCI: 5th International Conference, AI-HCI 2024, Held as Part of the 26th HCI International Conference, HCII 2024, Washington, DC, USA, June 29–July 4, 2024, Proceedings, Part II},
pages = {407–423},
numpages = {17},
keywords = {Human-Centered AI, Explainable AI, Model Robustness, Human Gaze},
location = {Washington DC, USA}
}

@article{10.1016/j.inffus.2025.103032,
author = {Holzinger, Andreas and Luka\v{c}, Niko and Rozajac, Dzemail and Johnston, Emile and Kocic, Veljka and Hoerl, Bernhard and Gollob, Christoph and Nothdurft, Arne and Stampfer, Karl and Schweng, Stefan and Del Ser, Javier},
title = {Enhancing trust in automated 3D point cloud data interpretation through explainable counterfactuals},
year = {2025},
issue_date = {Jul 2025},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {119},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2025.103032},
doi = {10.1016/j.inffus.2025.103032},
journal = {Inf. Fusion},
month = apr,
numpages = {15},
keywords = {Explainable AI, Point cloud data, Counterfactual reasoning, Information fusion, Interpretability, Human-centered AI}
}

@inproceedings{10.5555/3721488.3721836,
author = {Yadollahi, Elmira and Dogan, Fethiye Irmak and Romeo, Marta and Kontogiorgos, Dimosthenis and Qian, Peizhu and Zhang, Yan},
title = {3rd Workshop on Explainability in Human-Robot Collaboration: Real-World Concerns},
year = {2025},
publisher = {IEEE Press},
abstract = {Robots powered by AI and machine learning are increasingly capable of collaboration and social interaction with humans, leading to a demand to develop new approaches to ensure their transparency and explainable behaviour. As explainable AI (XAI) seeks to clarify AI decisions, its integration into physical robots often creates an illusion of explainability-raising questions about whether current approaches truly enhance understanding. The 3rd Workshop on Explainability in Human-Robot Collaboration aims to address the real-world concerns associated with developing explainable and transparent robots through a focused, multi-faceted panel discussion and a series of paper presentations. In this workshop, we will focus on refining when and how explanations should be provided, integrating human communication principles to enhance trust and transparency in human-robot collaboration through both technical and user-centred solutions.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1994–1996},
numpages = {3},
keywords = {explainable robotics, human-centered robot explanations, xai},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.24963/kr.2024/87,
author = {Rago, Antonio and Martinez, Maria Vanina},
title = {Advancing interactive explainable AI via belief change theory},
year = {2024},
isbn = {978-1-956792-05-8},
url = {https://doi.org/10.24963/kr.2024/87},
doi = {10.24963/kr.2024/87},
abstract = {As AI models become ever more complex and intertwined in humans' daily lives, greater levels of interactivity of explainable AI (XAI) methods are needed. In this paper, we propose the use of belief change theory as a formal foundation for operators that model the incorporation of new information, i.e. user feedback in interactive XAI, to logical representations of data-driven classifiers. We argue that this type of formalisation provides a framework and a methodology to develop interactive explanations in a principled manner, providing warranted behaviour and favouring transparency and accountability of such interactions. Concretely, we first define a novel, logic-based formalism to represent explanatory information shared between humans and machines. We then consider real world scenarios for interactive XAI, with different prioritisations of new and existing knowledge, where our formalism may be instantiated. Finally, we analyse a core set of belief change postulates, discussing their suitability for our real world settings and pointing to particular challenges that may require the relaxation or reinterpretation of some of the theoretical assumptions underlying existing operators.},
booktitle = {Proceedings of the 21st International Conference on Principles of Knowledge Representation and Reasoning},
articleno = {87},
numpages = {7},
location = {Hanoi, Vietnam},
series = {KR '24}
}

@inproceedings{10.1007/978-3-031-61315-9_17,
author = {Schmager, Stefan and Gupta, Samrat and Pappas, Ilias and Vassilakopoulou, Polyxeni},
title = {Designing for AI Transparency in Public Services: A User-Centred Study of Citizens’ Preferences},
year = {2024},
isbn = {978-3-031-61314-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-61315-9_17},
doi = {10.1007/978-3-031-61315-9_17},
abstract = {Enhancing transparency in AI enabled public services has the potential to improve their adoption and service delivery. Hence, it is important to identify effective design strategies for AI transparency in public services. To this end, we conduct this empirical qualitative study providing insights for responsible deployment of AI in practice by public organizations. We design an interactive prototype for a Norwegian public welfare service organization which aims to use AI to support sick leaves related services. Qualitative analysis of citizens’ data collected through survey, think-aloud interactions with the prototype, and open-ended questions revealed three key themes related to: articulating information in written form, representing information in graphical form, and establishing the appropriate level of information detail for improving AI transparency in public service delivery. This study advances research pertaining to design of public service portals and has implications for AI implementation in the public sector.},
booktitle = {HCI in Business, Government and Organizations: 11th International Conference, HCIBGO 2024, Held as Part of the 26th HCI International Conference, HCII 2024, Washington, DC, USA, June 29 – July 4, 2024, Proceedings, Part I},
pages = {237–253},
numpages = {17},
keywords = {Transparency, Human-Centered AI, Action Design Research, Design Preferences},
location = {Washington DC, USA}
}

@inproceedings{10.1145/3708557.3716150,
author = {Bhat, Maalvika},
title = {Designing AI Interfaces for Transparent Decision-Making and Ethical Reflection},
year = {2025},
isbn = {9798400714092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708557.3716150},
doi = {10.1145/3708557.3716150},
abstract = {As artificial intelligence (AI) systems increasingly mediate high-stakes decisions in domains such as healthcare, finance, and education, ensuring transparency and ethical accountability in AI interfaces is critical. However, existing interfaces often obscure algorithmic processes, leading to overtrust, disengagement, or misinterpretation of AI-generated outputs. My research explores how interface design—including presentation modes, interactive explainability tools, and speculative design interventions—can shape user perceptions, foster critical reflection, and enhance AI literacy. By integrating controlled experiments, participatory design, and qualitative analysis, my work aims to develop AI interfaces that not only communicate algorithmic decisions effectively but also encourage users to critically assess the ethical implications of AI technologies. I examine the trade-offs between transparency, usability, and engagement, investigating how interfaces can balance cognitive load while making ethical considerations more salient. Through this doctoral consortium, I seek mentorship and feedback on designing adaptive transparency mechanisms and mitigating overtrust in engaging AI interfaces.},
booktitle = {Companion Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {211–214},
numpages = {4},
keywords = {Explainable AI, AI Literacy, AI Transparency, Technology-Mediated Learning, Cognitive Interaction, Interaction Design, User Research, Trust, Control, Agency, Design Theory},
location = {
},
series = {IUI '25 Companion}
}

@inproceedings{10.1145/3490099.3511140,
author = {Ooge, Jeroen and Kato, Shotallo and Verbert, Katrien},
title = {Explaining Recommendations in E-Learning: Effects on Adolescents' Trust},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511140},
doi = {10.1145/3490099.3511140},
abstract = {In the scope of explainable artificial intelligence, explanation techniques are heavily studied to increase trust in recommender systems. However, studies on explaining recommendations typically target adults in e-commerce or media contexts; e-learning has received less research attention. To address these limits, we investigated how explanations affect adolescents’ initial trust in an e-learning platform that recommends mathematics exercises with collaborative filtering. In a randomized controlled experiment with 37&nbsp;adolescents, we compared real explanations with placebo and no explanations. Our results show that real explanations significantly increased initial trust when trust was measured as a multidimensional construct of competence, benevolence, integrity, intention to return, and perceived transparency. Yet, this result did not hold when trust was measured one-dimensionally. Furthermore, not all adolescents attached equal importance to explanations and trust scores were high overall. These findings underline the need to tailor explanations and suggest that dynamically learned factors may be more important than explanations for building initial trust. To conclude, we thus reflect upon the need for explanations and recommendations in e-learning in low-stakes and high-stakes situations.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {93–105},
numpages = {13},
keywords = {XAI, education, explainability, interpretability, teenagers},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inproceedings{10.1145/3687272.3688319,
author = {Wen, Ruchen and Ferraro, Francis and Matuszek, Cynthia},
title = {GPT-4 as a Moral Reasoner for Robot Command Rejection},
year = {2024},
isbn = {9798400711787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687272.3688319},
doi = {10.1145/3687272.3688319},
abstract = {To support positive, ethical human-robot interactions, robots need to be able to respond to unexpected situations in which societal norms are violated, including rejecting unethical commands. Implementing robust communication for robots is inherently difficult due to the variability of context in real-world settings and the risks of unintended influence during robots’ communication. HRI researchers have begun exploring the potential use of LLMs as a solution for language-based communication, which will require an in-depth understanding and evaluation of LLM applications in different contexts. In this work, we explore how an existing LLM responds to and reasons about a set of norm-violating requests in HRI contexts. We ask human participants to assess the performance of a hypothetical GPT-4-based robot on moral reasoning and explanatory language selection as it compares to human intuitions. Our findings suggest that while GPT-4 performs well at identifying norm violation requests and suggesting non-compliant responses, its flaws in not matching the linguistic preferences and context sensitivity of humans prevent it from being a comprehensive solution for moral communication between humans and robots. Based on our results, we provide a four-point recommendation for the community in incorporating LLMs into HRI systems.},
booktitle = {Proceedings of the 12th International Conference on Human-Agent Interaction},
pages = {54–63},
numpages = {10},
keywords = {command rejection, large language models in HRI, moral communication, moral reasoning, robot explanation},
location = {Swansea, United Kingdom},
series = {HAI '24}
}

@article{10.1016/j.ijhcs.2025.103444,
author = {Calisto, Francisco Maria and Abrantes, Jo\~{a}o Maria and Santiago, Carlos and Nunes, Nuno J. and Nascimento, Jacinto C.},
title = {Personalized explanations for clinician-AI interaction in breast imaging diagnosis by adapting communication to expertise levels},
year = {2025},
issue_date = {Mar 2025},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {197},
number = {C},
issn = {1071-5819},
url = {https://doi.org/10.1016/j.ijhcs.2025.103444},
doi = {10.1016/j.ijhcs.2025.103444},
journal = {Int. J. Hum.-Comput. Stud.},
month = apr,
numpages = {25},
keywords = {Human–computer interaction, Intelligent agents, Personalized medicine, Medical imaging, Breast cancer}
}

@inproceedings{10.1145/3643562.3672613,
author = {Zhang, Yang and Zong, Ruohan and Shang, Lanyu and Yue, Zhenrui and Zeng, Huimin and Liu, Yifan and Wang, Dong},
title = {Tripartite Intelligence: Synergizing Deep Neural Network, Large Language Model, and Human Intelligence for Public Health Misinformation Detection (Archival Full Paper)},
year = {2024},
isbn = {9798400705540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643562.3672613},
doi = {10.1145/3643562.3672613},
abstract = {The threat of rapidly spreading health misinformation through social media during crises like COVID-19 emphasizes the importance of addressing both clear falsehoods and complex misinformation, including conspiracy theories and subtle distortions. This paper designs a novel tripartite collective intelligence approach that integrates deep neural networks (DNNs), large language models (LLMs), and crowdsourced human intelligence (HI) to collaboratively detect complex forms of public health misinformation on social media. Our design is inspired by the collaborative strengths of DNNs, LLMs, and HI, which complement each other. We observe that DNNs efficiently handle large datasets for initial misinformation screening but struggle with complex content and rely on high-quality training data. LLMs enhance misinformation detection with improved language understanding but may sometimes provide eloquent yet factually incorrect explanations, risking misinformation mislabeling. HI provides critical thinking and ethical judgment superior to DNNs and LLMs but is slower and more costly in misinformation detection. In particular, we develop TriIntel , a tripartite collaborative intelligence framework that leverages the collective intelligence of DNNs, LLMs, and HI to tackle the public health information detection problem under a novel few-shot and uncertainty-aware maximum likelihood estimation framework. Evaluation results on a real-world public health misinformation detection application related to COVID-19 show that TriIntel outperforms representative DNNs, LLMs, and human-AI collaboration baselines in accurately detecting public health misinformation under a diverse set of evaluation scenarios.},
booktitle = {Proceedings of the ACM Collective Intelligence Conference},
pages = {63–75},
numpages = {13},
keywords = {Collective Intelligence, Human-AI Collaboration, Large Language Model, Misinformation},
location = {Boston, MA, USA},
series = {CI '24}
}

@inproceedings{10.1145/2962132.2962141,
author = {Tamime, Reham Al and Hall, Wendy and Giordano, Richard},
title = {Medical Science in Wikipedia: The Construction of Scientific Knowledge in Open Science Projects},
year = {2016},
isbn = {9781450344814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2962132.2962141},
doi = {10.1145/2962132.2962141},
abstract = {Wikipedia has challenged the way traditional encyclopedia knowledge is built and contested by creating an open socio-technical environment that allows non-domain experts to contribute to scientific and medical knowledge. The open nature of Wikipedia has been successful, but there are concerns about the quality and trustworthiness of its articles. The goal of my research is to build a theoretical framework to explain the dynamic of knowledge building in crowd-sourcing based environments like Wikipedia and judge the trustworthiness of the medical articles based on the dynamic network data. By applying Actor Network Theory and Social Network Analysis, the contribution of my research is theoretical and practical as to build a theory on the dynamics of knowledge building in Wikipedia across times and to offer insights for developing citizen science crowd-sourcing platforms by better understanding how editors interact to build health science content.},
booktitle = {Proceedings of the 12th International Symposium on Open Collaboration Companion},
articleno = {4},
numpages = {4},
keywords = {wikipedia, trust, facts' dynamics, citizen science, Open collaboration},
location = {Berlin, Germany},
series = {OpenSym '16}
}

@inproceedings{10.5555/3666122.3669148,
author = {Yang, Tao and Wang, Yuwang and Lu, Yan and Zheng, Nanning},
title = {DisDiff: unsupervised disentanglement of diffusion probabilistic models},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Targeting to understand the underlying explainable factors behind observations and modeling the conditional generation process on these factors, we connect disentangled representation learning to Diffusion Probabilistic Models (DPMs) to take advantage of the remarkable modeling ability of DPMs. We propose a new task, disentanglement of (DPMs): given a pre-trained DPM, without any annotations of the factors, the task is to automatically discover the inherent factors behind the observations and disentangle the gradient fields of DPM into sub-gradient fields, each conditioned on the representation of each discovered factor. With disentangled DPMs, those inherent factors can be automatically discovered, explicitly represented, and clearly injected into the diffusion process via the sub-gradient fields. To tackle this task, we devise an unsupervised approach named DisDiff, achieving disentangled representation learning in the framework of DPMs. Extensive experiments on synthetic and real-world datasets demonstrate the effectiveness of DisDiff.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {3026},
numpages = {27},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{10.1145/3701716.3715871,
author = {Xu, Da and Zhang, Danqing and Ruan, Chuanwei and Zheng, Lingling and Yang, Bo and Yang, Guangyu and Xu, Shuyuan and Wang, Haixun},
title = {Tutorial on Landing Generative AI in Industrial Social and E-commerce Recsys},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715871},
doi = {10.1145/3701716.3715871},
abstract = {Over the past two years, generative AI (GenAI) has evolved rapidly, influencing interdisciplinary fields including social and e-commerce Recsys. Despite several exciting research advances, landing GenAI innovations in real-world Recsys remains challenging due to the sophistication of modern industrial product and systems. Our tutorial begins with a brief overview of industrial Recsys and GenAI fundamentals (including LLMOps), followed by the ongoing efforts and opportunities to enhance existing Recsys data and model with foundation models. We then explore how GenAI's curation and reasoning capabilities can be integrated into Recsys-for example, by repurposing raw content, incorporating external knowledge for display and creative optimization, and generating personalized insights/explanations to foster transparency and trust. Following this, the tutorial highlights how AI agents can reshape Recsys by introducing interactive reasoning and action loops, moving beyond traditional passive feedback models. Lastly, we shed insights on real-world solutions for human-AI alignment and responsible GenAI practices. A key feature of the tutorial is the presentation of the holistic AI, Infrastructure, LLMOps, and Product roadmap, including evaluation and responsible AI practices, based on production solutions from LinkedIn, Amazon, Meta, TikTok, Microsoft, and Instacart. Real-world case studies and demos are further provided for illustration. While GenRecsys is still in its early stages, this tutorial provides valuable insights and practical strategies for the Recsys and GenAI communities, bridging scientific research and applied solutions in this novel and rapidly growing field.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {57–60},
numpages = {4},
keywords = {generative ai, industrial system, information retrieval, large language model, recommender system},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3531146.3534638,
author = {Robertson, Samantha and D\'{\i}az, Mark},
title = {Understanding and Being Understood: User Strategies for Identifying and Recovering From Mistranslations in Machine Translation-Mediated Chat},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534638},
doi = {10.1145/3531146.3534638},
abstract = {Machine translation (MT) is now widely and freely available, and has the potential to greatly improve cross-lingual communication. In order to use MT reliably and safely, end users must be able to assess the quality of system outputs and determine how much they can rely on them to guide their decisions and actions. However, it can be difficult for users to detect and recover from mistranslations due to limited language skills. In this work we collected 19 MT-mediated role-play conversations in housing and employment scenarios, and conducted in-depth interviews to understand how users identify and recover from translation errors. Participants communicated using four language pairs: English, and one of Spanish, Farsi, Igbo, or Tagalog. We conducted qualitative analysis to understand user challenges in light of limited system transparency, strategies for recovery, and the kinds of translation errors that proved more or less difficult for users to overcome. We found that users broadly lacked relevant and helpful information to guide their assessments of translation quality. Instances where a user erroneously thought they had understood a translation correctly were rare but held the potential for serious consequences in the real world. Finally, inaccurate and disfluent translations had social consequences for participants, because it was difficult to discern when a disfluent message was reflective of the other person’s intentions, or an artifact of imperfect MT. We draw on theories of grounding and repair in communication to contextualize these findings, and propose design implications for explainable AI (XAI) researchers, MT researchers, as well as collaboration among them to support transparency and explainability in MT. These directions include handling typos and non-standard grammar common in interpersonal communication, making MT in interfaces more visible to help users evaluate errors, supporting collaborative repair of conversation breakdowns, and communicating model strengths and weaknesses to users.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2223–2238},
numpages = {16},
keywords = {computer-mediated communication, explainable machine learning, human-AI interaction, machine translation},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3605390.3610822,
author = {Cavagnetto, Nicola and Venditti, Roberto and Cocchioni, Matteo and Bonelli, Stefano},
title = {Demo: SectorX, an en-route ATC simulator for AI-based decision support to Air Traffic Controllers: A case study in the MAHALO project},
year = {2023},
isbn = {9798400708060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605390.3610822},
doi = {10.1145/3605390.3610822},
abstract = {The EU funded MAHALO Project investigated the effects of transparent and conformal Machine Learning models in support of the safety-critical time-pressured task of Conflict Detection and Resolution for en-route Air Traffic Controllers. The experimental phase was conducted in Italy and Sweden, involving 36 Air Traffic Controllers playing simulated en-route Air Traffic Control scenarios. These scenarios were presented through Sector X, an en-route Air Traffic Control simulator with an ecological User Interface based on the Maastricht Upper Area Control's Controller Working Position. The researchers investigated transparency by introducing three different explainability levels through three visual explanations, and conformance by training with three different conflict resolution styles in the Machine Learning model. The researchers will present the UI designed to achieve transparency to the participants of the demo sessions, who will be able to play Air Traffic Control scenarios.},
booktitle = {Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {56},
numpages = {3},
keywords = {Transparency, Human-Computer Interaction, Human-AI Teaming, Explainable AI, Conformance},
location = {Torino, Italy},
series = {CHItaly '23}
}

@inproceedings{10.1145/3613905.3651080,
author = {Okolo, Chinasa T. and Lin, Hongjin},
title = {"You can’t build what you don’t understand": Practitioner Perspectives on Explainable AI in the Global South},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651080},
doi = {10.1145/3613905.3651080},
abstract = {AI for Social Good (AI4SG) has been advocated as a way to address social impact problems using emerging technologies, but little research has examined practitioner motivations behind building these tools and how practitioners make such tools understandable to stakeholders and end users, e.g., through leveraging techniques such as explainable AI (XAI). In this study, we interviewed 12 AI4SG practitioners to understand their experiences developing social impact technologies and their perceptions of XAI, focusing on projects in the Global South. While most of our participants were aware of XAI, many did not incorporate these techniques due to a lack of domain expertise, difficulty incorporating XAI into their existing workflows, and perceiving XAI as less valuable for end users with low levels of AI and digital literacy. Our work reflects on the shortcomings of XAI for real-world use and advocates for a reimagined agenda for human-centered explainability research.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {11},
numpages = {10},
keywords = {AI for Social Good, Artificial Intelligence, Explainable AI, Human-Centered Design, Social Impact},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3654777.3676368,
author = {Tian, Yuan and Kummerfeld, Jonathan K. and Li, Toby Jia-Jun and Zhang, Tianyi},
title = {SQLucid: Grounding Natural Language Database Queries with Interactive Explanations},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676368},
doi = {10.1145/3654777.3676368},
abstract = {Though recent advances in machine learning have led to significant improvements in natural language interfaces for databases, the accuracy and reliability of these systems remain limited, especially in high-stakes domains. This paper introduces SQLucid, a novel user interface that bridges the gap between non-expert users and complex database querying processes. SQLucid addresses existing limitations by integrating visual correspondence, intermediate query results, and editable step-by-step SQL explanations in natural language to facilitate user understanding and engagement. This unique blend of features empowers users to understand and refine SQL queries easily and precisely. Two user studies and one quantitative experiment were conducted to validate SQLucid’s effectiveness, showing significant improvement in task completion accuracy and user confidence compared to existing interfaces. Our code is available at https://github.com/magic-YuanTian/SQLucid.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {12},
numpages = {20},
keywords = {Databases, Explanations, Natural Language Interfaces},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@article{10.1007/s10676-024-09762-w,
author = {Wood, Nathan Gabriel},
title = {Explainable AI in the military domain},
year = {2024},
issue_date = {Jun 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {1388-1957},
url = {https://doi.org/10.1007/s10676-024-09762-w},
doi = {10.1007/s10676-024-09762-w},
abstract = {Artificial intelligence (AI) has become nearly ubiquitous in modern society, from components of mobile applications to medical support systems, and everything in between. In societally impactful systems imbued with AI, there has been increasing concern related to opaque AI, that is, artificial intelligence where it is unclear how or why certain decisions are reached. This has led to a recent boom in research on “explainable AI” (XAI), or approaches to making AI more explainable and understandable to human users. In the military domain, numerous bodies have argued that autonomous and AI-enabled weapon systems ought not incorporate unexplainable AI, with the International Committee of the Red Cross and the United States Department of Defense both explicitly including explainability as a relevant factor in the development and use of such systems. In this article, I present a cautiously critical assessment of this view, arguing that explainability will be irrelevant for many current and near-future autonomous systems in the military (which do not incorporate any AI), that it will be trivially incorporated into most military systems which do possess AI (as these generally possess simpler AI systems), and that for those systems with genuinely opaque AI, explainability will prove to be of more limited value than one might imagine. In particular, I argue that explainability, while indeed a virtue in design, is a virtue aimed primarily at designers and troubleshooters of AI-enabled systems, but is far less relevant for users and handlers actually deploying these systems. I further argue that human–machine teaming is a far more important element of responsibly using AI for military purposes, adding that explainability may undermine efforts to improve human–machine teamings by creating a prima facie sense that the AI, due to its explainability, may be utilized with little (or less) potential for mistakes. I conclude by clarifying that the arguments are not against XAI in the military, but are instead intended as a caution against over-inflating the value of XAI in this domain, or ignoring the limitations and potential pitfalls of this approach.},
journal = {Ethics and Inf. Technol.},
month = apr,
numpages = {13},
keywords = {Autonomous weapon systems, Artificial intelligence, AI, Explainability, Human–machine interaction}
}

@inproceedings{10.1145/2950290.2994160,
author = {Herbsleb, James},
title = {Building a socio-technical theory of coordination: why and how (outstanding research award)},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2994160},
doi = {10.1145/2950290.2994160},
abstract = {Research aimed at understanding and addressing coordination breakdowns experienced in global software development (GSD) projects at Lucent Technologies took a path from open-ended qualitative exploratory studies to quantitative studies with a tight focus on a key problem – delay – and its causes. Rather than being directly associated with delay, multi-site work items involved more people than comparable same-site work items, and the number of people was a powerful predictor of delay. To counteract this, we developed and deployed tools and practices to support more effective communication and expertise location. After conducting two case studies of open source development, an extreme form of GSD, we realized that many tools and practices could be effective for multi-site work, but none seemed to work under all conditions. To achieve deeper insight, we developed and tested our Socio-Technical Theory of Coordination (STTC) in which the dependencies among engineering decisions are seen as defining a constraint satisfaction problem that the organization can solve in a variety of ways. I conclude by explaining how we applied these ideas to transparent development environments, then sketch important open research questions.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {2–10},
numpages = {9},
keywords = {transparent environments, socio-technical theory of coordination, open source, global software development, empirical studies, collaboration, Coordination},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3336499.3338010,
author = {Qian, Kun and Burdick, Douglas and Gurajada, Sairam and Popa, Lucian},
title = {Learning Explainable Entity Resolution Algorithms for Small Business Data using SystemER},
year = {2019},
isbn = {9781450368230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336499.3338010},
doi = {10.1145/3336499.3338010},
abstract = {The 2019 FEIII CALI data challenge aims at linking different representations of the same real-world entities across multiple public datasets that collect identification and activity data about small to medium enterprises (SMEs) in California. We formalize this challenge as a learning-based entity resolution (ER) task, the goal of which is to learn a high-precision and high-recall pair-wise ER model that classifies small business entity pairs into matches and non-matches. Realistic ER tasks usually involve a pipeline of laborintensive and error-prone tasks, such as data preprocesing, gathering of training data, feature engineering, and model tuning. In this task, we apply an advanced human-in-the-loop system, named SystemER, to learn ER algorithms for SME entities. Powered by active learning and via a carefully designed user interface, SystemER can learn high-quality explainable ER algorithms with low human effort, while achieving high-accuracy on the datasets provided by the FEIII CALI data challenge.},
booktitle = {Proceedings of the 5th Workshop on Data Science for Macro-Modeling with Financial and Economic Datasets},
articleno = {8},
numpages = {6},
keywords = {small business, human-in-the-loop, SystemER, Entity resolution},
location = {Amsterdam, Netherlands},
series = {DSMM'19}
}

@inproceedings{10.5555/3535850.3535971,
author = {Qian, Peizhu and Unhelkar, Vaibhav},
title = {Evaluating the Role of Interactivity on Improving Transparency in Autonomous Agents},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Autonomous agents are increasingly being deployed amongst human end-users. Yet, human users often have little knowledge of how these agents work or what they will do next. This lack of transparency has already resulted in unintended consequences during AI use: a concerning trend which is projected to increase with the proliferation of autonomous agents. To curb this trend and ensure safe use of AI, assisting users in establishing an accurate understanding of agents that they work with is essential. In this work, we present AI teacher, a user-centered Explainable AI framework to address this need for autonomous agents that follow a Markovian policy. Our framework first computes salient instructions of agent behavior by estimating a user's mental model and utilizing algorithms for sequential decision-making. Next, in contrast to existing solutions, these instructions are presented interactively to the end-users, thereby enabling a personalized approach to improving AI transparency. We evaluate our framework, with emphasis on its interactive features, through experiments with human participants. The experiment results suggest that, relative to non-interactive approaches, interactive teaching can both reduce the amount of time it takes for humans to create accurate mental models of these agents and is subjectively preferred by human users.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1083–1091},
numpages = {9},
keywords = {shared mental models, machine teaching, human-AI collaboration, explainable AI, Monte-Carlo tree search},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3590003.3590107,
author = {Steindl, Sebastian and Surner, Martin},
title = {CIP-ES: Causal Input Perturbation for Explanation Surrogates},
year = {2023},
isbn = {9781450399449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590003.3590107},
doi = {10.1145/3590003.3590107},
abstract = {With current advances in Machine Learning and its growing use in high-impact scenarios, the demand for interpretable and explainable models becomes crucial. Causality research tries to go beyond statistical correlations by focusing on causal relationships, which is fundamental for Interpretable and Explainable Artificial Intelligence. In this paper, we perturb the input for explanation surrogates based on causal graphs. We present an approach to combine surrogate-based explanations with causal knowledge. We apply the perturbed data to the Local Interpretable Model-agnostic Explanations (LIME) approach to showcase how causal graphs improve explanations of surrogate models. We thus integrate features from both domains by adding a causal component to local explanations. The proposed approach enables explanations that suit the expectations of the user by having the user define an appropriate causal graph. Accordingly, these expectations are true to the user. We demonstrate the suitability of our method using real world data.},
booktitle = {Proceedings of the 2023 2nd Asia Conference on Algorithms, Computing and Machine Learning},
pages = {569–574},
numpages = {6},
keywords = {Surrogate Models, Interpretability, Explainable AI, Causality, Causability},
location = {Shanghai, China},
series = {CACML '23}
}

@inproceedings{10.1145/3613905.3638195,
author = {Lee, Christine P.},
title = {Design, Development, and Deployment of Context-Adaptive AI Systems for Enhanced User Adoption},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3638195},
doi = {10.1145/3613905.3638195},
abstract = {My research centers on the development of context-adaptive AI systems to improve end-user adoption through the integration of technical methods. I deploy these AI systems across various interaction modalities, including user interfaces and embodied agents like robots, to expand their practical applicability. My research unfolds in three key stages: design, development, and deployment. In the design phase, user-centered approaches were used to understand user experiences with AI systems and create design tools for user participation in crafting AI explanations. In the ongoing development stage, a safety-guaranteed AI system for a robot agent was created to automatically provide adaptive solutions and explanations for unforeseen scenarios. The next steps will involve the implementation and evaluation of context-adaptive AI systems in various interaction forms. I seek to prioritize human needs in technology development, creating AI systems that tangibly benefit end-users in real-world applications and enhance interaction experiences.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {429},
numpages = {5},
keywords = {Human-AI interaction, human-robot interaction, user-centered design},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3631700.3664900,
author = {Portaz, Miguel and Manjarres, Angeles and Santos, Olga C.},
title = {Harmonizing Ethical Principles: Feedback Generation Approaches in Modeling Human Factors for Assisted Psychomotor Systems},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3664900},
doi = {10.1145/3631700.3664900},
abstract = {As the demand for personalized and adaptive learning experiences increase, there is a urgent need for providing effective feedback mechanisms within critical systems, such as in psychomotor learning systems. This proposal introduces an approach for the integration of retrieval-augmented generation tools to provide comprehensive and insightful feedback to users. By combining the strengths of retrieval-based techniques and generative models, these tools offer the potential to enhance learning outcomes by delivering tailored feedback that is both informative and engaging. The proposal also emphasises the importance of incorporating explainability and transparency concepts. Following the hybrid intelligence paradigm it is possible to ensure that the feedback provided by these tools is not only accurate but also understandable to humans. This approach fosters trust and promotes a deeper understanding of the psychomotor learning process, empowering users and facilitators to make informed decisions about the psychomotor learning path. The hybrid intelligence paradigm, which combines the strengths of both human and artificial intelligence, plays a crucial role in the deployment of these solutions. By taking advantage of the cognitive capabilities of human experts alongside the computational power of artificial intelligence algorithms, it is possible to offer personalised feedback that takes into account both technical accuracy and pedagogical effectiveness. Through these collaborative efforts it is also possible to create learning environments that are inclusive, adaptable, and beneficial to lifelong learning. In conclusion, this proposal introduces retrieval-augmented generation tools for providing feedback in psychomotor learning systems, which represents a significant step towards in its personalization, and whose ethical implications align with the new regulations on the implementation of intelligent technologies in critical systems.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {380–385},
numpages = {6},
keywords = {Collaborative Learning, Ethics, Human-Centered, Hybrid Intelligence, Intelligent Psychomotor Systems, Retrieval Augmented Generation, XAI},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@article{10.1016/j.websem.2024.100856,
author = {Vidal, Maria-Esther and Chudasama, Yashrajsinh and Huang, Hao and Purohit, Disha and Torrente, Maria},
title = {Integrating Knowledge Graphs with Symbolic AI: The Path to Interpretable Hybrid AI Systems in Medicine},
year = {2025},
issue_date = {Jan 2025},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {84},
number = {C},
issn = {1570-8268},
url = {https://doi.org/10.1016/j.websem.2024.100856},
doi = {10.1016/j.websem.2024.100856},
journal = {Web Semant.},
month = jan,
numpages = {8},
keywords = {Knowledge Graphs, Neuro-symbolic systems, Semantic Data Management, Valid link prediction, Counterfactual prediction, KG-based applications}
}

@article{10.1016/j.engappai.2024.109561,
author = {Krai\v{s}nikovi\'{c}, Ceca and Harb, Robert and Plass, Markus and Zoughbi, Wael Al and Holzinger, Andreas and M\"{u}ller, Heimo},
title = {Fine-tuning language model embeddings to reveal domain knowledge: An explainable artificial intelligence perspective on medical decision making},
year = {2025},
issue_date = {Jan 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {139},
number = {PB},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2024.109561},
doi = {10.1016/j.engappai.2024.109561},
journal = {Eng. Appl. Artif. Intell.},
month = jan,
numpages = {15},
keywords = {Pathology reports, Large language models in pathology, Bidirectional Encoder Representations from Transformers model, Language model for German, Domain-language adaptation, Fine-tuning, Analysis of embeddings, Pathology-specific tasks, Digital pathology, Interpretable medical decision scores}
}

@mastersthesis{10.5555/AAI29097887,
author = {Balayan, Vladimir},
advisor = {Pedro, Saleiro, and Ludwig, Krippahl, and Calvente, de Barahona, Pedro Manuel Corr\^{e}a and Emanuel, da Gra\c{c}a Martins, Bruno},
title = {Human-Interpretable Explanations for Black-Box Machine Learning Models: An Application to Fraud Detection},
year = {2020},
isbn = {9798819344118},
publisher = {Universidade NOVA de Lisboa (Portugal)},
abstract = {Machine Learning (ML) has been increasingly used to aid humans making high-stakes decisions in a wide range of areas, from public policy to criminal justice, education, healthcare, or financial services. However, it is very hard for humans to grasp the rationale behind every ML model's prediction, hindering trust in the system. The field of Explainable Artificial Intelligence (XAI) emerged to tackle this problem, aiming to research and develop methods to make those "black-boxes" more interpretable, but there is still no major breakthrough. Additionally, the most popular explanation methods — LIME and SHAP — produce very low-level feature attribution explanations, being of limited usefulness to personas without any ML knowledge.This work was developed at Feedzai, a fintech company that uses ML to prevent financial crime. One of the main Feedzai products is a case management application used by fraud analysts to review suspicious financial transactions flagged by the ML models. Fraud analysts are domain experts trained to look for suspicious evidence in transactions but they do not have ML knowledge, and consequently, current XAI methods do not suit their information needs. To address this, we present JOEL, a neural network-based framework to jointly learn a decision-making task and associated domain knowledge explanations. JOEL is tailored to human-in-the-loop domain experts that lack deep technical ML knowledge, providing high-level insights about the model's predictions that very much resemble the experts' own reasoning. Moreover, by collecting the domain feedback from a pool of certified experts (human teaching), we promote seamless and better quality explanations. Lastly, we resort to semantic mappings between legacy expert systems and domain taxonomies to automatically annotate a bootstrap training set, overcoming the absence of concept-based human annotations. We validate JOEL empirically on a real-world fraud detection dataset, at Feedzai. We show that JOEL can generalize the explanations from the bootstrap dataset. Furthermore, obtained results indicate that human teaching is able to further improve the explanations prediction quality.},
note = {AAI29097887}
}

@article{10.1287/msom.2023.0093,
author = {Hou, Ting and Li, Meng and Tan, Yinliang (Ricky) and Zhao, Huazhong},
title = {Physician Adoption of AI Assistant},
year = {2024},
issue_date = {September-October 2024},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {26},
number = {5},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2023.0093},
doi = {10.1287/msom.2023.0093},
abstract = {Problem definition: Artificial intelligence (AI) assistants—software agents that can perform tasks or services for individuals—are among the most promising AI applications. However, little is known about the adoption of AI assistants by service providers (i.e., physicians) in a real-world healthcare setting. In this paper, we investigate the impact of the AI smartness (i.e., whether the AI assistant is powered by machine learning intelligence) and the impact of AI transparency (i.e., whether physicians are informed of the AI assistant). Methodology/results: We collaborate with a leading healthcare platform to run a field experiment in which we compare physicians’ adoption behavior, that is, adoption rate and adoption timing, of smart and automated AI assistants under transparent and non-transparent conditions. We find that the smartness can increase the adoption rate and shorten the adoption timing, whereas the transparency can only shorten the adoption timing. Moreover, the impact of AI transparency on the adoption rate is contingent on the smartness level of the AI assistant: the transparency increases the adoption rate only when the AI assistant is not equipped with smart algorithms and fails to do so when the AI assistant is smart. Managerial implications: Our study can guide platforms in designing their AI strategies. Platforms should improve the smartness of AI assistants. If such an improvement is too costly, the platform should transparentize the AI assistant, especially when it is not smart.Funding: This research was supported by a Behavioral Research Assistance Grant from the C. T. Bauer College of Business, University of Houston. H. Zhao acknowledges support from Hong Kong General Research Fund [9043593]. Y. (R.) Tan acknowledges generous support from CEIBS Research [Grant AG24QCS].Supplemental Material: The online appendix is available at .},
journal = {Manufacturing &amp; Service Operations Management},
month = sep,
pages = {1639–1655},
numpages = {17},
keywords = {health intelligence, operational transparency, medical platform, field experiment, generative AI, chatbot}
}

@inproceedings{10.1145/3638380.3638426,
author = {Colley, Ashley and Kalving, Matilda and H\"{a}kkil\"{a}, Jonna and V\"{a}\"{a}n\"{a}nen, Kaisa},
title = {Exploring Tangible Explainable AI (TangXAI): A User Study of Two XAI Approaches},
year = {2024},
isbn = {9798400717079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638380.3638426},
doi = {10.1145/3638380.3638426},
abstract = {Explainable AI (XAI) has garnered significant attention as a theoretical subject in the research community. However, the practical application of XAI, particularly in the realm of user interfaces, remains limited. Moreover, evaluations of these interfaces from the perspective of end-users are scarce. In this paper, we introduce and evaluate two innovative tangible XAI interface concepts. The tangible interfaces capitalize on the widely recognized advantages of data physicalization, offering users a more intuitive and hands-on experience. We implemented two distinct XAI approaches within this tangible framework: feature relevance and local explanations. These approaches were applied to real-world use cases: recommending recipes and selecting jogging routes, respectively. The findings of our Wizard of Oz study indicate that participants had some challenges in distinguishing between the primary objectives of the XAI interface and the typical interactions associated with an AI recommender system. However, tangibility seems to support users’ understanding of AI’s explanations and enables users to reflect on their trust in the AI model.},
booktitle = {Proceedings of the 35th Australian Computer-Human Interaction Conference},
pages = {679–683},
numpages = {5},
keywords = {Explainable AI, HCI, TUI, TangXAI, XAI, human centered AI, tangible interaction},
location = {Wellington, New Zealand},
series = {OzCHI '23}
}

@inproceedings{10.5555/3306127.3332017,
author = {Lage, Isaac and Lifschitz, Daphna and Doshi-Velez, Finale and Amir, Ofra},
title = {Toward Robust Policy Summarization},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {AI agents are being developed to help people with high stakes decision-making processes from driving cars to prescribing drugs. It is therefore becoming increasingly important to develop "explainable AI" methods that help people understand the behavior of such agents. Summaries of agent policies can help human users anticipate agent behavior and facilitate more effective collaboration. Prior work has framed agent summarization as a machine teaching problem where examples of agent behavior are chosen to maximize reconstruction quality under the assumption that people do inverse reinforcement learning to infer an agent's policy from demonstrations. We compare summaries generated under this assumption to summaries generated under the assumption that people use imitation learning. We show through simulations that in some domains, there exist summaries that produce high-quality reconstructions under different models, but in other domains, only matching the summary extraction model to the reconstruction model produces high-quality reconstructions. These results highlight the importance of assuming correct computational models for how humans extrapolate from a summary, suggesting human-in-the-loop approaches to summary extraction.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2081–2083},
numpages = {3},
keywords = {policy summarization, explainable AI},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3627673.3679099,
author = {Xu, Da and Zhang, Danqing and Zheng, Lingling and Yang, Bo and Yang, Guangyu and Xu, Shuyuan and Liang, Cindy},
title = {Tutorial on Landing Generative AI in Industrial Social and E-commerce Recsys},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679099},
doi = {10.1145/3627673.3679099},
abstract = {Over the past two years, GAI has evolved rapidly, influencing various fields including social and e-commerce Recsys. Despite exciting advances, landing these innovations in real-world Recsys remains challenging due to the sophistication of modern industrial product and systems. Our tutorial begins with a brief overview of building industrial Recsys and GAI fundamentals, followed by the ongoing efforts and opportunities to enhance personalized recommendations with foundation models.We then explore the integration of curation capabilities into Recsys, such as repurposing raw content, incorporating external knowledge, and generating personalized insights/explanations to foster transparency and trust. Next, the tutorial illustrates how AI agents can transform Recsys through interactive reasoning and action loops, shifting away from traditional passive feedback models. Finally, we shed insights on real-world solutions for human-AI alignment and responsible GAI practices.A critical component of the tutorial is detailing the AI, Infrastructure, LLMOps, and Product roadmap (including the evaluation and responsible AI practices) derived from the production solutions in LinkedIn, Amazon, TikTok, and Microsoft. While GAI in Recsys is still in its early stages, this tutorial provides valuable insights and practical solutions for the Recsys and GAI communities.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5538–5542},
numpages = {5},
keywords = {generative AI, industrial system, information retrieval, large language model, recommender system},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3419249.3420099,
author = {Alm, Cecilia Ovesdotter and Alvarez, Alberto and Font, Jos\'{e} and Liapis, Antonios and Pederson, Thomas and Salo, Johan},
title = {Invisible AI-driven HCI Systems – When, Why and How},
year = {2020},
isbn = {9781450375795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419249.3420099},
doi = {10.1145/3419249.3420099},
abstract = {The InvisibleAI (InvAI’20) workshop aims to systematically discuss a growing class of interactive systems that invisibly remove some decision-making tasks away from humans to machines, based on recent advances in artificial intelligence (AI), data science, and sensor or actuation technology. While the interest in the affordances as well as the risks of hidden pervasive AI are high on the agenda in public debate, discussion on the topic is needed within the human-computer interaction (HCI) community. In particular, we want to gather insights, ideas, and models for approaching the use of barely noticeable AI decision-making in systems design from a human-centered perspective, so as to make the most out of the automated systems and algorithms that support human activity both as designers and users. Concurrently, these systems should safeguard that humans remain in charge when it counts (high stakes decisions, privacy, monitoring lack of explainability and fairness, etc.). What to automate and what not to automate is often a system designer’s choice &nbsp;[8]. By taking the established concept of explicit interaction between a system and its user as a point of departure, and inviting authors to provide examples from their own research, we aim to stimulate dynamic discussion while keeping the workshop concrete and system design-focused. The workshop especially directs itself to participants from the interaction design, AI, and HCI communities. The targeted scientific outcome of the workshop is an up-to-date ontology of invisible AI-HCI systems and hybrid human-AI collaboration mechanisms, and approaches. Additionally, we expect that the workgroups and the roundtables will provide starting points shaping continued discussions, new collaborations, and innovative scientific contributions that springboard from the workgroups’ findings. The focus of the proposed workshop involves the bridging of two spaces of computational research that impact user experiences and societal domains (HCI and AI). Thus, the proposed workshop topic aligns well with the theme of this year’s NordiCHI conference which is Shaping Experiences, Shaping Society.},
booktitle = {Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society},
articleno = {130},
numpages = {3},
keywords = {human-machine collaboration, artificial intelligence, Human-computer interaction},
location = {Tallinn, Estonia},
series = {NordiCHI '20}
}

@inbook{10.5555/3716662.3716751,
author = {Nannini, Luca},
title = {Habemus a Right to an Explanation: So What? - A Framework on Transparency-Explainability Functionality and Tensions in the EU AI Act},
year = {2025},
publisher = {AAAI Press},
abstract = {The European Union's Artificial Intelligence Act (AI Act), finalized in February 2024, mandates transparency and explainability requirements for AI systems to enable effective oversight and safeguard fundamental rights. Yet the practical implementation of these requirements faces challenges due to tensions between the need for meaningful explanations and their potential risks. This research proposes the Transparency-Explainability Functionality and Tensions (TEFT) framework to analyze the interplay of legal, technical, and socio-ethical factors shaping the realization of algorithmic transparency and explainability in the EU context. Through a two-pronged approach combining a focused literature review and an in-depth examination of the AI Act's provisions, we identify key friction points and challenges in operationalizing the right to explanation. The TEFT framework maps the interests and incentives of various stakeholders, including AI providers &amp; deployers, oversight bodies, and affected individuals, while considering their goals, expected benefits, risks, possible negative impacts, and context to algorithmic explainability.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1023–1035},
numpages = {13}
}

@inproceedings{10.24963/kr.2024/73,
author = {Vasileiou, Stylianos Loukas and Kumar, Ashwin and Yeoh, William and Son, Tran Cao and Toni, Francesca},
title = {Dialectical reconciliation via structured argumentative dialogues},
year = {2024},
isbn = {978-1-956792-05-8},
url = {https://doi.org/10.24963/kr.2024/73},
doi = {10.24963/kr.2024/73},
abstract = {We present a novel framework designed to extend model reconciliation approaches, commonly used in human-aware planning, for enhanced human-AI interaction. By adopting a structured argumentation-based dialogue paradigm, our framework enables dialectical reconciliation to address knowledge discrepancies between an explainer (AI agent) and an explainee (human user), where the goal is for the explainee to understand the explainer's decision. We formally describe the operational semantics of our proposed framework, providing theoretical guarantees. We then evaluate the framework's efficacy "in the wild" via computational and human-subject experiments. Our findings suggest that our framework offers a promising direction for fostering effective human-AI interactions in domains where explainability is important.},
booktitle = {Proceedings of the 21st International Conference on Principles of Knowledge Representation and Reasoning},
articleno = {73},
numpages = {11},
location = {Hanoi, Vietnam},
series = {KR '24}
}

@article{10.1145/3481585,
author = {Kaptein, Frank and Kiefer, Bernd and Cully, Antoine and Celiktutan, Oya and Bierman, Bert and Rijgersberg-peters, Rifca and Broekens, Joost and Van Vught, Willeke and Van Bekkum, Michael and Demiris, Yiannis and Neerincx, Mark A.},
title = {A Cloud-based Robot System for Long-term Interaction: Principles, Implementation, Lessons Learned},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
url = {https://doi.org/10.1145/3481585},
doi = {10.1145/3481585},
abstract = {Making the transition to long-term interaction with social-robot systems has been identified as one of the main challenges in human-robot interaction. This article identifies four design principles to address this challenge and applies them in a real-world implementation: cloud-based robot control, a modular design, one common knowledge base for all applications, and hybrid artificial intelligence for decision making and reasoning. The control architecture for this robot includes a common Knowledge-base (ontologies), Data-base, “Hybrid Artificial Brain” (dialogue manager, action selection and explainable AI), Activities Centre (Timeline, Quiz, Break and Sort, Memory, Tip of the Day,  ( ldots  ) ), Embodied Conversational Agent (ECA, i.e., robot and avatar), and Dashboards (for authoring and monitoring the interaction). Further, the ECA is integrated with an expandable set of (mobile) health applications. The resulting system is a Personal Assistant for a healthy Lifestyle (PAL), which supports diabetic children with self-management and educates them on health-related issues (48 children, aged 6–14, recruited via hospitals in the Netherlands and in Italy). It is capable of autonomous interaction “in the wild” for prolonged periods of time without the need for a “Wizard-of-Oz” (up until 6 months online). PAL is an exemplary system that provides personalised, stable and diverse, long-term human-robot interaction.},
journal = {J. Hum.-Robot Interact.},
month = oct,
articleno = {8},
numpages = {27},
keywords = {pervasive lifestyle support, conversational agents, long-term human-robot interaction, Cloud-based robots}
}

@inproceedings{10.1145/3643491.3660283,
author = {Schmitt, Vera and Csomor, Bal\'{a}zs Patrik and Meyer, Joachim and Villa-Areas, Luis-Felipe and Jakob, Charlott and Polzehl, Tim and M\"{o}ller, Sebastian},
title = {Evaluating Human-Centered AI Explanations: Introduction of an XAI Evaluation Framework for Fact-Checking},
year = {2024},
isbn = {9798400705526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643491.3660283},
doi = {10.1145/3643491.3660283},
abstract = {The rapidly increasing amount of online information and the advent of Generative Artificial Intelligence (GenAI) make the manual verification of information impractical. Consequently, AI systems are deployed to detect disinformation and deepfakes. Prior studies have indicated that combining AI and human capabilities yields enhanced performance in detecting disinformation. Furthermore, the European Union (EU) AI Act mandates human supervision for AI applications in areas impacting essential human rights, like freedom of speech, necessitating that AI systems be transparent and provide adequate explanations to ensure comprehensibility. Extensive research has been conducted on incorporating explainability (XAI) attributes to augment AI transparency, yet these often miss a human-centric assessment. The effectiveness of such explanations also varies with the user’s prior knowledge and personal attributes. Therefore, we developed a framework for validating XAI features for the collaborative human-AI fact-checking task. The framework allows the testing of XAI features with objective and subjective evaluation dimensions and follows human-centric design principles when displaying information about the AI system to the users. The framework was tested in a crowdsourcing experiment with 433 participants, including 406 crowdworkers and 27 journalists for the collaborative disinformation detection task. The tested XAI features increase the AI system’s perceived usefulness, understandability, and trust. With this publication, the XAI evaluation framework is made open source.},
booktitle = {Proceedings of the 3rd ACM International Workshop on Multimedia AI against Disinformation},
pages = {91–100},
numpages = {10},
keywords = {Human-centered eXplanations, blind trust in AI systems, objective and subjective evaluation of eXplanations},
location = {Phuket, Thailand},
series = {MAD '24}
}

@inproceedings{10.5555/3618408.3619140,
author = {Krishna, Satyapriya and Ma, Jiaqi and Lakkaraju, Himabindu},
title = {Towards bridging the gaps between the right to explanation and the right to be forgotten},
year = {2023},
publisher = {JMLR.org},
abstract = {The Right to Explanation and the Right to be Forgotten are two important principles outlined to regulate algorithmic decision making and data usage in real-world applications. While the right to explanation allows individuals to request an actionable explanation for an algorithmic decision, the right to be forgotten grants them the right to ask for their data to be deleted from all the databases and models of an organization. Intuitively, enforcing the right to be forgotten may trigger model updates which in turn invalidate previously provided explanations, thus violating the right to explanation. In this work, we investigate the technical implications arising due to the interference between the two aforementioned regulatory principles, and propose the first algorithmic framework to resolve the tension between them. To this end, we formulate a novel optimization problem to generate explanations that are robust to model updates due to the removal of training data instances by data deletion requests. We then derive an efficient approximation algorithm to handle the combinatorial complexity of this optimization problem. We theoretically demonstrate that our method generates explanations that are provably robust to worst-case data deletion requests with bounded costs in case of linear models and certain classes of non-linear models. Extensive experimentation with real-world datasets demonstrates the efficacy of the proposed framework},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {732},
numpages = {19},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@phdthesis{10.5555/AAI27998733,
author = {Daniels, Zachary Alan and Michmizos, Konstantinos and Moustakides, George and Li, Fuxin},
advisor = {N, Metaxas, Dimitris},
title = {Explanation-Driven Learning-Based Models for Visual Recognition Tasks},
year = {2020},
isbn = {9798557093446},
publisher = {Rutgers The State University of New Jersey, School of Graduate Studies},
abstract = {Safety-critical applications (e.g., autonomous vehicles, human-machine teaming, and automated medical diagnosis) often require the use of computational agents that are capable of understanding and reasoning about the high-level content of real-world scene images in order to make rational and grounded decisions that can be trusted by humans. Many of these agents rely on machine learning-based models which are increasingly being treated as black-boxes. One way to increase model interpretability is to make explainability a core principle of the model, e.g., by forcing deep neural networks to explicitly learn grounded and interpretable features. In this thesis, I provide a high-level overview of the field of explainable/interpretable machine learning and review some existing approaches for interpreting neural networks used for computer vision tasks. I also introduce four novel approaches for making convolutional neural networks (CNNs) more interpretable by utilizing explainability as a guiding principle when designing the model architecture. Finally, I discuss some possible future research directions involving explanation-driven machine learning.},
note = {AAI27998733}
}

@inproceedings{10.1145/3549737.3549808,
author = {Vouros, George},
title = {Tutorial on Explainable Deep Reinforcement Learning: One framework, three paradigms and many challenges.},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549737.3549808},
doi = {10.1145/3549737.3549808},
abstract = {Interpretability, explainability and transparency are key issues to introducing Artificial Intelligence closed—box methods in many critical domains: This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability and fairness, and has important consequences towards keeping the human in the loop in high levels of automation, especially in critical cases for decision making. Reinforcement learning methods, and especially their deep versions, are closed-box methods that support agents to act autonomously in the real world. This tutorial will provide a formal specification of the deep reinforcement learning explainability problems, and will present the necessary components of a general explainable reinforcement learning framework. Based on this framework will provide distinct explainability paradigms towards solving explainability problems, with examples from state-of-the-art methods and real-world cases. The tutorial will conclude identifying open questions and important challenges. The tutorial is based on the survey paper on “Explainable Deep Reinforcement Learning” State of the Art and Challenges” [1].},
booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
articleno = {67},
numpages = {1},
keywords = {Transparency, Interpretability, Explainability, Deep Reinforcement Learning, Deep Learning},
location = {Corfu, Greece},
series = {SETN '22}
}

@inproceedings{10.1007/978-3-030-77772-2_2,
author = {Cirqueira, Douglas and Helfert, Markus and Bezbradica, Marija},
title = {Towards Design Principles for User-Centric Explainable AI in Fraud Detection},
year = {2021},
isbn = {978-3-030-77771-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77772-2_2},
doi = {10.1007/978-3-030-77772-2_2},
abstract = {Experts rely on fraud detection and decision support systems to analyze fraud cases, a growing problem in digital retailing and banking. With the advent of Artificial Intelligence (AI) for decision support, those experts face the black-box problem and lack trust in AI predictions for fraud. Such an issue has been tackled by employing Explainable AI (XAI) to provide experts with explained AI predictions through various explanation methods. However, fraud detection studies supported by XAI lack a user-centric perspective and discussion on how principles are deployed, both important requirements for experts to choose an appropriate explanation method. On the other hand, recent research in Information Systems (IS) and Human-Computer Interaction highlights the need for understanding user requirements to develop tailored design principles for decision support systems. In this research, we adopt a design science research methodology and IS theoretical lens to develop and evaluate design principles, which align fraud expert’s tasks with explanation methods for Explainable AI decision support. We evaluate the utility of these principles using an information quality framework to interview experts in banking fraud, plus a simulation. The results show that the principles are an useful tool for designing decision support systems for fraud detection with embedded user-centric Explainable AI.},
booktitle = {Artificial Intelligence in HCI: Second International Conference, AI-HCI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings},
pages = {21–40},
numpages = {20},
keywords = {Explainable AI, Fraud detection, Decision support systems, Artificial intelligence, Design principles, HCI, Human-AI interaction, Human-centered AI}
}

@inproceedings{10.1145/3328519.3329130,
author = {Thirumuruganathan, Saravanan and Ouzzani, Mourad and Tang, Nan},
title = {Explaining Entity Resolution Predictions: Where are we and What needs to be done?},
year = {2019},
isbn = {9781450367912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328519.3329130},
doi = {10.1145/3328519.3329130},
abstract = {Entity resolution (ER) seeks to identify the set of tuples in a dataset that refer to the same real-world entity. It is one of the fundamental and well studied problems in data integration with applications in diverse domains such as banking, insurance, e-commerce, and so on. Machine Learning and Deep Learning based methods provide the state-of-the-art results. For practitioners, it is often challenging to understand why the classifier made a particular prediction. While there has been extensive work in the ML community on explaining classifier predictions, we found that a direct application of those techniques is not appropriate for ER. There is a huge gap between the needs of lay ER practitioners and the explanation community. In this paper, we provide a comprehensive taxonomy of these challenges, discuss research opportunities and propose preliminary solutions.},
booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
articleno = {10},
numpages = {6},
location = {Amsterdam, Netherlands},
series = {HILDA '19}
}

@article{10.1145/3721283,
author = {Al Ghanmi, Hanouf and Ahmadjee, Sabreen and Bahsoon, Rami},
title = {Evaluating Explanations Needs in Blockchain Smart Contracts to Reconcile Surprises},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3721283},
doi = {10.1145/3721283},
abstract = {Smart contracts on the blockchain play an important role in decentralised systems by automating and executing agreements without the need for intermediaries. As these contracts become integral to various domains, ensuring users’ understanding of their functioning is paramount. This article investigates the need for explanations in smart contracts, drawing inspiration from contract law principles and established practices in Explainable Artificial intelligence (XAI). It introduces key purposes—justification, clarification, compliance and consent to design explainability. Additionally, the study proposes a novel assessment framework informed by the Metacognitive Explanation-Based (MEB) theory to systematically evaluate surprise potential in smart contracts lacking explanations. We use surprise as a guiding factor to systematically identify areas requiring improvement in terms of justification, clarification, compliance and consent. To demonstrate the utility of the assessment approach, we evaluate two decentralised lending projects, uncovering potential surprises. One of the key observations is the lack of setting information, especially concerning compliance, consent and decision justification. This absence of information has heightened the potential for surprises. In the process of validating the explanation purposes, we implement techniques to improve the design of the assessed smart contracts. Further, the research explores the trade-offs involved in integrating explanations, providing nuanced insights into economic implications such as increased deployment and execution costs. This work contributes to the broader comprehension of smart contract explainability requirements and lays out a theoretical foundation for a generic evaluation method. It aims to facilitate the development of more human-centric and comprehensible smart contracts.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {Blockchain, smart contracts, explainability, requirements}
}

@inproceedings{10.1145/3411764.3445385,
author = {Jacobs, Maia and He, Jeffrey and F. Pradier, Melanie and Lam, Barbara and Ahn, Andrew C. and McCoy, Thomas H. and Perlis, Roy H. and Doshi-Velez, Finale and Gajos, Krzysztof Z.},
title = {Designing AI for Trust and Collaboration in Time-Constrained Medical Decisions: A Sociotechnical Lens},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445385},
doi = {10.1145/3411764.3445385},
abstract = {Major depressive disorder is a debilitating disease affecting 264 million people worldwide. While many antidepressant medications are available, few clinical guidelines support choosing among them. Decision support tools (DSTs) embodying machine learning models may help improve the treatment selection process, but often fail in clinical practice due to poor system integration. We use an iterative, co-design process to investigate clinicians’ perceptions of using DSTs in antidepressant treatment decisions. We identify ways in which DSTs need to engage with the healthcare sociotechnical system, including clinical processes, patient preferences, resource constraints, and domain knowledge. Our results suggest that clinical DSTs should be designed as multi-user systems that support patient-provider collaboration and offer on-demand explanations that address discrepancies between predictions and current standards of care. Through this work, we demonstrate how current trends in explainable AI may be inappropriate for clinical environments and consider paths towards designing these tools for real-world medical systems.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {659},
numpages = {14},
keywords = {major depressive disorder, healthcare, decision support tools},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1109/TPAMI.2024.3489597,
author = {Zhang, Rui and Du, Xingbo and Yan, Junchi and Zhang, Shihua},
title = {The Decoupling Concept Bottleneck Model},
year = {2025},
issue_date = {Feb. 2025},
publisher = {IEEE Computer Society},
address = {USA},
volume = {47},
number = {2},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2024.3489597},
doi = {10.1109/TPAMI.2024.3489597},
abstract = {The Concept Bottleneck Model (CBM) is an interpretable neural network that leverages high-level concepts to explain model decisions and conduct human-machine interaction. However, in real-world scenarios, the deficiency of informative concepts can impede the model's interpretability and subsequent interventions. This paper proves that insufficient concept information can lead to an inherent dilemma of concept and label distortions in CBM. To address this challenge, we propose the Decoupling Concept Bottleneck Model (DCBM), which comprises two phases: 1) DCBM for prediction and interpretation, which decouples heterogeneous information into explicit and implicit concepts while maintaining high label and concept accuracy, and 2) DCBM for human-machine interaction, which automatically corrects labels and traces wrong concepts via mutual information estimation. The construction of the interaction system can be formulated as a light min-max optimization problem. Extensive experiments expose the success of alleviating concept/label distortions, especially when concepts are insufficient. In particular, we propose the Concept Contribution Score (CCS) to quantify the interpretability of DCBM. Numerical results demonstrate that CCS can be guaranteed by the Jensen-Shannon divergence constraint in DCBM. Moreover, DCBM expresses two effective human-machine interactions, including forward intervention and backward rectification, to further promote concept/label accuracy via interaction with human experts.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = feb,
pages = {1250–1265},
numpages = {16}
}

@inproceedings{10.1145/3430665.3460419,
author = {Rohlfing, Katharina J.},
title = {Under Co-construction: Toward the Social Design of Explainable AI Systems},
year = {2021},
isbn = {9781450382144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430665.3460419},
doi = {10.1145/3430665.3460419},
abstract = {Technological advancements in machine learning affecting humans' lives on the one hand and also regulatory initiatives fostering transparency in algorithmic decision making on the other hand drive a recent surge of interest in explainable AI (XAI). Explainability is discussed as a solution to sociotechnical challenges such as intelligent software providing incomprehensible decisions or big data enabling fast learning but becoming too complex to fully comprehend and judge its achievements. With explainable AI, more insights into the functions, decisions, and usefulness of algorithms are expected.If an explanation is successful, it results in an understanding. Current XAI research is centering around one-way interaction from which solutions to achieve understanding are derived. In the presentation, I will point to an important resource for achieving understanding that has been overlooked so far: the interaction with the addressee [1].Whereas in current XAI research, the addressee (explainee) is mostly seen as a passive receiver, I argue that the explainee can provide an active and crucial contribution to the process of understanding resulting in the explanation being tailored to a particular form of understanding and thus gaining on relevance. Within the co-constructive view [1], both partners scaffold and monitor each other-perpetuating, thus, the process of explaining as a joint endeavor toward a goal. I argue that such an endeavor can be implemented in explainable and interactive AI.},
booktitle = {Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {1},
numpages = {1},
keywords = {explainability, co-construction and interaction},
location = {Virtual Event, Germany},
series = {ITiCSE '21}
}

@article{10.1145/3492855,
author = {Kou, Ziyi and Shang, Lanyu and Zhang, Yang and Wang, Dong},
title = {HC-COVID: A Hierarchical Crowdsource Knowledge Graph Approach to Explainable COVID-19 Misinformation Detection},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {GROUP},
url = {https://doi.org/10.1145/3492855},
doi = {10.1145/3492855},
abstract = {The proliferation of social media has promoted the spread of misinformation that raises many concerns in our society. This paper focuses on a critical problem of explainable COVID-19 misinformation detection that aims to accurately identify and explain misleading COVID-19 claims on social media. Motivated by the lack of COVID-19 relevant knowledge in existing solutions, we construct a novel crowdsource knowledge graph based approach to incorporate the COVID-19 knowledge facts by leveraging the collaborative efforts of expert and non-expert crowd workers. Two important challenges exist in developing our solution: i) how to effectively coordinate the crowd efforts from both expert and non-expert workers to generate the relevant knowledge facts for detecting COVID-19 misinformation; ii) How to leverage the knowledge facts from the constructed knowledge graph to accurately explain the detected COVID-19 misinformation. To address the above challenges, we develop HC-COVID, a hierarchical crowdsource knowledge graph based framework that explicitly models the COVID-19 knowledge facts contributed by crowd workers with different levels of expertise and accurately identifies the related knowledge facts to explain the detection results. We evaluate HC-COVID using two public real-world datasets on social media. Evaluation results demonstrate that HC-COVID significantly outperforms state-of-the-art baselines in terms of the detection accuracy of misleading COVID-19 claims and the quality of the explanations.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {36},
numpages = {25},
keywords = {human-ai collaboration, explainable misinformation detection, covid19}
}

@article{10.1145/3711000,
author = {Kapania, Shivani and Wang, Ruiyi and Li, Toby Jia-Jun and Li, Tianshi and Shen, Hong},
title = { 'I'm Categorizing LLM as a Productivity Tool': Examining Ethics of LLM Use in HCI Research Practices},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3711000},
doi = {10.1145/3711000},
abstract = {Large language models are increasingly applied in real-world scenarios, including research and education. These models, however, come with well-known ethical issues, which may manifest in unexpected ways in human-computer interaction research due to the extensive engagement with human subjects. This paper reports on research practices related to LLM use, drawing on 16 semi-structured interviews and a survey with 50 HCI researchers. We discuss the ways in which LLMs are already being utilized throughout the entire HCI research pipeline, from ideation to system development and paper writing. While researchers described nuanced understandings of ethical issues, they were rarely or only partially able to identify and address those ethical concerns in their own projects. This lack of action and reliance on workarounds was explained through the perceived lack of control and distributed responsibility in the LLM supply chain, the conditional nature of engaging with ethics, and competing priorities. Finally, we reflect on the implications of our findings and present opportunities to shape emerging norms of engaging with large language models in HCI research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW102},
numpages = {26},
keywords = {hci research, large language models, research ethics, research practices}
}

@inproceedings{10.1007/978-3-031-70396-6_13,
author = {Maldonado, Andrea and Frey, Christian M. M. and Tavares, Gabriel Marques and Rehwald, Nikolina and Seidl, Thomas},
title = {GEDI: Generating Event Data with&nbsp;Intentional Features for&nbsp;Benchmarking Process Mining},
year = {2024},
isbn = {978-3-031-70395-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-70396-6_13},
doi = {10.1007/978-3-031-70396-6_13},
abstract = {Process mining solutions include enhancing performance, conserving resources, and alleviating bottlenecks in organizational contexts. However, as in other data mining fields, success hinges on data quality and availability. Existing analyses for process mining solutions lack diverse and ample data for rigorous testing, hindering insights’ generalization. To address this, we propose Generating Event Data with Intentional features, a framework producing event data sets satisfying specific meta-features. Considering the meta-feature space that defines feasible event logs, we observe that existing real-world datasets describe only local areas within the overall space. Hence, our framework aims at providing the capability to generate an event data benchmark, which covers unexplored regions. Therefore, our approach leverages a discretization of the meta-feature space to steer generated data towards regions, where a combination of meta-features is not met yet by existing benchmark datasets. Providing a comprehensive data pool enriches process mining analyses, enables methods to capture a wider range of real-world scenarios, and improves evaluation quality. Moreover, it empowers analysts to uncover correlations between meta-features and evaluation metrics, enhancing explainability and solution effectiveness. Experiments demonstrate GEDI’s ability to produce a benchmark of intentional event data sets and robust analyses for process mining tasks.},
booktitle = {Business Process Management: 22nd International Conference, BPM 2024, Krakow, Poland, September 1–6, 2024, Proceedings},
pages = {221–237},
numpages = {17},
keywords = {Data Generation, Benchmarking, Event Log Features, Hyperparameter Optimization},
location = {Krakow, Poland}
}

@inproceedings{10.1145/3600211.3604729,
author = {Nannini, Luca},
title = {Explainability in Process Mining: A Framework for Improved Decision-Making},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604729},
doi = {10.1145/3600211.3604729},
abstract = {This research project aims to develop and validate explanatory facilities to enhance information reception of process mining solutions, which could inform and be translated to other business intelligence platforms. Process mining, a nascent field for analyzing event data stored in information systems, faces challenges in adoption, engagement, and comprehensive explainability frameworks. The research problem lies in the difficulties organizations face when understanding the return on investment and integration requirements associated with process mining operationalization. Furthermore, users often struggle to comprehend the elaboration and representation of process outputs. This issue is compounded by the limited application of Explainable AI (XAI) in process mining, which so far has been predominantly focused on prediction and monitoring activities without a holistic view of explainability trade-offs.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {975–976},
numpages = {2},
keywords = {AI Ethics, AI policy, Explainable AI, Human-Computer Interaction, Responsible Process Mining},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@inproceedings{10.1145/3643834.3660707,
author = {Zaj\k{a}c, Hubert Dariusz and Ribeiro, Jorge Miguel Neves and Ingala, Silvia and Gentile, Simona and Wanjohi, Ruth and Gitau, Samuel Nguku and Carlsen, Jonathan Frederik and Nielsen, Michael Bachmann and Andersen, Tariq Osman},
title = {"It depends": Configuring AI to Improve Clinical Usefulness Across Contexts},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3660707},
doi = {10.1145/3643834.3660707},
abstract = {Artificial Intelligence (AI) repeatedly match or outperform radiologists in lab experiments. However, real-world implementations of radiological AI-based systems are found to provide little to no clinical value. This paper explores how to design AI for clinical usefulness in different contexts. We conducted 19 design sessions and design interventions with 13 radiologists from 7 clinical sites in Denmark and Kenya, based on three iterations of a functional AI-based prototype. Ten sociotechnical dependencies were identified as crucial for the design of AI in radiology. We conceptualised four technical dimensions that must be configured to the intended clinical context of use: AI functionality, AI medical focus, AI decision threshold, and AI Explainability. We present four design recommendations on how to address dependencies pertaining to the medical knowledge, clinic type, user expertise level, patient context, and user situation that condition the configuration of these technical dimensions.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {874–889},
numpages = {16},
keywords = {AI Interaction, Human-Centred AI, Machine Learning, Performance Optimisation, Transferability, Usability, User-Centred Design},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3411763.3441347,
author = {Osman Andersen, Tariq and Nunes, Francisco and Wilcox, Lauren and Kaziunas, Elizabeth and Matthiesen, Stina and Magrabi, Farah},
title = {Realizing AI in Healthcare: Challenges Appearing in the Wild},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3441347},
doi = {10.1145/3411763.3441347},
abstract = {The last several years have shown a strong growth of Artificial Intelligence (AI) technologies with promising results for many areas of healthcare. HCI has contributed to these discussions, mainly with studies on explainability of advanced algorithms. However, there are only few AI-systems based on machine learning algorithms that make it to the real world and everyday care. This challenging move has been named the “last mile” of AI in healthcare, emphasizing the sociotechnical uncertainties and unforeseen learnings from involving users in the design or use of AI-based systems. The aim of this workshop is to set the stage for a new wave of HCI research that accounts for and begins to develop new insights, concepts, and methods, for transitioning from development to implementation and use of AI in healthcare. Participants are invited to collaboratively define an HCI research agenda focused on healthcare AI in the wild, which will require examining end-user engagements and questioning underlying concepts of AI in healthcare.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {108},
numpages = {5},
keywords = {artificial intelligence, human computer interaction},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@article{10.1016/j.aei.2024.102404,
author = {Zhang, Zhongfei and Qu, Ting and Zhao, Kuo and Zhang, Kai and Zhang, Yongheng and Guo, Wenyou and Liu, Lei and Chen, Zefeng},
title = {Enhancing trusted synchronization in open production logistics: A platform framework integrating blockchain and digital twin under social manufacturing},
year = {2024},
issue_date = {Aug 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {C},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2024.102404},
doi = {10.1016/j.aei.2024.102404},
journal = {Adv. Eng. Inform.},
month = aug,
numpages = {24},
keywords = {Social manufacturing, Blockchain, Digital twin, Production logistics, Trusted synchronization}
}

@article{10.1016/j.eswa.2022.117230,
author = {Henckaerts, Roel and Antonio, Katrien and C\^{o}t\'{e}, Marie-Pier},
title = {When stakes are high: Balancing accuracy and transparency with Model-Agnostic Interpretable Data-driven suRRogates},
year = {2022},
issue_date = {Sep 2022},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {202},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2022.117230},
doi = {10.1016/j.eswa.2022.117230},
journal = {Expert Syst. Appl.},
month = sep,
numpages = {13},
keywords = {XAI, Segmentation, Insurance, Global surrogate, GLM, Feature selection}
}

@inproceedings{10.1145/3411763.3441338,
author = {Omeiza, Daniel and Anjomshoae, Sule and Kollnig, Konrad and Camburu, Oana-Maria and Fr\"{a}mling, Kary and Kunze, Lars},
title = {Towards Explainable and Trustworthy Autonomous Physical Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3441338},
doi = {10.1145/3411763.3441338},
abstract = {The safe deployment of autonomous physical systems in real-world scenarios requires them to be explainable and trustworthy, especially in critical domains. In contrast with ‘black-box’ systems, explainable and trustworthy autonomous physical systems will lend themselves to easy assessments by system designers and regulators. This promises to pave ways for easy improvements that can lead to enhanced performance, and as well, increased public trust. In this one-day virtual workshop, we aim to gather a globally distributed group of researchers and practitioners to discuss the opportunities and social challenges in the design, implementation, and deployment of explainable and trustworthy autonomous physical systems, especially in a post-pandemic era. Interactions will be fostered through panel discussions and a series of spotlight talks. To ensure lasting impact of the workshop, we will conduct a pre-workshop survey which will examine the public perception of the trustworthiness of autonomous physical systems. Further, we will publish a summary report providing details about the survey as well as the identified challenges resulting from the workshop’s panel discussions.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {88},
numpages = {3},
keywords = {Explainability, collaboration, human-machine interaction, trust},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.24963/kr.2023/57,
author = {Rago, Antonio and Li, Hengzhi and Toni, Francesca},
title = {Interactive explanations by conflict resolution via argumentative exchanges},
year = {2023},
isbn = {978-1-956792-02-7},
url = {https://doi.org/10.24963/kr.2023/57},
doi = {10.24963/kr.2023/57},
abstract = {As the field of explainable AI (XAI) is maturing, calls for interactive explanations for (the outputs of) AI models are growing, but the state-of-the-art predominantly focuses on static explanations. In this paper, we focus instead on interactive explanations framed as conflict resolution between agents (i.e. AI models and/or humans) by leveraging on computational argumentation. Specifically, we define Argumentative eXchanges (AXs) for dynamically sharing, in multiagent systems, information harboured in individual agents' quantitative bipolar argumentation frameworks towards resolving conflicts amongst the agents. We then deploy AXs in the XAI setting in which a machine and a human interact about the machine's predictions. We identify and assess several theoretical properties characterising AXs that are suitable for XAI. Finally, we instantiate AXs for XAI by defining various agent behaviours, e.g. capturing counterfactual patterns of reasoning in machines and highlighting the effects of cognitive biases in humans. We show experimentally (in a simulated environment) the comparative advantages of these behaviours in terms of conflict resolution, and show that the strongest argument may not always be the most effective.},
booktitle = {Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning},
articleno = {57},
numpages = {11},
location = {Rhodes, Greece},
series = {KR '23}
}

@inproceedings{10.1145/3375627.3375866,
author = {Smart, Andrew and James, Larry and Hutchinson, Ben and Wu, Simone and Vallor, Shannon},
title = {Why Reliabilism Is not Enough: Epistemic and Moral Justification in Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375866},
doi = {10.1145/3375627.3375866},
abstract = {In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of em justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed wide adoption of machine learning? We argue that, in general, people implicitly adoptreliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method citegoldman2012reliabilism. We argue that, in cases where model deployments require em moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral "wrapper'' around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification---moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {372–377},
numpages = {6},
keywords = {epistemology, explainability, interpretability, machine learning, moral justification, neural networks},
location = {New York, NY, USA},
series = {AIES '20}
}

@article{10.1016/j.compeleceng.2024.110006,
author = {Bai, Hao and Gao, Jian-Hong and Liu, Tong and Guo, Zi-Yi and Guo, Mou-Fa},
title = {Explainable incremental learning for high-impedance fault detection in distribution networks},
year = {2025},
issue_date = {Mar 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2024.110006},
doi = {10.1016/j.compeleceng.2024.110006},
journal = {Comput. Electr. Eng.},
month = apr,
numpages = {17},
keywords = {Distribution network, High impedance fault, Discrete wavelet transform, Incremental learning, Model explainability, Backpropagation neural network}
}

@inproceedings{10.1007/978-3-031-73195-2_10,
author = {Jiang, Hai and Luo, Ao and Liu, Xiaohong and Han, Songchen and Liu, Shuaicheng},
title = {LightenDiffusion: Unsupervised Low-Light Image Enhancement with&nbsp;Latent-Retinex Diffusion Models},
year = {2024},
isbn = {978-3-031-73194-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-73195-2_10},
doi = {10.1007/978-3-031-73195-2_10},
abstract = {In this paper, we propose a diffusion-based unsupervised framework that incorporates physically explainable Retinex theory with diffusion models for low-light image enhancement, named LightenDiffusion. Specifically, we present a content-transfer decomposition network that performs Retinex decomposition within the latent space instead of image space as in previous approaches, enabling the encoded features of unpaired low-light and normal-light images to be decomposed into content-rich reflectance maps and content-free illumination maps. Subsequently, the reflectance map of the low-light image and the illumination map of the normal-light image are taken as input to the diffusion model for unsupervised restoration with the guidance of the low-light feature, where a self-constrained consistency loss is further proposed to eliminate the interference of normal-light content on the restored results to improve overall visual quality. Extensive experiments on publicly available real-world benchmarks show that the proposed LightenDiffusion outperforms state-of-the-art unsupervised competitors and is comparable to supervised methods while being more generalizable to various scenes. Our code is available at .},
booktitle = {Computer Vision – ECCV 2024: 18th European Conference, Milan, Italy, September 29–October 4, 2024, Proceedings, Part XLVIII},
pages = {161–179},
numpages = {19},
keywords = {Image restoration, Low-light image enhancement, Diffusion models, Retinex theory},
location = {Milan, Italy}
}

@inproceedings{10.1109/ProComm52174.2021.00005,
author = {Duin, Ann Hill and Pedersen, Isabel},
title = {Working Alongside Non-Human Agents},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ProComm52174.2021.00005},
doi = {10.1109/ProComm52174.2021.00005},
abstract = {We coexist with non-human AI agents, and we now must plan for human and non-human-agent teaming, for cooperation and collaboration, as a means to expand collaborative intelligence in our ongoing quest for user advocacy. For practice and experimentation, we provide links to current non-human agents. We then distinguish automation and autonomy, and discuss humanness design, teaming. A deeper understanding of usability and ethical considerations for working alongside these systems, deploying robots and building bonds and trust with nonhuman agents, begins with differentiation of automation and autonomy, human-autonomy teaming, and a humanness design approach as a means to prevent undesirable autonomy. While TPC scholarship attends to privacy, accountability, safety and security, and transparency and explainability, we need additional vigilance regarding fairness and non-discrimination, human control of technology, TPC professional responsibility, and continued promotion of human values as we work alongside non-human agents.},
booktitle = {2021 IEEE International Professional Communication Conference (ProComm)},
pages = {1–5},
numpages = {5},
location = {Pittsburgh, PA, USA}
}

@article{10.1016/j.ijhcs.2019.05.006,
author = {Janssen, Christian P. and Donker, Stella F. and Brumby, Duncan P. and Kun, Andrew L.},
title = {History and future of human-automation interaction},
year = {2019},
issue_date = {Nov 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {1071-5819},
url = {https://doi.org/10.1016/j.ijhcs.2019.05.006},
doi = {10.1016/j.ijhcs.2019.05.006},
journal = {Int. J. Hum.-Comput. Stud.},
month = nov,
pages = {99–107},
numpages = {9},
keywords = {Robotics, Automated vehicles, Ethics, Divided attention, Situated systems, Embodied systems, Autonomous agents, Safety-critical systems, Human-automation interaction, Automation}
}

@inproceedings{10.1007/978-3-031-76824-8_5,
author = {Gu, Yaoqin and Sheng, Youyu and Duan, Yujia and Zhang, Jingyu},
title = {How Do Different HMIs of Autonomous Driving Monitoring Systems Influence the Perceived Safety of Robotaxis Passengers? a Field Study},
year = {2024},
isbn = {978-3-031-76823-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-76824-8_5},
doi = {10.1007/978-3-031-76824-8_5},
abstract = {Despite their significant potential to transform urban mobility, passengers still harbor safety concerns regarding robotaxis. Effective Human-Machine Interfaces (HMIs) may enhance users’ safety perceptions of these systems. Yet, strategies to improve such perceptions are not well-explored. To elucidate how system differences influence passenger perceptions, we conducted a field study involving a comparative analysis of two commercially operated robotaxi systems, System A and System B. Subsequently, we gathered data from passengers of both systems to assess their perceptions of safety. The results indicated that under normal operating conditions, passengers using both systems reported similar levels of safety. However, in the event of incidents, perceived safety significantly declined among passengers who used System B, whereas it remained stable for passengers using System A which provides more explanation on the nature of the incidents. These findings offer crucial insights for operators and provide a valuable resource for future transportation research.},
booktitle = {HCI International 2024 – Late Breaking Papers: 26th International Conference on Human-Computer Interaction, HCII 2024, Washington, DC, USA, June 29 – July 4, 2024, Proceedings, Part VIII},
pages = {51–60},
numpages = {10},
keywords = {Robotaxis, HMIs, Autonomous driving, Perceived safety},
location = {Washington DC, USA}
}

@article{10.1016/j.chb.2017.03.019,
author = {Kefalidou, Genovefa},
title = {When immediate interactive feedback boosts optimization problem solving},
year = {2017},
issue_date = {August 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {73},
number = {C},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2017.03.019},
doi = {10.1016/j.chb.2017.03.019},
abstract = {In past, feedback in problem solving was found to improve human performance and focused mainly on learning applications. Interactive tools supporting decision-making and general problem-solving processes have long being developed to assist operations but not in optimization problem solving. Optimization problem solving is currently addressed within Operational Research (OR) through computational algorithms that aim to find the best solution in a problem (e.g. routing problem). Limited investigation there is on how computerized interactivity and metacognitive support (e.g. feedback and planning) can support optimization problem solving. This paper reports on human performance on Capacitated Vehicle Routing Problems (CVRPs) using paper-based problems and two different versions of an interactive computerized tool (one version with live explanatory and directive feedback alongside planning (strategy) support; one version without strategy support but with live explanatory feedback). Results suggest that human performance did not change when people were given paper-based post-problem feedback. On the contrary, participants' performance improved significantly when they used either version of the interactive tool that facilitated both live feedback support. No differences in performance across the two versions were observed. Implications on current theories and design implications for future optimization systems are discussed. Participants solved paper-based and computerized interactive CVRPs.Paper-based post-problem feedback did not affect human performance.Concurrent interactive feedback boosts human optimization performance.Concurrent explanatory feedback improves performance quality but reduces speed.Computerized metacognitive support improves human performance on CVRPs.},
journal = {Comput. Hum. Behav.},
month = aug,
pages = {110–124},
numpages = {15},
keywords = {Metacognition, Interactive route optimization, Human-in-the-loop, Human performance, Concurrent feedback}
}

@article{10.14778/3611479.3611522,
author = {Wu, Siyuan and U, Leong Hou and Karras, Panagiotis},
title = {k-Best Egalitarian Stable Marriages for Task Assignment},
year = {2023},
issue_date = {July 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611479.3611522},
doi = {10.14778/3611479.3611522},
abstract = {In a two-sided market with each agent ranking individuals on the other side according to their preferences, such as location or incentive, the stable marriage problem calls to find a perfect matching among the two sides such that no pair of agents prefers each other to their assigned matches. Recent studies show that the number of solutions can be large in practice. Yet the classic solution by the Gale-Shapley (GS) algorithm is optimal for agents on the one side and pessimal for those on the other side. Some algorithms find a stable marriage that optimizes a measure of the cumulative satisfaction of all agents, such as egalitarian cost. However, in many real-world circumstances, a decision-maker needs to examine a set of solutions that are stable and attentive to both sides and choose among them based on expert knowledge. With such a disposition, it is necessary to identify a set of high-quality stable marriages and provide transparent explanations for any reassigned matches to the decision-maker. In this paper, we provide efficient algorithms that find the k-best stable marriages by egalitarian cost. Our exhaustive experimental study using real-world data and realistic preferences demonstrates the efficacy and efficiency of our solution.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3240–3252},
numpages = {13}
}

@article{10.1016/j.ijhcs.2024.103380,
author = {Schmude, Timoth\'{e}e and Koesten, Laura and M\"{o}ller, Torsten and Tschiatschek, Sebastian},
title = {Information that matters: Exploring information needs of people affected by algorithmic decisions},
year = {2025},
issue_date = {Jan 2025},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {193},
number = {C},
issn = {1071-5819},
url = {https://doi.org/10.1016/j.ijhcs.2024.103380},
doi = {10.1016/j.ijhcs.2024.103380},
journal = {Int. J. Hum.-Comput. Stud.},
month = jan,
numpages = {18},
keywords = {Explainable AI, Understanding, Information needs, Affected stakeholders, Question-driven explanations, Qualitative methods}
}

@inproceedings{10.5555/3666122.3669430,
author = {Schwettmann, Sarah and Shaham, Tamar Rott and Materzynska, Joanna and Chowdhury, Neil and Li, Shuang and Andreas, Jacob and Bau, David and Torralba, Antonio},
title = {FIND: a function description benchmark for evaluating interpretability methods},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions are procedurally constructed across textual and numeric domains, and involve a range of real-world complexities, including noise, composition, approximation, and bias. We evaluate methods that use pretrained language models (LMs) to produce code-based and natural language descriptions of function behavior. Additionally, we introduce a new interactive method in which an Automated Interpretability Agent (AIA) generates function descriptions. We find that an AIA, built with an off-the-shelf LM augmented with black-box access to functions, can sometimes infer function structure—acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, FIND also reveals that LM-based descriptions capture global function behavior while missing local details. These results suggest that FIND will be useful for characterizing the performance of more sophisticated interpretability methods before they are applied to real-world models.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {3308},
numpages = {28},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{10.1145/3637528.3671482,
author = {Zhao, Chen and Chen, Feng and Wu, Xintao and Li, Jundong and Chen, Haifeng},
title = {3rd Workshop on Ethical Artificial Intelligence: Methods and Applications (EAI)},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671482},
doi = {10.1145/3637528.3671482},
abstract = {Ethical AI has become increasingly important, and it has been attracting attention from academia and industry, due to its increased popularity in real-world applications with fairness concerns. It also places fundamental importance on ethical considerations in determining legitimate and illegitimate uses of AI. Organizations that apply ethical AI have clearly stated well-defined review processes to ensure adherence to legal guidelines. Therefore, the wave of research at the intersection of ethical AI in data mining and machine learning has also influenced other fields of science, including computer vision, natural language processing, reinforcement learning, and social science. Despite these successes, ethical AI still faces many challenges, such as a lack of interpretable and explainable methods for fairness-aware deep learning models, etc. Consequently, there is an urgent need to bring experts and researchers together at prestigious venues to discuss ethical AI, which has been rarely seen in previous KDD conferences. This workshop will provide a premium platform for both research and industry from different backgrounds to exchange ideas on opportunities, challenges, and cutting-edge techniques in ethical AI.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6751–6752},
numpages = {2},
keywords = {ethical artificial intelligence, fairness-aware, machine learning},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3640543.3645164,
author = {Tilekbay, Bekzat and Yang, Saelyne and Lewkowicz, Michal Adam and Suryapranata, Alex and Kim, Juho},
title = {ExpressEdit: Video Editing with Natural Language and Sketching},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645164},
doi = {10.1145/3640543.3645164},
abstract = {Informational videos serve as a crucial source for explaining conceptual and procedural knowledge to novices and experts alike. When producing informational videos, editors edit videos by overlaying text/images or trimming footage to enhance the video quality and make it more engaging. However, video editing can be difficult and time-consuming, especially for novice video editors who often struggle with expressing and implementing their editing ideas. To address this challenge, we first explored how multimodality—natural language (NL) and sketching, which are natural modalities humans use for expression—can be utilized to support video editors in expressing video editing ideas. We gathered 176 multimodal expressions of editing commands from 10 video editors, which revealed the patterns of use of NL and sketching in describing edit intents. Based on the findings, we present ExpressEdit, a system that enables editing videos via NL text and sketching on the video frame. Powered by LLM and vision models, the system interprets (1) temporal, (2) spatial, and (3) operational references in an NL command and spatial references from sketching. The system implements the interpreted edits, which then the user can iterate on. An observational study (N=10) showed that ExpressEdit enhanced the ability of novice video editors to express and implement their edit ideas. The system allowed participants to perform edits more efficiently and generate more ideas by generating edits based on user’s multimodal edit commands and supporting iterations on the editing commands. This work offers insights into the design of future multimodal interfaces and AI-based pipelines for video editing.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {515–536},
numpages = {22},
keywords = {human-AI interaction, multimodal input, video editing},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{10.1145/3539597.3572332,
author = {de Rijke, Maarten},
title = {Beyond-Accuracy Goals, Again},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3572332},
doi = {10.1145/3539597.3572332},
abstract = {Improving the performance of information retrieval systems tends to be narrowly scoped. Often, better prediction performance is considered the only metric of improvement. As a result, work on improving information retrieval methods usually focuses on im- proving the methods' accuracy. Such a focus is myopic. Instead, as researchers and practitioners we should adopt a richer perspective measuring the performance of information retrieval systems. I am not the first to make this point (see, e.g., [4]), but I want to highlight dimensions that broaden the scope considered so far and offer a number of examples to illustrate what this would mean for our research agendas.First, trustworthiness is a prerequisite for people, organizations, and societies to use AI-based, and, especially, machine learning- based systems in general, and information retrieval systems in particular. Trust can be gained in an intrinsic manner by revealing the inner workings of an AI-based system, i.e., through explainability. Or it can be gained extrinsically by showing, in a principled or empirical manner, that a system upholds verifiable guarantees. Such guarantees should obtained for the following dimensions (at a minimum): (i) accuracy, including well-defined and explained contexts of usage; (ii) reliability, including exhibiting parity with respect to sensitive attributes; (iii) repeatable and reproducible results, including audit trails; (iv) resilience to adversarial examples, distributional shifts; and (v) safety, including privacy-preserving search and recommendation.Second, in information retrieval, our experiments are mostly conducted in controlled laboratory environments. Extrapolating this information to evaluate the real-world effects often remains a challenge. This is particularly true when measuring the impact of information retrieval systems across broader scales, both temporally and spatially. Conducting controlled experimental trials for evaluating real-world impacts of information retrieval systems can result in depicting a snapshot situation, where systems are tailored towards that specific environment. As society is constantly changing, the requirements set for information retrieval systems are changing as well, resulting in short-term and long-term feedback loops with interactions between society and information retrieval systems.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {2–3},
numpages = {2},
keywords = {evaluation, measurement, performance},
location = {Singapore, Singapore},
series = {WSDM '23}
}

@inproceedings{10.1145/3643834.3661559,
author = {Lee, Christine P and Praveena, Pragathi and Mutlu, Bilge},
title = {REX: Designing User-centered Repair and Explanations to Address Robot Failures},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661559},
doi = {10.1145/3643834.3661559},
abstract = {Robots in real-world environments continuously engage with multiple users and encounter changes that lead to unexpected conflicts in fulfilling user requests. Recent technical advancements (e.g., large-language models (LLMs), program synthesis) offer various methods for automatically generating repair plans that address such conflicts. In this work, we understand how automated repair and explanations can be designed to improve user experience with robot failures through two user studies. In our first, online study (n = 162), users expressed increased trust, satisfaction, and utility with the robot performing automated repair and explanations. However, we also identified risk factors—safety, privacy, and complexity—that require adaptive repair strategies. The second, in-person study (n = 24) elucidated distinct repair and explanation strategies depending on the level of risk severity and type. Using a design-based approach, we explore automated repair with explanations as a solution for robots to handle conflicts and failures, complemented by adaptive strategies for risk factors. Finally, we discuss the implications of incorporating such strategies into robot designs to achieve seamless operation among changing user needs and environments.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {2911–2925},
numpages = {15},
keywords = {failures, human-robot interaction, program repair, robot, user-centered design, vignette study},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3473856.3473886,
author = {Faulhaber, Anja K. and Ni, Ina and Schmidt, Ludger},
title = {The Effect of Explanations on Trust in an Assistance System for Public Transport Users and the Role of the Propensity to Trust},
year = {2021},
isbn = {9781450386456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473856.3473886},
doi = {10.1145/3473856.3473886},
abstract = {The present study aimed to investigate whether explanations increase trust in an assistance system. Moreover, we wanted to take the role of the individual propensity to trust in technology into account. We conducted an empirical study in a virtual reality environment where 40 participants interacted with a specific assistance system for public transport users. The study was in a 2x2 mixed design with the within-subject factor assistance system feature (trip planner and connection request) and the between-subject factor explanation (with or without). We measured trust as explicit trust via a questionnaire and as implicit trust via an operationalization of the participants’ behavior. The results showed that trust propensity predicted explicit trust, and explanations increased explicit trust significantly. This was not the case for implicit trust, though, suggesting that explicit and implicit trust do not necessarily coincide. In conclusion, our results complement the literature on explainable artificial intelligence and trust in automation and provide topics for future research regarding the effect of explanations on trust in assistance systems or other technologies.},
booktitle = {Proceedings of Mensch Und Computer 2021},
pages = {303–310},
numpages = {8},
location = {Ingolstadt, Germany},
series = {MuC '21}
}

@article{10.1007/s00146-023-01633-0,
author = {Wulff, Kristin and Finnestrand, Hanne},
title = {Creating meaningful work in the age of AI: explainable AI, explainability, and why it matters to organizational designers},
year = {2023},
issue_date = {Aug 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {39},
number = {4},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-023-01633-0},
doi = {10.1007/s00146-023-01633-0},
abstract = {In this paper, we contribute to research on enterprise artificial intelligence (AI), specifically to organizations improving the customer experiences and their internal processes through using the type of AI called machine learning (ML). Many organizations are struggling to get enough value from their AI efforts, and part of this is related to the area of explainability. The need for explainability is especially high in what is called black-box ML models, where decisions are made without anyone understanding how an AI reached a particular decision. This opaqueness creates a user need for explanations. Therefore, researchers and designers create different versions of so-called eXplainable AI (XAI). However, the demands for XAI can reduce the accuracy of the predictions the AI makes, which can reduce the perceived usefulness of the AI solution, which, in turn, reduces the interest in designing the organizational task structure to benefit from the AI solution. Therefore, it is important to ensure that the need for XAI is as low as possible. In this paper, we demonstrate how to achieve this by optimizing the task structure according to sociotechnical systems design principles. Our theoretical contribution is to the underexplored field of the intersection of AI design and organizational design. We find that explainability goals can be divided into two groups, pattern goals and experience goals, and that this division is helpful when defining the design process and the task structure that the AI solution will be used in. Our practical contribution is for AI designers who include organizational designers in their teams, and for organizational designers who answer that challenge.},
journal = {AI Soc.},
month = jan,
pages = {1843–1856},
numpages = {14},
keywords = {Artificial intelligence, Machine learning, Explainability, Sociotechnical systems (STS), Organizational design}
}

@article{10.1007/s10994-023-06501-y,
author = {Mignone, Paolo and Corizzo, Roberto and Ceci, Michelangelo},
title = {Distributed and explainable GHSOM for anomaly detection in sensor networks},
year = {2024},
issue_date = {Jul 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {113},
number = {7},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-023-06501-y},
doi = {10.1007/s10994-023-06501-y},
abstract = {The identification of anomalous activities is a challenging and crucially important task in sensor networks. This task is becoming increasingly complex with the increasing volume of data generated in real-world domains, and greatly benefits from the use of predictive models to identify anomalies in real time. A key use case for this task is the identification of misbehavior that may be caused by involuntary faults or deliberate actions. However, currently adopted anomaly detection methods are often affected by limitations such as the inability to analyze large-scale data, a reduced effectiveness when data presents multiple densities, a strong dependence on user-defined threshold configurations, and a lack of explainability in the extracted predictions. In this paper, we propose a distributed deep learning method that extends growing hierarchical self-organizing maps, originally designed for clustering tasks, to address anomaly detection tasks. The SOM-based modeling capabilities of the method enable the analysis of data with multiple densities, by exploiting multiple SOMs organized as a hierarchy. Our map-reduce implementation under Apache Spark allows the method to process and analyze large-scale sensor network data. An automatic threshold-tuning strategy reduces user efforts and increases the robustness of the method with respect to noisy instances. Moreover, an explainability component resorting to instance-based feature ranking emphasizes the most salient features influencing the decisions of the anomaly detection model, supporting users in their understanding of raised alerts. Experiments are conducted on five real-world sensor network datasets, including wind and photovoltaic energy production, vehicular traffic, and pedestrian flows. Our results show that the proposed method outperforms state-of-the-art anomaly detection competitors. Furthermore, a scalability analysis reveals that the method is able to scale linearly as the data volume presented increases, leveraging multiple worker nodes in a distributed computing setting. Qualitative analyses on the level of anomalous pollen in the air further emphasize the effectiveness of our proposed method, and its potential in determining the level of danger in raised alerts.},
journal = {Mach. Learn.},
month = jan,
pages = {4445–4486},
numpages = {42},
keywords = {Anomaly detection, Self-organizing maps, Distributed learning, Sensor networks, Explainable AI}
}

@inproceedings{10.1007/978-3-031-19818-2_41,
author = {Chen, Honglin and Venkatesh, Rahul and Friedman, Yoni and Wu, Jiajun and Tenenbaum, Joshua B. and Yamins, Daniel L. K. and Bear, Daniel M.},
title = {Unsupervised Segmentation in&nbsp;Real-World Images via&nbsp;Spelke Object Inference},
year = {2022},
isbn = {978-3-031-19817-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-19818-2_41},
doi = {10.1007/978-3-031-19818-2_41},
abstract = {Self-supervised, category-agnostic segmentation of real-world images is a challenging open problem in computer vision. Here, we show how to learn static grouping priors from motion self-supervision by building on the cognitive science concept of a Spelke Object: a set of physical stuff that moves together. We introduce the Excitatory-Inhibitory Segment Extraction Network (EISEN), which learns to extract pairwise affinity graphs for static scenes from motion-based training signals. EISEN then produces segments from affinities using a novel graph propagation and competition network. During training, objects that undergo correlated motion (such as robot arms and the objects they move) are decoupled by a bootstrapping process: EISEN explains away the motion of objects it has already learned to segment. We show that EISEN achieves a substantial improvement in the state of the art for self-supervised image segmentation on challenging synthetic and real-world robotics datasets.},
booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIX},
pages = {719–735},
numpages = {17},
location = {Tel Aviv, Israel}
}

@inproceedings{10.1609/aaai.v38i17.29950,
author = {Zhu, Hai and Zhao, Qingyang and Shang, Weiwei and Wu, Yuren and Liu, Kai},
title = {LimeAttack: local explainable method for textual hard-label adversarial attack},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i17.29950},
doi = {10.1609/aaai.v38i17.29950},
abstract = {Natural language processing models are vulnerable to adversarial examples. Previous textual adversarial attacks adopt model internal information (gradients or confidence scores) to generate adversarial examples. However, this information is unavailable in the real world. Therefore, we focus on a more realistic and challenging setting, named hard-label attack, in which the attacker can only query the model and obtain a discrete prediction label. Existing hard-label attack algorithms tend to initialize adversarial examples by random substitution and then utilize complex heuristic algorithms to optimize the adversarial perturbation. These methods require a lot of model queries and the attack success rate is restricted by adversary initialization. In this paper, we propose a novel hard-label attack algorithm named LimeAttack, which leverages a local explainable method to approximate word importance ranking, and then adopts beam search to find the optimal solution. Extensive experiments show that LimeAttack achieves the better attacking performance compared with existing hard-label attack under the same query budget. In addition, we evaluate the effectiveness of LimeAttack on large language models and some defense methods, and results indicate that adversarial examples remain a significant threat to large language models. The adversarial examples crafted by LimeAttack are highly transferable and effectively improve model robustness in adversarial training.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2202},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inproceedings{10.1007/978-3-031-56992-0_24,
author = {Vear, Craig and Poltronieri, Fabrizio and DiDonato, Balandino and Zhang, Yawen and Benerradi, Johann and Hutchinson, Simon and Turowski, Paul and Shell, Jethro and Malekmohamadi, Hossein},
title = {Building an Embodied Musicking Dataset for Co-creative Music-Making},
year = {2024},
isbn = {978-3-031-56991-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-56992-0_24},
doi = {10.1007/978-3-031-56992-0_24},
abstract = {In this paper, we present our findings of the design, development and deployment of a proof-of-concept dataset that captures some of the physiological, musicological, and psychological aspects of embodied musicking. After outlining the conceptual elements of this research, we explain the design of the dataset and the process of capturing the data. We then introduce two tests we used to evaluate the dataset: a) using data science techniques and b) a practice-based application in an AI-robot digital score. The results from these tests are conflicting: from a data science perspective the dataset could be considered questionable, but when applied to a real-world musicking situation performers reported it was transformative and felt to be ‘co-creative. We discuss this duality and pose some important questions for future study. However, we feel that the datatset contains a set of relationships that are useful to explore in the creation of music.},
booktitle = {Artificial Intelligence in Music, Sound, Art and Design: 13th International Conference, EvoMUSART 2024, Held as Part of EvoStar 2024, Aberystwyth, UK, April 3–5, 2024, Proceedings},
pages = {373–388},
numpages = {16},
keywords = {dataset, music performance, embodied AI},
location = {Aberystwyth, United Kingdom}
}

@inproceedings{10.1145/3663384.3663391,
author = {Deacon, Thomas and Plumbley, Mark D.},
title = {Working with AI Sound: Exploring the Future of Workplace AI Sound Technologies},
year = {2024},
isbn = {9798400710179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663384.3663391},
doi = {10.1145/3663384.3663391},
abstract = {The workplace is a site for the rapid development and deployment of Artificial Intelligence (AI) systems. However, our research suggests that their adoption could already be hindered by critical issues such as trust, privacy, and security. This paper examines the integration of AI-enabled sound technologies in the workplace, with a focus on enhancing well-being and productivity through a soundscape approach while addressing ethical concerns. To explore these concepts, we used scenario-based design and structured feedback sessions with knowledge workers from open-plan offices and those working from home. To do this, we present initial design concepts for AI sound analysis and control systems. Based on the perspectives gathered, we present user requirements and concerns, particularly regarding privacy and the potential for workplace surveillance, emphasising the need for user consent and levels of transparency in AI deployments. Navigating these ethical considerations is a key implication of the study. We advocate for novel ways to incorporate people’s involvement in the design process through co-design and serious games to shape the future of AI audio technologies in the workplace.},
booktitle = {Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
articleno = {2},
numpages = {21},
keywords = {AI sound systems, focus groups, knowledge workers, open-plan offices, personalised sound systems, privacy, sound monitoring, soundscape, thematic analysis, work-from-home, workplace acoustics},
location = {Newcastle upon Tyne, United Kingdom},
series = {CHIWORK '24}
}

@inproceedings{10.1145/3706598.3713913,
author = {Wang, Huichen Will and Birnbaum, Larry and Setlur, Vidya},
title = {Jupybara: Operationalizing a Design Space for Actionable Data Analysis and Storytelling with LLMs},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713913},
doi = {10.1145/3706598.3713913},
abstract = {Mining and conveying actionable insights from complex data is a key challenge of exploratory data analysis (EDA) and storytelling. To address this challenge, we present a design space for actionable EDA and storytelling. Synthesizing theory and expert interviews, we highlight how semantic precision, rhetorical persuasion, and pragmatic relevance underpin effective EDA and storytelling. We also show how this design space subsumes common challenges in actionable EDA and storytelling, such as identifying appropriate analytical strategies and leveraging relevant domain knowledge. Building on the potential of LLMs to generate coherent narratives with commonsense reasoning, we contribute Jupybara, an AI-enabled assistant for actionable EDA and storytelling implemented as a Jupyter Notebook extension. Jupybara employs two strategies—design-space-aware prompting and multi-agent architectures—to operationalize our design space. An expert evaluation confirms Jupybara’s usability, steerability, explainability, and reparability, as well as the effectiveness of our strategies in operationalizing the design space framework with LLMs.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1005},
numpages = {24},
keywords = {Actionable Insights, Human-AI Collaboration, Multi-Agent System, Large Language Model, Exploratory Data Analysis, Data Storytelling, Data Science, Semantics, Rhetoric, Pragmatics.},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3514221.3517854,
author = {Huang, Zezhou and Wu, Eugene},
title = {Reptile: Aggregation-level Explanations for Hierarchical Data},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3517854},
doi = {10.1145/3514221.3517854},
abstract = {Users often can see from overview-level statistics that some results look "off", but are rarely able to characterize even the type of error. Reptile is an iterative human-in-the-loop explanation and cleaning system for errors in hierarchical data. Users specify an anomalous distributive aggregation result (a complaint), and Reptile recommends drill-down operations to help the user "zoom-in" on the underlying errors. Unlike prior explanation systems that intervene on raw records, Reptile intervenes by learning a group's expected statistics, and ranks drill-down sub-groups by how much the intervention fixes the complaint. This group-level formulation supports a wide range of error types (missing, duplicates, value errors) and uniquely leverages the distributive properties of the user complaint. Further, the learning-based intervention lets users provide domain expertise that Reptile learns from.In each drill-down iteration, Reptile must train a large number of predictive models. We thus extend factorized learning from count-join queries to aggregation-join queries, and develop a suite of optimizations that leverage the data's hierarchical structure. These optimizations reduce runtimes by &gt;6\texttimes{} compared to a Lapack-based implementation. When applied to real-world Covid-19 and African farmer survey data, Reptile correctly identifies 21/30 (vs 2 using existing explanation approaches) and 20/22 errors. Reptile has been deployed in Ethiopia and Zambia, and used to clean nation-wide farmer survey data; the clean data has been used to design national drought insurance policies.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {399–413},
numpages = {15},
keywords = {data cleaning, data factorisation, data mining, data provenance, explanations, exploratory data analysis, functional dependencies, provenance},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@article{10.1016/j.envsoft.2021.105159,
author = {Razavi, Saman},
title = {Deep learning, explained: Fundamentals, explainability, and bridgeability to process-based modelling},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {144},
number = {C},
issn = {1364-8152},
url = {https://doi.org/10.1016/j.envsoft.2021.105159},
doi = {10.1016/j.envsoft.2021.105159},
journal = {Environ. Model. Softw.},
month = oct,
numpages = {21},
keywords = {Hydrology, Earth systems, Process-based modelling, Artificial neural networks, Deep learning, Machine learning, Artificial intelligence}
}

@inproceedings{10.1145/3628096.3629046,
author = {Anuyah, Oghenemaro and Wan, Ruyuan and Adejoro, Cornelius and Yeh, Tom and Metoyer, Ronald and Badillo-Urquiola, Karla},
title = {Cultural Considerations in AI Systems for the Global South: A Systematic Review},
year = {2024},
isbn = {9798400708879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628096.3629046},
doi = {10.1145/3628096.3629046},
abstract = {The field of Artificial Intelligence (AI) is leading transformative impacts across different sectors. However, these advancements are often developed with a Western-centric focus, neglecting the cultural diversity in regions such as the Global South. In this paper, we synthesize twelve research papers focusing on cultural considerations in designing AI systems for the Global South. Our findings revealed a significant focus on domains like healthcare and intelligent assistants and challenges, including usability, AI transparency, and data availability, that can hinder the design and deployment of culturally sensitive AI systems. Moreover, the results demonstrated the need to integrate cultural values to increase the acceptance and usefulness of AI systems for regions in the Global South. This paper guides future research towards developing culturally and contextually relevant AI systems for Africa, highlighting the need for increased visibility and representation of African researchers in HCI and AI domains.},
booktitle = {Proceedings of the 4th African Human Computer Interaction Conference},
pages = {125–134},
numpages = {10},
keywords = {Artificial Intelligence, Culture, Design Research, Global South, Systematic Review},
location = {East London, South Africa},
series = {AfriCHI '23}
}

@inproceedings{10.1609/aaai.v38i19.30160,
author = {Tan, Zhen and Chen, Tianlong and Zhang, Zhenyu and Liu, Huan},
title = {Sparsity-guided holistic explanation for LLMs with interpretable inference-time intervention},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i19.30160},
doi = {10.1609/aaai.v38i19.30160},
abstract = {Large Language Models (LLMs) have achieved unprecedented breakthroughs in various natural language processing domains. However, the enigmatic "black-box" nature of LLMs remains a significant challenge for interpretability, hampering transparent and accountable applications. While past approaches, such as attention visualization, pivotal subnetwork extraction, and concept-based analyses, offer some insight, they often focus on either local or global explanations within a single dimension, occasionally falling short in providing comprehensive clarity. In response, we propose a novel methodology anchored in sparsity-guided techniques, aiming to provide a holistic interpretation of LLMs. Our framework, termed SparseCBM, innovatively integrates sparsity to elucidate three intertwined layers of interpretation: input, subnetwork, and concept levels. In addition, the newly introduced dimension of interpretable inference-time intervention facilitates dynamic adjustments to the model during deployment. Through rigorous empirical evaluations on real-world datasets, we demonstrate that SparseCBM delivers a profound understanding of LLM behaviors, setting it apart in both interpreting and ameliorating model inaccuracies. Codes are provided in supplements.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2412},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inproceedings{10.1145/3529190.3534775,
author = {Schobesberger, Martin and Huber, Jaroslava and Gr\"{u}nberger, Stefan and Haslgr\"{u}bler, Michael and Ferscha, Alois},
title = {Designing Proactive Safety Systems for Industrial Workers Using Intelligent Mechanisms},
year = {2022},
isbn = {9781450396318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529190.3534775},
doi = {10.1145/3529190.3534775},
abstract = {In the process of increasing industrial productivity, the aspect of worker safety plays an important but often neglected part. The following workshop paper discusses potential implementations of a priorly designed concept of a HumanAI based assistive and proactive safety system in a specific industrial use case. The goal is to address safety threats which occur during a polymer recycling process (shredding, melting, and producing plastic granulate) during human-machine interaction by encouraging a meaningful transition from the more traditional view of safety (Safety-I), to a more modern and flexible approach to safety (Safety-II), potentially applying intelligent systems. The principles of Safety-I and Safety-II as well as the STOP principle are explained alongside general considerations for safety and assistance systems. This introduction is followed by a detailed description and safety analysis of the chosen representative real-world use case before a transition in the STOP principle from Safety-I to Safety-II is proposed and illustrated in an in depth example. The contribution of this workshop paper is an introduction of Safety-II mechanisms to replace or enhance established Safety-I mechanisms in the STOP principle to increase worker safety.},
booktitle = {Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {480–485},
numpages = {6},
keywords = {worker safety, polymer recycling, humanAI, emergency break assistant, assistance systems, activity recognition, accident prevention, Safety-II},
location = {Corfu, Greece},
series = {PETRA '22}
}

@article{10.1007/s11704-023-3008-x,
author = {Liu, Hengyu and Zhang, Tiancheng and Li, Fan and Yu, Minghe and Yu, Ge},
title = {A probabilistic generative model for tracking multi-knowledge concept mastery probability},
year = {2024},
issue_date = {Jun 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-023-3008-x},
doi = {10.1007/s11704-023-3008-x},
abstract = {Knowledge tracing aims to track students’ knowledge status over time to predict students’ future performance accurately. In a real environment, teachers expect knowledge tracing models to provide the interpretable result of knowledge status. Markov chain-based knowledge tracing (MCKT) models, such as Bayesian Knowledge Tracing, can track knowledge concept mastery probability over time. However, as the number of tracked knowledge concepts increases, the time complexity of MCKT predicting student performance increases exponentially (also called explaining away problem). When the number of tracked knowledge concepts is large, we cannot utilize MCKT to track knowledge concept mastery probability over time. In addition, the existing MCKT models only consider the relationship between students’ knowledge status and problems when modeling students’ responses but ignore the relationship between knowledge concepts in the same problem. To address these challenges, we propose an inTerpretable pRobAbilistiC gEnerative moDel (TRACED), which can track students’ numerous knowledge concepts mastery probabilities over time. To solve explain away problem, we design long and short-term memory (LSTM)-based networks to approximate the posterior distribution, predict students’ future performance, and propose a heuristic algorithm to train LSTMs and probabilistic graphical model jointly. To better model students’ exercise responses, we proposed a logarithmic linear model with three interactive strategies, which models students’ exercise responses by considering the relationship among students’ knowledge status, knowledge concept, and problems. We conduct experiments with four real-world datasets in three knowledge-driven tasks. The experimental results show that TRACED outperforms existing knowledge tracing methods in predicting students’ future performance and can learn the relationship among students, knowledge concepts, and problems from students’ exercise sequences. We also conduct several case studies. The case studies show that TRACED exhibits excellent interpretability and thus has the potential for personalized automatic feedback in the real-world educational environment.},
journal = {Front. Comput. Sci.},
month = jan,
numpages = {16},
keywords = {probabilistic graphical model, deep learning, knowledge tracing, learner modeling}
}

@article{10.1016/j.robot.2025.104937,
author = {Zhou, Haotian and Lin, Yunhan and Yan, Longwu and Min, Huasong},
title = {Achieving adaptive tasks from human instructions for robots using large language models and behavior trees},
year = {2025},
issue_date = {May 2025},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {187},
number = {C},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2025.104937},
doi = {10.1016/j.robot.2025.104937},
journal = {Robot. Auton. Syst.},
month = apr,
numpages = {12},
keywords = {Large language models, Behavior trees, Adaptive tasks, Reactive policy, Behavior tree generation}
}

@article{10.1145/3665502,
author = {Li, Shuyang and Prasad Majumder, Bodhisattwa and McAuley, Julian},
title = {Self-Supervised Bot Play for Transcript-Free Conversational Critiquing with Rationales},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3665502},
doi = {10.1145/3665502},
abstract = {Conversational critiquing in recommender systems offers a way for users to engage in multi-turn conversations to find items they enjoy. For users to trust an agent and give effective feedback, the recommender system must be able to explain its suggestions and rationales. We develop a two-part framework for training multi-turn conversational critiquing in recommender systems that provide recommendation rationales that users can effectively interact with to receive better recommendations. First, we train a recommender system to jointly suggest items and explain its reasoning via subjective rationales. We then fine-tune this model to incorporate iterative user feedback via self-supervised bot-play. Experiments on three real-world datasets demonstrate that our system can be applied to different recommendation models across diverse domains to achieve state-of-the-art performance in multi-turn recommendation. Human studies show that systems trained with our framework provide more useful, helpful, and knowledgeable suggestions in warm- and cold-start settings. Our framework allows us to use only product reviews during training, avoiding the need for expensive dialog transcript datasets that limit the applicability of previous conversational recommender agents.},
journal = {ACM Trans. Recomm. Syst.},
month = aug,
articleno = {7},
numpages = {20},
keywords = {Conversational recommendation, critiquing}
}

@inproceedings{10.5555/3692070.3693872,
author = {Shaham, Tamar Rott and Schwettmann, Sarah and Wang, Franklin and Rajaram, Achyuta and Hernandez, Evan and Andreas, Jacob and Torralba, Antonio},
title = {A multimodal automated interpretability agent},
year = {2024},
publisher = {JMLR.org},
abstract = {This paper describes MAIA, a Multimodal Automated Interpretability Agent. MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery. It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior. These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results. Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior. We evaluate applications of MAIA to computer vision models. We first characterize MAIA's ability to describe (neuron-level) features in learned representations of images. Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters. We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {1802},
numpages = {29},
location = {Vienna, Austria},
series = {ICML'24}
}

@inproceedings{10.1109/ICRA48506.2021.9561893,
author = {Kottinger, Justin and Almagor, Shaull and Lahijanian, Morteza},
title = {MAPS-X: Explainable Multi-Robot Motion Planning via Segmentation},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICRA48506.2021.9561893},
doi = {10.1109/ICRA48506.2021.9561893},
abstract = {Traditional multi-robot motion planning (MMP) focuses on computing trajectories for multiple robots acting in an environment, such that the robots do not collide when the trajectories are taken simultaneously. In safety-critical applications, a human supervisor may want to verify that the plan is indeed collision-free. In this work, we propose a notion of explanation for a plan of MMP, based on visualization of the plan as a short sequence of images representing time segments, where in each time segment the trajectories of the agents are disjoint, clearly illustrating the safety of the plan. We show that standard notions of optimality (e.g., makespan) may create conflict with short explanations. Thus, we propose meta-algorithms, namely multi-agent plan segmenting-X (MAPS-X) and its lazy variant, that can be plugged on existing centralized sampling-based tree planners X to produce plans with good explanations using a desirable number of images. We demonstrate the efficacy of this explanation-planning scheme and extensively evaluate the performance of MAPS-X and its lazy variant in various environments and agent dynamics.},
booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
pages = {7994–8000},
numpages = {7},
location = {Xi'an, China}
}

@article{10.1016/j.asoc.2024.112274,
author = {Ye, Guangze and Wu, Wen and Shi, Liye and Hu, Wenxin and Chen, Xi and He, Liang},
title = {A personality-guided preference aggregator for ephemeral group recommendation},
year = {2024},
issue_date = {Dec 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {PA},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2024.112274},
doi = {10.1016/j.asoc.2024.112274},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {16},
keywords = {Group recommendation, Personality traits, Data sparsity}
}

@inproceedings{10.1145/3531146.3533237,
author = {Ramesh, Divya and Kameswaran, Vaishnav and Wang, Ding and Sambasivan, Nithya},
title = {How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533237},
doi = {10.1145/3531146.3533237},
abstract = {Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‘high-risk’ AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‘boon’ of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1917–1928},
numpages = {12},
keywords = {algorithmic accountability, algorithmic fairness, human-ai interaction, instant loans, socio-technical systems},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{10.1007/s11280-023-01170-2,
author = {Jin, Langjunqing and Zhao, Feng and Jin, Hai},
title = {HTSE: hierarchical time-surface model for temporal knowledge graph embedding},
year = {2023},
issue_date = {Sep 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-023-01170-2},
doi = {10.1007/s11280-023-01170-2},
abstract = {Representation learning based on temporal knowledge graphs (TKGs) has attracted widespread interest, and temporal knowledge graph embedding (TKGE) expresses time entity and relation tokens and exhibit strong dynamics. Despite the significance of the dynamics and the persistent updates in TKGs, most studies have been devoted to static knowledge graphs. Moreover, previous temporal works ignored the semantic hierarchies observed in knowledge modelling cases, which are common in real-world applications. Inaccurate semantic expressions caused by incomplete projections might not capture complex topological structures very well. To solve this problem, a novel hierarchicaltime-surfaceembedding (HTSE) model is proposed for the representation learning of entities, relations and time. Specifically, a unified relation-oriented hierarchical space aims to distinguish relations at different semantic levels of a hierarchy, and entities can naturally reflect the corresponding hierarchy. Then, a time surface aims to enhance the temporal characteristics, and quadruples are learned through exponential mapping and tangent planes in the time surface. According to extensive experiments, HTSE can achieve remarkable performance on five benchmark datasets, outperforming baseline models for time scope prediction, temporal link prediction and hierarchical relation embedding tasks.Furthermore, the qualitative analysis is used to demonstrate the explainable strategy for hierarchical embeddings and their significance in TKGs.},
journal = {World Wide Web},
month = may,
pages = {2947–2967},
numpages = {21},
keywords = {Temporal prediction, Time surface, Semantic hierarchy, Knowledge graph embedding}
}

@inproceedings{10.1145/3511808.3557202,
author = {Fu, Dongqi and Ban, Yikun and Tong, Hanghang and Maciejewski, Ross and He, Jingrui},
title = {DISCO: Comprehensive and Explainable Disinformation Detection},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557202},
doi = {10.1145/3511808.3557202},
abstract = {Disinformation refers to false information deliberately spread to influence the general public, and the negative impact of disinformation on society can be observed in numerous issues, such as political agendas and manipulating financial markets. In this paper, we identify prevalent challenges and advances related to automated disinformation detection from multiple aspects and propose a comprehensive and explainable disinformation detection framework called DISCO. It leverages the heterogeneity of disinformation and addresses the opaqueness of prediction. Then we provide a demonstration of DISCO on a real-world fake news detection task with satisfactory detection accuracy and explanation. The demo video and source code of DISCO is now publicly available https://github.com/DongqiFu/DISCO. We expect that our demo could pave the way for addressing the limitations of identification, comprehension, and explainability as a whole.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {4848–4852},
numpages = {5},
keywords = {graph augmentation, explanation, disinformation detection},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1007/978-3-030-58920-2_13,
author = {Aslansefat, Koorosh and Sorokos, Ioannis and Whiting, Declan and Tavakoli Kolagari, Ramin and Papadopoulos, Yiannis},
title = {SafeML: Safety Monitoring of Machine Learning Classifiers Through Statistical Difference Measures},
year = {2020},
isbn = {978-3-030-58919-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58920-2_13},
doi = {10.1007/978-3-030-58920-2_13},
abstract = {Ensuring safety and explainability of machine learning (ML) is a topic of increasing relevance as data-driven applications venture into safety-critical application domains, traditionally committed to high safety standards that are not satisfied with an exclusive testing approach of otherwise inaccessible black-box systems. Especially the interaction between safety and security is a central challenge, as security violations can lead to compromised safety. The contribution of this paper to addressing both safety and security within a single concept of protection applicable during the operation of ML systems is active monitoring of the behavior and the operational context of the data-driven system based on distance measures of the Empirical Cumulative Distribution Function (ECDF). We investigate abstract datasets (XOR, Spiral, Circle) and current security-specific datasets for intrusion detection (CICIDS2017) of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures. Our preliminary findings indicate that there is a meaningful correlation between ML decisions and the ECDF-based distances measures of the input features. Thus, they can provide a confidence level that can be used for a) analyzing the applicability of the ML system in a given field (safety/security) and b) analyzing if the field data was maliciously manipulated. (Our preliminary code and results are available at .)},
booktitle = {Model-Based Safety and Assessment: 7th International Symposium, IMBSA 2020, Lisbon, Portugal, September 14–16, 2020, Proceedings},
pages = {197–211},
numpages = {15},
keywords = {Domain adaptation, Statistical difference, Artificial Intelligence, Deep Learning, Machine Learning, SafeML, Safety},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3580305.3599215,
author = {Zhao, Chen and Chen, Feng and Wu, Xintao and Chen, Haifeng and Zhou, Jiayu},
title = {2nd Workshop on Ethical Artificial Intelligence: Methods and Applications (EAI)},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599215},
doi = {10.1145/3580305.3599215},
abstract = {Ethical AI has become increasingly important, and it has been attracting attention from academia and industry, due to its increased popularity in real-world applications with fairness concerns. It also places fundamental importance on ethical considerations in determining legitimate and illegitimate uses of AI. Organizations that apply ethical AI have clearly stated well-defined review processes to ensure adherence to legal guidelines. Therefore, the wave of research at the intersection of ethical AI in data mining and machine learning has also influenced other fields of science, including computer vision, natural language processing, reinforcement learning, and social science. Despite these successes, ethical AI still faces many challenges, such as a lack of interpretable and explainable methods for fairness-aware deep learning models, etc. Consequently, there is an urgent need to bring experts and researchers together at prestigious venues to discuss ethical AI, which has been rarely seen in previous KDD conferences. This workshop will provide a premium platform for both research and industry from different backgrounds to exchange ideas on opportunities, challenges, and cutting-edge techniques in ethical AI.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5903–5904},
numpages = {2},
keywords = {machine learning, fairness-aware, ethical artificial intelligence},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1007/978-3-031-35741-1_6,
author = {George, Micah Wilson and Gaikwad, Nandini and Duffy, Vincent G. and Greenwood, Allen G.},
title = {Digital Twin Modelling for Human-Centered Ergonomic Design},
year = {2023},
isbn = {978-3-031-35740-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-35741-1_6},
doi = {10.1007/978-3-031-35741-1_6},
abstract = {Simulation turns out to be one of the most important aspects of ergonomics, as it allows testing the interaction of the people and the system before they are brought into effect to improve the design process. Digital Twin is a concept that utilizes simulation to implement the representation of the model of a system or product in a virtual manner so that its purpose and working can be analyzed and evaluated before bringing it into the real world to serve its purpose as expected. This report focuses on how Digital Twin is a powerful industrial tool and explains its distinct properties along with its applications through case studies that provide explanations for the required methodology to implement Digital Twin in a simulation software called FlexSim. The results show how important it is in today’s world to carry out proper simulations, i.e., implementing the Digital Twin to provide critical solutions in workplace ergonomics and other areas.},
booktitle = {Digital Human Modeling and Applications in Health, Safety, Ergonomics and Risk Management: 14th International Conference, DHM 2023, Held as Part of the 25th HCI International Conference, HCII 2023, Copenhagen, Denmark, July 23–28, 2023, Proceedings, Part I},
pages = {58–69},
numpages = {12},
keywords = {Ergonomics, Digital Twin, Simulation},
location = {Copenhagen, Denmark}
}

@article{10.1002/ail2.41,
author = {Vasu, Bhavan and Hu, Brian and Dong, Bo and Collins, Roddy and Hoogs, Anthony},
title = {Explainable, interactive content‐based image retrieval},
year = {2021},
issue_date = {December 2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1002/ail2.41},
doi = {10.1002/ail2.41},
abstract = {Quantifying the value of explanations in a human‐in‐the‐loop (HITL) system is difficult. Previous methods either measure explanation‐specific values that do not correspond to user tasks and needs or poll users on how useful they find the explanations to be. In this work, we quantify how much explanations help the user through a utility‐based paradigm that measures change in task performance when using explanations vs not. Our chosen task is content‐based image retrieval (CBIR), which has well‐established baselines and performance metrics independent of explainability. We extend an existing HITL image retrieval system that incorporates user feedback with similarity‐based saliency maps (SBSM) that indicate to the user which parts of the retrieved images are most similar to the query image. The system helps the user understand what it is paying attention to through saliency maps, and the user helps the system understand their goal through saliency‐guided relevance feedback. Using the MS‐COCO dataset, a standard object detection and segmentation dataset, we conducted extensive, crowd‐sourced experiments validating that SBSM improves interactive image retrieval. Although the performance increase is modest in the general case, in more difficult cases such as cluttered scenes, using explanations yields an 6.5% increase in accuracy. To the best of our knowledge, this is the first large‐scale user study showing that visual saliency map explanations improve performance on a real‐world, interactive task. Our utility‐based evaluation paradigm is general and potentially applicable to any task for which explainability can be incorporated.We describe a utility‐based user study paradigm to investigate the effectiveness of explanations on an image retrieval task. The task requires a human‐in‐the‐loop, where similarity‐based saliency maps are presented to the user as a means to explain why an image result was returned. Our results show that explanations improve image retrieval performance on cluttered scenes, as well as increase trust in the system.

image
image},
journal = {Applied AI Letters},
month = nov,
numpages = {11},
keywords = {user study, saliency, image retrieval, explainable AI}
}

@inproceedings{10.5555/3666122.3668304,
author = {Okawa, Maya and Lubana, Ekdeep Singh and Dick, Robert P. and Tanaka, Hidenori},
title = {Compositional abilities emerge multiplicatively: exploring diffusion models on a synthetic task},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhibits a sudden "emergence" due to multiplicative reliance on the performance of constituent tasks, partially explaining emergent phenomena seen in generative models; and (iii) composing concepts with lower frequency in the training data to generate out-of-distribution samples requires considerably more optimization steps compared to generating in-distribution samples. Overall, our study lays a foundation for understanding emergent capabilities and compositionality in generative models from a data-centric perspective.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {2182},
numpages = {23},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{10.1145/3447548.3467148,
author = {Li, Xiao-Hui and Shi, Yuhan and Li, Haoyang and Bai, Wei and Cao, Caleb Chen and Chen, Lei},
title = {An Experimental Study of Quantitative Evaluations on Saliency Methods},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467148},
doi = {10.1145/3447548.3467148},
abstract = {It has been long debated that eXplainable AI (XAI) is an important technology for model and data exploration, validation, and debugging. To deploy XAI into actual systems, an executable and comprehensive evaluation of the quality of generated explanation is highly in demand. In this paper, we briefly summarize the status quo of the quantitative metrics of different properties of XAI including evaluation on faithfulness, localization, sensitivity check, and stability. With an exhaustive experimental study based on them, we conclude that among all the typical methods we compare, no single explanation method dominates others in all metrics. Nonetheless, Gradient-weighted Class Activation Mapping (Grad-CAM) and Randomly Input Sampling for Explanation (RISE) perform fairly well in most of the metrics. We further present a novel utilization of the evaluation results to diagnose the classification bases for models. Hopefully, this valuable work could serve as a guide for future research.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3200–3208},
numpages = {9},
keywords = {model diagnosis, metrics, explainable artificial intelligence, evaluation},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3605390.3610829,
author = {De Luca, Ernesto William and Purificato, Erasmo and Boratto, Ludovico and Marrone, Stefano and Sansone, Carlo},
title = {First Workshop on User Perspectives in Human-Centred Artificial Intelligence (HCAI4U)},
year = {2023},
isbn = {9798400708060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605390.3610829},
doi = {10.1145/3605390.3610829},
abstract = {The emerging concept of Human-Centred Artificial Intelligence (HCAI) involves the amplification, augmentation, empowerment, and enhancement of individuals. The goal of HCAI is to ensure that AI meets our needs while also operating transparently, delivering fair and equitable outcomes, and respecting privacy, all while preserving human control. This approach involves multiple stakeholders, such as researchers, developers, business leaders, policy makers, and users, who are affected in various ways by the implementation and evaluation of AI systems. The primary focus of the First Workshop on User Perspectives in Human-Centred Artificial Intelligence (HCAI4U) is to examine the potential positive and negative impacts of automated decision-making systems on end-users, as well as how their interaction with AI is influenced by human-centred aspects of reliability, safety, and fairness. The workshop aims to facilitate discussion and exchange of ideas among the community on advances in developing trustworthy, fair, and privacy-preserving systems, as well as user interfaces that are explainable, with a specific focus on the users’ perception in real-world scenarios rather than solely on the algorithmic and model performance. Additionally, HCAI4U aims to foster cross-disciplinary and interdisciplinary discussions between experts from various research fields, such as computer science, psychology, sociology, law, medicine, business, etc., to discuss problems and synergies in this exciting research topic.},
booktitle = {Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {36},
numpages = {3},
keywords = {User Perspectives, Trustworthiness, Reliability, Human-Computer Interaction, Human-Centred Artificial Intelligence, Fairness, Explainability, Artificial Intelligence},
location = {Torino, Italy},
series = {CHItaly '23}
}

@article{10.1016/j.chb.2023.108105,
author = {Rauschnabel, Philipp A. and Felix, Reto and Heller, Jonas and Hinsch, Chris},
title = {The 4C framework: Towards a holistic understanding of consumer engagement with augmented reality},
year = {2024},
issue_date = {May 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {154},
number = {C},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2023.108105},
doi = {10.1016/j.chb.2023.108105},
journal = {Comput. Hum. Behav.},
month = may,
numpages = {12},
keywords = {Augmented Reality, Content, Consumer, Context, Engagement, Metaverse}
}

@inproceedings{10.1007/978-3-031-24667-8_2,
author = {Schmidt-Wolf, Melanie and Feil-Seifer, David},
title = {Vehicle-To-Pedestrian Communication Feedback Module: A Study on&nbsp;Increasing Legibility, Public Acceptance and&nbsp;Trust},
year = {2022},
isbn = {978-3-031-24666-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-24667-8_2},
doi = {10.1007/978-3-031-24667-8_2},
abstract = {Vehicle pedestrian communication is extremely important when developing autonomy for an autonomous vehicle. Enabling bidirectional nonverbal communication between pedestrians and autonomous vehicles will lead to an improvement of pedestrians’ safety in autonomous driving. The autonomous vehicle should provide feedback to the human about what it is about to do. The user study presented in this paper investigated several possible options for an external vehicle display for effective nonverbal communication between an autonomous vehicle and a human. The result of this study will guide the development of the feedback module to optimize for public acceptance and trust in the autonomous vehicle’s decision while being legible to the widest range of potential users. The results of this study show that participants prefer symbols over text, lights and road projection. We plan to elaborate and focus on the selected interaction modes via Virtual Reality and in the real world in ongoing and future studies.},
booktitle = {Social Robotics: 14th International Conference, ICSR 2022, Florence, Italy, December 13–16, 2022, Proceedings, Part I},
pages = {14–23},
numpages = {10},
keywords = {Trust, Public acceptance, Legibility, eHMI, V2P, Autonomous vehicle},
location = {Florence, Italy}
}

@inproceedings{10.1145/3498851.3498926,
author = {Chen, Xiangtai and Tang, Tao and Ren, Jing and Lee, Ivan and Chen, Honglong and Xia, Feng},
title = {Heterogeneous Graph Learning for Explainable Recommendation over Academic Networks},
year = {2022},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3498926},
doi = {10.1145/3498851.3498926},
abstract = {With the explosive growth of new graduates with research degrees every year, unprecedented challenges arise for early-career researchers to find a job at a suitable institution. This study aims to understand the behavior of academic job transition and hence recommend suitable institutions for PhD graduates. Specifically, we design a deep learning model to predict the career move of early-career researchers and provide suggestions. The design is built on top of scholarly/academic networks, which contains abundant information about scientific collaboration among scholars and institutions. We construct a heterogeneous scholarly network to facilitate the exploring of the behavior of career moves and the recommendation of institutions for scholars. We devise an unsupervised learning model called HAI (Heterogeneous graph Attention InfoMax) which aggregates attention mechanism and mutual information for institution recommendation. Moreover, we propose scholar attention and meta-path attention to discover the hidden relationships between several meta-paths. With these mechanisms, HAI provides ordered recommendations with explainability. We evaluate HAI upon a real-world dataset against baseline methods. Experimental results verify the effectiveness and efficiency of our approach.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {29–36},
numpages = {8},
keywords = {recommender systems, heterogeneous networks, graph learning, explainability, academic social networks},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@article{10.1016/j.cie.2024.110310,
author = {Bavaresco, Rodrigo and Ren, Yutian and Barbosa, Jorge and Li, G.P.},
title = {An ontology-based framework for worker’s health reasoning enabled by machine learning},
year = {2024},
issue_date = {Jul 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {193},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2024.110310},
doi = {10.1016/j.cie.2024.110310},
journal = {Comput. Ind. Eng.},
month = jul,
numpages = {16},
keywords = {Knowledge representation, Ontology, Deep learning, Reasoning, Occupational health and safety}
}

@article{10.1016/j.knosys.2022.109278,
author = {Li, Minglei and Li, Xiang and Jiang, Yuchen and Zhang, Jiusi and Luo, Hao and Yin, Shen},
title = {Explainable multi-instance and multi-task learning for COVID-19 diagnosis and lesion segmentation in CT images},
year = {2022},
issue_date = {Sep 2022},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {252},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2022.109278},
doi = {10.1016/j.knosys.2022.109278},
journal = {Know.-Based Syst.},
month = sep,
numpages = {16},
keywords = {Explainable multi-instance learning, Adaptive multi-task learning, COVID-19, Lesion segmentation, Automated diagnosis}
}

@inproceedings{10.1145/3412382.3459209,
author = {Rathore, Hemant},
title = {Designing Adversarial Robust and Explainable Malware Detection System for Android based Smartphones: PhD Forum Abstract},
year = {2021},
isbn = {9781450380980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412382.3459209},
doi = {10.1145/3412382.3459209},
abstract = {Android smartphones and malware have grown exponentially in the last decade. Literature suggests that the current malware detection systems cannot cope with the present security challenges. Thus researchers are developing next-generation malware detection systems/models using the machine and deep learning. However, the proposed systems/models have poor explainability and are vulnerable against adversarial attacks, which will jeopardize their adoption in the future security ecosystem. Thus, we aim to construct adversarial robust malware detection models by first acting as an adversary to find vulnerabilities in models and then proposing preventive countermeasures. We also aim to improve models' explain-ability to win security community confidence before real-world implementation.},
booktitle = {Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co-Located with CPS-IoT Week 2021)},
pages = {412–413},
numpages = {2},
location = {Nashville, TN, USA},
series = {IPSN '21}
}

@book{10.5555/3383726,
author = {Curry, Edward},
title = {Real-time Linked Dataspaces: Enabling Data Ecosystems for Intelligent Systems},
year = {2019},
isbn = {3030296644},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {This open access book explores the dataspace paradigm as a best-effort approach to data management within data ecosystems. It establishes the theoretical foundations and principles of real-time linked dataspaces as a data platform for intelligent systems. The book introduces a set of specialized best-effort techniques and models to enable loose administrative proximity and semantic integration for managing and processing events and streams. The book is divided into five major parts: Part I "Fundamentals and Concepts" details the motivation behind and core concepts of real-time linked dataspaces, and establishes the need to evolve data management techniques in order to meet the challenges of enabling data ecosystems for intelligent systems within smart environments. Further, it explains the fundamental concepts of dataspaces and the need for specialization in the processing of dynamic real-time data. Part II "Data Support Services" explores the design and evaluation of critical services, including catalog, entity management, query and search, data service discovery, and human-in-the-loop. In turn, Part III "Stream and Event Processing Services" addresses the design and evaluation of the specialized techniques created for real-time support services including complex event processing, event service composition, stream dissemination, stream matching, and approximate semantic matching. Part IV "Intelligent Systems and Applications" explores the use of real-time linked dataspaces within real-world smart environments. In closing, Part V "Future Directions" outlines future research challenges for dataspaces, data ecosystems, and intelligent systems. Readers will gain a detailed understanding of how the dataspace paradigm is now being used to enable data ecosystems for intelligent systems within smart environments. The book covers the fundamental theory, the creation of new techniques needed for support services, and lessons learned from real-world intelligent systems and applications focused on sustainability. Accordingly, it will benefit not only researchers and graduate students in the fields of data management, big data, and IoT, but also professionals who need to create advanced data management platforms for intelligent systems, smart environments, and data ecosystems.}
}

@inproceedings{10.1007/978-3-031-32636-3_1,
author = {Bella, Giampaolo},
title = {Interactional Freedom and&nbsp;Cybersecurity},
year = {2022},
isbn = {978-3-031-32635-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-32636-3_1},
doi = {10.1007/978-3-031-32636-3_1},
abstract = {We have become accustomed to the news of more and more cunning attacks to real-world systems, and equally accustomed to try to fix them even though further attacks may come. I discuss how to tackle and ultimately resolve this tedious and infamous attack-fix-loop practice by distilling out five paradigms to achieve cybersecurity: democratic, dictatorial, beautiful, invisible and explainable security. While each of these has distinctive features, various combinations, at some rate, of them may coexist, with the final aim of improving the way security measures account for the human element. Towards the end of the paper, I conjecture how the paradigms could be used to improve the ultimate security measure of our times, a Security Operation Centre. May I remark that many of the observations made below derive from my personal and current understanding and would require a number of experiments to be fully confirmed.},
booktitle = {Innovative Security Solutions for Information Technology and Communications: 15th International Conference, SecITC 2022, Virtual Event, December 8–9, 2022, Revised Selected Papers},
pages = {1–16},
numpages = {16},
keywords = {Explainable Security, Invisible Security, Beautiful security, Dictatorial security, Democratic security}
}

@inproceedings{10.1145/3557915.3560978,
author = {Tenzer, Mark and Rasheed, Zeeshan and Shafique, Khurram},
title = {Learning citywide patterns of life from trajectory monitoring},
year = {2022},
isbn = {9781450395298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3557915.3560978},
doi = {10.1145/3557915.3560978},
abstract = {The recent proliferation of real-world human mobility datasets has catalyzed geospatial and transportation research in trajectory prediction, demand forecasting, travel time estimation, and anomaly detection. However, these datasets also enable, more broadly, a descriptive analysis of intricate systems of human mobility. We formally define patterns of life analysis as a natural, explainable extension of online unsupervised anomaly detection, where we not only monitor a data stream for anomalies but also explicitly extract normal patterns over time. To learn patterns of life, we adapt Grow When Required (GWR) episodic memory from research in computational biology and neurorobotics to a new domain of geospatial analysis. This biologically-inspired neural network, related to self-organizing maps (SOM), constructs a set of "memories" or prototype traffic patterns incrementally as it iterates over the GPS stream. It then compares each new observation to its prior experiences, inducing an online, unsupervised clustering and anomaly detection on the data. We mine patterns-of-interest from the Porto taxi dataset, including both major public holidays and newly-discovered transportation anomalies, such as festivals and concerts which, to our knowledge, have not been previously acknowledged or reported in prior work. We anticipate that the capability to incrementally learn normal and abnormal road transportation behavior will be useful in many domains, including smart cities, autonomous vehicles, and urban planning and management.},
booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
articleno = {43},
numpages = {12},
keywords = {self-organizing feature maps, patterns of life, geospatial analysis, biological neural networks, anomaly detection},
location = {Seattle, Washington},
series = {SIGSPATIAL '22}
}

@article{10.1007/s11280-021-00912-4,
author = {Huang, Yafan and Zhao, Feng and Gui, Xiangyu and Jin, Hai},
title = {Path-enhanced explainable recommendation with knowledge graphs},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {5},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-021-00912-4},
doi = {10.1007/s11280-021-00912-4},
abstract = {Recommender systems, which are used to predict user requirements precisely, play a vital role in the modern internet industry. As an effective tool with rich semantics, knowledge graphs have recently attracted growing research attention in enhancing recommendation results. By mining multihop relations (i.e., paths) between user-item interactions within a knowledge graph, implicit user preferences and other side information can be clearly revealed. Nevertheless, existing knowledge graph-based recommendation methods have two fundamental limitations. First, the indiscriminate utilization of user-item path sets conveys unclear information and negatively influences explainability. Moreover, obtaining reliable recommendation results with these methods requires large amounts of prior knowledge, which indicates that they show poor performance in terms of accuracy and handling cold-start issues. To address these issues, we propose a novel model called the Path-enhanced Recurrent Network (PeRN). Specifically, PeRN integrates a recurrent neural network encoder with a metapath-based entropy encoder to increase explainability and accuracy and reduce cold-start costs. The recurrent network encoder has a strong ability to represent sequential path semantics in a knowledge graph, while the entropy encoder, as an efficient statistical analysis tool, leverages metapath information to differentiate paths in a single user-item interaction. A path extraction algorithm with a bidirectional scheme is also proposed to make PeRN more feasible. The experimental results on two real-world datasets demonstrate our significant improvements with reasonable explanations, promising accuracy and a minimal amount of prior knowledge compared with several state-of-the-art baselines.},
journal = {World Wide Web},
month = sep,
pages = {1769–1789},
numpages = {21},
keywords = {Recurrent neural network, Metapath, Knowledge graph, Recommender system}
}

@phdthesis{10.5555/AAI29352355,
author = {Nan, Zhang},
advisor = {Bon-Gang, Hwang, and Yujie, Lu,},
title = {Development of a Behavior-Based Analysis Framework for Promoting RPV Deployment in Singapore},
year = {2021},
isbn = {9798352684467},
publisher = {National University of Singapore (Singapore)},
abstract = {Residential Photovoltaic (RPV) systems are one type of building-integrated photovoltaics (BIPV) specialized for residential buildings. The deployment of RPV holds great potential to solve energy poverty and reduce greenhouse gas emissions because residential buildings consume approximately 38% of total electricity. However, it is challenging to make heterogeneous households adopt the system, causing a relatively slow development speed for RPV, especially in Southeast Asian countries. Singapore, an equatorial country with abundant solar resources, has installed 443.6 MWp total PV capacity but RPV only accounts for 3.5% by Q1 2021. To promote RPV deployment, researchers have conducted different analyses in many aspects, including exploring the driving factors for RPV adoptions, analyzing how RPV diffuses under different scenarios, and designing optimal energy policies for leveraging RPV adoption in the district.Household owners are the primary consumer of RPV systems, and they are heterogeneous in their adoption decisions and their energy usage behaviors. Behavioral factors are essential in driving RPV diffusion because RPV adoption is typical consumer behavior. However, there lacks a comprehensive study that links the RPV adoption, diffusion, and policy analysis in a unified framework in considering the complex consumers' behaviors. In conventional RPV adoption analysis, non-linear relationships are common in analyzing adoption behaviors, but they are often ignored in the existing studies due to the limitations of conventional data analysis approaches. On the other hand, Artificial Neural Networks (ANNs) are robust in dealing with non-linear relationships, but they lack interpretation capability. This leads to an Explainable Artificial Intelligence (XAI) issue and makes ANNs unsuitable for adoption analysis. Pertinent studies of diffusion analysis focused on using agent-based modeling (ABM) to simulate the consumers' technology adoption behaviors but did not capture consumers' countermeasures for investment volatility which impedes the RPV adoption and diffusion and undermines its economic feasibility. In addition, the existing research ignores the consumers' energy behavior after RPV installation, which may, in turn, influences the performance of energy policies.This research aims to analyze the mechanism of RPV adoption, diffusion and policy design considering the consumers' RPV adoption behaviors and afteradoption energy behaviors. An Artificial Intelligence (AI) assisted agent-based simulation framework is constructed to link behaviors among stakeholders in the process (household owners, policymakers). The proposed research framework consists of three critical objectives: individual behavior analysis, agent-based diffusion modeling, and energy policy optimization.(1) To resolve the XAI issue, this research firstly proposes a six-step analytical procedure based on a hybrid-ANN by integrating the behavior theory and the network weight-based method.(2) To capture investment volatility, this research integrated the real options analysis (ROA) approach into an agent-based diffusion model to quantify consumer's defer options in their decisions. The agent-based simulation model is able to simulate RPV diffusion in the network from individuals' behaviors and critical driving factors identified from objective one. (3) To explore the designing of optimal RPV policy considering consumers' adoption and energy usage behaviors by promising a simulation-optimization approach based on the agent-based model developed in objective two.},
note = {AAI29352355}
}

@inproceedings{10.1145/3351095.3375673,
author = {Oswald, Marion and Powell, David},
title = {Can an algorithmic system be a 'friend' to a police officer's discretion? ACM FAT 2020 translation tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375673},
doi = {10.1145/3351095.3375673},
abstract = {This tutorial aims to increase understanding of the importance of discretion in police decision-making. It will guide computer scientists, policy-makers, lawyers and others in considering practical and technical issues crucial to avoiding the prejudicial and instead develop algorithms that are supportive - a 'friend'- to legitimate discretionary decision-making. It combines explanation of the relevant law and related literature with discussion based upon deep operational experience in the area of preventative and protective policing work.Autonomy and discretion are fundamental to police work, not only in relation to strategy and policy but for day-to-day operational decisions taken by front line officers. Such discretion 'recognizes the fallibility of interfacing rules with their field of application.' (Hildebrandt 2016). This discretion is not unbounded however and English common law expects discretion to be exercised reasonably and fairly. Conversely, discretion must not be fettered unlawfully, by failing to take a relevant factor into account when making a decision, or by abdicating responsibility to another person, body or 'thing'. Algorithmic systems have the potential to contribute to factors relevant to the decision in question at the point of interaction between their outputs and the real-world outcome for the victim, offender and/or community.Algorithmic decision tools present a number of challenges to legitimate discretionary police decision-making. Unnuanced outputs could be highly influential on the human decision-maker (Cooke and Michie 2012) and may undermine discretionary power to deal with atypical cases and 'un-thought of' factors that rely upon uncodified knowledge (Oswald 2018).Practical and technical considerations will be crucial to developing MLA that are supportive to discretionary decision-making. These include the methodological approach, design of the humancomputer interface having regard the decision-maker's responsibility to give reasons for their decision, the avoidance of unnuanced or over-confident framing of results, understanding of the policing context in which the MLA will operate, and consideration of the implications of organisational culture and processes to the MLA's influence.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {698},
numpages = {1},
keywords = {algorithms, discretion, machine learning, police},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@article{10.1145/3555178,
author = {Kou, Ziyi and Zhang, Yang and Zhang, Daniel and Wang, Dong},
title = {CrowdGraph: A Crowdsourcing Multi-modal Knowledge Graph Approach to Explainable Fauxtography Detection},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555178},
doi = {10.1145/3555178},
abstract = {Human-centric fauxtography is a category of multi-modal posts that spread misleading information on online information distribution and sharing platforms such as online social media. The reason of a human-centric post being fauxtography is closely related to its multi-modal content that consists of diversified human and non-human subjects with complex and implicit relationships. In this paper, we focus on an explainable fauxtography detection problem where the goal is to accurately identify and explain why a human-centric social media post is fauxtography (or not). Our problem is motivated by the limitations of current fauxtography detection solutions that focus primarily on the detection task but ignore the important aspect of explaining their results (e.g., why a certain component of the post delivers the misinformation). Two important challenges exist in solving our problem: 1) it is difficult to capture the implicit relations and attributions of different subjects in a fauxtography post given the fact that many of such knowledge is shared between different crowd workers; 2) it is not a trivial task to create a multi-modal knowledge graph from crowd workers to identify and explain human-centric fauxtography posts with multi-modal contents. To address the above challenges, we develop CrowdGraph, a crowdsourcing based multi-modal knowledge graph approach to address the explainable fauxtography detection problem. We evaluate the performance of CrowdGraph by creating a real-world dataset that consists of human-centric fauxtography posts from Twitter and Reddit. The results show that CrowdGraph not only detects the fauxtography posts more accurately than the state-of-the-arts but also provides well-justified explanations to the detection results with convincing evidence.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {287},
numpages = {28},
keywords = {social media, multi modal information, crowdsourcing}
}

@inproceedings{10.5555/3504035.3504214,
author = {Bansal, Gagan and Weld, Daniel S.},
title = {A coverage-based utility model for identifying unknown unknowns},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {A classifier's low confidence in prediction is often indicative of whether its prediction will be wrong; in this case, inputs are called known unknowns. In contrast, unknown unknowns (UUs) are inputs on which a classifier makes a high confidence mistake. Identifying UUs is especially important in safety-critical domains like medicine (diagnosis) and law (recidivism prediction). Previous work by Lakkaraju et al. (2017) on identifying unknown unknowns assumes that the utility of each revealed UU is independent of the others, rather than considering the set holistically. While this assumption yields an efficient discovery algorithm, we argue that it produces an incomplete understanding of the classifier's limitations. In response, this paper proposes a new class of utility models that rewards how well the discovered UUs cover (or "explain") a sample distribution of expected queries. Although choosing an optimal cover is intractable, even if the UUs were known, our utility model is monotone submodular, affording a greedy discovery strategy. Experimental results on four datasets show that our method outperforms bandit-based approaches and achieves within 60.9% utility of an omniscient, tractable upper bound.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {179},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1145/3600160.3604986,
author = {Goeman, Victor and de Ruck, Dairo and Boh\'{e}, Ilse and Lapon, Jorn and Naessens, Vincent},
title = {IoT Security Seminar: Raising Awareness and Sharing Critical Knowledge},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600160.3604986},
doi = {10.1145/3600160.3604986},
abstract = {The security of the Internet of Things (IoT) devices has become a major concern as the number of connected devices continues to increase. Despite this concern, there is a lack of training opportunities to educate IoT developers on security measures. While there are ample ICT and Network Management courses for developers, there is a lack of security courses scoped for this audience. One of the reasons is that raising cybersecurity awareness and increasing the security expertise of developers presents a significant challenge due to the complexity of IoT security. This work presents a cybersecurity seminar that tackles these challenges. It is aimed at various actors in the IoT device development cycle (e.g. software designers, developers and managers) to raise IoT security awareness and share critical knowledge. It cultivates the basics of both offensive and defensive security through a custom-built vulnerable IoT firmware image with vulnerabilities found in real-world IoT devices. This intentionally vulnerable image is accompanied by a detailed walkthrough explaining various exploitation and mitigation techniques. Our seminar has been held multiple times in both industry and academics and consistently received very positive feedback. It has been successful in educating participants about the importance of IoT security and providing them with additional knowledge and skills to take action in their own practices.},
booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security},
articleno = {62},
numpages = {8},
keywords = {Awareness, Cybersecurity, Education, IoT},
location = {Benevento, Italy},
series = {ARES '23}
}

@inbook{10.1007/978-3-030-31423-1_2,
author = {Martinez, Maria Vanina and Simari, Gerardo I.},
title = {Explanation-Friendly Query Answering Under Uncertainty},
year = {2022},
isbn = {978-3-030-31422-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-31423-1_2},
abstract = {Many tasks often regarded as requiring some form of intelligence to perform can be seen as instances of query answering over a semantically rich knowledge base. In this context, two of the main problems that arise are: (i) uncertainty, including both inherent uncertainty (such as events involving the weather) and uncertainty arising from lack of sufficient knowledge; and (ii) inconsistency, which involves dealing with conflicting knowledge. These unavoidable characteristics of real world knowledge often yield complex models of reasoning; assuming these models are mostly used by humans as decision-support systems, meaningful explainability of their results is a critical feature. These lecture notes are divided into two parts, one for each of these basic issues. In Part&nbsp;1, we present basic probabilistic graphical models and discuss how they can be incorporated into powerful ontological languages; in Part&nbsp;2, we discuss both classical inconsistency-tolerant semantics for ontological query answering based on the concept of repair and other semantics that aim towards more flexible yet principled ways to handle inconsistency. Finally, in both parts we ponder the issue of deriving different kinds of explanations that can be attached to query results.},
booktitle = {Reasoning Web. Explainable Artificial Intelligence},
pages = {65–103},
numpages = {39}
}

@article{10.1145/3429444,
author = {Zou, Deqing and Zhu, Yawei and Xu, Shouhuai and Li, Zhen and Jin, Hai and Ye, Hengkai},
title = {Interpreting Deep Learning-based Vulnerability Detector Predictions Based on Heuristic Searching},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3429444},
doi = {10.1145/3429444},
abstract = {Detecting software vulnerabilities is an important problem and a recent development in tackling the problem is the use of deep learning models to detect software vulnerabilities. While effective, it is hard to explain why a deep learning model predicts a piece of code as vulnerable or not because of the black-box nature of deep learning models. Indeed, the interpretability of deep learning models is a daunting open problem. In this article, we make a significant step toward tackling the interpretability of deep learning model in vulnerability detection. Specifically, we introduce a high-fidelity explanation framework, which aims to identify a small number of tokens that make significant contributions to a detector’s prediction with respect to an example. Systematic experiments show that the framework indeed has a higher fidelity than existing methods, especially when features are not independent of each other (which often occurs in the real world). In particular, the framework can produce some vulnerability rules that can be understood by domain experts for accepting a detector’s outputs (i.e., true positives) or rejecting a detector’s outputs (i.e., false-positives and false-negatives). We also discuss limitations of the present study, which indicate interesting open problems for future research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {23},
numpages = {31},
keywords = {vulnerability detection, sensitivity analysis, deep learning, Explainable AI}
}

@inproceedings{10.1145/3442442.3453708,
author = {Rozanec, Joze M.},
title = {Explainable Demand Forecasting: A Data Mining Goldmine},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3453708},
doi = {10.1145/3442442.3453708},
abstract = {Demand forecasting is a crucial component of demand management. Value is provided to the organization through accurate forecasts and insights into the reasons driving the forecasts to increase confidence and assist decision-making. In this Ph.D., we aim to develop state-of-the-art demand forecasting models for irregular demand, develop explainability mechanisms to avoid exposing models fine-grained information regarding the model features, create a recommender system to assist users on decision-making and develop mechanisms to enrich knowledge graphs with feedback provided by the users through artificial intelligence-powered feedback modules. We have already developed models for accurate forecasts regarding steady and irregular demand and architecture to provide forecast explanations that preserve sensitive information regarding model features. These explanations highlighting real-world events that provide insights on the general context captured through the dataset features while highlighting actionable items and suggesting datasets for future data enrichment.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {723–724},
numpages = {2},
keywords = {Machine learning, Lumpy demand, Explainable Artificial Intelligence (XAI), Demand forecasting, Decision support systems, Artificial Intelligence (AI)},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1016/j.compag.2022.107182,
author = {Gautron, Romain and Maillard, Odalric-Ambrym and Preux, Philippe and Corbeels, Marc and Sabbadin, R\'{e}gis},
title = {Reinforcement learning for crop management support: Review, prospects and challenges},
year = {2022},
issue_date = {Sep 2022},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {200},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2022.107182},
doi = {10.1016/j.compag.2022.107182},
journal = {Comput. Electron. Agric.},
month = sep,
numpages = {14},
keywords = {crop management, decision support system, machine learning, multi-armed bandit, reinforcement learning}
}

@article{10.1504/IJAOSE.2017.087656,
title = {Developing ePartners for human-robot teams in space based on ontologies and formal abstraction hierarchies},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {5},
number = {4},
issn = {1746-1375},
url = {https://doi.org/10.1504/IJAOSE.2017.087656},
doi = {10.1504/IJAOSE.2017.087656},
abstract = {Manned space missions are typically performed by teams composed of humans as well as technical systems and are situated in complex, dynamic and safety-critical domains. Intelligent electronic partners ePartners can play an important role here to support human-robot teams in their collaborative problem solving process when things do not go as planned. To engage in effective team collaboration, ePartners, humans and robots must align their communication at the right level of abstraction. In this paper, an approach is put forward to represent the functionality of human-robot teams in a formal manner using abstraction hierarchies. In this way, formal relations between domain knowledge at the system, functional and mission-oriented levels are established and reasoning rules are used to navigate through these relations. As a consequence, ePartners are equipped with the ability to reason about the status of a mission, propose solutions in non-nominal situations and provide explanations for the proposed solutions. The approach has been implemented within a mobile application on a tablet that can be used to support astronaut-robot teams during space missions. The application has been evaluated during an experiment at the European Space Research and Technology Centre ESTEC in the context of a Mars mission.},
journal = {Int. J. Agent-Oriented Softw. Eng.},
month = jan,
pages = {366–398},
numpages = {33}
}

@inproceedings{10.1145/3580305.3599363,
author = {Zhao, Yu and Deng, Pan and Liu, Junting and Jia, Xiaofeng and Zhang, Jianwei},
title = {Generative Causal Interpretation Model for Spatio-Temporal Representation Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599363},
doi = {10.1145/3580305.3599363},
abstract = {Learning, interpreting, and predicting from complex and high-dimensional spatio-temporal data is a natural ability of humans and other intelligent agents, and one of the most important and difficult challenges of AI. Although objects may present different observed phenomena under different situations, their causal mechanism and generation rules are stable and invariant. Different from most existing studies that focus on dynamic correlation, we explore the latent causal structure and mechanism of causal descriptors in the spatio-temporal dimension at the microscopic level, thus revealing the generation principle of observation. In this paper, we regard the causal mechanism as a spatio-temporal causal process modulated by non-stationary exogenous variables. To this end, we propose a theoretically-grounded Generative Causal Interpretation Model (GCIM), which infers explanatory-capable microscopic causal descriptors from observational data via spatio-temporal causal representations. The core of GCIM is to estimate the prior distribution of causal descriptors by using the spatio-temporal causal structure and transition process under the constraints of identifiable conditions, thus extending the Variational Auto Encoder (VAE). Furthermore, our method is able to automatically capture domain information from observations to model non-stationarity. We further analyze the model identifiability, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments on synthetic and real-world datasets show that GCIM can successfully identify latent causal descriptors and structures, and accurately predict future data.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3537–3548},
numpages = {12},
keywords = {spatio-temporal representation learning, generative causal model},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3351095.3372830,
author = {Barocas, Solon and Selbst, Andrew D. and Raghavan, Manish},
title = {The hidden assumptions behind counterfactual explanations and principal reasons},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372830},
doi = {10.1145/3351095.3372830},
abstract = {Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established "principal reason" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant---and withholding others.These "feature-highlighting explanations" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear.In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes.We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden.While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world---and the subjective choices necessary to compensate for this---must be understood before these techniques can be usefully implemented.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {80–89},
numpages = {10},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@article{10.1016/j.neucom.2024.127798,
author = {Oneto, Luca and Ridella, Sandro and Anguita, Davide},
title = {Towards algorithms and models that we can trust: A theoretical perspective},
year = {2024},
issue_date = {Aug 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {592},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2024.127798},
doi = {10.1016/j.neucom.2024.127798},
journal = {Neurocomput.},
month = aug,
numpages = {19},
keywords = {Trustworthy machine learning, Technical metrics, Ethical metrics, Deterministic algorithms and models, Randomized algorithms and models, Generalization, Complexity-based methods, PAC-bayes, Algorithmic distribution stability, Differential privacy}
}

@article{10.1007/s00146-023-01733-x,
author = {de Seta, Gabriele and Shchetvina, Anya},
title = {Imagining machine vision: Four visual registers from the Chinese AI industry},
year = {2023},
issue_date = {Oct 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {39},
number = {5},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-023-01733-x},
doi = {10.1007/s00146-023-01733-x},
abstract = {Machine vision is one of the main applications of artificial intelligence. In China, the machine vision industry makes up more than a third of the national AI market, and technologies like face recognition, object tracking and automated driving play a central role in surveillance systems and social governance projects relying on the large-scale collection and processing of sensor data. Like other novel articulations of technology and society, machine vision is defined, developed and explained by different actors through the work of imagination. In this article, we draw on the concept of sociotechnical imaginaries to understand how Chinese companies represent machine vision. Through a qualitative multimodal analysis of the corporate websites of leading industry players, we identify a cohesive sociotechnical imaginary of machine vision, and explain how four distinct visual registers contribute to its articulation. These four registers, which we call computational abstraction, human–machine coordination, smooth everyday, and dashboard realism, allow Chinese tech companies to articulate their global ambitions and competitiveness through narrow and opaque representations of machine vision technologies.},
journal = {AI Soc.},
month = aug,
pages = {2267–2284},
numpages = {18},
keywords = {Artificial intelligence, China, Machine vision, Sociotechnical imaginaries, Tech industry, Visual culture}
}

@inproceedings{10.1145/3379503.3403554,
author = {Wiegand, Gesa and Eiband, Malin and Haubelt, Maximilian and Hussmann, Heinrich},
title = {“I’d like an Explanation for That!”Exploring Reactions to Unexpected Autonomous Driving},
year = {2020},
isbn = {9781450375160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379503.3403554},
doi = {10.1145/3379503.3403554},
abstract = {Autonomous vehicles are complex systems that may behave in unexpected ways. From the drivers’ perspective, this can cause stress and lower trust and acceptance of autonomous driving. Prior work has shown that explanation of system behavior can mitigate these negative effects. Nevertheless, it remains unclear in which situations drivers actually need an explanation and what kind of interaction is relevant to them. Using thematic analysis of real-world experience reports, we first identified 17 situations in which a vehicle behaved unexpectedly. We then conducted a think-aloud study (N = 26) in a driving simulator to validate these situations and enrich them with qualitative insights about drivers’ need for explanation. We identified six categories to describe the main concerns and topics during unexpected driving behavior (emotion and evaluation, interpretation and reason, vehicle capability, interaction, future driving prediction and explanation request times). Based on these categories, we suggest design implications for autonomous vehicles, in particular related to collaboration insights, user mental models and explanation requests.},
booktitle = {22nd International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {36},
numpages = {11},
keywords = {Unexpected driving behavior, Explainable AI, Autonomous driving.},
location = {Oldenburg, Germany},
series = {MobileHCI '20}
}

@article{10.1016/j.dss.2022.113892,
author = {Schmitz, Hans Christian and Lutz, Bernhard and Wolff, Dominik and Neumann, Dirk},
title = {When machines trade on corporate disclosures: Using text analytics for investment strategies},
year = {2023},
issue_date = {Feb 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {165},
number = {C},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2022.113892},
doi = {10.1016/j.dss.2022.113892},
journal = {Decis. Support Syst.},
month = feb,
numpages = {16},
keywords = {Trading strategies, Corporate disclosures, Text mining, Machine learning, Decision support}
}

@article{10.3233/SW-222993,
author = {Alam, Mehwish and Buscaldi, Davide and Cochez, Michael and Osborne, Francesco and Recupero, Diego Reforgiato and Sack, Harald and Gottschalk, Simon and Demidova, Elena and Alam, Mehwish and Buscaldi, Davide and Cochez, Michael and Osborne, Francesco and Refogiato Recupero, Diego and Sack, Harald},
title = {Tab2KG: Semantic table interpretation with lightweight semantic profiles},
year = {2022},
issue_date = {2022},
publisher = {IOS Press},
address = {NLD},
volume = {13},
number = {3},
issn = {1570-0844},
url = {https://doi.org/10.3233/SW-222993},
doi = {10.3233/SW-222993},
abstract = {Tabular data plays an essential role in many data analytics and machine learning tasks. Typically, tabular data does not possess any machine-readable semantics. In this context, semantic table interpretation is crucial for making data analytics workflows more robust and explainable. This article proposes Tab2KG – a novel method that targets at the interpretation of tables with previously unseen data and automatically infers their semantics to transform them into semantic data graphs. We introduce original lightweight semantic profiles that enrich a domain ontology’s concepts and relations and represent domain and table characteristics. We propose a one-shot learning approach that relies on these profiles to map a tabular dataset containing previously unseen instances to a domain ontology. In contrast to the existing semantic table interpretation approaches, Tab2KG relies on the semantic profiles only and does not require any instance lookup. This property makes Tab2KG particularly suitable in the data analytics context, in which data tables typically contain new instances. Our experimental evaluation on several real-world datasets from different application domains demonstrates that Tab2KG outperforms state-of-the-art semantic table interpretation baselines.},
journal = {Semant. Web},
month = jan,
pages = {571–597},
numpages = {27},
keywords = {one-shot learning, semantic profiles, domain knowledge graphs, Semantic table interpretation}
}

@article{10.1016/j.is.2021.101830,
author = {Mandilaras, George and Papadakis, George and Gagliardelli, Luca and Simonini, Giovanni and Thanos, Emmanouil and Giannakopoulos, George and Bergamaschi, Sonia and Palpanas, Themis and Koubarakis, Manolis and Lara-Clares, Alicia and Fari\~{n}a, Antonio},
title = {Reproducible experiments on Three-Dimensional Entity Resolution with JedAI},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {102},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2021.101830},
doi = {10.1016/j.is.2021.101830},
journal = {Inf. Syst.},
month = dec,
numpages = {12},
keywords = {Reproducibility, Progressive methods, Batch methods, Entity Resolution}
}

@inproceedings{10.1145/3209978.3210013,
author = {Su, Yu and Hassan Awadallah, Ahmed and Wang, Miaosen and White, Ryen W.},
title = {Natural Language Interfaces with Fine-Grained User Interaction: A Case Study on Web APIs},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210013},
doi = {10.1145/3209978.3210013},
abstract = {The rapidly increasing ubiquity of computing puts a great demand on next-generation human-machine interfaces. Natural language interfaces, exemplified by virtual assistants like Apple Siri and Microsoft Cortana, are widely believed to be a promising direction. However, current natural language interfaces provide users with little help in case of incorrect interpretation of user commands. We hypothesize that the support of fine-grained user interaction can greatly improve the usability of natural language interfaces. In the specific setting of natural language interface to web APIs, we conduct a systematic study to verify our hypothesis. To facilitate this study, we propose a novel modular sequence-to-sequence model to create interactive natural language interfaces. By decomposing the complex prediction process of a typical sequence-to-sequence model into small, highly-specialized prediction units called modules, it becomes straightforward to explain the model prediction to the user, and solicit user feedback to correct possible prediction errors at a fine-grained level. We test our hypothesis by comparing an interactive natural language interface with its non-interactive version through both simulation and human subject experiments with real-world APIs. We show that with the interactive natural language interface, users can achieve a higher success rate and a lower task completion time, which lead to greatly improved user satisfaction.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {855–864},
numpages = {10},
keywords = {web api, user feedback, natural language interface},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3447548.3470794,
author = {Pang, Guansong and Aggarwal, Charu},
title = {Toward Explainable Deep Anomaly Detection},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470794},
doi = {10.1145/3447548.3470794},
abstract = {Anomaly explanation, also known as anomaly localization, is as important as, if not more than, anomaly detection in many real-world applications. However, it is challenging to build explainable detection models due to the lack of anomaly-supervisory information and the unbounded nature of anomaly; most existing studies exclusively focus on the detection task only, including the recently emerging deep learning-based anomaly detection that leverages neural networks to learn expressive low-dimensional representations or anomaly scores for the detection task. Deep learning models, including deep anomaly detection models, are often constructed as black boxes, which have been criticized for the lack of explainability of their prediction results. To tackle this explainability issue, there have been numerous techniques introduced over the years, many of which can be utilized or adapted to offer highly explainable detection results.  This tutorial aims to present a comprehensive review of the advances in deep learning-based anomaly detection and explanation. We first review popular state-of-the-art deep anomaly detection methods from different categories of approaches, followed by the introduction of a number of principled approaches used to provide anomaly explanation for deep detection models. Through this tutorial, we aim to promote the development in algorithms, theories and evaluation of explainable deep anomaly detection in the machine learning and data mining community. The slides and other materials of the tutorial are made publicly available at https://tinyurl.com/explainableDeepAD.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4056–4057},
numpages = {2},
keywords = {outlying feature selection, explainable machine learning, deep learning, anomaly localization, anomaly explanation, anomaly detection},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@article{10.1007/s00146-022-01577-x,
author = {Barsotti, Flavia and Ko\c{c}er, R\"{u}ya G\"{o}khan},
title = {MinMax fairness: from Rawlsian Theory of Justice to solution for algorithmic bias},
year = {2022},
issue_date = {Jun 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {39},
number = {3},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-022-01577-x},
doi = {10.1007/s00146-022-01577-x},
abstract = {This paper presents an intuitive explanation about why and how Rawlsian Theory of Justice (Rawls in A theory of justice, Harvard University Press, Harvard, 1971) provides the foundations to a solution for algorithmic bias. The contribution of the paper is to discuss and show why Rawlsian ideas in their original form (e.g. the veil of ignorance, original position, and allowing inequalities that serve the worst-off) are relevant to operationalize fairness for algorithmic decision making. The paper also explains how this leads to a specific MinMaxfairness solution, which addresses the basic challenges of algorithmic justice. We combine substantive elements of Rawlsian perspective with an intuitive explanation in order to provide accessible and practical insights. The goal is to propose and motivate why and how the MinMaxfairness solution derived from Rawlsian principles overcomes some of the current challenges for algorithmic bias and highlight the benefits provided when compared to other approaches. The paper presents and discusses the solution by building a bridge between the qualitative theoretical aspects and the quantitative technical approach.},
journal = {AI Soc.},
month = nov,
pages = {961–974},
numpages = {14},
keywords = {Algorithmic bias, Fairness, Rawlsian&nbsp;Justice, Ethics, AI systems}
}

@inproceedings{10.1145/3311957.3359452,
author = {Bhargava, Rahul and Chung, Anna and Gaikwad, Neil S. and Hope, Alexis and Jen, Dennis and Rubinovitz, Jasmin and Sald\'{\i}as-Fuentes, Bel\'{e}n and Zuckerman, Ethan},
title = {Gobo: A System for Exploring User Control of Invisible Algorithms in Social Media},
year = {2019},
isbn = {9781450366922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311957.3359452},
doi = {10.1145/3311957.3359452},
abstract = {In recent years, there has been an unprecedented growth in content that is shared and presented on social media platforms. Along with this growth, however, there is an increasing concern over the lack of control social media users have on the content they are shown by invisible algorithms. In this paper, we introduce Gobo, an open-source social media browser system that enables users to manage and filter content from multiple platforms on their own. Gobo aims to help users control what's hidden from their feeds, add perspectives from outside their network to help them break filter bubbles, and explore why they see certain content on their feed. Through an iterative design process, we've built and deployed Gobo in the wild and conducted a pilot study in the form of a survey to understand how the users respond to the shift of control from invisible algorithms to themselves. Our initial findings suggest that Gobo has potential to provide an alternate design space to enhance control, transparency, and explainability in social media.},
booktitle = {Companion Publication of the 2019 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {151–155},
numpages = {5},
keywords = {algorithmic accountability, algorithmic transparency},
location = {Austin, TX, USA},
series = {CSCW '19 Companion}
}

@inproceedings{10.1007/978-3-030-10928-8_23,
author = {Yu, Tong and Kveton, Branislav and Wen, Zheng and Bui, Hung and Mengshoel, Ole J.},
title = {: Online Spectral Learning for Single Topic Models},
year = {2018},
isbn = {978-3-030-10927-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-10928-8_23},
doi = {10.1007/978-3-030-10928-8_23},
abstract = {We study the problem of learning a latent variable model online from a stream of data. Latent variable models are popular because they can explain observed data through unobserved concepts. These models have traditionally been studied in the offline setting. In the online setting, online expectation maximization (EM) is arguably the most popular approach for learning latent variable models. Although online EM is computationally efficient, it typically converges to a local optimum. In this work, we develop a new online learning algorithm for latent variable models, which we call .  converges to the global optimum, and we derive a sublinear upper bound on its n-step regret in a single topic model. In both synthetic and real-world experiments, we show that  performs similarly to or better than online EM with tuned hyper-parameters.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10–14, 2018, Proceedings, Part II},
pages = {379–395},
numpages = {17},
keywords = {Online learning, Spectral method, Topic models},
location = {Dublin, Ireland}
}

@article{10.1109/COMST.2024.3412852,
author = {Chaccour, Christina and Saad, Walid and Debbah, M\'{e}rouane and Han, Zhu and Vincent Poor, H.},
title = {Less Data, More Knowledge: Building Next-Generation Semantic Communication Networks},
year = {2025},
issue_date = {Feb. 2025},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {1553-877X},
url = {https://doi.org/10.1109/COMST.2024.3412852},
doi = {10.1109/COMST.2024.3412852},
abstract = {Semantic communication is viewed as a revolutionary paradigm that can potentially transform how we design and operate wireless communication systems. However, despite a recent surge of research activities in this area, remarkably, the research landscape is still limited in at least three ways. First and foremost, the definition of a “semantic communication system” is ambiguous and varies widely between different studies. This lack of consensus makes it challenging to develop rigorous and scalable frameworks for building semantic communication networks. Secondly, current approaches to building semantic communication networks are limited by their reliance on data-driven and information-driven AI-augmented networks. These networks remain “tied” to the data, which limits their ability to perform versatile logic. In contrast, knowledge-driven and reasoning-driven AI-native networks would allow for more flexible and powerful communication capabilities. However, there is currently a lack of technical foundations to support such networks. Thirdly, the concept of “semantic representation” is not well understood yet, and its role in embedding meaning and structure in data transferred across wireless network is still a subject of active research. The development of semantic representations that are minimalist, generalizable, and efficient is critical to enabling the transmitter and receiver to generate content via a minimally semantic representation. To address these limitations, in this tutorial, we propose the first rigorous and holistic vision of an end-to-end semantic communication network that is founded on novel concepts from artificial intelligence (AI), causal reasoning, transfer learning, and minimum description length theory. We first discuss how the design of semantic communication networks requires a move from data-driven AI-augmented networks, in which wireless networks remain “tied” to data, towards reasoning-driven AI-native networks which can perform versatile logic and generalizable intelligence. We then distinguish the concept of semantic communications from several other approaches that have been conflated with it. We opine that building effective and efficient semantic communication systems necessitates surpassing the creation of new encoder and decoder types at the transmitter/receiver side, or developing an “AI for wireless” framework that only extracts application features or fine-tunes wireless protocols/algorithms. Then, we identify the main tenets that are needed to build an end-to-end semantic communication network. Among those building blocks of a semantic communication network, we highlight the necessity of creating semantic representations of data that satisfy the key properties of minimalism, generalizability, and efficiency so as to faithfully represent the data and enable the transmitter and receiver to do more with less. We then explain how those representations can form the basis of a so-called semantic language that will allow a transmitter and receiver to communicate at a semantic level. We then concretely define the concept of reasoning by investigating the fundamentals of causal representation learning and their role in designing reasoning-driven semantic communication networks. For such reasoning-driven networks, we propose novel and essential semantic communication key performance indicators (KPIs) and metrics, including new “reasoning capacity” measures that could surpass Shannon’s bound to capture the imminent convergence of computing and communication resources. Finally, we explain how semantic communications can be scaled to large-scale networks such as 6G and beyond cellular networks. In a nutshell, we expect this tutorial to provide a unified and self-contained reference on how to properly build, design, analyze, and deploy next-generation semantic communication networks.},
journal = {Commun. Surveys Tuts.},
month = feb,
pages = {37–76},
numpages = {40}
}

@inproceedings{10.1609/aaai.v37i13.27057,
author = {Alam, Firoj and Dalvi, Fahim and Durrani, Nadir and Sajjad, Hassan and Khan, Abdul Rafae and Xu, Jia},
title = {ConceptX: a framework for latent concept analysis},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i13.27057},
doi = {10.1609/aaai.v37i13.27057},
abstract = {The opacity of deep neural networks remains a challenge in deploying solutions where explanation is as important as precision. We present ConceptX, a human-in-the-loop framework for interpreting and annotating latent representational space in pre-trained Language Models (pLMs). We use an unsupervised method to discover concepts learned in these models and enable a graphical interface for humans to generate explanations for the concepts. To facilitate the process, we provide auto-annotations of the concepts (based on traditional linguistic ontologies). Such annotations enable development of a linguistic resource that directly represents latent concepts learned within deep NLP models. These include not just traditional linguistic concepts, but also task-specific or sensitive concepts (words grouped based on gender or religious connotation) that helps the annotators to mark bias in the model. The framework consists of two parts (i) concept discovery and (ii) annotation platform.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1988},
numpages = {3},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@phdthesis{10.5555/AAI28771459,
author = {Fu, Zuohui and Hao, Wang, and Qingyao, Ai,},
advisor = {Gerard, de Melo, and Yongfeng, Zhang,},
title = {Towards Human-Centered Recommender Systems},
year = {2021},
isbn = {9798759965664},
publisher = {Rutgers The State University of New Jersey, School of Graduate Studies},
abstract = {Recently, there has been extensive interest in developing intelligent human-centered AI (artificial intelligence) systems that support human participation so as to facilitate cooperation between humans and machines. As one of the typical decision making paradigm in AI, recommender systems which have become an integral part of our lives, are a particularly pervasive form of AI system that can aid in decision-making in the face of ever-growing amounts of information. It now becomes imaginable and achievable with the help of advanced artificial intelligence, especially the modern deep learning based recommender systems that is known for its superior representation and predictive power, have made great strides in accuracy and effectiveness. Meanwhile, it also raises a number of important challenges:1) How can we actively incorporate human participation into the decision-making procedure of recommender systems? It aims to integrate human participation as guidance to keep the decision-making process consistent with human feedback to maintain the trustworthiness to human beings. 2) How can we ensure that explanations are provided such that users can better understand why particular items are being recommended? In this aspect, explainable recommendation can be leveraged to not only assist the agent to provide high-quality recommendation results but also offering personalized and intuitive explanations with better user engagement, which are important for several modern recommender systems such as e-commerce and social media platforms etc. 3) How can we alleviate biases in recommender systems? Seldom progress has been explored to mitigate the biases that arise in human-centered recommender systems so as to hurt user satisfaction and trust towards the recommendation service.In this thesis, we proposes several novel methods to fill these gaps. In particular, for improved human understanding, we introduce an adversarial semantic learning framework for cross-lingual settings understanding. For human integration, a human-in-the-loop conversational recommender system with external graph structure is introduced. To ensure fair explanations, we mitigate the unfairness within graph-based explainable reasoning in the recommender system. Finally, for human-system cooperation, we present a popularity debiasing framework to integrate user interaction and debiased dialogue stat management in a conversational recommender system.We not only extensively evaluate our proposed approaches on multiple real-world recommendation datasets, but also contribute open public datasets to the community. The experimental results demonstrate the effectiveness of the proposed methods in achieving satisfying prediction accuracy, mitigating bias, and providing users with understandable explanations.},
note = {AAI28771459}
}

@inproceedings{10.1007/978-981-96-3522-1_38,
author = {Finkel, Marcel and Timm, Lara and Erle, Lukas and Arntz, Alexander and Helgert, Andr\'{e} and Stra\ss{}mann, Carolin and Eimler, Sabrina C.},
title = {Robot or&nbsp;Employee? Exploring People’s Choice for&nbsp;or&nbsp;Against an&nbsp;Interaction with&nbsp;a&nbsp;Social Robot},
year = {2025},
isbn = {978-981-96-3521-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-96-3522-1_38},
doi = {10.1007/978-981-96-3522-1_38},
abstract = {Employing social robots in public spaces to support employees at work is a frequently discussed scenario. However, the success of robotic systems often depends on people’s willingness to initiate interactions with them. This makes understanding people’s usage decisions crucial, yet only limited research has been done on why people select publicly accessible social robots over alternatives, such as human employees. Amongst various factors, people’s diversity characteristics are likely to influence this decision, such as people’s locus of control when using technology and their self-efficacy in human-robot interaction. To investigate this choice for or against using a robot, a field study (N = 65) was conducted in two public libraries in the Ruhr area (Germany). Participants had to decide to interact with a robot or an employee and were subsequently asked to explain their decision via a questionnaire and an interview. Results reveal that the decision could neither be explained by people’s locus of control when using technology nor by other diversity characteristics. Furthermore, no significant differences in self-efficacy in human-robot interaction between users who chose the robot instead of the human employee were found. Finally, the qualitative findings point to general interest in robots and people’s differences in dealing with novelty as reasons for their choice. Overall, our findings offer insights into the decision for or against the usage of a robot, which are relevant to both, research and the deployment of social robots in public spaces.},
booktitle = {Social Robotics: 16th International Conference, ICSR + AI 2024, Odense, Denmark, October 23–26, 2024, Proceedings, Part I},
pages = {446–459},
numpages = {14},
keywords = {human-robot interaction, use choice, locus of control, self-efficacy, diversity characteristics, field study},
location = {Odense, Denmark}
}

@inproceedings{10.1145/3301275.3302289,
author = {Cai, Carrie J. and Jongejan, Jonas and Holbrook, Jess},
title = {The effects of example-based explanations in a machine learning interface},
year = {2019},
isbn = {9781450362726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301275.3302289},
doi = {10.1145/3301275.3302289},
abstract = {The black-box nature of machine learning algorithms can make their predictions difficult to understand and explain to end-users. In this paper, we propose and evaluate two kinds of example-based explanations in the visual domain, normative explanations and comparative explanations (Figure 1), which automatically surface examples from the training set of a deep neural net sketch-recognition algorithm. To investigate their effects, we deployed these explanations to 1150 users on QuickDraw, an online platform where users draw images and see whether a recognizer has correctly guessed the intended drawing. When the algorithm failed to recognize the drawing, those who received normative explanations felt they had a better understanding of the system, and perceived the system to have higher capability. However, comparative explanations did not always improve perceptions of the algorithm, possibly because they sometimes exposed limitations of the algorithm and may have led to surprise. These findings suggest that examples can serve as a vehicle for explaining algorithmic behavior, but point to relative advantages and disadvantages of using different kinds of examples, depending on the goal.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {258–262},
numpages = {5},
keywords = {machine learning, human-AI interaction, explainable AI, example-based explanations},
location = {Marina del Ray, California},
series = {IUI '19}
}

@inproceedings{10.1145/3531056.3542775,
author = {Ogenrwot, Daniel and Tabo, Geoffrey Olok and Aber, Kevin and Nakatumba-Nabende, Joyce},
title = {From Undergraduate (Software) Capstone Projects to Start-ups: Challenges and Opportunities in Higher Institutions of Learning},
year = {2022},
isbn = {9781450396639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531056.3542775},
doi = {10.1145/3531056.3542775},
abstract = {The capstone project is a fundamental part of almost all science and engineering degrees. It is not only a requirement for the partial fulfillment of an accredited university programme but also a method of assessing the students’ general mastery of concepts, critical thinking, problem-solving, and transferable skills. Annually, final-year undergraduate students offering computing programmes in Uganda build innovative software solutions to real-world problems within and outside their community. Anecdotal evidence indicates that most of those innovations have the potential for commercialization and transformation into technology-based businesses. However, limited progress has been made to commercialize students’ projects, and promising solutions are “buried” within academic reports. To this end, our research aims to explain the challenges and opportunities in the commercialization of students’ capstone projects across two (2) undergraduate computing programmes (Bachelor of Science in Computer Science and Bachelor of Information Technology) offered at Gulu University in Uganda. Using exploratory research design, we reviewed eighty-six (86) capstone projects, curricula, and a facilitated students &amp; stakeholders’ workshop report. This paper articulates factors hindering the commercialization of undergraduate software capstone projects and recommends mitigating measures. It also proposes a framework for extending capstone course design from a traditional curriculum structure to an inclusive industry and community-oriented approach capable of turning ideas into business start-ups. The findings from this research are expected to inform higher institutions of learning in Africa in developing novel pedagogical approaches for orchestrating (software) capstone project courses that are inclusive and profitable beyond the academic setting.},
booktitle = {Proceedings of the Federated Africa and Middle East Conference on Software Engineering},
pages = {73–82},
numpages = {10},
keywords = {Start-ups, Software Engineering, Commercialization, Capstone Projects},
location = {Cairo-Kampala, Egypt},
series = {FAMECSE '22}
}

@phdthesis{10.5555/AAI28869734,
author = {Bansal, Gagan and Jamie, Morgenstern, and Besmira, Nushi,},
advisor = {S, Weld, Daniel},
title = {Three Maxims for Developing Human-Centered AI for Decision Making},
year = {2022},
isbn = {9798780639954},
publisher = {University of Washington},
abstract = {We focus on AI-advised decision making, where AI systems (e.g., classifiers) are deployed to assist users to make better decisions (e.g., in healthcare, finance, and criminal justice). While the dominant development practice deploys the most "accurate" autonomous AI to assist users, we argue that in order for AI to augment users, we should shift the focus of research to developing human-centered AI (HCAI). HCAI systems additional requirements atop those of autonomous AI. They are not just capable but also: trustworthy and dependable, they communicate and coordinate their reasoning with users, and complement users' expertise. We specifically develop and study three relevant maxims for developing HCAI systems: 1) help users understand when to trust AI recommendations, 2) preserve user's mental model of AI's trustworthiness, and 3) train AI to optimize for team performance.Through experiments on various tasks that involve AI-assisted decision making, we show that a) contrary to expectations, current XAI methods may be insufficient for helping users understand when to rely on AI recommendations. b) It is easier for users to create a mental model of AI's trustworthiness when its error boundary (i.e., regions where it errs) is simple and deterministic. c) The current practice of updates to AI systems (e.g., to improve its accuracy) can result in models that violate user trust, e.g., by introducing errors on examples on which the system was previously correct; however, we also show that its possible to create models that preserve trust by considering compatibility of updates during the training process. d) For a simple setting, we formally show that by accommodating the user's mental model in the AI's training process, we can train a model that results in higher a human-AI team performance than the team performance achieved with the most accurate AI. Finally, we discuss open problems and future work in developing HCAI including enabling explanatory dialogs (as opposed to static, one-shot explanations) and enabling user control of AI behavior. Overall, the problems and results in this thesis show the richness and interdisciplinary nature of the challenge of developing human-centered AI.},
note = {AAI28869734}
}

@inproceedings{10.1007/978-3-030-29726-8_1,
author = {Holzinger, Andreas and Kickmeier-Rust, Michael and M\"{u}ller, Heimo},
title = {KANDINSKY Patterns as IQ-Test for Machine Learning},
year = {2019},
isbn = {978-3-030-29725-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29726-8_1},
doi = {10.1007/978-3-030-29726-8_1},
abstract = {AI follows the notion of human intelligence which is unfortunately not a clearly defined term. The most common definition given by cognitive science as mental capability, includes, among others, the ability to think abstract, to reason, and to solve problems from the real world. A hot topic in current AI/machine learning research is to find out whether and to what extent algorithms are able to learn abstract thinking and reasoning similarly as humans can do – or whether the learning outcome remains on purely statistical correlation. In this paper we provide some background on testing intelligence, report some preliminary results from 271 participants of our online study on explainability, and propose to use our Kandinsky Patterns as an IQ-Test for machines. Kandinsky Patterns are mathematically describable, simple, self-contained hence controllable test data sets for the development, validation and training of explainability in AI. Kandinsky Patterns are at the same time easily distinguishable from human observers. Consequently, controlled patterns can be described by both humans and computers. The results of our study show that the majority of human explanations was made based on the properties of individual elements in an image (i.e., shape, color, size) and the appearance of individual objects (number). Comparisons of elements (e.g., more, less, bigger, smaller, etc.) were significantly less likely and the location of objects, interestingly, played almost no role in the explanation of the images. The next step is to compare these explanations with machine explanations.},
booktitle = {Machine Learning and Knowledge Extraction: Third IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE 2019, Canterbury, UK, August 26–29, 2019, Proceedings},
pages = {1–14},
numpages = {14},
keywords = {Artificial intelligence, Human intelligence, Intelligence testing, IQ-Test, Explainable-AI, Interpretable machine learning},
location = {Canterbury, United Kingdom}
}

@inproceedings{10.1145/3240323.3240362,
author = {Kleinerman, Akiva and Rosenfeld, Ariel and Kraus, Sarit},
title = {Providing explanations for recommendations in reciprocal environments},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240362},
doi = {10.1145/3240323.3240362},
abstract = {Automated platforms which support users in finding a mutually beneficial match, such as online dating and job recruitment sites, are becoming increasingly popular. These platforms often include recommender systems that assist users in finding a suitable match. While recommender systems which provide explanations for their recommendations have shown many benefits, explanation methods have yet to be adapted and tested in recommending suitable matches. In this paper, we introduce and extensively evaluate the use of "reciprocal explanations" - explanations which provide reasoning as to why both parties are expected to benefit from the match. Through an extensive empirical evaluation, in both simulated and real-world dating platforms with 287 human participants, we find that when the acceptance of a recommendation involves a significant cost (e.g., monetary or emotional), reciprocal explanations outperform standard explanation methods, which consider the recommendation receiver alone. However, contrary to what one may expect, when the cost of accepting a recommendation is negligible, reciprocal explanations are shown to be less effective than the traditional explanation methods.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {22–30},
numpages = {9},
keywords = {reciprocal recommender systems, online-dating application, explanations},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

@inproceedings{10.5555/3524938.3525201,
author = {Dutta, Sanghamitra and Wei, Dennis and Yueksel, Hazar and Chen, Pin-Yu and Liu, Sijia and Varshney, Kush R.},
title = {Is there a trade-off between fairness and accuracy? a perspective using mismatched hypothesis testing},
year = {2020},
publisher = {JMLR.org},
abstract = {A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {263},
numpages = {11},
series = {ICML'20}
}

@article{10.1016/j.neucom.2021.07.097,
author = {Fdez-S\'{a}nchez, J.A. and Pascual-Triana, J.D. and Fern\'{a}ndez, A. and Herrera, F.},
title = {Learning interpretable multi-class models by means of hierarchical decomposition: Threshold Control for Nested Dichotomies},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {463},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.07.097},
doi = {10.1016/j.neucom.2021.07.097},
journal = {Neurocomput.},
month = nov,
pages = {514–524},
numpages = {11},
keywords = {Hierarchical decomposition, Transparency, Nested Dichotomies, Multi-class classification, Interpretability and explainability}
}

@phdthesis{10.5555/AAI29350071,
author = {Chatterjee, Joyjit},
title = {The Blessings of Explainable Ai in Operations &amp; Maintenance of Wind Turbines},
year = {2021},
publisher = {University of Hull (United Kingdom)},
abstract = {Wind turbines play an integral role in generating clean energy, but regularly suffer from operational inconsistencies and failures leading to unexpected downtimes and significant Operations &amp; Maintenance (O&amp;M) costs. Condition-Based Monitoring (CBM) has been utilised in the past to monitor operational inconsistencies in turbines by applying signal processing techniques to vibration data. The last decade has witnessed growing interest in leveraging Supervisory Control &amp; Acquisition (SCADA) data from turbine sensors towards CBM. Machine Learning (ML) techniques have been utilised to predict incipient faults in turbines and forecast vital operational parameters with high accuracy by leveraging SCADA data and alarm logs. More recently, Deep Learning (DL) methods have outperformed conventional ML techniques, particularly for anomaly prediction. Despite demonstrating immense promise in transitioning to Artificial Intelligence (AI), such models are generally black-boxes that cannot provide rationales behind their predictions, hampering the ability of turbine operators to rely on automated decision making. We aim to help combat this challenge by providing a novel perspective on Explainable AI (XAI) for trustworthy decision support. This thesis revolves around three key strands of XAI - DL, Natural Language Generation (NLG) and Knowledge Graphs (KGs), which are investigated by utilising data from an operational turbine. We leverage DL and NLG to predict incipient faults and alarm events in the turbine in natural language as well as generate human-intelligible O&amp;M strategies to assist engineers in fixing/averting the faults. We also propose specialised DL models which can predict causal relationships in SCADA features as well as quantify the importance of vital parameters leading to failures. The thesis finally culminates with an interactive Question- Answering (QA) system for automated reasoning that leverages multimodal domain-specific information from a KG, facilitating engineers to retrieve O&amp;M strategies with natural language questions. By helping make turbines more reliable, we envisage wider adoption of wind energy sources towards tackling climate change.},
note = {AAI29350071}
}

@article{10.1145/3368270,
author = {Zhuo, Hankz Hankui and Zha, Yantian and Kambhampati, Subbarao and Tian, Xin},
title = {Discovering Underlying Plans Based on Shallow Models},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3368270},
doi = {10.1145/3368270},
abstract = {Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or action models in hand. Previous approaches either discover plans by maximally “matching” observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing action models to best explain the observed actions, assuming that complete action models are available. In real-world applications, however, target plans are often not from plan libraries, and complete action models are often not available, since building complete sets of plans and complete action models are often difficult or expensive. In this article, we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Specifically, we propose two approaches, DUP and RNNPlanner, to discover target plans based on vector representations of actions. DUP explores the EM-style (Expectation Maximization) framework to capture local contexts of actions and discover target plans by optimizing the probability of target plans, while RNNPlanner aims to leverage long-short term contexts of actions based on RNNs (Recurrent Neural Networks) framework to help recognize target plans. In the experiments, we empirically show that our approaches are capable of discovering underlying plans that are not from plan libraries without requiring action models provided. We demonstrate the effectiveness of our approaches by comparing its performance to traditional plan recognition approaches in three planning domains. We also compare DUP and RNNPlanner to see their advantages and disadvantages.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {18},
numpages = {30},
keywords = {shallow model, recurrent neural networks, action representation, Plan recognition}
}

@inproceedings{10.1145/3227609.3227645,
author = {Zupanc, Kaja and Bosni\'{c}, Zoran},
title = {Increasing accuracy of automated essay grading by grouping similar graders},
year = {2018},
isbn = {9781450354899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3227609.3227645},
doi = {10.1145/3227609.3227645},
abstract = {Automated essay evaluation is a widely used practical solution for replacing time-consuming manual grading of student essays. Automated systems are used in combination with human graders in different high-stake assessments, where grading models are learned on essays datasets scored by different graders. Despite the unified grading rules, human graders can unintentionally introduce subjective bias into scores. Consequently, a grading model has to learn from a data that represents a noisy relationship between essay attributes and its grade. We propose an approach for separating a set of essays into subsets that represent similar graders, which uses an explanation methodology and clustering. The results confirm our assumption that learning from the ensemble of separated models can significantly improve the average prediction accuracy on artificial and real-world datasets.},
booktitle = {Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics},
articleno = {35},
numpages = {6},
keywords = {prediction accuracy, explanations of predictions, clustering, automated essay evaluation},
location = {Novi Sad, Serbia},
series = {WIMS '18}
}

@article{10.1016/j.asoc.2017.03.046,
author = {Hassaballah, M. and Ghareeb, A.},
title = {A framework for objective image quality measures based on intuitionistic fuzzy sets},
year = {2017},
issue_date = {August 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {57},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.03.046},
doi = {10.1016/j.asoc.2017.03.046},
abstract = {Graphical abstractDisplay Omitted HighlightsThis paper introduces a framework for using intuitionistic fuzzy sets (IFSs) theory in image processing, specifically in image comparison.Existing similarity measures on the IFSs space are discussed and highlighted their properties.This paper also introduces an intuitionistic fuzzy based image quality index measure.Besides, construction of neighborhood-based similarity is also proposed for improving the perceived visual quality of these IFS-based similarity measures.The proposed framework is tested on real world images under various types of image distortions and the obtained results are encourages. Measuring the distance or similarity objectively between images is an essential and a challenging problem in various image processing and pattern recognition applications. As it is very difficult to find a certain measure that can be successfully applied to all kinds of images comparisons-related problems in the same time, it is appropriate to look for new approaches for measuring the similarity. Several similarity measures tested on numerical cases are developed in the literature based on intuitionistic fuzzy sets (IFSs) without evaluation on real data. This paper introduces a framework for using the similarity measures on IFSs in image processing field, specifically for image comparison. First, some existing similarity measures are discussed and highlighted their properties. Then, modeling digital images using IFSs is explained. Moreover, the paper introduces an intuitionistic fuzzy based image quality index measure. Second, for improving the perceived visual quality of these IFS-based similarity measures, construction of neighborhood-based similarity is proposed, which takes into consideration homogeneity of images. Finally, the proposed framework is verified on real world natural images under various types of image distortions. Experimental results confirm the effectiveness of the proposed framework in measuring the similarity between images.},
journal = {Appl. Soft Comput.},
month = aug,
pages = {48–59},
numpages = {12},
keywords = {Similarity measures, Intuitionistic fuzzy sets, Image similarity, Image processing, Image comparison}
}

@article{10.1287/msom.2023.1226,
author = {Adjerid, Idris and Ayvaci, Mehmet U. S. and \"{O}zer, \"{O}zalp},
title = {Value of Algorithm-Enabled Process Innovation: The Case of Sepsis},
year = {2023},
issue_date = {July-August 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {25},
number = {4},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2023.1226},
doi = {10.1287/msom.2023.1226},
abstract = {Problem definition: Algorithm-enabled decision support has an increasingly important role in supporting the day-to-day operations of healthcare organizations. Yet, fully realizing the value of algorithmic decision support lies critically in the opportunity to re-engineer the related processes and redefine roles in ways that make organizations more effective. We study how and when algorithm-enabled process innovation (AEPI) creates value in light of dynamic operational environments (i.e., workload) and behavioral responses to algorithmic predictions (i.e., algorithmic accuracy). Our context is an AEPI effort around a rule-based decision-support algorithm for early detection of sepsis—a costly condition that is the leading cause of death for hospitalized patients. We collaborated with a large U.S.-based hospital system and examined whether AEPI developed for sepsis care (sepsis AEPI) impacts patient mortality and when this impact is stronger or weaker. Methodology/results: We utilize a rich set of clinical and nonclinical data in empirically examining the impact of sepsis AEPI on patient mortality. We leverage the staggered implementation of sepsis AEPI across hospital units and conduct our estimation on a carefully matched sample. The matching utilizes data on patient vitals and the logic behind the algorithm to create a robust comparison group consisting of patient visits for which sepsis AEPI would have triggered an alert if it had been in place. Our empirical analysis shows that sepsis AEPI reduces the likelihood of death from sepsis (45% relative reduction in mortality risk due to sepsis). A higher-than-usual workload and an increase in the average number of inaccurate alert experience at a hospital unit (e.g., an oncology unit, which provides care for cancer patients), in general, reduces the effectiveness of AEPI. We also identify diminishing mortality benefits over prolonged periods of adoption; evaluation of the moderators over time helps explain this diminishing impact. Managerial implications: Our findings suggest that streamlining sepsis-care processes through a predictive algorithm (i.e., algorithm-based monitoring of real-time patient data and providing predictions, streamlined communication channels for coordinating care for a patient with sepsis prediction, and a more standardized process for sepsis diagnosis and treatment) can reduce the loss of life from sepsis. For the 3,739 sepsis patients in our study period, AEPI’s benefits would translate to 181 lives saved. We show that such value, however, is sensitive to operational and behavioral factors as the algorithm becomes a routine part of the day-to-day operations of the hospital.Funding: Financial support from University Hospitals is gratefully acknowledged.Supplemental Material: The online appendix is available at .},
journal = {Manufacturing &amp; Service Operations Management},
month = jul,
pages = {1545–1566},
numpages = {22},
keywords = {data-driven decisions, process innovation, compliance, algorithms, sepsis, healthcare operations}
}

@inproceedings{10.1145/3397481.3450640,
author = {Subramonyam, Hariharan and Seifert, Colleen and Adar, Eytan},
title = {ProtoAI: Model-Informed Prototyping for AI-Powered Interfaces},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450640},
doi = {10.1145/3397481.3450640},
abstract = {When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. However, AI poses challenges to design due to its dynamic behavior in response to training data, end-user data, and feedback. Designers must consider AI’s uncertainties and offer adaptations such as explainability, error recovery, and automation vs. human task control. Unfortunately, current prototyping tools assume a black-box view of AI, forcing designers to work with separate tools to explore machine learning models, understand model performance, and align interface choices with model behavior. This introduces friction to rapid and iterative prototyping. We propose Model-Informed Prototyping (MIP), a workflow for AIX design that combines model exploration with UI prototyping tasks. Our system, ProtoAI, allows designers to directly incorporate model outputs into interface designs, evaluate design choices across different inputs, and iteratively revise designs by analyzing model breakdowns. We demonstrate how ProtoAI can readily operationalize human-AI design guidelines. Our user study finds that designers can effectively engage in MIP to create and evaluate AI-powered interfaces during AIX design.},
booktitle = {Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {48–58},
numpages = {11},
keywords = {AI-Powered Interfaces, Design-by-Instance, Human-Centered AI},
location = {College Station, TX, USA},
series = {IUI '21}
}

@article{10.1007/s11334-019-00339-1,
author = {Narizzano, Massimo and Pulina, Luca and Tacchella, Armando and Vuotto, Simone},
title = {Property specification patterns at work: verification and inconsistency explanation},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {3–4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-019-00339-1},
doi = {10.1007/s11334-019-00339-1},
abstract = {Property specification patterns (PSPs) have been proposed to ease the formalization of requirements, yet enable automated verification thereof. In particular, the internal consistency of specifications written with PSPs can be checked automatically with the use of, for example, linear temporal logic (LTL) satisfiability solvers. However, for most practical applications, the expressiveness of PSPs is too restricted to enable writing useful requirement specifications, and proving that a set of requirements is inconsistent can be worthless unless a minimal set of conflicting requirements is extracted to help designers to correct a wrong specification. In this paper, we extend PSPs by considering Boolean as well as atomic numerical assertions, we contribute an encoding from extended PSPs to LTL formulas, and we present an algorithm computing inconsistency explanations, i.e., irreducible inconsistent subsets of the original set of requirements. Our extension enables us to reason about the internal consistency of functional requirements which would not be captured by basic PSPs. Experimental results demonstrate that our approach can check and explain (in)consistencies in specifications with nearly two thousand requirements generated using a probabilistic model, and that it enables effective handling of real-world case studies.},
journal = {Innov. Syst. Softw. Eng.},
month = sep,
pages = {307–323},
numpages = {17},
keywords = {Inconsistency explanation, LTL satisfiability checking, Property specifications patterns, Consistency of requirements}
}

@inproceedings{10.1145/3460231.3478880,
author = {Ronen, Royi and Berezin, Hilik and Preizler, Rotem and Kasturi, Gopal and Ezzour, AJ and Bhanavase, Sayalee and Hauon, Edan and Nir, Oron},
title = {Generic Automated Lead Ranking in Dynamics CRM},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3478880},
doi = {10.1145/3460231.3478880},
abstract = {We developed a generic framework which enables Customer Relationship Management (CRM) organizations to deploy an automated ranking system for leads (commonly known as ‘lead scoring’). Leads are records that represent non-customers who might become customers. Lead ranking is a fundamental CRM problem with many flavors. Ranking serves as a prioritization management tool for CRM organizations, with many characteristics similar to those of recommender systems.We present the system with its most recent developments, emphasizing challenges that go beyond the core of the learning algorithm, and that have played an instrumental role in maturing the system into a trustable feature, robust to different types of organizations and datasets. Particularly, we present features which enable Human in the Loop [1], a dominant concept in both configuration and result consumption. Another type of features demonstrates the addition of domain knowledge into the machine learning based process.We present the concepts of feature selection, with and without human help, prediction explanations, insights on model inputs, data quality issues, training for UX consistency, and actionability for each individual prediction.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {757–759},
numpages = {3},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{10.5555/2936924.2937091,
author = {Tian, Xin and Zhuo, Hankz Hankui and Kambhampati, Subbarao},
title = {Discovering Underlying Plans Based on Distributed Representations of Actions},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or domain models in hand. Previous approaches either discover plans by maximally "matching" observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing domain models to best explain the observed actions, assuming complete domain models are available. In real world applications, however, target plans are often not from plan libraries and complete domain models are often not available, since building complete sets of plans and complete domain models are often difficult or expensive. In this paper we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Our approach is capable of discovering underlying plans that are not from plan libraries, without requiring domain models provided. We empirically demonstrate the effectiveness of our approach by comparing its performance to traditional plan recognition approaches in three planning domains.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems},
pages = {1135–1143},
numpages = {9},
keywords = {planning, plan recognition, distributed representation},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@article{10.1504/IJBIS.2016.075256,
author = {Chakravorty, Satya S. and Dulaney, Ronald E. and Franza, Richard M.},
title = {ERP implementation failures: a case study and analysis},
year = {2016},
issue_date = {March 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {21},
number = {4},
issn = {1746-0972},
url = {https://doi.org/10.1504/IJBIS.2016.075256},
doi = {10.1504/IJBIS.2016.075256},
abstract = {Despite the pervasiveness of ERP systems, there is a serious concern regarding the failure of ERP implementations. One explanation for many ERP implementation failures may be 'escalation of commitment'. Escalation of commitment refers to the propensity of decision-makers to continue investing in a failing course of action. Using escalation of commitment as a framework, this research describes an ERP implementation failure in a packaging manufacturing company and makes two significant contributions. First, this research describes the dynamics of 'real world' ERP implementation failures. Second, in studying these dynamics, relevant insights for improving ERP implementation successes are provided for both academicians and practioners.},
journal = {Int. J. Bus. Inf. Syst.},
month = mar,
pages = {462–476},
numpages = {15}
}

@inproceedings{10.1007/978-3-031-04987-3_4,
author = {Li, Xianxue and Song, Tingying},
title = {Establishment and Validation of Flight Crew Training Cost Model},
year = {2022},
isbn = {978-3-031-04986-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-04987-3_4},
doi = {10.1007/978-3-031-04987-3_4},
abstract = {As one of the important part of civil aircraft’s life cycle cost, training cost is the key factor for manufacturer’s consideration of market competitiveness and for airline’s aircraft procurement. A mathematical model of flight crew training cost is established to study the impact of cockpit layout, human machine interface and operational procedure to different type of aircraft’s flight crew training cost. This model is validated by using typical aircraft’s training cost and can be used to explain and predict flight crew’s training time.},
booktitle = {HCI in Mobility, Transport, and Automotive Systems: 4th International Conference, MobiTAS 2022, Held as Part of the 24th HCI International Conference, HCII 2022, Virtual Event, June 26 – July 1, 2022, Proceedings},
pages = {62–71},
numpages = {10},
keywords = {Model, Training cost, Aircraft}
}

@article{10.1007/s10458-019-09406-0,
author = {Shmaryahu, Dorin and Shani, Guy and Hoffmann, J\"{o}rg},
title = {Comparative criteria for partially observable contingent planning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {5},
issn = {1387-2532},
url = {https://doi.org/10.1007/s10458-019-09406-0},
doi = {10.1007/s10458-019-09406-0},
abstract = {In contingent planning under partial observability with sensing actions, agents actively use sensing to discover meaningful facts about the world. The solution can be represented as a plan tree or graph, branching on various possible observations. Typically in contingent planning one seeks a satisfying plan leading to a goal state at each leaf. In many applications, however, one may prefer some satisfying plans to others, such as plans that lead to the goal with a lower average cost. However, methods such as average cost make an implicit assumption concerning the probabilities of outcomes, which may not apply when the stochastic dynamics of the environment are unknown. We focus on the problem of providing valid comparative criteria for contingent plan trees and graphs, allowing us to compare two plans and decide which one is preferable. We suggest a set of such comparison criteria—plan simplicity, dominance, and best and worst plan costs.We also argue that in some cases certain branches of the plan correspond to an unlikely combination of mishaps, and can be ignored, and provide methods for pruning such unlikely branches before comparing the plan graphs. We explain these criteria, and discuss their validity, correlations, and application to real world problems. We also suggest efficient algorithms for computing the comparative criteria where needed. We provide experimental results, showing that existing contingent planners provide diverse plans, that can be compared using these criteria.},
journal = {Autonomous Agents and Multi-Agent Systems},
month = sep,
pages = {481–517},
numpages = {37},
keywords = {Partial observability, Plan tree, Comparative Criteria, Contingent planning, Planning}
}

@inproceedings{10.1007/978-3-031-36049-7_3,
author = {Li, Keli and Li, Guoxin},
title = {Study on the Impact of Service Robot Autonomy on Customer Satisfaction},
year = {2023},
isbn = {978-3-031-36048-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-36049-7_3},
doi = {10.1007/978-3-031-36049-7_3},
abstract = {The rapid development of artificial intelligence technology has accelerated the promotion and application of service robot in the market. Although technology has provided service robot with increasingly autonomous functions, more research is needed on how service robot with different levels of autonomy affects customer satisfaction in service scenarios. Guided by the theory of affordance, this study examined whether service robot operational and decisional autonomy would have effects on customer satisfaction and explored explanatory mechanism. Adopting an experimental vignette method (EVM), the study reveals that direct effect of service robot operational autonomy and indirect effect of decisional autonomy on customer satisfaction, and functional affordance played a positive mediating role in the impact of service robot autonomy on customer satisfaction. The results extend and enrich the relevant literature on human-machine interaction and customer satisfaction research. Our results also provide marketing insights for enterprises to improve autonomous robot design and enhance customer relationships.},
booktitle = {HCI in Business, Government and Organizations: 10th International Conference, HCIBGO 2023, Held as Part of the 25th HCI International Conference, HCII 2023, Copenhagen, Denmark, July 23–28, 2023, Proceedings, Part II},
pages = {30–40},
numpages = {11},
keywords = {Service Robot, Autonomy, Affordance Perception, Functional Affordance, Customer Satisfaction},
location = {Copenhagen, Denmark}
}

@article{10.1016/j.cie.2018.08.011,
author = {Mattsson, Sandra and Fast-Berglund, \r{A}sa and Li, Dan and Thorvald, Peter},
title = {Forming a cognitive automation strategy for Operator 4.0 in complex assembly},
year = {2020},
issue_date = {Jan 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {139},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2018.08.011},
doi = {10.1016/j.cie.2018.08.011},
journal = {Comput. Ind. Eng.},
month = jan,
numpages = {6},
keywords = {Human-automation interaction, Industry 4.0, Operator 4.0, Complexity, Ergonomics, Human factors}
}

@article{10.1007/s10676-025-09821-w,
author = {Saxena, Vageesh and Tam\`{o}-Larrieux, Aurelia and Van Dijck, Gijs and Spanakis, Gerasimos},
title = {Responsible guidelines for authorship attribution tasks in NLP: Responsible guidelines for authorship attribution tasks in NLP},
year = {2025},
issue_date = {Jun 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {1388-1957},
url = {https://doi.org/10.1007/s10676-025-09821-w},
doi = {10.1007/s10676-025-09821-w},
abstract = {Authorship Attribution (AA) approaches in Natural Language Processing (NLP) are important in various domains, including forensic analysis and cybercrime. However, they pose Ethical, Legal, and Societal Implications/Aspects (ELSI/ELSA) challenges that remain underexplored. Inspired by foundational AI ethics guidelines and frameworks, this research introduces a comprehensive framework of responsible guidelines that focuses on AA tasks in NLP, which are tailored to different stakeholders and development phases. These guidelines are structured around four core principles: privacy and data protection, fairness and non-discrimination, transparency and explainability, and societal impact. Furthermore, to illustrate a practical application of our guidelines, we apply them to a recent AA study that targets identifying and linking potential human trafficking vendors. We believe the proposed guidelines can assist researchers and practitioners in justifying their decisions, assisting ethical committees in promoting responsible practices, and identifying ethical concerns related to NLP-based AA approaches. Our study aims to contribute to ensuring the responsible development and deployment of AA tools.},
journal = {Ethics and Inf. Technol.},
month = mar,
numpages = {28},
keywords = {Responsible AI, Authorship attribution (AA), Natural language processing (NLP), Privacy &amp; data protection, Fairness &amp; non-discrimination, Transparency &amp; Explainability, Societal impact}
}

@article{10.1007/s11023-020-09537-4,
author = {van de Poel, Ibo},
title = {Embedding Values in Artificial Intelligence (AI) Systems},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {3},
issn = {0924-6495},
url = {https://doi.org/10.1007/s11023-020-09537-4},
doi = {10.1007/s11023-020-09537-4},
abstract = {Organizations such as the EU High-Level Expert Group on AI and the IEEE have recently formulated ethical principles and (moral) values that should be adhered to in the design and deployment of artificial intelligence (AI). These include respect for autonomy, non-maleficence, fairness, transparency, explainability, and accountability. But how can we ensure and verify that an AI system actually respects these values? To help answer this question, I propose an account for determining when an AI system can be said to embody certain values. This account understands embodied values as the result of design activities intended to embed those values in such systems. AI systems are here understood as a special kind of sociotechnical system that, like traditional sociotechnical systems, are composed of technical artifacts, human agents, and institutions but—in addition—contain artificial agents and certain technical norms that regulate interactions between artificial agents and other elements of the system. The specific challenges and opportunities of embedding values in AI systems are discussed, and some lessons for better embedding values in AI systems are drawn.},
journal = {Minds Mach.},
month = sep,
pages = {385–409},
numpages = {25},
keywords = {Multi-agent system, Norms, Artificial agent, Institution, Value embedding, Sociotechnical system, Ethics, Values, Artificial intelligence}
}

@inproceedings{10.1145/3361331.3361337,
author = {Carmichael, Peter and Morisset, Charles and Gro\ss{}, Thomas},
title = {SHRUBS: simulating influencing human behaviour in security},
year = {2020},
isbn = {9781450372855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361331.3361337},
doi = {10.1145/3361331.3361337},
abstract = {An organisational requirement of no unauthorised personnel permitted in a restricted area may have a security policy such as all employees must wear identification badges and employees must challenge people who are not wearing a badge. An employee's choice to wear/not wear their badge can be strongly related to how they perceive the security policy, which we call the compliance attitude. Peoples behaviour towards a security policy can influence other peoples compliance attitudes, such as challenging those not wearing a badge influences the compliance attitudes of the people who are challenged and those observing the challenge. The exchange of a challenge by social interaction between two or more people can create a social influence, whereby a persons choice to wear their badge is nudged. For the organisation, the problem is assessing how these social influences propagate throughout the compliance attitudes for particular security policies of all employees. We present SHRUBS which is a work in progress. It is a tool evaluating global security compliance attitudes of human agents. SHRUBS simulates behaviour for security policies where social interaction is present. It is built from conclusions in psychology, behavioural economics and human factors in security. SHRUBS takes as input, a list of behaviour parameters describing agent behaviour and returns the global compliance attitude from a set of traces formed through simulation. We demonstrate the application of SHRUBS with a running example to illustrate how one might mitigate against poor compliance attitudes amongst agents. We then go onto discuss the validation possibilities and explain this with a real world data set that we have collected. We envision that future versions of the tool would enable organisations to make more informed security policy decisions about employee behaviour, such as the best behavioural intervention to use.},
booktitle = {Proceedings of the 8th Workshop on Socio-Technical Aspects in Security and Trust},
articleno = {6},
numpages = {11},
location = {San Juan, Puerto Rico},
series = {STAST '18}
}

@phdthesis{10.5555/AAI28753684,
author = {Henriques, Nuno Andrade da Cruz},
advisor = {Ferreira, Coelho, Helder Manuel and Leonel, Garcia-Marques,},
title = {Sensai+Expanse: Prediction of Emotional Valence Changes on Humans in Context by an Artificial Agent Towards Empathy},
year = {2020},
isbn = {9798471139794},
publisher = {Universidade de Lisboa (Portugal)},
abstract = {The field of Cognitive Science is broader enough on the interdisciplinary study of the brain, mind, and intelligence with a scientific research community gaining momentum over the last few decades. Specifically, joining the two fields of psychology and artificial intelligence (AI) one may envision agents, embodied or not, human-like or wearable, with the ability to significantly change the way humans live. This research conceive the artificial agent as a non-anthropomorphic with adaptive empathy for human-agent interaction (HAI) synergy towards better companionship. Therefore, the main objectives of this research are (a) to build a predictive model for each human user on context-based emotional valence changes; and (b) to study the age, gender, and human behaviour neutrality and robustness of the artificial agent regarding the prediction ability. The context include geographically located data from sensors, text sentiment analysis, and human emotional valence self-report, all timestamped events, using a common mobile device such as a smartphone. Also, to analyse and discuss the results on how to leverage such a model to adapt interaction strategies in order to foster higher levels of empathy between a non-anthropomorphic agent and its interacting human. For these goals SensAI+Expanse is developed where SensAI acts as an embodied nearby agent and Expanse encompass the machine learning resources in efficient manner, i.e., a distributed, fault-tolerant, mobile and Cloud-based platform from scratch as a research tool to continuously, online, gather and process data towards automated machine learning (AutoML) and prediction.The study is designed with a methodology in place to avoid the Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies bias. This goal is accomplished by collecting data in the wild and worldwide by making use of the publicly accessible Google Play repository for the Android™ SensAI smartphone application. Eligible participants are diverse in age, gender, and behaviour on self-reporting emotional valence. In order to balance the gender distribution by age a dichotomy approach using age median (M = 34) is used. Regarding participation duration, two thirds (33/49) of the eligible individuals for analysis remain interacting for the required minimum of four weeks. The analysis of the results show evidence of significant behaviour differences between some age and gender combinations regarding self-reported emotional valence. Furthermore, the results from a comparison study between state-of-the-art algorithms revealed Extreme Gradient Boosting on average the best model for prediction (F1 = 0:91) with efficient energy use, and explainable using feature importance inspection. Moreover, the artificial agent remained neutral regarding human demographics and, simultaneously, able to reveal individual idiosyncrasies. Therefore, this research contributions include results with evidence, restricted to population and data samples available, of differences in behaviour amongst some combinations of age ranges versus gender. The main contribution is a novel platform for studies regarding human emotional valence changes in context. This system may complement and supersede (eventually) traditional long-list self-appraisal questionnaires. The SensAI+Expanse platform contributes with several parts such as a mobile device application (SensAI) able to adapt and learn in order to predict emotional valence states with high performance, a cloud computing (Cloud) service (SensAI Expanse) with ready-to-action analysis and processing modules towards AutoML. Additionally, smartphone sensing add a contribution for continuous, non-invasive and personalised health check. In the future, developments about human-agent relationships regarding affective interactions are foreseen. Further, the measurement of empathetic reactions and evaluating outcomes may be used to verify and validate health status thus improving care and significantly change the way humans live.},
note = {AAI28753684}
}

@inproceedings{10.1007/978-3-030-78361-7_29,
author = {Huang, Yu-Hsiung and Chen, Wei-Chun and Hsu, Su-Chu},
title = {Creative Design of Gaussian Sensor System with Encoding and Decoding},
year = {2021},
isbn = {978-3-030-78360-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78361-7_29},
doi = {10.1007/978-3-030-78361-7_29},
abstract = {Stuart Hall proposed the “encoding/decoding model of communication” for the theoretical method of media information production and dissemination in 1973. In 1980, he further proposed the research of classic contemporary culture titled “Encoding/Decoding”, which explained how media producers can “encode” an object, feeling, and ideas. Message in the media to achieve the purpose of disseminating information. In addition, “decoding” is the process and method of how the media message can be perceived by the “receiver” after being transformed and translated. It has been explained that the concept of encoding and decoding has a great influence on the research of different cultural media communication from the analogy to the digital age through&nbsp;many&nbsp;kinds of research. However, with the rapid development of digital media technology, we are faced with the production methods of information in tangible and intangible media, and most of them are translated in virtual form in programming languages or digital symbols. Encoding and decoding of digital symbols and codes has gradually changed the way we understand perception.In this paper, we propose the “Gaussian Sensor System”, which consists of three parts: Gausstoys magnetic sensor module, video/audio encoding and decoding, and interactive installation art. We&nbsp;used damped oscillator magnetic balance and Gausstoys sensor as a tangible user interface (TUI), and integrated the Gaussian sensor into the interactive installation art. When the user intervenes with the floating magnet device and disturbs the magnetic field, the gaussian sensor will “encode” the human analogy behavior. Then the data of human behavior is transformed into visual&nbsp;video and sound feedback. The&nbsp;RGB color&nbsp;of visual&nbsp;video and frequency feedback&nbsp;of audio on the screen is the “decoding” of perception. Therefore, in our Gaussian Sensor System, “balance” is generated through the floating magnetic force in our artwork. After the user “intervenes”, the entire behavior is transformed into a digital reproduction of video/audio and then transmitted to the user the perception feedback of color and sound. Our creative design has been&nbsp;shown&nbsp;to “Tsing Hua Effects 2020: STEM with A” Technology and Art Festival of Tsing Hua University in Taiwan and “Art Gallery, 2016 SIGGRAPH Asia” in Macau. In the past, many applications of Gaussian Sensor were used in interactive games or interactive learning, but our application was in “interactive installation art”. We applied the damped oscillator magnetic balance as a tangible interface device, which is quite rare in HCI applications or interactive art. In the future, the media in the digital age that we are facing will gradually transform real-world cognitions through digital programming languages to produce new perceptions. At present, many kinds of research&nbsp;in HCI&nbsp;field have explained how to experience hearing, taste, and touch in digital media. The encoding and decoding of digital media will be one of the important topics in HCI in the future. Our&nbsp;creative&nbsp;design can be applied to more HCI or TUI research fields in the future&nbsp;and&nbsp;drive&nbsp;users to experience more diverse perceptions through digital media.},
booktitle = {Human Interface and the Management of Information. Information-Rich and Intelligent Environments: Thematic Area, HIMI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II},
pages = {385–395},
numpages = {11},
keywords = {Gaussian Sensor, Encoding and Decoding, Damped oscillation, Tangible User Interface, Interactive installation art}
}

@article{10.1145/3663682.3663686,
author = {Raman, Roopa and Sullivan, Nicholas},
title = {Sharing Workarounds in Health IT-Enabled Patient-Care Work: Impact on Clinicians},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0095-0033},
url = {https://doi.org/10.1145/3663682.3663686},
doi = {10.1145/3663682.3663686},
abstract = {Healthcare providers engaged in health information technology (HIT)-enabled patient-care work tend to resort to workarounds when problems arise in prescribed practices. Hospital administration discourages using such informal workarounds. Yet, due to the interdependent collaborative nature of hospital work, there is a tendency for clinicians to share informally with their colleagues the workarounds that they find useful. In this study, we investigate how the sharing of workarounds impacts the clinicians involved. Our qualitative field study within an inpatient hospital system allowed us to perform rich case studies on a set of four instantiations of the sharing of workarounds in electronic medication administration record (eMAR)-enabled medication administration work. Our findings offer the counterintuitive insight that positive outcomes can be realized from sharing workarounds, even when they are problematic. Our exploratory inductive research model presents a set of five propositions that explain two different ways in which the sharing of workarounds creates positive value for the clinicians involved. We encourage future research to investigate other consequences, and routes leading to those consequences, from the sharing of workarounds in various other IT-enabled contexts. Cumulatively, such research would inform, enrich, and even challenge traditional notions that largely prefer avoiding informal workarounds in the IT-enabled workplace.},
journal = {SIGMIS Database},
month = may,
pages = {42–71},
numpages = {30},
keywords = {electronic medical records, health information technology, it workarounds, patient-care practices, qualitative research}
}

@inproceedings{10.1145/2851581.2892338,
author = {Taamneh, Salah and Dcosta, Malcolm and Kwon, Kyeong-An and Pavlidis, Ioannis},
title = {SubjectBook: Hypothesis-Driven Ubiquitous Visualization for Affective Studies},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2892338},
doi = {10.1145/2851581.2892338},
abstract = {Analyzing affective studies is challenging because they feature multimodal data, such as psychometric scores, imaging sequences, and signals from wearable sensors, with the latter streaming continuously for hours on end. Meaningful visual representations of such data can greatly facilitate insights and qualitative analysis. Various tools that were proposed to tackle this problem provide visualizations of the original data only; they do not support higher level abstractions. In this paper, we introduce SubjectBook, an interactive web-based tool for synchronizing, visualizing, exploring, and analyzing affective datasets. Uniquely, SubjectBook operates at three levels of abstraction, mirroring the stages of quantitative analysis in hypothesis-driven research. The top level uses a grid visualization to show the study's significant outcomes across subjects. The middle level summarizes, for each subject, context information along with the explanatory and response measurements in a construct reminiscent of an ID card. This enables the analyst to appreciate within subject phenomena. Finally, the bottom level brings together detailed information concerning the inner and outer state of human subjects along with their real-world interactions - a visualization fusion that supports cause and effect reasoning at the experimental session level. SubjectBook was evaluated on a case study focused on driving behaviors.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {1483–1489},
numpages = {7},
keywords = {qualitative analysis, physiological signals, data visualization, affective studies, affective datasets, affective computing},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

@inproceedings{10.1145/3686169.3686185,
author = {Ehsan, Upol and Riedl, Mark},
title = {Explainable AI Reloaded: Challenging the XAI Status Quo in the Era of Large Language Models},
year = {2024},
isbn = {9798400710421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686169.3686185},
doi = {10.1145/3686169.3686185},
abstract = {When the initial vision of Explainable (XAI) was articulated, the most popular framing was to open the (proverbial) “black-box” of AI so that we could understand the inner workings. With the advent of Large Language Models (LLMs), the very ability to open the black-box is increasingly limited. Especially when it comes to non-technical end-users. In this paper, we challenge the assumption of “opening” the black-box in the LLM era and argue for a shift in our XAI expectations. Highlighting the epistemic blind spots of an algorithm-centered XAI view, we argue that a human-centered perspective can be a path forward. We operationalize the argument by synthesizing XAI research along three dimensions: explainability outside the black-box, explainability around the edges of the black box, and explainability that leverages infrastructural seams. We conclude with takeaways that reflexively inform XAI as a domain.},
booktitle = {Proceedings of the Halfway to the Future Symposium},
articleno = {8},
numpages = {8},
keywords = {Explainable AI, Generative AI, Large Language Models},
location = {Santa Cruz, CA, USA},
series = {HttF '24}
}

@article{10.1016/j.robot.2022.104170,
author = {Luperto, Matteo and Romeo, Marta and Monroy, Javier and Renoux, Jennifer and Vuono, Alessandro and Moreno, Francisco-Angel and Gonzalez-Jimenez, Javier and Basilico, Nicola and Borghese, N. Alberto},
title = {User feedback and remote supervision for assisted living with mobile robots: A field study in long-term autonomy},
year = {2022},
issue_date = {Sep 2022},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {155},
number = {C},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2022.104170},
doi = {10.1016/j.robot.2022.104170},
journal = {Robot. Auton. Syst.},
month = sep,
numpages = {18},
keywords = {Field study, Long-term autonomy, Socially Assistive Robots}
}

@article{10.1016/j.eswa.2022.117232,
author = {Bravo-Rocca, Gusseppe and Liu, Peini and Guitart, Jordi and Dholakia, Ajay and Ellison, David and Falkanger, Jeffrey and Hodak, Miroslav},
title = {Scanflow: A multi-graph framework for Machine Learning workflow management, supervision, and debugging▪},
year = {2022},
issue_date = {Sep 2022},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {202},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2022.117232},
doi = {10.1016/j.eswa.2022.117232},
journal = {Expert Syst. Appl.},
month = sep,
numpages = {19},
keywords = {Concept drift, Containerization, Robustness, Graph, Symbolic knowledge, Machine Learning}
}

@article{10.1016/j.ijcci.2022.100460,
author = {Szczuka, Jessica M. and Strathmann, Clara and Szymczyk, Natalia and Mavrina, Lina and Kr\"{a}mer, Nicole C.},
title = {How do children acquire knowledge about voice assistants? A longitudinal field study on children’s knowledge about how voice assistants store and process data},
year = {2022},
issue_date = {Sep 2022},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {33},
number = {C},
issn = {2212-8689},
url = {https://doi.org/10.1016/j.ijcci.2022.100460},
doi = {10.1016/j.ijcci.2022.100460},
journal = {Int. J. Child-Comp. Interact.},
month = sep,
numpages = {15},
keywords = {Voice assistants, Child–Computer Interaction, Data storage, Data processing, Secrets}
}

@article{10.1145/3710906,
author = {Darian, Shiva and Robledo Yamamoto, Fujiko and Voida, Amy},
title = {Data Siloing as Infrastructural Activism},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3710906},
doi = {10.1145/3710906},
abstract = {The refugee support ecosystem in the U.S.-Mexico Borderplex is resource-scarce, dynamic, and transitional, requiring intensive information work. To address coordination challenges among the cross-sector sociotechnical community working at one of the largest ports of entry for asylum seekers, the county government of El Paso, TX, USA, has proposed a centralized information system for use by all refugee-serving organizations on the U.S. side of the border. However, stakeholder responses have varied, with some organizations approving, and others resisting the proposal. This research investigates the nuanced dynamics of information infrastructures among stakeholders from different refugee-serving organizations working in the Borderplex, explaining how they navigate pressures to centralize their information infrastructures amid myriad concerns while considering the costs of not doing so, particularly for the refugees. Through a combination of ethnographic fieldwork, semi-structured interviews, and thematic analysis, we explore why some of the organizations in the Borderplex are choosing to silo their data—in support of financial freedom, mission malleability, and maintaining privacy in a liminal context—as a form of infrastructural activism. Our findings contribute to discussions of non-use and deliberate disconnection, highlighting the complex political and practical dimensions of technology (non-)adoption.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW008},
numpages = {24},
keywords = {asylum seekers, database infrastructure, human service organization, immigration, migration, organizational networks, refugees}
}

@inproceedings{10.1145/3357384.3358160,
author = {Lin, Jionghao and Pan, Shirui and Lee, Cheng Siong and Oviatt, Sharon},
title = {An Explainable Deep Fusion Network for Affect Recognition Using Physiological Signals},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358160},
doi = {10.1145/3357384.3358160},
abstract = {Affective computing is an emerging research area which provides insights on human's mental state through human-machine interaction. During the interaction process, bio-signal analysis is essential to detect human affective changes. Currently, machine learning methods to analyse bio-signals are the state of the art to detect the affective states, but most empirical works mainly deploy traditional machine learning methods rather than deep learning models due to the need for explainability. In this paper, we propose a deep learning model to process multimodal-multisensory bio-signals for affect recognition. It supports batch training for different sampling rate signals at the same time, and our results show significant improvement compared to the state of the art. Furthermore, the results are interpreted at the sensor- and signal- level to improve the explainaibility of our deep learning model.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2069–2072},
numpages = {4},
keywords = {multimodal fusion, explainability, deep learning, affect recognition},
location = {Beijing, China},
series = {CIKM '19}
}

@article{10.1016/j.sigpro.2015.08.003,
author = {Velasco, Jose and Mart\'{\i}n-Arguedas, Carlos J. and Macias-Guarasa, Javier and Pizarro, Daniel and Mazo, Manuel},
title = {Proposal and validation of an analytical generative model of SRP-PHAT power maps in reverberant scenarios},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {119},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2015.08.003},
doi = {10.1016/j.sigpro.2015.08.003},
abstract = {The algorithms for acoustic source localization based on PHAT filtering have been profusely used with good results in reverberant and noisy environments. However, there are very few studies that give a formal explanation of their robustness, most of them providing just an empirical validation or showing results on simulated data. In this work we present a novel analytical model for predicting the behavior of both the SRP-PHAT power maps and the GCC-PHAT functions. The results show that they are only affected by the signal bandwidth, the microphone array topology, and the room geometry, being independent of the spectral content of the received signal. The proposed model is shown to be valid in reverberant environments and under far and near field conditions. Using this result, an analysis study on how the aforementioned factors affect the SRP-PHAT power maps is presented providing well supported theoretical and practical considerations. The model validation is based on both synthetic and real data, obtaining in all cases a high accuracy of the model to reproduce the SRP-PHAT power maps, both in anechoic and non-anechoic scenarios, becoming thus an excellent tool to be exploited for the improvement of real world relevant applications related to acoustic localization. HighlightsA novel parametric analytical model to predict SRP-PHAT power maps is formulated.An exhaustive evaluation is done on both synthetic and real data.Results show high accuracy for very different acoustical and geometrical conditions.The paper also addresses practical issues in the model implementation.},
journal = {Signal Process.},
month = feb,
pages = {209–228},
numpages = {20},
keywords = {Steered Response Power (SRP), Phase Transform (PHAT), Microphone arrays, Generalized Cross-Correlation (GCC), Acoustic Source Location (ASL)}
}

@inproceedings{10.1007/978-3-031-70074-3_12,
author = {Speith, Timo and Xu, Jing},
title = {Explainability and&nbsp;Transparency in&nbsp;Practice: A Comparison Between Corporate and&nbsp;National AI Ethics Guidelines in&nbsp;Germany and&nbsp;China},
year = {2024},
isbn = {978-3-031-70073-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-70074-3_12},
doi = {10.1007/978-3-031-70074-3_12},
abstract = {As artificial intelligence (AI) continues to permeate various sectors, ethical concerns, particularly around transparency and explainability, have grown. While research has focused on making AI systems transparent or explainable and exploring the connection between these two principles and other ethical dimensions (e.g., fairness and responsibility), little is known about their practical applications in corporate settings. Against this background, this study conducts a comparative analysis of corporate and national AI ethics guidelines in Germany and China to uncover how transparency and explainability are operationalized in practice. The insights contribute not only to understanding corporate needs concerning transparency and explainability but also reveal distinct approaches taken by countries in fostering ethical AI. Overall, this study informs researchers, industry practitioners, and policymakers about the nuances of ethical AI implementation in global contexts.},
booktitle = {Explainable and Transparent AI and Multi-Agent Systems: 6th International Workshop, EXTRAAMAS 2024, Auckland, New Zealand, May 6–10, 2024, Revised Selected Papers},
pages = {205–223},
numpages = {19},
keywords = {Explainability, Transparency, Explainable AI, Ethical AI, AI Ethics, Ethics Guidelines, Trustworthy AI, Comparative Analysis},
location = {Auckland, New Zealand}
}

@article{10.1007/s00146-021-01258-1,
author = {Paltieli, Guy},
title = {The political imaginary of National AI Strategies},
year = {2022},
issue_date = {Dec 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {37},
number = {4},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-021-01258-1},
doi = {10.1007/s00146-021-01258-1},
abstract = {In the past few years, several democratic governments have published their National AI Strategies (NASs). These documents outline how AI technology should be implemented in the public sector and explain the policies that will ensure the ethical use of personal data. In this article, I examine these documents as political texts and reconstruct the political imaginary that underlies them. I argue that these documents intervene in contemporary democratic politics by suggesting that AI can help democracies overcome some of the challenges they are facing. To achieve this, NASs use different kinds of imaginaries—democratic, sociotechnical and data—that help citizens envision how a future AI democracy might look like. As part of this collective effort, a new kind of relationship between citizens and governments is formed. Citizens are seen as autonomous data subjects, but at the same time, they are expected to share their personal data for the common good. As a result, I argue, a new kind of political imaginary is developed in these documents. One that maintains a human-centric approach while championing a vision of collective sovereignty over data. This kind of political imaginary can become useful in understanding the roles of citizens and governments in this technological age.},
journal = {AI Soc.},
month = dec,
pages = {1613–1624},
numpages = {12},
keywords = {Political theory, Imaginaries, Consent, Big data, AI}
}

@article{10.1145/3440959.3440966,
author = {Kearns, Michael and Roth, Aaron},
title = {Ethical algorithm design},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3440959.3440966},
doi = {10.1145/3440959.3440966},
abstract = {In this letter, we summarize the research agenda that we survey in our recent book The Ethical Algorithm, which is intended for a general, nontechnical audience. At a high level, this research agenda proposes formalizing the ethical and social values that we want our algorithms to maintain --- values including privacy, fairness, and explainability --- and then to embed these social values directly into our algorithms as part of their design. This broad research area is most mature in the area of privacy, specifically differential privacy. It is off to a good start in emerging areas like algorithmic fairness, and seems promising for more nebulous goals like explainability, if only we can find the right definitions. Most work in this area to date analyzes algorithms as isolated components, but game-theoretic and economic analysis will become increasingly important as we try and study the effects of algorithmic interventions in larger sociotechnical systems.},
journal = {SIGecom Exch.},
month = dec,
pages = {31–36},
numpages = {6},
keywords = {privacy, game theory, fairness, explainability}
}

@inproceedings{10.1145/3605390.3605402,
author = {Rezzani, Andrea and De Angeli, Antonella and Men\'{e}ndez Blanco, Mar\'{\i}a and Dorfmann, Max},
title = {The space of user aggression in Human-Robot Interaction},
year = {2023},
isbn = {9798400708060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605390.3605402},
doi = {10.1145/3605390.3605402},
abstract = {Aggression as a social behaviour towards technology has been highlighted in Human-Computer Interaction over a decade ago and recently found a renewed interest in the niche of robot abuse. However, a lack of operationalisation and definition of this behaviour resulted in specific-domain observations that hinder generalisations of the determinants that explain it. This paper presents a theoretically grounded workshop that aims to explore how university students imagine aggression towards robots through speculative design. A total of 69 participants, divided into seven workshops, took part in elaborating stories of aggression towards robots using the inspiration cards “Humans against Robots”. The results highlighted nuances of aggression that do not entirely fit the traditional definitions of psychological aggression, nor the term abuse, prompting a new operationalisation of this concept. Aggression towards robots is presented using four different behaviours: 1) Outburst against annoying tools; 2) Clash with conflicting companions; 3) Oppression of faithful tools; 4) Rebellion against unfriendly companions. These forms are represented in an orthogonal space according to the function of aggression (reactive vs proactive) and the identity attributed to the robot (subject vs object). This paper supports the importance of investigating sociotechnical imaginaries to understand the heterogeneity of behaviours and determinants that can explain aggression against robots. The space of user aggression in Human-Robot Interaction (HRI) allows for distinguishing between the different sociotechnical imaginaries that aggression against robots evokes and that designers and researchers might consider when exploring this topic.},
booktitle = {Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {6},
numpages = {13},
keywords = {Speculative design, Sociotechnical Imaginaries, Robot Abuse, Inspiration cards, Human-Robot Interaction},
location = {Torino, Italy},
series = {CHItaly '23}
}

@article{10.1287/orsc.2021.1560,
author = {Yu, Siyu and Greer, Lindred L.},
title = {The Role of Resources in the Success or Failure of Diverse Teams: Resource Scarcity Activates Negative Performance-Detracting Resource Dynamics in Social Category Diverse Teams},
year = {2023},
issue_date = {January-February 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {34},
number = {1},
issn = {1526-5455},
url = {https://doi.org/10.1287/orsc.2021.1560},
doi = {10.1287/orsc.2021.1560},
abstract = {Increasing the social category diversity of work teams is top of mind for many organizations. However, such efforts may not always be sufficiently resourced, given the numerous resource demands facing organizations. In this paper, we offer a novel take on the relationship between social category diversity and team performance, seeking to understand the role resources may play in both altering and explaining the performance dynamics of diverse teams. Specifically, our resource framework explains how the effects of social category diversity on team performance can be explained by intrateam resource cognitions and behaviors and are dependent on team resource availability. We propose that in the face of scarcity in a focal resource (i.e., budget), diverse (but not homogenous) teams generalize this scarcity perception to fear that all resources (i.e., staff, time, etc.) are scarce, prompting performance-detracting power struggles over resources within the team. We find support for our model in three multimethod team-level studies, including two laboratory studies of interacting teams and a field study of work teams in research and development firms. Our resource framework provides a new lens to study the success or failure of diverse teams by illuminating a previously overlooked danger in diverse teams (negative resource cognitions (scarcity spillover bias) and behaviors (intrateam power struggles)), which offers enhanced explanatory power over prior explanations. This resource framework for the study of team diversity also yields insight into how to remove the roadblocks that may occur in diverse teams, highlighting the necessity of resource sufficiency for the success of diverse teams.},
journal = {Organization Science},
month = jan,
pages = {24–50},
numpages = {27},
keywords = {scarcity, resources, conflict, power, diversity, groups, teams}
}

@inproceedings{10.1145/3657054.3657130,
author = {Lee Hui Shan, Alvina and Shankararaman, Venky and Ouh, Eng Lieh},
title = {Enhancing Government Service Delivery: A Case Study of ACQAR Implementation and Lessons Learned from ChatGPT Integration in a Singapore Government Agency},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657054.3657130},
doi = {10.1145/3657054.3657130},
abstract = {This paper presents the pilot implementation of AI Based Citizen Question-Answer Recommender (ACQAR) as an attempt to enhance citizen service delivery within a Singaporean government agency. Drawing insights from previous studies on the Empath library's use in Service Level Agreement (SLA) prediction and the implementation of the Citizen Question-Answer system (CQAS), we redesigned the pilot system, ACQAR. ACQAR integrates the outputs from Empath X SLA predictor and CQAS as essential inputs to the ChatGPT engine, creating contextually aware responses for customer service officers to use as responses to the citizens.Empath X SLA predictor anticipates the expected service response time based on citizens' emotional states, while CQAS recommends answers for faster and more efficient officer responses. This paper provides a comprehensive blueprint for governments aiming to enhance citizen service delivery by fusing sentiment analysis, SLA prediction, question-answer models, and ChatGPT. The proposed system design aims to revolutionize government-citizen interactions, delivering empathetic, efficient, and tailored responses without violating SLAs.Although the full-scale deployment of ACQAR is pending, this paper outlines a foundational step towards the practical development and implementation of an intelligent system by sharing the trial outcomes of ACQAR. By leveraging ChatGPT, this system holds the potential to significantly enhance citizen satisfaction, foster trust in government services, and strengthen overall government-citizen relationships.Additionally, the paper addresses inherent challenges associated with ChatGPT, including data opacity, potential misinformation, and occasional errors, especially critical in government decision-making. Upholding public administration's core values of transparency and accountability, the paper emphasizes the importance of AI explainability in ChatGPT's adoption within government agencies. Strategies proposed include prompt engineering, data governance, and the adoption of interpretability tools such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) to enhance understanding and align ChatGPT's decision-making processes with these principles.},
booktitle = {Proceedings of the 25th Annual International Conference on Digital Government Research},
pages = {645–653},
numpages = {9},
keywords = {Citizen Services, Information Retrieval, Question Answering, Service Innovation, Text Analytics},
location = {Taipei, Taiwan},
series = {dg.o '24}
}

@article{10.1109/TSE.2022.3220713,
author = {Peng, Kewen and Chakraborty, Joymallya and Menzies, Tim},
title = {FairMask: Better Fairness via Model-Based Rebalancing of Protected Attributes},
year = {2023},
issue_date = {April 2023},
publisher = {IEEE Press},
volume = {49},
number = {4},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2022.3220713},
doi = {10.1109/TSE.2022.3220713},
abstract = {&lt;italic&gt;Context&lt;/italic&gt;: Machine learning software can generate models that inappropriately discriminate against specific protected social groups (e.g., groups based on gender, ethnicity, etc.). Motivated by those results, software engineering researchers have proposed many methods for mitigating those discriminatory effects. While those methods are effective in mitigating bias, few of them can provide explanations on what is the root cause of bias. &lt;italic&gt;Objective&lt;/italic&gt;: We aim to better detect and mitigate algorithmic discrimination in machine learning software problems. &lt;italic&gt;Method&lt;/italic&gt;: Here we propose &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;${{sf FairMask}}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant="sans-serif"&gt;FairMask&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="peng-ieq1-3220713.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, a &lt;italic&gt;model-based&lt;/italic&gt; extrapolation method that is capable of both mitigating bias and explaining the cause. In our &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;${{sf FairMask}}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant="sans-serif"&gt;FairMask&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="peng-ieq2-3220713.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; approach, protected attributes are represented by models learned from the other independent variables (and these models offer extrapolations over the space between existing examples). We then use the extrapolation models to relabel protected attributes later seen in testing data or deployment time. Our approach aims to offset the biased predictions of the classification model by rebalancing the distribution of protected attributes. &lt;italic&gt;Results&lt;/italic&gt;: The experiments of this paper show that, without compromising (original) model performance, &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;${{sf FairMask}}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant="sans-serif"&gt;FairMask&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="peng-ieq3-3220713.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; can achieve significantly better group and individual fairness (as measured in different metrics) than benchmark methods. Moreover, compared to another instance-based rebalancing method, our model-based approach shows faster runtime and thus better scalability. &lt;italic&gt;Conclusion&lt;/italic&gt;: Algorithmic decision bias can be removed via extrapolation that corrects the misleading latent correlation between the protected attributes and other non-protected ones. As evidence for this, our proposed &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;${{sf FairMask}}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant="sans-serif"&gt;FairMask&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="peng-ieq4-3220713.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; is not only performance-wise better (measured by fairness and performance metrics) than two state-of-the-art fairness algorithms. &lt;italic&gt;Reproduction Package&lt;/italic&gt;: In order to better support open science, all scripts and data used in this study are available online at &lt;uri&gt;https://github.com/anonymous12138/biasmitigation&lt;/uri&gt;.},
journal = {IEEE Trans. Softw. Eng.},
month = apr,
pages = {2426–2439},
numpages = {14}
}

@article{10.1109/TITS.2022.3187532,
author = {Bhattacharyya, Kinjal and Laharotte, Pierre-Antoine and Burianne, Arthur and Faouzi, Nour-Eddin El},
title = {Assessing Connected Vehicle’s Response to Green Light Optimal Speed Advisory From Field Operational Test and Scaling Up},
year = {2023},
issue_date = {June 2023},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {1524-9050},
url = {https://doi.org/10.1109/TITS.2022.3187532},
doi = {10.1109/TITS.2022.3187532},
abstract = {What are the main factors related to road or service configuration influencing the response behaviour of connected vehicles? How does it evolve with respect to the Market Penetration Rate (MPR) of connected vehicles? Here are some questions raised by this paper with a focus made on Green Light Optimal Speed Advisory (GLOSA) strategy. Such a system, based on V2I communication, aims at providing speed advice/recommendations when approaching an intersection to adjust speed and enhance fuel consumption. The message is displayed on the Human Machine Interface (HMI) of the connected vehicles and a response is expected from the driver. This paper derives its interest in the response behaviour of the driver to HMI. It develops a two-stage methodology based on (i) Field Operational Test to collect realistic inputs (e.g., response rate, delay, deceleration profile, etc.) and (ii) a simulated environment used for extending the findings to non-observed cases (e.g. higher MPR). Besides, the methodology that is well-fitted for generic evaluation and comparison of pilots sites’ conclusions, one further contribution lies in the process to select the explaining factors. Factors are targeted among features of (i) the road configuration (e.g. number of lanes), (ii) the service configuration (e.g. activation distance), or (iii) the individual route choice and traffic conditions. Among others, it is highlighted that the activation distance plays a significant role in the response behaviour and, depending on the cycle duration, a short activation distance might be completely inefficient, while a true environmental impact requires high MPR.},
journal = {Trans. Intell. Transport. Sys.},
month = jun,
pages = {6725–6736},
numpages = {12}
}

@inproceedings{10.1007/978-3-030-49044-7_18,
author = {Peifer, Corinna and Kluge, Annette and Rummel, Nikol and Kolossa, Dorothea},
title = {Fostering Flow Experience in HCI to Enhance and Allocate Human Energy},
year = {2020},
isbn = {978-3-030-49043-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-49044-7_18},
doi = {10.1007/978-3-030-49044-7_18},
abstract = {Motivation explains the direction, intensity and persistence of human behavior and thus plays a crucial role in the mobilization and allocation of available energy. An experience that occurs during motivated action is flow. Flow is perceived as highly rewarding for its own sake and, thus, in flow all attention is directed towards the task at hand, leading to an experience of absorption. At the same time, attention is shielded from irrelevant stimuli and the activity feels easy and effortless. This suggests that flow is a highly efficient state in terms of energy expenditure. Studies addressing the physiology of flow support this assumption. Accordingly, for an optimal use of energy, it is of interest to promote flow in relevant work processes. In HCI, for example, in production work, flow promotion could be enabled by a real-time measure of the operator’s flow state in combination with automated adjustments in the work system to achieve, sustain, or extend flow. Such a real-time measure should not interrupt a person, as traditional self-report measures do. A combination of physiological measures (e.g., heart rate variability, skin conductance, and blink rate) provides a promising starting point to find such a real-time measure. Automated adjustments first require the identification of design approaches that affect flow within the work system. Using the example of work in manufacturing, the concept of flow, its measurement, and potential design approaches for automated adaptation are presented, and their application in HCI processes is discussed.},
booktitle = {Engineering Psychology and Cognitive Ergonomics. Mental Workload, Human Physiology, and Human Energy: 17th International Conference, EPCE 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings, Part I},
pages = {204–220},
numpages = {17},
keywords = {Human energy, Flow experience, Automated adaptation},
location = {Copenhagen, Denmark}
}

@article{10.4018/IJEBR.349930,
author = {Srivastava, Arpita and Srivastava, Nidhi and Chadha, Priyanka and Gera, Rajat},
title = {Mobile Shopping Apps Adoption: A Systematic Review of Theories and Future Research Directions},
year = {2024},
issue_date = {Jul 2024},
publisher = {IGI Global},
address = {USA},
volume = {20},
number = {1},
issn = {1548-1131},
url = {https://doi.org/10.4018/IJEBR.349930},
doi = {10.4018/IJEBR.349930},
abstract = {In this study, 22 research papers published on consumer behavior and electronic commerce journals were selected and reviewed based on inclusion criteria of high-quality journals; the aim of this exercise was to conduct an analysis of the state-of the-art research in theories used for studies of MSA. Through this exercise, 22 consumer-level theories were grouped into seven categories. For each theory, its definition, current application in m-shopping apps (MSA) adoption research and suggestions for future research are presented. The review indicates the lack of a universal model of consumer and the predominant effect of cognitive factors i.e., perceived ease of use and utilitarian benefits in explaining usage and adoption intentions. The effects of hedonic benefits, experiential constructs and social influence are contextual and not homogenous. Most of the studies have adopted the behavioral theories of technology adoption by extending or modifying them or as components of multi theoretic approach. A relative lack of theories from socio-psychological, relationship marketing, experiential and emotion dominant disciplines is noted. There is an over-reliance on theories related to information systems, consumer behavior, and predominance of quantitative research-based techniques. Most of the studies focus on consumers pre-adoption intentions compared to usage behavior and consumer loyalty and fail to address the challenges of consumer retention, in-app purchase and consumer loyalty. A relative lack of cross-cultural and cross-country comparative studies is noted. A summary of theories applied and proposed for this study are presented along with proposed research questions. This review provides practitioners and policy makers with a framework of antecedent factors which can be applied for effective marketing strategies according to the stage of consumer adoption of MSA. While keeping in mind specific findings related to this literature review as well as the necessity to guide research in the future that deals with mobile and technology utilization in MSA, a research agenda is introduced.},
journal = {Int. J. E-Bus. Res.},
month = jul,
pages = {1–26},
numpages = {26},
keywords = {Mobile Shopping Apps, Adoption Behavior, Consumer Behavior and Continuous Usage}
}

@article{10.1007/s10606-023-09478-3,
author = {Farshchian, Babak A. and Mikalsen, Marius},
title = {Using a Service Lens to Better Understand Practices –and Vice Versa},
year = {2023},
issue_date = {Sep 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {3},
issn = {0925-9724},
url = {https://doi.org/10.1007/s10606-023-09478-3},
doi = {10.1007/s10606-023-09478-3},
abstract = {Many studies of practices involve service exchange, and many service researchers have discovered the central role that sociotechnical practices play in service –in particular, within the service-dominant logic school of thought. In this paper, we propose an analytical lens that builds on this mutual interest to understand complex practices involving service exchange. Practice researchers can gain new insights regarding practices embedded in service ecosystems. At the same time, service researchers can better explain actor behavior by looking deeper at sociotechnical practices. We develop a concept toolbox based on practice and service-dominant logic research literature. We illustrate the usefulness of the toolbox through an interpretative case study of public service to include children with disabilities in leisure activities. Seeing practices as parts of larger multi-stakeholder service ecosystems 1) can help us better explain behavior in those practices and understand how they are affected by other overlapping practices, 2) brings forward the importance of value and how multiple actors need to interact in order to create value for each other, and 3) enriches service-dominant logic with a focus on sociotechnical aspects that are central to many practice studies.},
journal = {Comput. Supported Coop. Work},
month = aug,
pages = {499–551},
numpages = {53},
keywords = {Practice, Service, Value, Service ecosystem, Boundary resource, Practice-centered computing, Service-dominant logic, S-D logic, Value co-creation, Resource integration, Ecosystem, Social inclusion, Disability, Children}
}

@inproceedings{10.1145/3375627.3375872,
author = {Osoba, Osonde A. and Boudreaux, Benjamin and Yeung, Douglas},
title = {Steps Towards Value-Aligned Systems},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375872},
doi = {10.1145/3375627.3375872},
abstract = {Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are now indispensable tools that help us manage the flood of information we use to try to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the body of research focused on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment so far has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {332–336},
numpages = {5},
keywords = {ml fairness, sociotechnical systems, systems analysis, value alignment},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3295750.3298972,
author = {Kilian, Melanie A.},
title = {Where to Go and What to Do: Towards Understanding Task-Based Information Behavior at Transitional Spaces},
year = {2019},
isbn = {9781450360258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295750.3298972},
doi = {10.1145/3295750.3298972},
abstract = {In public transitional spaces, such as airports, users are faced with diverse challenges regarding information interaction and use. These challenges arise due to the scheduled and/or location-dependent procedures users are required to perform. Understanding what these users need or desire in the context of such spaces, what information is on offer, both online and in situ, and how these aspects interrelate is important to facilitate the design of systems that are accepted by the users concerned. However, very little is known about human information behavior (HIB) in public transitional spaces. As a starting point to understand how behavior in such spaces relates to or differs from information behavior in other contexts, holistically, I will create an explanatory model of airport information behavior by conducting an exploratory grounded theory based field study and relating my findings to those of existing models.},
booktitle = {Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
pages = {413–416},
numpages = {4},
keywords = {models of information behavior, information tasks, information needs, information behavior, grounded theory, field study},
location = {Glasgow, Scotland UK},
series = {CHIIR '19}
}

@article{10.1007/s10676-025-09826-5,
author = {Hammerschmidt, Teresa},
title = {Navigating the Nexus of ethical standards and moral values},
year = {2025},
issue_date = {Jun 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {1388-1957},
url = {https://doi.org/10.1007/s10676-025-09826-5},
doi = {10.1007/s10676-025-09826-5},
abstract = {This study examines how ethical standards established by stakeholders such as developers and policymakers provide top-down guidance aligned with deontological ethics or utilitarian goals. It also highlights a complementary bottom-up approach, rooted in virtue ethics, in which individuals engage in ethical deliberations shaped by their moral values. Both approaches have limitations, and, at times, ethical standards can clash with moral values, thus blurring lines of responsibilities. Deontological principles may offer a structured framework, but often lack adaptability to diverse cultural contexts; bottom-up approaches foster intrinsic moral intentions, but universal applicability may be challenging, thus raising moral dilemmas. Through a theoretical literature review, this study explains how different ontological and normative ethical perceptions lead to moral dilemmas in various AI application scenarios (e.g., algorithmically managed platforms, crime detection systems, medical AI assistants). It addresses top-down and bottom-up approaches that may help account for moral dilemmas ethically. The study discusses the balance between top-down regulatory frameworks and bottom-up community-driven ethics to navigate the complex ethical landscape of AI applications, whose increasing capabilities alter expectations of AI’s agency and morality. This study calls for holistic and multi-objective ethical frameworks that incorporate diverse normative ethical perspectives and recognizes context-specific ontologies throughout the AI lifecycle. It emphasizes a nuanced and context-specific combination of top-down standards (e.g., regulatory oversight, clear guidelines) and bottom-up fostering of moral values (e.g., by improving ethical knowledge). This tailored and ongoing reflection of ethical standards and moral values accounts for an ethical development, deployment, and utilization of AI technologies.},
journal = {Ethics and Inf. Technol.},
month = mar,
numpages = {19},
keywords = {Literature review, AI ethics, AI governance, Normative ethics}
}

@inproceedings{10.1007/978-3-030-90963-5_36,
author = {L\'{o}pez-Gonz\'{a}lez, M\'{o}nica},
title = {Applying Human Cognition to Assured Autonomy},
year = {2021},
isbn = {978-3-030-90962-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90963-5_36},
doi = {10.1007/978-3-030-90963-5_36},
abstract = {The scaled deployment of semi- and fully autonomous systems undeniably depends on assured autonomy. This reality, however, has become far more complex than expected because it necessarily demands an integrated tripartite solution not yet achieved: consensus-based standards and compliance across industry, scientific innovation within artificial intelligence R&amp;D of explainability, and robust end-user education. In this is paper I present my human-centered approach to the design, development, and deployment of autonomous systems and break down how human factors such as cognitive and behavioral insights into how we think, feel, act, plan, make decisions, and problem-solve are foundational to assuring autonomy.},
booktitle = {HCI International 2021 - Late Breaking Papers: Multimodality, EXtended Reality, and Artificial Intelligence: 23rd HCI International Conference, HCII 2021,  Virtual Event, July 24–29, 2021, Proceedings},
pages = {474–488},
numpages = {15},
keywords = {Autonomous vehicles, Explainability, Trust, Human factors, Artificial intelligence, Assured autonomy}
}

@article{10.1080/13600869.2017.1298547,
author = {Vedder, Anton and Naudts, Laurens},
title = {Accountability for the use of algorithms in a big data environment},
year = {2017},
issue_date = {July 2017},
publisher = {Routledge},
address = {USA},
volume = {31},
number = {2},
issn = {1360-0869},
url = {https://doi.org/10.1080/13600869.2017.1298547},
doi = {10.1080/13600869.2017.1298547},
abstract = {Accountability is the ability to provide good reasons in order to explain and to justify actions, decisions and policies for a hypothetical forum of persons or organisations. Since decision-makers, both in the private and in the public sphere, increasingly rely on algorithms operating on Big Data for their decision-making, special mechanisms of accountability concerning the making and deployment of algorithms in that setting become gradually more urgent. In the upcoming General Data Protection Regulation, the importance of accountability and closely related concepts, such as transparency, as guiding protection principles, is emphasised. Yet, the accountability mechanisms inherent in the regulation cannot be appropriately applied to algorithms operating on Big Data and their societal impact. First, algorithms are complex. Second, algorithms often operate on a random group level, which may pose additional difficulties when interpreting and articulating the risks of algorithmic decision-making processes. In light of the possible significance of the impact on human beings, the complexities and the broader scope of algorithms in a big data setting call for accountability mechanisms that transcend the mechanisms that are now inherent in the regulation.},
journal = {Int. Rev. Law Comput. Technol.},
month = may,
pages = {206–224},
numpages = {19}
}

@article{10.1016/j.inffus.2024.102321,
author = {Wang, Xiao and Wang, Yutong and Yang, Jing and Jia, Xiaofeng and Li, Lijun and Ding, Weiping and Wang, Fei-Yue},
title = {The survey on multi-source data fusion in cyber-physical-social systems: Foundational infrastructure for industrial metaverses and industries 5.0},
year = {2024},
issue_date = {Jul 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {107},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2024.102321},
doi = {10.1016/j.inffus.2024.102321},
journal = {Inf. Fusion},
month = jul,
numpages = {16},
keywords = {Multi-source data fusion, CPSS, Industrial metaverses, Parallel manufacturing, Social manufacturing}
}

@inproceedings{10.1145/3593013.3594033,
author = {Chan, Alan and Salganik, Rebecca and Markelius, Alva and Pang, Chris and Rajkumar, Nitarshan and Krasheninnikov, Dmitrii and Langosco, Lauro and He, Zhonghao and Duan, Yawen and Carroll, Micah and Lin, Michelle and Mayhew, Alex and Collins, Katherine and Molamohammadi, Maryam and Burden, John and Zhao, Wanru and Rismani, Shalaleh and Voudouris, Konstantinos and Bhatt, Umang and Weller, Adrian and Krueger, David and Maharaj, Tegan},
title = {Harms from Increasingly Agentic Algorithmic Systems},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594033},
doi = {10.1145/3593013.3594033},
abstract = {Research in Fairness, Accountability, Transparency, and Ethics (FATE)1 has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed, typically without strong regulatory barriers, threatening the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms, rather than just responding to them. Anticipation of harms is especially important given the rapid pace of developments in machine learning (ML). Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency – notably, these include systemic and/or long-range impacts, often on marginalized or unconsidered stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {651–666},
numpages = {16},
keywords = {FATE, agency, algorithmic systems, autonomy, delayed impacts, ethics, harms, negative externalities, power, safety, sociotechnical systems},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@inproceedings{10.1145/3078072.3084338,
author = {Alarcon-Licona, Susana and Loke, Lian},
title = {Autistic Children's Use of Technology and Media: A Fieldwork Study},
year = {2017},
isbn = {9781450349215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078072.3084338},
doi = {10.1145/3078072.3084338},
abstract = {This qualitative field study conducted at a school for autistic children aimed to explain how low and medium functioning autistic children use technology at school. Additionally, it explored education professionals and parents' attitudes and concerns regarding technology use by children. Data-collection methods included focus groups with nine education professionals, naturalistic observations of fifteen children with autism in therapy sessions, and interviews with parents of four of the children. Results revealed that (1) children are regular users of digital technology and media, (2) there are different patterns of use of technologies with possible correlations to accessibility and behavioural manifestations of autism, (3) education professionals and parents have positive attitude towards the use of technology by the children, but also recognize disadvantages to its use.},
booktitle = {Proceedings of the 2017 Conference on Interaction Design and Children},
pages = {651–658},
numpages = {8},
keywords = {media, interactive technology, field study, digital technology, children, autism, assistive technology},
location = {Stanford, California, USA},
series = {IDC '17}
}

@article{10.4018/IJT.291553,
author = {Matteucci, Matteo and Fossa, Fabio and Arrigoni, Stefano and Caruso, Giandomenico and Cholakkal, Hafeez Husain and Dahal, Pragyan and Cheli, Federico},
title = {Operationalizing the Ethics of Connected and Automated Vehicles: An Engineering Perspective},
year = {2022},
issue_date = {Oct 2022},
publisher = {IGI Global},
address = {USA},
volume = {13},
number = {1},
issn = {1947-3451},
url = {https://doi.org/10.4018/IJT.291553},
doi = {10.4018/IJT.291553},
abstract = {In response to the many social impacts of automated mobility, in September 2020 the European Commission published Ethics of Connected and Automated Vehicles, a report in which recommendations on road safety, privacy, fairness, explainability, and responsibility are drawn from a set of eight overarching principles. This paper presents the results of an interdisciplinary research where philosophers and engineers joined efforts to operationalize the guidelines advanced in the report. To this aim, we endorse a function-based working approach to support the implementation of values and recommendations into the design of automated vehicle technologies. Based on this, we develop methodological tools to tackle issues related to personal autonomy, explainability, and privacy as domains that most urgently require fine-grained guidance due to the associated ethical risks. Even though each tool still requires further inquiry, we believe that our work might already prove the productivity of the function-based approach and foster its adoption in the CAV scientific community.},
journal = {Int. J. Technoethics},
month = feb,
pages = {1–20},
numpages = {20},
keywords = {Privacy, Principles, Explainability, Ethics, Connected and Automated Vehicles, Autonomy, Application}
}

@inproceedings{10.1007/978-3-031-43895-0_17,
author = {Qian, Yiming and Li, Liangzhi and Fu, Huazhu and Wang, Meng and Peng, Qingsheng and Tham, Yih Chung and Cheng, Chingyu and Liu, Yong and Goh, Rick Siow Mong and Xu, Xinxing},
title = {Category-Independent Visual Explanation for&nbsp;Medical Deep Network Understanding},
year = {2023},
isbn = {978-3-031-43894-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43895-0_17},
doi = {10.1007/978-3-031-43895-0_17},
abstract = {Visual explanations have the potential to improve our understanding of deep learning models and their decision-making process, which is critical for building transparent, reliable, and trustworthy AI systems. However, existing visualization methods have limitations, including their reliance on categorical labels to identify regions of interest, which may be inaccessible during model deployment and lead to incorrect diagnoses if an incorrect label is provided. To address this issue, we propose a novel category-independent visual explanation method called Hessian-CIAM. Our algorithm uses the Hessian matrix, which is the second-order derivative of the activation function, to weigh the activation weight in the last convolutional layer and generate a region of interest heatmap at inference time. We then apply an SVD-based post-process to create a smoothed version of the heatmap. By doing so, our algorithm eliminates the need for categorical labels and modifications to the deep learning model. To evaluate the effectiveness of our proposed method, we compared it to seven state-of-the-art algorithms using the Chestx-ray8 dataset. Our approach achieved a 55% higher IoU measurement than classical GradCAM and a 17% higher IoU measurement than EigenCAM. Moreover, our algorithm obtained a Judd AUC score of 0.70 on the glaucoma retinal image database, demonstrating its potential applicability in various medical applications. In summary, our category-independent visual explanation method, Hessian-CIAM, generates high-quality region of interest heatmaps that are not dependent on categorical labels, making it a promising tool for improving our understanding of deep learning models and their decision-making process, particularly in medical applications.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part II},
pages = {181–191},
numpages = {11},
location = {Vancouver, BC, Canada}
}

@phdthesis{10.5555/AAI28114747,
author = {Lakkaraju, Katyaini},
advisor = {Jure, Leskovec, and Percy, Liang, and Cynthia, Rudin,},
title = {Human-Centric Machine Learning: Enabling Machine Learning for High-Stakes Decision-Making},
year = {2018},
isbn = {9798662539532},
publisher = {Stanford University},
address = {Stanford, CA, USA},
abstract = {Domains such as law, healthcare, and public policy often involve highly consequential decisions which are predominantly made by human decision-makers. The growing availability of data pertaining to such decisions offers an unprecedented opportunity to develop machine learning models which can help humans in making better decisions. However, the applicability of machine learning to such scenarios is limited by certain fundamental challenges: a) The data is selectively labeled i.e., we only observe the outcomes of the decisions made by human decision-makers and not the counterfactuals. b) The data is prone to a variety of selection biases and confounding effects. c) The successful adoption of the models that we develop depends on how well decision-makers can understand and trust their functionality, however, most of the existing machine learning models are primarily optimized for predictive accuracy and are not very interpretable. In this dissertation, we develop novel computational frameworks which address the aforementioned challenges, thus, paving the way for large-scale deployment of machine learning models and algorithms to address problems of significant societal impact. We first discuss how to build interpretable predictive models and explanations of complex black box models which can be readily understood and consequently trusted by human decision-makers. We then outline novel evaluation strategies which allow us to reliably compare the quality of human and algorithmic decision-making while accounting for challenges such as selective labels and confounding effects. Lastly, we present approaches which can diagnose and characterize biases (systematic errors) in human decisions and algorithmic predictions.},
note = {AAI28114747}
}

@article{10.14778/3415478.3415570,
author = {Stoyanovich, Julia and Howe, Bill and Jagadish, H. V.},
title = {Responsible data management},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415570},
doi = {10.14778/3415478.3415570},
abstract = {The need for responsible data management intensifies with the growing impact of data on society. One central locus of the societal impact of data are Automated Decision Systems (ADS), socio-legal-technical systems that are used broadly in industry, non-profits, and government. ADS process data about people, help make decisions that are consequential to people's lives, are designed with the stated goals of improving efficiency and promoting equitable access to opportunity, involve a combination of human and automated decision making, and are subject to auditing for legal compliance and to public disclosure. They may or may not use AI, and may or may not operate with a high degree of autonomy, but they rely heavily on data.In this article, we argue that the data management community is uniquely positioned to lead the responsible design, development, use, and oversight of ADS. We outline a technical research agenda that requires that we step outside our comfort zone of engineering for efficiency and accuracy, to also incorporate reasoning about values and beliefs. This seems high-risk, but one of the upsides is being able to explain to our children what we do and why it matters.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3474–3488},
numpages = {15}
}

@article{10.1145/3609861,
author = {Cetina Presuel, Rodrigo and Martinez Sierra, Jose M.},
title = {The Adoption of Artificial Intelligence in Bureaucratic Decision-making: A Weberian Perspective},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
url = {https://doi.org/10.1145/3609861},
doi = {10.1145/3609861},
abstract = {This work questions AI´s role in bureaucratic decision-making. The Weberian conception of bureaucracy, based around the concept of an ideal bureaucracy in which authority is distributed, delegated, clearly delimited and hierarchical and that enshrines the following of formal rules, task specialization through division of labor, legal certainty and a predilection for efficiency in recordable and accountable decisions can serve as a framework to orient how governments should approach the adoption of artificial intelligence (AI) given the many problems associated with its careless deployment. Using theoretical analysis, this work explains Weberian ideas of bureaucracy and contrasts them with real-life cases of implementation of AI in bureaucratic decision-making, often with detrimental results for society. After identifying and framing issues related to AI, e.g., lack of transparency, attempts to shift accountability from humans to technology, the exacerbation of bias and potential for systemic discrimination, the paper proposes Weberian prescriptions that should help public administration make careful decisions about the adoption of AI and the consequences of its implementation. The article also engages with Weber critically, rejecting the notion that public administrators do not engage in politics and asserting that AI decision-making is necessarily political as well, as it entails exercising power over citizens.},
journal = {Digit. Gov.: Res. Pract.},
month = mar,
articleno = {6},
numpages = {20},
keywords = {Weberian bureaucracy, Digital Government, artificial intelligence, automated decision-making, Al bias}
}

@inproceedings{10.1007/978-981-99-8718-4_4,
author = {Reimann, Merle and van de Graaf, Jesper and van Gulik, Nina and van de Sanden, Stephanie and Verhagen, Tibert and Hindriks, Koen},
title = {Social Robots in&nbsp;the&nbsp;Wild and&nbsp;the&nbsp;Novelty Effect},
year = {2023},
isbn = {978-981-99-8717-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-8718-4_4},
doi = {10.1007/978-981-99-8718-4_4},
abstract = {We designed a wine recommendation robot and deployed it in a small supermarket. In a study aimed to evaluate our design we found that people with no intent to buy wine were interacting with the robot rather than the intended audience of wine-buying customers. Behavioural data, moreover, suggests a very different evaluation of the robot than the surveys that were completed. We also found that groups were interacting more with the robot than individuals, a finding that has been reported more often in the literature. All of these findings taken together suggest that a novelty effect may have been at play. It also suggests that field studies should take this effect more seriously. The main contribution of our work is in identifying and proposing a set of indicators and thresholds that can be used to identify that a novelty effect is present. We argue that it is important to focus more on measuring attitudes towards robots that may explain behaviour due to novelty effects. Our findings also suggest research should focus more on verifying whether real user needs are met.},
booktitle = {Social Robotics: 15th International Conference, ICSR 2023, Doha, Qatar, December 3–7, 2023, Proceedings, Part II},
pages = {38–48},
numpages = {11},
keywords = {Retail, Socially Assistive Robots, Novelty effect indicators},
location = {Doha, Qatar}
}

@article{10.1145/3708504,
author = {Tsakalakis, Niko and Stalla-Bourdillon, Sophie and Huynh, Dong and Moreau, Luc},
title = {A typology of explanations to support Explainability-by-Design},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3708504},
doi = {10.1145/3708504},
abstract = {As automated decision-making permeates almost all aspects of everyday life, capabilities to generate meaningful explanations for various stakeholders (i.e., decision-makers, addressees of decisions including individuals, auditors, and regulators) should be carefully deployed. This article presents a typology of explanations intended to support the first pillar of an explainability-by-design strategy. Its production has been achieved by pursuing a responsible innovation approach and introducing a new persona within the research and innovation process, i.e., a legal engineer, whose role is to work at the interface of two teams, the compliance and the engineering teams, and to oversee the process of requirement elicitation, which is often opinionated and narrowing. Once explanation requirements have been derived from applicable regulatory requirements, compliance rules, or business policies, they have been mapped to the dimensions of the typology to produce fine-grained explanation requirements, forming computable building blocks that can then be translated into system requirements during the technical design phase. The typology has been co-created with industry partners operating in two sectors: finance and education. Two pilot studies have thus been conducted to test both the feasibility of the generation and computation of explanations on the basis of the typology and the usefulness of the outputs in the light of the state-of-the-art. The typology comprises nine hierarchical dimensions. It can be leveraged to operate a stand-alone classifier of explanations that acts as detective controls within a broader partially automated compliance strategy. A machine-readable format of the typology is provided in the form of a light ontology.},
journal = {ACM J. Responsib. Comput.},
month = feb,
articleno = {1},
numpages = {36},
keywords = {Artificial intelligence, explainability, typology, data protection, automated decisions}
}

@article{10.1016/j.eswa.2023.121220,
author = {Giudici, Paolo and Centurelli, Mattia and Turchetta, Stefano},
title = {Artificial Intelligence risk measurement},
year = {2024},
issue_date = {Jan 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {235},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2023.121220},
doi = {10.1016/j.eswa.2023.121220},
journal = {Expert Syst. Appl.},
month = jan,
numpages = {9},
keywords = {Machine learning, Sustainability, Accuracy, Fairness, Explainability, Financial risk management}
}

@article{10.1016/j.dss.2024.114215,
author = {Ojo, Adegboyega and Rizun, Nina and Walsh, Grace and Mashinchi, Mona Isazad and Venosa, Maria and Rao, Manohar Narayana},
title = {Prioritising national healthcare service issues from free text feedback – A computational text analysis &amp; predictive modelling approach},
year = {2024},
issue_date = {Jun 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {181},
number = {C},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2024.114215},
doi = {10.1016/j.dss.2024.114215},
journal = {Decis. Support Syst.},
month = jun,
numpages = {15},
keywords = {Policy &amp; programme monitoring, Computational grounded theory, Policy analytics, Issue valence and salience, Service quality, Theory of change, Maternity service experience}
}

@article{10.1007/s10111-019-00595-y,
author = {Naikar, Neelam and Elix, Ben},
title = {Designing for self-organisation in sociotechnical systems: resilience engineering, cognitive work analysis, and the diagram of work organisation possibilities},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {1},
issn = {1435-5558},
url = {https://doi.org/10.1007/s10111-019-00595-y},
doi = {10.1007/s10111-019-00595-y},
abstract = {In designing sociotechnical systems, accounting for the phenomenon of self-organisation is critical. Empirical studies show that workers in these systems adapt not just their individual behaviours, but also their collective structures to deal with complex work environments. The concept of self-organisation can explain how such adaptations can be achieved spontaneously, continuously, and relatively seamlessly, and why this phenomenon is important for dealing with instability, uncertainty, and unpredictability in the task demands. However, existing design approaches such as resilience engineering and cognitive work analysis are limited in their capacity to design for self-organisation. This paper demonstrates that the diagram of work organisation possibilities, a recent addition to cognitive work analysis, provides a sound theoretical basis for designing for self-organisation. That is, it shows how essential components of the diagram are aligned with the concept of self-organisation and are well-grounded in empirical observations of adaptation in a variety of sociotechnical systems, specifically emergency management, military, and healthcare systems. Consequently, designs based on this diagram should have the potential to facilitate the emergence of new spatial, temporal, and functional organisational structures from the flexible actions of individual, interacting actors, thereby enhancing a system’s capacity for dealing with a dynamic, ambiguous work environment. Future research should focus on validating these ideas and demonstrating their value in industrial settings.},
journal = {Cogn. Technol. Work},
month = feb,
pages = {23–37},
numpages = {15},
keywords = {Work design, Organisational structure, Emergence, Flexibility, Adaptation}
}

@inproceedings{10.5555/3692070.3692401,
author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios N. and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael I. and Gonzalez, Joseph E. and Stoica, Ion},
title = {Chatbot arena: an open platform for evaluating LLMs by human preference},
year = {2024},
publisher = {JMLR.org},
abstract = {Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowd-sourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. The platform is publicly available at https://chat.lmsys.org.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {331},
numpages = {30},
location = {Vienna, Austria},
series = {ICML'24}
}

@inproceedings{10.1145/3576914.3587485,
author = {Wetzels, Jos and Dos Santos, Daniel and Ghafari, Mohammad},
title = {Insecure by Design in the Backbone of Critical Infrastructure},
year = {2023},
isbn = {9798400700491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576914.3587485},
doi = {10.1145/3576914.3587485},
abstract = {We inspected 45 actively deployed Operational Technology (OT) product families from ten major vendors and found that every system suffers from at least one trivial vulnerability. We reported a total of 53 weaknesses, stemming from insecure by design practices or basic security design failures. They enable attackers to take a device offline, manipulate its operational parameters, and execute arbitrary code without any constraint. We discuss why vulnerable products are often security certified and appear to be more secure than they actually are, and we explain complicating factors of OT risk management.},
booktitle = {Proceedings of Cyber-Physical Systems and Internet of Things Week 2023},
pages = {7–12},
numpages = {6},
keywords = {Vulnerability, operational technology, secure by design},
location = {San Antonio, TX, USA},
series = {CPS-IoT Week '23}
}

@inproceedings{10.1145/3639592.3639620,
author = {Poghosyan, Arnak and Harutyunyan, Ashot and Bunarjyan, Tigran and Baloian, Nelson},
title = {Optimizing SaaS Solutions for Enhanced Sustainability and Predictive Management of Cloud Assets},
year = {2024},
isbn = {9798400716225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639592.3639620},
doi = {10.1145/3639592.3639620},
abstract = {Identifying waste resources in modern-scale cloud infrastructures is a critical sustainability issue since it helps free up additional capacities for extra tasks and improves the performance of the systems while optimizing their costs. It is already well-recognized as a challenging task for human operators regarding manual and massive action efforts. At the same time, the problem is quite complicated – a complete and satisfactory solution is yet to be achieved. The paper proposes a novel and AI-driven approach to the problem. Applying rule induction learning across the history of service deployment instances to the log event data of the underlying entities, we extract conditions that lead to specific patterns, such as Resource Termination, thus providing a predictive mechanism for detecting objects subject to such actions in a real-time fashion. This explainable recommender system (called Cloud Sweeper) serves as an AI operations assistant for cloud users and Site Reliability Engineers (SRE) in their administrative duties.},
booktitle = {Proceedings of the 2023 6th Artificial Intelligence and Cloud Computing Conference},
pages = {204–210},
numpages = {7},
keywords = {AI Ops, Automated SaaS management, cloud waste management, explainable AI, log event data, rule induction learning},
location = {Kyoto, Japan},
series = {AICCC '23}
}

@article{10.1145/3665223,
author = {Regimbal, Juliette and Blum, Jeffrey R. and Kuo, Cyan and Cooperstock, Jeremy R.},
title = {IMAGE: An Open-Source, Extensible Framework for Deploying Accessible Audio and Haptic Renderings of Web Graphics},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3665223},
doi = {10.1145/3665223},
abstract = {For accessibility practitioners, creating and deploying novel multimedia interactions for people with disabilities is a nontrivial task. As a result, many projects aiming to support such accessibility needs come and go or never make it to a public release. To reduce the overhead involved in deploying and maintaining a system that transforms web content into multimodal renderings, we created an open source, modular microservices architecture as part of the IMAGE project. This project aims to design richer means of interacting with web graphics than is afforded by a screen reader and text descriptions alone. To benefit the community of accessibility software developers, we discuss this architecture and explain how it provides support for several multimodal processing pipelines. Beyond illustrating the initial use case that motivated this effort, we further describe two use cases outside the scope of our project to explain how a team could use the architecture to develop and deploy accessible solutions for their own work. We then discuss our team’s experience working with the IMAGE architecture, informed by discussions with six project members, and provide recommendations to other practitioners considering applying the framework to their own accessibility projects.},
journal = {ACM Trans. Access. Comput.},
month = jul,
articleno = {11},
numpages = {17},
keywords = {Systems and architecture, Web accessibility, Multimodal interaction, Blind, Low vision}
}

@article{10.1016/j.robot.2022.104132,
author = {Xiao, Xuesu and Wang, Zizhao and Xu, Zifan and Liu, Bo and Warnell, Garrett and Dhamankar, Gauraang and Nair, Anirudh and Stone, Peter},
title = {APPL: Adaptive Planner Parameter Learning},
year = {2022},
issue_date = {Aug 2022},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {154},
number = {C},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2022.104132},
doi = {10.1016/j.robot.2022.104132},
journal = {Robot. Auton. Syst.},
month = aug,
numpages = {15},
keywords = {Motion planning, Machine learning, Mobile robot navigation}
}

@inproceedings{10.1145/3613904.3642773,
author = {Kazemitabaar, Majeed and Ye, Runlong and Wang, Xiaoning and Henley, Austin Zachary and Denny, Paul and Craig, Michelle and Grossman, Tovi},
title = {CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642773},
doi = {10.1145/3613904.3642773},
abstract = {Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student’s incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI’s unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {650},
numpages = {20},
keywords = {AI assistants, AI tutoring, class deployment, design guidelines, educational technology, generative AI, intelligent tutoring systems, large language models, programming education},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1109/ICRA.2016.7487451,
author = {Knight, Heather and Simmons, Reid},
title = {Laban head-motions convey robot state: A call for robot body language},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICRA.2016.7487451},
doi = {10.1109/ICRA.2016.7487451},
abstract = {Functional robots are an increasing presence in shared human-machine environments. Humans efficiently parse motion expressions, gaining an immediate impression of an agent's current action and state. Past work has shown that motion can effectively reveal a robot's current task objective to bystanders and collaborators, however, the layering of expression on pre-existing robot task motions has yet to be explored. Rather than showing us what the robot is doing, these layered motion characteristics leverage the how of the task motions to convey additional robot attitudes, e.g., confidence, adherence to deadline or flexibility of attention. To lay the foundations for this objective, we adapt the Laban Efforts, a system from dance and acting training in use for over 50 years. We operationalize features representing the four Laban Efforts (Time, Space, Weight, and Flow) to the movements of a 2-DOF Nao head and a 4-DOF Keepon robot during simple dance and look-for-someone behaviors. Using online survey, we collect 1028 motion ratings for 72 robot motion videos depicting contrasting Effort motion examples. We achieve statistically significant legibility results for all four Effort implementations. Even without human degrees of freedom, we find that robot motion patterns can convey complex expressions to people.},
booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
pages = {2881–2888},
numpages = {8},
location = {Stockholm, Sweden}
}

@article{10.1007/s00146-022-01467-2,
author = {Fabbri, Matteo},
title = {Social influence for societal interest: a pro-ethical framework for improving human decision making through multi-stakeholder recommender systems},
year = {2022},
issue_date = {Apr 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {38},
number = {2},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-022-01467-2},
doi = {10.1007/s00146-022-01467-2},
abstract = {In the contemporary digital age, recommender systems (RSs) play a fundamental role in managing information on online platforms: from social media to e-commerce, from travels to cultural consumptions, automated recommendations influence the everyday choices of users at an unprecedented scale. RSs are trained on users’ data to make targeted suggestions to individuals according to their expected preference, but their ultimate impact concerns all the multiple stakeholders involved in the recommendation process. Therefore, whilst RSs are useful to reduce information overload, their deployment comes with significant ethical challenges, which are still largely unaddressed because of proprietary constraints and regulatory gaps that limit the effects of standard approaches to explainability and transparency. In this context, I address the ethical and social implications of automated recommendations by proposing a pro-ethical design framework aimed at reorienting the influence of RSs towards societal interest. In particular, after highlighting the problem of explanation for RSs, I discuss the application of beneficent informational nudging to the case of conversational recommender systems (CRSs), which rely on user-system dialogic interactions. Subsequently, through a comparison with standard recommendations, I outline the incentives for platforms and providers in adopting this approach and its benefits for both individual users and society.},
journal = {AI Soc.},
month = may,
pages = {995–1002},
numpages = {8},
keywords = {Explainability, Social influence, Recommender systems, AI ethics}
}

@inproceedings{10.1145/3462462.3468882,
author = {Zhang, Hantian and Shahbazi, Nima and Chu, Xu and Asudeh, Abolfazl},
title = {FairRover: explorative model building for fair and responsible machine learning},
year = {2021},
isbn = {9781450384865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462462.3468882},
doi = {10.1145/3462462.3468882},
abstract = {The potential harms and drawbacks of automated decision making has become a challenge as data science blends into our lives. In particular, fairness issues with deployed machine learning models have drawn significant attention from the research community. Despite the myriad of algorithmic fairness work in various research communities, in practice data scientists still face many roadblocks in ensuring the fairness of their machine learning models. This is primarily because there does not exist an end-to-end system that guides the users in building a fair machine learning model in a responsible way from model auditing, to model explanation, to bias mitigation.We propose a explorative model building system FairRover for responsible fair model building. FairRover guides users in (1) discovering the potential biases in the model; (2) providing explanation to the discovered biases so as to help users in understanding potential causes of the biases; and (3) mitigating the most important biases selected by the users. Because of the impossibility theorem of fairness, and the well-known trade-off between fairness and accuracy, it is generally impossible to achieve a completely fair and accurate machine learning model. Therefore, this responsible model building process is naturally performed iteratively until a satisfying trade-off is reached. Human users are involved in the loop to make various decisions guided by FairRover.We demonstrate a case study on the Adult Census dataset, which shows how FairRover guides users in iteratively building a fair income prediction model in a responsible way. We discuss the current limitations of FairRover and future work.},
booktitle = {Proceedings of the Fifth Workshop on Data Management for End-To-End Machine Learning},
articleno = {5},
numpages = {10},
location = {Virtual Event, China},
series = {DEEM '21}
}

@article{10.1007/s10676-019-09519-w,
author = {Mecacci, Giulio and Santoni de Sio, Filippo},
title = {Meaningful human control as reason-responsiveness: the case of dual-mode vehicles},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {1388-1957},
url = {https://doi.org/10.1007/s10676-019-09519-w},
doi = {10.1007/s10676-019-09519-w},
abstract = {In this paper, in line with the general framework of value-sensitive design, we aim to operationalize the general concept of “Meaningful Human Control” (MHC) in order to pave the way for its translation into more specific design&nbsp;requirements. In particular, we focus on the operationalization of the first of the two conditions (Santoni de Sio and Van den Hoven 2018) investigated: the so-called ‘tracking’ condition. Our investigation is led in relation to one specific subcase of automated system: dual-mode driving systems (e.g. Tesla ‘autopilot’). First, we connect and compare meaningful human control with a concept of control very popular in engineering and traffic psychology (Michon 1985), and we explain to what extent tracking resembles and differs from it. This will help clarifying the extent to which the idea of meaningful human control is connected to, but also goes beyond, current notions of control in engineering and psychology. Second, we take the systematic analysis of practical reasoning as&nbsp;traditionally presented in the philosophy of human action (Anscombe, Bratman, Mele)&nbsp;and we adapt it to offer a general framework where different types of reasons and&nbsp;agents are identified according to their relation to an automated system’s behaviour.&nbsp;This framework is meant to help explaining what reasons and what agents (should) play a role in controlling a given system, thereby enabling policy makers to produce&nbsp;usable guidelines and engineers to design systems that properly respond to selected&nbsp;human reasons. In the final part, we discuss a practical example of how our framework&nbsp;could be employed in designing automated driving systems.},
journal = {Ethics and Inf. Technol.},
month = jun,
pages = {103–115},
numpages = {13},
keywords = {Ethics of human–robot interaction, Responsible innovation in self-driving cars, Proximity scale of reasons, Accountability for autonomous systems, Ethics of self-driving cars, Meaningful human control}
}

@inproceedings{10.1145/3290605.3300484,
author = {Ashktorab, Zahra and Jain, Mohit and Liao, Q. Vera and Weisz, Justin D.},
title = {Resilient Chatbots: Repair Strategy Preferences for Conversational Breakdowns},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300484},
doi = {10.1145/3290605.3300484},
abstract = {Text-based conversational systems, also referred to as chatbots, have grown widely popular. Current natural language understanding technologies are not yet ready to tackle the complexities in conversational interactions. Breakdowns are common, leading to negative user experiences. Guided by communication theories, we explore user preferences for eight repair strategies, including ones that are common in commercially-deployed chatbots (e.g., confirmation, providing options), as well as novel strategies that explain characteristics of the underlying machine learning algorithms. We conducted a scenario-based study to compare repair strategies with Mechanical Turk workers (N=203). We found that providing options and explanations were generally favored, as they manifest initiative from the chatbot and are actionable to recover from breakdowns. Through detailed analysis of participants' responses, we provide a nuanced understanding on the strengths and weaknesses of each repair strategy.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {repair, grounding, conversational breakdown, conversational agents, chatbots},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3531146.3533106,
author = {Goetze, Trystan S.},
title = {Mind the Gap: Autonomous Systems, the Responsibility Gap, and Moral Entanglement},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533106},
doi = {10.1145/3531146.3533106},
abstract = {When a computer system causes harm, who is responsible? This question has renewed significance given the proliferation of autonomous systems enabled by modern artificial intelligence techniques. At the root of this problem is a philosophical difficulty known in the literature as the responsibility gap. That is to say, because of the causal distance between the designers of autonomous systems and the eventual outcomes of those systems, the dilution of agency within the large and complex teams that design autonomous systems, and the impossibility of fully predicting how autonomous systems will behave once deployed, determining who is morally responsible for harms caused by autonomous systems is unclear at a conceptual level. I review past work on this topic, criticizing prior works for suggesting workarounds rather than philosophical answers to the conceptual problem presented by the responsibility gap. The view I develop, drawing on my earlier work on vicarious moral responsibility, explains why computing professionals are ethically required to take responsibility for the systems they design, despite not being blameworthy for the harms these systems may cause.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {390–400},
numpages = {11},
keywords = {accountability, autonomous systems, computer ethics, ethics of artificial intelligence, lethal autonomous weapons systems (LAWS), moral responsibility, professional responsibility},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3616855.3635699,
author = {Bernard, Nolwenn and Kostric, Ivica and Balog, Krisztian},
title = {IAI MovieBot 2.0: An Enhanced Research Platform with Trainable Neural Components and Transparent User Modeling},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635699},
doi = {10.1145/3616855.3635699},
abstract = {While interest in conversational recommender systems has been on the rise, operational systems suitable for serving as research platforms for comprehensive studies are currently lacking. This paper introduces an enhanced version of the IAI MovieBot conversational movie recommender system, aiming to evolve it into a robust and adaptable platform for conducting user-facing experiments. The key highlights of this enhancement include the addition of trainable neural components for natural language understanding and dialogue policy, transparent and explainable modeling of user preferences, along with improvements in the user interface and research infrastructure.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {1042–1045},
numpages = {4},
keywords = {conversational ai, conversational recommender systems},
location = {Merida, Mexico},
series = {WSDM '24}
}

@article{10.1007/s10676-020-09528-0,
author = {Skerker, Michael and Purves, Duncan and Jenkins, Ryan},
title = {Autonomous weapons systems and the moral equality of combatants},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {1388-1957},
url = {https://doi.org/10.1007/s10676-020-09528-0},
doi = {10.1007/s10676-020-09528-0},
abstract = {To many, the idea of autonomous weapons systems (AWS) killing human beings is grotesque. Yet critics have had difficulty explaining why it should make a significant moral difference if a human combatant is killed by an AWS as opposed to being killed by a human combatant. The purpose of this paper is to explore the roots of various deontological concerns with AWS and to consider whether these concerns are distinct from any concerns that also apply to long-distance, human-guided weaponry. We suggest that at least one major driver of the intuitive moral aversion to lethal AWS is that their use disrespects their human targets by violating the martial contract between human combatants. On our understanding of this doctrine, service personnel cede a right not to be directly targeted with lethal violence to other human agents alone. Artificial agents, of which AWS are one example, cannot understand the value of human life. A human combatant cannot transfer his privileges of targeting enemy combatants to a robot. Therefore, the human duty-holder who deploys AWS breaches the martial contract between human combatants and disrespects the targeted combatants. We consider whether this novel deontological objection to AWS forms the foundation of several other popular yet imperfect deontological objections to AWS.},
journal = {Ethics and Inf. Technol.},
month = sep,
pages = {197–209},
numpages = {13},
keywords = {Military ethics, Just war theory, Moral equality of combatants, Lethal autonomous weapons}
}

@inproceedings{10.1145/3600160.3605025,
author = {Stelkens-Kobsch, Tim H. and Boumann, Hilke and Piekert, Florian and Schaper, Meilin and Carstengerdes, Nils},
title = {A Concept-Based Validation Approach to Validate Security Systems for Protection of Interconnected Critical Infrastructures},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600160.3605025},
doi = {10.1145/3600160.3605025},
abstract = {When it comes to securing critical infrastructures, it is evident to not only provide a toolbox which allows to detect when vulnerabilities are exploited but also to support the operations in performing mitigation procedures. This paper explains how a validation was conducted in the Horizon 2020 project PRAETORIAN to evaluate the operational feasibility of a system which observes and manages security within interconnected critical infrastructures. To this end, a concept-based approach involving presentation of scenarios with the help of narrations and visual elements, hands-on experience as well as discussions and questionnaires was used. Some results are discussed to demonstrate the applicability of this approach.},
booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security},
articleno = {110},
numpages = {10},
keywords = {Critical infrastructure protection, cyber-physical security, cybersecurity, hybrid security, validation},
location = {Benevento, Italy},
series = {ARES '23}
}

@inproceedings{10.1007/978-3-030-77015-0_4,
author = {McKenna, H. Patricia},
title = {The Importance of Theory for Understanding Smart Cities: Making a Case for Ambient Theory},
year = {2021},
isbn = {978-3-030-77014-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77015-0_4},
doi = {10.1007/978-3-030-77015-0_4},
abstract = {This paper seeks to develop a theoretical foundation for ambient theory as a theory in support of advancing definitions and understandings of smart cities and regions. Through a review of the evolving research literature for the ambient and for smart cities, a conceptual framework is formulated consisting of components and characteristics constituting ambient theory for smart cities and regions. The framework is then operationalized for use in this paper, exploring the practical application of ambient theory in smart cities and regions. Using a case study approach together with an explanatory correlational design, elements such as technology-driven services, creative opportunities, and access to public data are explored. Drawing additionally on other works where this approach is employed, elements such as awareness, information and communication technologies (ICTs), interactivity, and sensing are provided as further examples showing the potential for promising relationships in support of ambient theory for smart cities. Ambient theory as advanced in this paper is discussed in terms of theory usefulness, parsimony, and type. Future directions are identified for explorations of ambient theory going forward for both research and practice in contributing to definitions and understandings of smart cities and regions.},
booktitle = {Distributed, Ambient and Pervasive Interactions: 9th International Conference, DAPI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings},
pages = {41–54},
numpages = {14},
keywords = {Adaptability, Ambient human-computer interaction, Ambient theory, Awareness, Correlation, Creativity, Information and Communication Technologies (ICTs), Interactivities, Sensing, Smart cities, Smart environments, Theory building}
}

@inproceedings{10.1145/3490099.3511158,
author = {Guti\'{e}rrez, Francisco and Htun, Nyi Nyi and Vanden Abeele, Vero and De Croon, Robin and Verbert, Katrien},
title = {Explaining Call Recommendations in Nursing Homes: a User-Centered Design Approach for Interacting with Knowledge-Based Health Decision Support Systems},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511158},
doi = {10.1145/3490099.3511158},
abstract = {Recommender systems are increasingly used in high-risk application domains, including healthcare. It has been shown that explanations are crucial in this context to support decision-making. This paper explores how to explain call recommendations to nursing home staff, providing insights into call priority, notifications, and resident information. We present the design and implementation of a recommender engine and a mobile application designed to support call recommendations and explain these recommendations that may contribute to residents’ safety and quality of care. More specifically, we report on the results of a user-centered design approach with residents (N=12) and healthcare professionals (N=4), and a final evaluation (N=12) after four months of deployment. The results show that our design approach provides a valuable tool for more accurate and efficient decision-making. The overall system encourages nursing home staff to provide feedback and annotate, resulting in more confidence in the system. We discuss usability issues, challenges, and reflections to be considered in future health recommender systems.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {162–172},
numpages = {11},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.1145/3700599,
author = {Francis, Anthony and P\'{e}rez-D’Arpino, Claudia and Li, Chengshu and Xia, Fei and Alahi, Alexandre and Alami, Rachid and Bera, Aniket and Biswas, Abhijat and Biswas, Joydeep and Chandra, Rohan and Chiang, Hao-Tien Lewis and Everett, Michael and Ha, Sehoon and Hart, Justin and How, Jonathan P. and Karnan, Haresh and Lee, Tsang-Wei Edward and Manso, Luis J. and Mirsky, Reuth and Pirk, S\"{o}ren and Singamaneni, Phani Teja and Stone, Peter and Taylor, Ada V. and Trautman, Peter and Tsoi, Nathan and V\'{a}zquez, Marynel and Xiao, Xuesu and Xu, Peng and Yokoyama, Naoki and Toshev, Alexander and Mart\'{\i}n-Mart\'{\i}n, Roberto},
title = {Principles and Guidelines for Evaluating Social Robot Navigation Algorithms},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
url = {https://doi.org/10.1145/3700599},
doi = {10.1145/3700599},
abstract = {A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this article, we pave the road toward common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributions include (a) a definition of a socially navigating robot as one that respects the principles of safety, comfort, legibility, politeness, social competency, agent understanding, proactivity, and responsiveness to context, (b) guidelines for the use of metrics, development of scenarios, benchmarks, datasets, and simulators to evaluate social navigation, and (c) a design of a social navigation metrics framework to make it easier to compare results from different simulators, robots, and datasets.},
journal = {J. Hum.-Robot Interact.},
month = feb,
articleno = {34},
numpages = {65},
keywords = {social robotics, robot navigation, datasets, benchmarks, simulators}
}

@inproceedings{10.1145/3502223.3502232,
author = {Zhang, Wen and Deng, Shumin and Chen, Mingyang and Wang, Liang and Chen, Qiang and Xiong, Feiyu and Liu, Xiangwen and Chen, Huajun},
title = {Knowledge Graph Embedding in E-commerce Applications: Attentive Reasoning, Explanations, and Transferable Rules},
year = {2022},
isbn = {9781450395656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502223.3502232},
doi = {10.1145/3502223.3502232},
abstract = {Knowledge Graphs (KGs), representing facts as triples, have been widely adopted in many applications. Reasoning tasks such as link prediction and rule induction are important for the development of KGs. Knowledge Graph Embeddings (KGEs) embedding entities and relations of a KG into continuous vector spaces, have been proposed for these reasoning tasks and proven to be efficient and robust. But the plausibility and feasibility of applying and deploying KGEs in real-work applications has not been well-explored. In this paper, we discuss and report our experiences of deploying KGEs in a real domain application: e-commerce. We first identity three important desiderata for e-commerce KG systems: 1) attentive reasoning, reasoning over a few target relations of more concerns instead of all; 2) explanation, providing explanations for a prediction to help both users and business operators understand why the prediction is made; 3) transferable rules, generating reusable rules to accelerate the deployment of a KG to new systems. While non existing KGE could meet all these desiderata, we propose a novel one, an explainable knowledge graph attention network that make prediction through modeling correlations between triples rather than purely relying on its head entity, relation and tail entity embeddings. It could automatically selects attentive triples for prediction and records the contribution of them at the same time, from which explanations could be easily provided and transferable rules could be efficiently produced. We empirically show that our method is capable of meeting all three desiderata in our e-commerce application and outperform typical baselines on datasets from real domain applications.},
booktitle = {Proceedings of the 10th International Joint Conference on Knowledge Graphs},
pages = {71–79},
numpages = {9},
keywords = {Rules, Representation Learning, Reasoning, Knowledge Graphs, Explainable AI, E-commerce},
location = {Virtual Event, Thailand},
series = {IJCKG '21}
}

@article{10.1007/s10270-024-01204-x,
author = {Li, Tong and Wang, Yiting and Wei, Xiang and Zhang, Xueying and Liu, Yu},
title = {MUREQ: a multilayer framework for analyzing and operationalizing visualization requirements},
year = {2024},
issue_date = {Oct 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-024-01204-x},
doi = {10.1007/s10270-024-01204-x},
abstract = {Understanding and interpreting vast amounts of information is pivotal in the contemporary data-rich age. Data visualization has emerged as a significant measure of comprehending these data. Similarly, an appropriate visualization can also enhance software modeling by providing straightforward and interactive representations. However, current data visualization methods predominantly require users to have data visualization-related expertise, which is usually challenging to obtain in reality. It is essential to bridge the gap between visualization requirements and visualization solutions for non-expert users, assisting them in automatically operationalizing their visualization requirements. This paper proposes a MUltilayer framework for analyzing and operationalizing visualization REQuirements that automatically derives appropriate visualization solutions based on users’ requirements. Specifically, we systematically investigate the connections among visualization requirements, visual variable characteristics, visual variable attributes, and visualization solutions, based on which we establish a conceptual framework that characterizes the relationships among different layers. Our proposal contributes to not only automatically operationalizing visualization requirements but also providing meaningful explanations for the derived visualization solutions. To promote our proposal and pragmatically benefit real users, we have developed and deployed a prototype tool based on the proposed framework, which is publicly available at . To evaluate our proposed framework, we conducted an initial controlled experiment with 44 participants to test the performance of the evolved mappings within our framework. Based on the expert’s feedback, we refined the mappings and incorporated a ranking system for visualization solutions tailored to specific requirements. To assess the current method, a subsequent experiment with another group of 44 participants and a focused case study involving two new participants were carried out. The results demonstrate that users perceive that the current method accelerates task completion, especially for complex tasks, by efficiently narrowing down options and prioritizing them. This approach is particularly advantageous for users with limited data visualization experience. Besides, the multilayer framework can be used to inspire the visualization of models in the software modeling community.},
journal = {Softw. Syst. Model.},
month = sep,
pages = {1123–1155},
numpages = {33},
keywords = {Visualization requirements operationalization, Multilayer analysis, Empirical evaluation, Prototype tool}
}

@inproceedings{10.1145/3448139.3448143,
author = {Khosravi, Hassan and Demartini, Gianluca and Sadiq, Shazia and Gasevic, Dragan},
title = {Charting the Design and Analytics Agenda of Learnersourcing Systems},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448143},
doi = {10.1145/3448139.3448143},
abstract = {Learnersourcing is emerging as a viable learner-centred and pedagogically justified approach for harnessing the creativity and evaluation power of learners as experts-in-training. Despite the increasing adoption of learnersourcing in higher education, understanding students’ behaviour while engaged in learnersourcing and best practices for the design and development of learnersourcing systems are still largely under-researched. This paper offers data-driven reflections and lessons learned from the development and deployment of a learnersourcing adaptive educational system called RiPPLE, which to date, has been used in more than 50-course offerings with over 12,000 students. Our reflections are categorised into examples and best practices on (1) assessing the quality of students’ contributions using accurate, explainable and fair approaches to data analysis, (2) incentivising students to develop high-quality contributions and (3) empowering instructors with actionable and explainable insights to guide student learning. We discuss the implications of these findings and how they may contribute to the growing literature on the development of effective learnersourcing systems and more broadly technological educational solutions that support learner-centred learning at scale.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {32–42},
numpages = {11},
keywords = {human-centred computing, explainable AI, crowdsourcing in education, Learnersourcing},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3428502.3428506,
author = {Ches\~{n}evar, Carlos Iv\'{a}n and Gonz\'{a}lez, Mar\'{\i}a Paula and Maguitman, Ana and Estevez, Elsa},
title = {A first approach towards integrating computational argumentation in cognitive cities},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428506},
doi = {10.1145/3428502.3428506},
abstract = {In the last years, the concept of Cognitive Smart City (CSC) emerged from the convergence of the Internet of Things, big data, smart city technologies, and artificial intelligence techniques. At the same time, computational argumentation has consolidated itself as a vibrant area in Artificial Intelligence (AI) which has engineered different approaches to reflect aspects of how humans build, exchange and analyze arguments in their daily lives, mainly to deal with controversial or inconsistent information. Thus, computational argumentation provides a valuable metaphor for reasoning on top of available data in order to draw conclusions and offer explanations for them. This paper discusses a first approach towards integrating computational argumentation in a layered model for cognitive cities, where the bottom layers comprise raw data collected from sensor, actuators and other artifacts deployed in the context of a smart city, and argumentation provides high-level intelligence abilities. We show how our approach can be paired with a layered model for computational argumentation. To illustrate our proposal, we analyze a case study, based on the DECIDE 2.0 framework.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {25–32},
numpages = {8},
keywords = {Explainable Artificial Intelligence, Computational Argumentation, Cognitive Cities Models},
location = {Athens, Greece},
series = {ICEGOV '20}
}

@article{10.1287/isre.2022.1153,
author = {Shaikh, Maha and Vaast, Emmanuelle},
title = {Algorithmic Interactions in Open Source Work},
year = {2023},
issue_date = {June 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {34},
number = {2},
issn = {1526-5536},
url = {https://doi.org/10.1287/isre.2022.1153},
doi = {10.1287/isre.2022.1153},
abstract = {This study focuses on algorithmic interactions in open source work. Algorithms are essential in open source because they remedy concerns incompletely addressed by parallel development or modularity. Following algorithmic interactions in open source allows us to map the operational performance of algorithms to understand how algorithms work with multiple other algorithms to accomplish work. Studying algorithms working together shows us how residual interdependencies of modularity and problems not resolved by dependence on parallel development are worked around to perform open source work. We examine the Linux Kernel case that reveals how algorithmic interactions facilitate open source work through the three processes of managing, organizing, and supervising development work. Our qualitative study theorizes how algorithmic interactions intensify through these processes that work together to facilitate development. We make a theoretical contribution to open source scholarship by explaining how algorithmic interactions navigate across module rigidity and enhance parallel development. Our work also reveals how, in open source, developers work to automate most tasks and augmentation is a bidirectional relationship of algorithms augmenting the work of developers and of developers augmenting the work of algorithms.This study focuses on algorithmic interactions in open source work. Algorithms are essential in open source because they remedy concerns incompletely addressed by parallel development or modularity. Following algorithmic interactions in open source allows us to map the performance of algorithms to understand the nature of work conducted by multiple algorithms functioning together. We zoom to the level of algorithmic interactions to show how residual interdependencies of modularity are worked around by algorithms. Moreover, the dependence on parallel development does not suffice to resolve all concerns related to the distributed work of open source. We examine the Linux Kernel case that reveals how algorithmic interactions facilitate open source work through the three processes of managing, organizing, and supervising development work. Our qualitative study theorizes how algorithmic interactions intensify through these processes that work together to facilitate development. We make a theoretical contribution to open source scholarship by explaining how algorithmic interactions navigate across module rigidity and enhance parallel development. Our work also reveals how, in open source, developers work to automate most tasks and augmentation is a bidirectional relationship of algorithms augmenting the work of developers and of developers augmenting the work of algorithms.History: Hemant Jain, Balaji Padmanabhan, Paul Pavlou, and Raghu Santanam, Senior Editors; Likoebe Maruping, Associate Editor.Supplemental Material: The online appendix is available at .},
journal = {Info. Sys. Research},
month = jun,
pages = {744–765},
numpages = {22},
keywords = {qualitative study, augmentation and automation, parallel development, modularity, open source work, algorithmic interactions}
}

@inproceedings{10.5555/3586210.3586319,
author = {Cayirci, Erdal and AlNaimi, Ramzan and AlNabet, Sara Salem},
title = {Computer Assisted Military Experimentations},
year = {2023},
publisher = {IEEE Press},
abstract = {Computer assisted military experimentation methodology and process are explained. The military processes that can benefit from computer assisted military experimentation are introduced and the best practices for each process are elaborated on. Finally, emerging new concepts and their potential impact on the military experimentation requirements are briefly discussed and the tutorial is concluded. During the tutorial, live demonstrations are made for geostrategic foresight development, defense planning, operational plan analysis, computer assisted military experimentation design and conducting a computer assisted military experiment.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1311–1324},
numpages = {14},
location = {Singapore, Singapore},
series = {WSC '22}
}

@article{10.1016/j.future.2019.04.051,
author = {Mualla, Yazan and Najjar, Amro and Daoud, Alaa and Galland, St\'{e}phane and Nicolle, Christophe and Yasar, Ansar-Ul-Haque and Shakshuki, Elhadi},
title = {Agent-based simulation of unmanned aerial vehicles in civilian applications: A systematic literature review and research directions},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {100},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.04.051},
doi = {10.1016/j.future.2019.04.051},
journal = {Future Gener. Comput. Syst.},
month = nov,
pages = {344–364},
numpages = {21},
keywords = {Civilian applications, Systematic literature review, Unmanned aerial vehicle, Agent-based simulation, Multi-agent systems}
}

@article{10.1007/s10796-022-10311-3,
author = {Zuo, Meihua and Angelopoulos, Spyros and Liang, Zhouyang and Ou, Carol X. J.},
title = {Blazing the Trail: Considering Browsing Path Dependence in Online Service Response Strategy},
year = {2022},
issue_date = {Aug 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {4},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-022-10311-3},
doi = {10.1007/s10796-022-10311-3},
abstract = {Competition on e-commerce platforms is becoming increasingly fierce, due to the ease of online searching for comparing products and services. We examine how the sequential browsing behavior of consumers can enable targeted marketing strategies on e-commerce platforms, by using clickstream data from one of the largest e-commerce platforms in Asia. We deploy duration analysis to i) explore how path dependence can better explain consumers’ sequential browsing behavior in different product categories, and ii) characterize the sequential browsing behavior of heterogeneous consumer groups. The findings of our work showcase i) the high accuracy of using sequential browsing path dependence to explain consumer behavior, ii) the patterns of their behavioral intentions and iii) the spell of the behavior of heterogeneous consumer groups. Our findings provide nuanced implications for strategically managing branding, marketing, and customer relations on e-commerce platforms. We discuss the implications of our findings for both research and practice, and we delineate an agenda for future research on the topic.},
journal = {Information Systems Frontiers},
month = jul,
pages = {1605–1619},
numpages = {15},
keywords = {Duration Analysis, Path Dependence, Consumer Behavior, Business Analytics, E-commerce}
}

@inproceedings{10.1007/978-3-031-75599-6_19,
author = {Buffa, Matteo},
title = {Resignifying Compliance Between Ontologies and Epistemologies of Law (Invited Paper)},
year = {2024},
isbn = {978-3-031-75598-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-75599-6_19},
doi = {10.1007/978-3-031-75599-6_19},
abstract = {This contribution is devoted to the topic of compliance in the legal-regulatory context, focusing on the need for new epistemologies of compliance that look at different, and interoperable, contexts. It is an attempt to understand and explain human and non-human cognitive processes at the basis of evaluations of a knowledge (and knowing) compliant capacity with the tension to reconstruct the different phases of the intelligibility of dispositions (and target documents that derive from them). The data analyzed so far allow us to imagine new (and plural) epistemic foundations capable of understanding and explaining this complexity. In particular, it seems to be possible to support a transition (linguistic, modeling, but also operational) from ontologies to epistemologies, all the more so when we are called upon to evaluate the issue of compliance.},
booktitle = {Advances in Conceptual Modeling: ER 2024 Workshops, AISA, CMLS, EmpER, QUAMES, JUSMOD, LLM4Modeling, Pittsburgh, PA, USA, October 28–31, 2024, Proceedings},
pages = {253–266},
numpages = {14},
keywords = {Compliance, Ontologies, Epistemologies, Philosophy of Law, GDPR},
location = {Pittsburg, PA, USA}
}

@article{10.1007/s10111-022-00705-3,
author = {Maggi, Davide and Romano, Richard and Carsten, Oliver and Winter, Joost C. F. De},
title = {When terminology hinders research: the colloquialisms of transitions of control in automated driving},
year = {2022},
issue_date = {Aug 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {3},
issn = {1435-5558},
url = {https://doi.org/10.1007/s10111-022-00705-3},
doi = {10.1007/s10111-022-00705-3},
abstract = {During the last 20 years, technological advancement and economic interests have motivated research on automated driving and its impact on drivers’ behaviour, especially after transitions of control. Indeed, once the Automated Driving System (ADS) reaches its operational limits, it is forced to request human intervention. However, the fast accumulation and massive quantity of produced studies and the gaps left behind by standards have led to an imprecise and colloquial use of terms which, as technology and research interest evolve, creates confusion. The goal of this survey is to compare how different taxonomies describe transitions of control, address the current use of widely adopted terms in the field of transitions of control and explain how their use should be standardized to enhance future research. The first outcome of this analysis is a schematic representation of the correspondence among the elements of the reviewed taxonomies. Then, the definitions of “takeover” and “handover” are clarified as two parallel processes occurring in every transition of control. A second set of qualifiers, which are necessary to unequivocally define a transition of control and identify the agent requesting the transition and the agent receiving the request (ADS or the driver), is provided. The “initiator” is defined as the agent requesting the transition to take place, and the “receiver” is defined as the agent receiving that request.},
journal = {Cogn. Technol. Work},
month = aug,
pages = {509–520},
numpages = {12},
keywords = {Human factors, Automated vehicles, Automation, Transitions of control, Taxonomies}
}

@article{10.1007/s11277-020-07538-1,
author = {Hussain, Md. Muzakkir and Beg, M. M. Sufyan and Alam, Mohammad Saad},
title = {Fog Computing for Big Data Analytics in IoT Aided Smart Grid Networks},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {114},
number = {4},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-020-07538-1},
doi = {10.1007/s11277-020-07538-1},
abstract = {The recent integration of Internet of Things and Cloud Computing (CC) technologies into a Smart Grid (SG) revolutionizes its operation. The scalable and unlimited Store Compute and Networking (SCN) resources offered by CC enables efficient Big Data Analytics of SG data. However, due to remote location of Cloud Data Centers and congested network traffic, the cloud often gives poor performance for latency and energy critical SG applications. Fog Computing (FC) is thus proposed as a model that distributes the SCN resources at the intermediary devices, termed as Fog Computing Nodes (FCN), viz. network gateways, battery powered servers, access points, etc. By executing application specific logic at those nodes, the FC astonishingly reduces the response time as well as energy consumption of network elements. In this paper, we propose a mathematical framework that explains the Planning and Placement of Fog computing in smart Grid (PPFG). Basically, the PPFG model is formulated as an Integer Linear Programming problem that determines the optimal location, the capacity and the number of FCNs, towards minimizing the average response delay and energy consumption of network elements. Since this optimization problem is trivially NP-Hard, we solve it using an evolutionary Non-dominated Sorting Genetic Algorithm. By running the model on an exemplary SG network, we demonstrate the operation of proposed PPFG model. In fact, we perform a complete analysis of the obtained Pareto Fronts (PF), in order to better understand the working of design constraints in the PPFG model. The PFs will enable the SG utilities and architectural designers to evaluate the pros and cons of each of the trade-off solutions, leading to intelligent planning, designing and deployment of FC based SG applications.},
journal = {Wirel. Pers. Commun.},
month = oct,
pages = {3395–3418},
numpages = {24},
keywords = {Smart grid, Fog computing, Cloud computing, Big data analytics}
}

@article{10.1016/S1353-4858(19)30120-5,
author = {Heritage, Ian},
title = {Protecting Industry 4.0: challenges and solutions as IT, OT and IP converge},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {2019},
number = {10},
issn = {1353-4858},
url = {https://doi.org/10.1016/S1353-4858(19)30120-5},
doi = {10.1016/S1353-4858(19)30120-5},
journal = {Netw. Secur.},
month = oct,
pages = {6–9},
numpages = {4}
}

@inproceedings{10.1007/978-981-97-5492-2_11,
author = {Zhu, Yong and Xiao, Shuai and Zhang, Zhuo and Wen, Jiabao and Xi, Meng and Yang, Jiachen},
title = {An Konwledge-Based Semi-supervised Active Learning Method for&nbsp;Precision Pest Disease Diagnostic},
year = {2024},
isbn = {978-981-97-5491-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-97-5492-2_11},
doi = {10.1007/978-981-97-5492-2_11},
abstract = {Over the past decade, deep learning (DL) has seen remarkable progress, and this advancement extends to the recognition of agricultural pests and diseases. However, the high expense of labeling agricultural images has posed a significant challenge for DL-based agricultural image analysis, thereby complicating the deployment of Internet of Things devices in agriculture. This paper introduces a knowledge-based, semi-supervised, and explanation-friendly active learning framework designed specifically for the recognition of agricultural pests and diseases. The proposed framework features an innovative active learning algorithm that efficiently selects both in-class and borderline samples. Additionally, it incorporates a semi-supervised strategy to harness the predictive capabilities of DL models, significantly reducing labeling costs. To integrate the scoring of individual samples by both active learning and semi-supervised methods, enhancing the diversity of sample selection, the framework includes a novel fusion strategy. The effectiveness of this approach is validated through experiments on a dataset of agricultural pest and disease images, with each module delivering promising results. This work offers a practical solution to reduce the costs associated with labeling agricultural data and to enhance the efficiency of model learning in the context of agricultural pest and disease recognition.},
booktitle = {Knowledge Science, Engineering and Management: 17th International Conference, KSEM 2024, Birmingham, UK, August 16–18, 2024, Proceedings, Part I},
pages = {136–147},
numpages = {12},
keywords = {Deep Learning, Active Learning, Semi-Supervised Learning, Pest Diagnostic},
location = {Birmingham, United Kingdom}
}

@article{10.1016/j.cose.2023.103489,
author = {Mikuleti\v{c}, Samanta and Vrhovec, Simon and Skela-Savi\v{c}, Brigita and \v{Z}vanut, Bo\v{s}tjan},
title = {Security and privacy oriented information security culture (ISC): Explaining unauthorized access to healthcare data by nursing employees},
year = {2024},
issue_date = {Jan 2024},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {136},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2023.103489},
doi = {10.1016/j.cose.2023.103489},
journal = {Comput. Secur.},
month = jan,
numpages = {14},
keywords = {Information security culture, Healthcare data, Electronic health records, EHR, Data breach, Information security, Nursing}
}

@inproceedings{10.1145/3600211.3604717,
author = {Fabbri, Matteo},
title = {Self-determination through explanation: an ethical perspective on the implementation of the transparency requirements for recommender systems set by the Digital Services Act of the European Union},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604717},
doi = {10.1145/3600211.3604717},
abstract = {In the contemporary information age, recommender systems (RSs) play a critical role in influencing online behaviour: from social media to e-commerce, from music streaming to news aggregators, individuals are constantly targeted by personalized recommendations suggesting contents that may interest them. Despite such diffusion, the extent to which recommendations influence users’ decisions is still underexplored, given that independent audits on the structure and functioning of RSs deployed on online platforms are usually prevented by proprietary constraints. The nudging potential of RSs can represent a risk for vulnerable people: indeed, judicial cases involving platforms’ responsibility for displaying recommendations that may lead to political radicalization or endangerment of minors have recently caught public attention. The Digital Services Act of the European Union (DSA) is the first supranational regulation that sets specific transparency and auditing requirements for RSs implemented by online platforms with the aim of enhancing users’ self-determination: in particular, it allows users to modify the parameters on which recommendations rely so to let them choose autonomously which kind of content they want to see. This research focuses on whether and how the enforcement of this regulation can mitigate the unfair consequences of the power imbalance between online platforms and users. To this aim, I discuss the harms arising from digital nudging based on RSs and propose explanations as a tool that can reduce the impact of those harms by increasing users’ awareness. Through a comparative analysis of relevant articles of the DSA, the General Data Protection Regulation (GDPR) and the AI Act, I outline how the provisions of the DSA fill some of the gaps left by other relevant European regulations, while leaving the so-called right to explanation substantially unaddressed. As a result of this analysis, I argue that, in order for the implementation of the DSA provisions on recommender systems to be effective, policy-makers should: 1) enhance users’ awareness through clear and easily accessible explanations on how the recommendation process works and how they can be influenced by it; 2) grant users the possibility of intervening directly on the strategies through which RSs target them on the platform’s interface.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {653–661},
numpages = {9},
keywords = {Digital Nudging, Digital Services Act, Recommender Systems, Regulation of AI, Transparency},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@inproceedings{10.1145/3605768.3623544,
author = {Tovanich, Natkamon and Kassoul, Myriam and Weidenholzer, Simon and Prat, Julien},
title = {Contagion in Decentralized Lending Protocols: A Case Study of Compound},
year = {2023},
isbn = {9798400702617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605768.3623544},
doi = {10.1145/3605768.3623544},
abstract = {We study financial contagion in Compound V2, a decentralized lending protocol deployed on the Ethereum blockchain. We explain how to construct the balance sheets of Compound's liquidity pools and use our methodology to characterize the financial network. Our analysis reveals that most users either borrow stablecoins or engage in liquidity mining. We then study the robustness of Compound through a series of stress tests, identifying the pools that are most likely to set off a cascade of defaults.},
booktitle = {Proceedings of the 2023 Workshop on Decentralized Finance and Security},
pages = {55–63},
numpages = {9},
keywords = {systemic risk, stress test, financial network, financial contagion, decentralized finance},
location = {Copenhagen, Denmark},
series = {DeFi '23}
}

@inproceedings{10.1145/3442188.3445937,
author = {Kroll, Joshua A.},
title = {Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445937},
doi = {10.1145/3442188.3445937},
abstract = {Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {758–771},
numpages = {14},
keywords = {AI ethics, AI principles, accountability, traceability, transparency},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{10.1145/2975941.2990289,
author = {Uchitel, Sebastian},
title = {Business process adaptation using discrete event controller synthesis},
year = {2016},
isbn = {9781450342148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2975941.2990289},
doi = {10.1145/2975941.2990289},
abstract = {Discrete Event Controller Synthesis is a fully automated procedure for producing operational reactive strategies for achieving declarative goals. In this talk I will discuss how synthesis at runtime can play a key role in achieving self-adaptation. I will discuss a reference architecture for self-adaptive systems and focus on the problem of dynamic controller update. I will also explain why discrete event controller synthesis is particularly well suited for runtime adaptation of business processes, in particular for dynamic update of workflows.},
booktitle = {Proceedings of the International Workshop on Formal Methods for Analysis of Business Systems},
pages = {3},
numpages = {1},
keywords = {Workflow Systems, Supervisory Control, Discrete Event Controller Systems, Controller Synthesis},
location = {Singapore, Singapore},
series = {ForMABS 2016}
}

@article{10.1177/1059712318824697,
author = {Imre, Mert and Oztop, Erhan and Nagai, Yukie and Ugur, Emre},
title = {Affordance-based altruistic robotic architecture for human–robot collaboration},
year = {2019},
issue_date = {Aug 2019},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {27},
number = {4},
issn = {1059-7123},
url = {https://doi.org/10.1177/1059712318824697},
doi = {10.1177/1059712318824697},
abstract = {This article proposes a computational model for altruistic behavior, shows its implementation on a physical robot, and presents the results of human–robot interaction experiments conducted with the implemented system. Inspired from the sensorimotor mechanisms of the primate brain, object affordances are utilized for both intention estimation and action execution, in particular, to generate altruistic behavior. At the core of the model is the notion that sensorimotor systems developed for movement generation can be used to process the visual stimuli generated by actions of the others, infer the goals behind, and take the necessary actions to help achieving these goals, potentially leading to the emergence of altruistic behavior. Therefore, we argue that altruistic behavior is not necessarily a consequence of deliberate cognitive processing but may emerge through basic sensorimotor processes such as error minimization, that is, minimizing the difference between the observed and expected outcomes. In the model, affordances also play a key role by constraining the possible set of actions that an observed actor might be engaged in, enabling a fast and accurate intention inference. The model components are implemented on an upper-body humanoid robot. A set of experiments are conducted validating the workings of the components of the model, such as affordance extraction and task execution. Significantly, to assess how human partners interact with our altruistic model deployed robot, extensive experiments with na\"{\i}ve subjects are conducted. Our results indicate that the proposed computational model can explain emergent altruistic behavior in reference to its biological counterpart and moreover engage human partners to exploit this behavior when implemented on an anthropomorphic robot.},
journal = {Adapt Behav},
month = aug,
pages = {223–241},
numpages = {19},
keywords = {goal inference, human–robot interaction, affordances, brain-inspired robotics, computational modeling, Altruistic behavior}
}

@article{10.1016/j.compedu.2023.104843,
author = {van der Linden, Sara and Papadopoulos, Pantelis M. and Nieveen, Nienke and McKenney, Susan},
title = {ReflAct: Formative assessment for teacher reflection in video-coaching settings},
year = {2023},
issue_date = {Oct 2023},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {203},
number = {C},
issn = {0360-1315},
url = {https://doi.org/10.1016/j.compedu.2023.104843},
doi = {10.1016/j.compedu.2023.104843},
journal = {Comput. Educ.},
month = oct,
numpages = {15},
keywords = {Video coaching, Professional development, ReflAct, Formative assessment, Teacher reflection, Teacher coaching, Pre-service teacher education, In-service teacher development}
}

@article{10.1016/j.rcim.2023.102578,
author = {KEUNG, K.L. and LEE, C.K.M. and XIA, Liqiao and LIU, Chao and LIU, Bufan and JI, P.},
title = {A cyber-physical robotic mobile fulfillment system in smart manufacturing: The simulation aspect},
year = {2023},
issue_date = {Oct 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {83},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2023.102578},
doi = {10.1016/j.rcim.2023.102578},
journal = {Robot. Comput.-Integr. Manuf.},
month = oct,
numpages = {14},
keywords = {Graph neural networks, Smart manufacturing, Cyber-physical production system, Robotic mobile fulfillment system}
}

@inproceedings{10.5555/3539845.3540108,
author = {Pan, Zhixin and Mishra, Prabhat},
title = {Hardware acceleration of explainable machine learning},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Machine learning (ML) is successful in achieving human-level performance in various fields. However, it lacks the ability to explain an outcome due to its black-box nature. While recent efforts on explainable ML has received significant attention, the existing solutions are not applicable in real-time systems since they map interpretability as an optimization problem, which leads to numerous iterations of time-consuming complex computations. To make matters worse, existing implementations are not amenable for hardware-based acceleration. In this paper, we propose an efficient framework to enable acceleration of explainable ML procedure with hardware accelerators. We explore the effectiveness of both Tensor Processing Unit (TPU) and Graphics Processing Unit (GPU) based architectures in accelerating explainable ML. Specifically, this paper makes three important contributions. (1) To the best of our knowledge, our proposed work is the first attempt in enabling hardware acceleration of explainable ML. (2) Our proposed solution exploits the synergy between matrix convolution and Fourier transform, and therefore, it takes full advantage of TPU's inherent ability in accelerating matrix computations. (3) Our proposed approach can lead to real-time outcome interpretation. Extensive experimental evaluation demonstrates that proposed approach deployed on TPU can provide drastic improvement in interpretation time (39x on average) as well as energy efficiency (69x on average) compared to existing acceleration techniques.},
booktitle = {Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe},
pages = {1127–1130},
numpages = {4},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@article{10.1016/j.neucom.2022.09.026,
author = {Tang, Hong and Ling, Xiangzheng and Li, Liangzhi and Xiong, Liyan and Yao, Yu and Huang, Xiaohui},
title = {One-shot pruning of gated recurrent unit neural network by sensitivity for time-series prediction},
year = {2022},
issue_date = {Nov 2022},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {512},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2022.09.026},
doi = {10.1016/j.neucom.2022.09.026},
journal = {Neurocomput.},
month = nov,
pages = {15–24},
numpages = {10},
keywords = {Time-series, Pruning, Gated recurrent units (GRU), Deep learning}
}

@article{10.1016/j.dss.2016.02.005,
author = {G\'{o}mez-Vallejo, H.J. and Uriel-Latorre, B. and Sande-Meijide, M. and Villamar\'{\i}n-Bello, B. and Pav\'{o}n, R. and Fdez-Riverola, F. and Glez-Pe\~{n}a, D.},
title = {A case-based reasoning system for aiding detection and classification of nosocomial infections},
year = {2016},
issue_date = {April 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {84},
number = {C},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2016.02.005},
doi = {10.1016/j.dss.2016.02.005},
abstract = {Nowadays, it is recognized worldwide that healthcare-associated infections are responsible for an increase in patient morbidity, mortality, and higher costs related to prolonged hospital stays. As electronic health data are increasingly available today, there is a unique opportunity to implement real-time decision support systems for automating the surveillance of healthcare-associated infections. As a consequence, different electronic surveillance systems have been implemented to date with varying degrees of success. However, there have been few instances in which clinical data and physician narratives with the potential to significantly improve electronic surveillance alternatives have been adopted. In this context, the present work introduces a case-based reasoning system for the automatic surveillance and diagnosis of healthcare-associated infections. The developed system makes use of different machine learning techniques in order to (i) automatically extract evidence from different types of data including clinical unstructured documents, (ii) incorporate static a priori knowledge handled by infection preventionists, and (iii) dynamically generate new knowledge as well as understandable explanations about the system's decisions. Results obtained from a real deployment in a public hospital belonging to the Spanish National Health System trained with 2569 samples belonging to 1800 patients during more than 10 consecutive months recognize the usefulness of the system. Display Omitted Automatic surveillance of healthcare-associated infections.Diagnostic decision support system aiding monitoring and control.Case-based reasoning system for classifying nosocomial infections.Static rule-based knowledge representation and dynamic induction process.Natural language processing for physician narratives and nurses' comments.},
journal = {Decis. Support Syst.},
month = apr,
pages = {104–116},
numpages = {13},
keywords = {Infection detection and classification, Healthcare-associated infections, Clinical decision support system, Case-based reasoning, Automatic surveillance}
}

@inproceedings{10.1145/3450614.3464617,
author = {Oyibo, Kiemute},
title = {EMVE-DeCK: A Theory-Based Framework for Designing and Tailoring Persuasive Technology},
year = {2021},
isbn = {9781450383677},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450614.3464617},
doi = {10.1145/3450614.3464617},
abstract = {Although the importance of tailoring persuasive technologies (PTs) has been discussed extensively in the literature, there is insufficient theory-driven guidance on how to employ social psychology theories in the design of PT interventions. In this paper, we provide an overview of the key frameworks in the extant literature for designing information systems, in general, and persuasive systems, in particular. Specifically, we identify their limitations, and propose a new framework called ”EMVE-DeCK Framework” based on the synthesis of the strengths of the existing frameworks. The EMVE-DeCK Framework, which is grounded in Bandura’s Triad of Reciprocal Determinism, comprises seven steps, which include: (1) Explain: Employ “Theory” to explain the target “Behavior” by uncovering the relationship between the “Behavioral Determinants” and the target “Behavior”; (2) Map: Map the significant “Behavioral Determinants” in the “Theory” domain to “Persuasive Strategies” in the “Technology” domain; (3) Validate: Validate the target users’ receptiveness to the “Persuasive Strategies” in the “Technology” domain; (4) Explicate: Employ “Theory” to explicate (explain) the adoption of the proposed persuasive “Technology” by uncovering the relationship between the user experience (UX) “Design Attributes” and the persuasive “Technology Adoption”; (5) Design: Design and implement theory-driven, tailored persuasive “Technology”; (6) Change: Deploy the persuasive “Technology” to change “Behavior” in the field; and (7) Knowledge: Contribute “Findings” to Knowledge. We discuss the framework in the context of PT interventions.},
booktitle = {Adjunct Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {257–267},
numpages = {11},
keywords = {tailoring, persuasive technology, personalization, framework},
location = {Utrecht, Netherlands},
series = {UMAP '21}
}

@inproceedings{10.1007/978-3-662-53525-7_9,
author = {Cotta, Carlos and Fern\'{a}ndez-Leiva, Antonio J. and Vega, Francisco Fern\'{a}ndez and Ch\'{a}vez, Francisco and Merelo, Juan J. and Castillo, Pedro A. and Camacho, David and R-Moreno, Mar\'{\i}a D.},
title = {Application Areas of Ephemeral Computing: A Survey},
year = {2016},
isbn = {9783662535240},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-53525-7_9},
doi = {10.1007/978-3-662-53525-7_9},
abstract = {It is increasingly common that computational devices with significant computing power are underexploited. Some of the reasons for that are due to frequent idle-time or to the low computational demand of the tasks they perform, either sporadically or in their regular duty. The exploitation of this otherwise-wasted computational power is a cost-effective solution for solving complex computational tasks. Individually device-wise, this computational power can sometimes comprise a stable, long-lasting availability window but it will more frequently take the form of brief, ephemeral bursts. Then, in this context a highly dynamic and volatile computational landscape emerges from the collective contribution of such numerous devices. Algorithms consciously running on this kind of environment require specific properties in terms of flexibility, plasticity and robustness. Bioinspired algorithms are particularly well suited to this endeavor, thanks to some of the features they inherit from their biological sources of inspiration, namely decentralized functioning, intrinsic parallelism, resilience, and adaptiveness. Deploying bioinspired techniques on this scenario, and conducting analysis and modelling of the underlying Ephemeral Computing environment will also pave the way for the application of other non-bioinspired techniques on this computational domain. Computational creativity and content generation in video games are applications areas of the foremost economical interest and are well suited to Ephemeral Computing due to their intrinsic ephemeral nature and the widespread abundance of gaming applications in all kinds of devices. In this paper, we will explain why and how they can be adapted to this new environment.},
booktitle = {Transactions on Computational Collective Intelligence XXIV - Volume 9770},
pages = {153–167},
numpages = {15},
keywords = {Evolutionary computation, Ephemeral computing, Distributed computing, Complex systems, Bioinspired optimization, Autonomic computing}
}

@inproceedings{10.1007/978-3-030-31095-0_3,
author = {De Vos, Marina and Kirrane, Sabrina and Padget, Julian and Satoh, Ken},
title = {ODRL Policy Modelling and Compliance Checking},
year = {2019},
isbn = {978-3-030-31094-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-31095-0_3},
doi = {10.1007/978-3-030-31095-0_3},
abstract = {This paper addresses the problem of constructing a policy pipeline that enables compliance checking of business processes against regulatory obligations. Towards this end, we propose an Open Digital Rights Language (ODRL) profile that can be used to capture the semantics of both business policies in the form of sets of required permissions and regulatory requirements in the form of deontic concepts, and present their translation into Answer Set Programming (via the Institutional Action Language (InstAL)) for compliance checking purposes. The result of the compliance checking is either a positive compliance result or an explanation pertaining to the aspects of the policy that are causing the non-compliance. The pipeline is illustrated using two (key) fragments of the General Data Protect Regulation, namely Articles 6 (Lawfulness of processing) and Articles 46 (Transfers subject to appropriate safeguards) and industrially-relevant use cases that involve the specification of sets of permissions that are needed to execute business processes. The core contributions of this paper are the ODRL profile, which is capable of modelling regulatory obligations and business policies, the exercise of modelling elements of GDPR in this semantic formalism, and the operationalisation of the model to demonstrate its capability to support personal data processing compliance checking, and a basis for explaining why the request is deemed compliant or not.},
booktitle = {Rules and Reasoning: Third International Joint Conference, RuleML+RR 2019, Bolzano, Italy, September 16–19, 2019, Proceedings},
pages = {36–51},
numpages = {16},
location = {Bolzano, Italy}
}

@article{10.1007/s11633-018-1154-7,
author = {Li, Hao and Wang, Yu-Ping and Mu, Tai-Jiang},
title = {Synthesizing Robot Programs with Interactive Tutor Mode},
year = {2019},
issue_date = {Aug 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {4},
issn = {1476-8186},
url = {https://doi.org/10.1007/s11633-018-1154-7},
doi = {10.1007/s11633-018-1154-7},
abstract = {With the rapid development of the robotic industry, domestic robots have become increasingly popular. As domestic robots are expected to be personal assistants, it is important to develop a natural language-based human-robot interactive system for end-users who do not necessarily have much programming knowledge. To build such a system, we developed an interactive tutoring framework, named “Holert”, which can translate task descriptions in natural language to machine-interpretable logical forms automatically. Compared to previous works, Holert allows users to teach the robot by further explaining their intentions in an interactive tutor mode. Furthermore, Holert introduces a semantic dependency model to enable the robot to “understand” similar task descriptions. We have deployed Holert on an open-source robot platform, Turtlebot 2. Experimental results show that the system accuracy could be significantly improved by 163.9% with the support of the tutor mode. This system is also efficient. Even the longest task session with 10 sentences can be handled within 0.7 s.},
journal = {Int. J. Autom. Comput.},
month = aug,
pages = {462–474},
numpages = {13},
keywords = {natural language understanding, intelligent robotic systems, program synthesis, semantic parsing, Human-robot interaction}
}

@inproceedings{10.1145/3209978.3210018,
author = {Lin, Jimmy and Mohammed, Salman and Sequiera, Royal and Tan, Luchen},
title = {Update Delivery Mechanisms for Prospective Information Needs: An Analysis of Attention in Mobile Users},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210018},
doi = {10.1145/3209978.3210018},
abstract = {Real-time summarization systems that monitor document streams to identify relevant content have a few options for delivering system updates to users. In a mobile context, systems could send push notifications to users' mobile devices, hoping to grab their attention immediately. Alternatively, systems could silently deposit updates into "inboxes" that users can access at their leisure. We refer to these mechanisms as push-based vs. pull-based, and present a two-year contrastive study that attempts to understand the effects of the delivery mechanism on mobile user behavior, in the context of the TREC Real-Time Summarization Tracks. Through a cluster analysis, we are able to identify three distinct and coherent patterns of behavior. As expected, we find that users are likely to ignore push notifications, but for those updates that users do pay attention to, content is consumed within a short amount of time. Interestingly, users bombarded with push notifications are less likely to consume updates on their own initiative and less likely to engage in long reading sessions---which is a common pattern for users who pull content from their inboxes. We characterize users as exhibiting "eager" or "apathetic" information consumption behavior as an explanation of these observations, and attempt to operationalize our findings into design recommendations.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {785–794},
numpages = {10},
keywords = {social media, real-time summarization, push notifications, mobile users},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@article{10.1016/j.asoc.2018.01.024,
author = {Pe\~{n}a, Alejandro and Bonet, Isis and Lochmuller, Christian and Chiclana, Francisco and G\'{o}ngora, Mario},
title = {Flexible inverse adaptive fuzzy inference model to identify the evolution of operational value at risk for improving operational risk management},
year = {2018},
issue_date = {Apr 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {65},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2018.01.024},
doi = {10.1016/j.asoc.2018.01.024},
journal = {Appl. Soft Comput.},
month = apr,
pages = {614–631},
numpages = {18},
keywords = {Basel II, Basel Committee on Banking Supervision, Risk management matrix, Operational value at risk, Loss distribution approach, Montecarlo sampling, Adaptive fuzzy inference model, Operational risk}
}

@inproceedings{10.1145/3230599.3230618,
author = {Mart\'{\i}nez-Casta\~{n}o, Rodrigo and Pichel, Juan C. and Losada, David E.},
title = {Building Python-Based Topologies for Massive Processing of Social Media Data in Real Time},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230618},
doi = {10.1145/3230599.3230618},
abstract = {In this paper we propose a streaming approach for real-time processing of huge amounts of data. CATENAE is a library for easy building and execution of Python topologies (e.g., web crawler, classifier). Topologies are designed for their deployment inside Docker containers and, thus, horizontal scaling, granular resource assignment and isolation can be achieved easily. Furthermore, micromodules can have its own dependencies (including the Python version), allowing the user to limit resources such as CPU or memory by instance. We describe an implementation of a use case composed of two topologies: (1) a crawler for tracking users in social media and (2) an early risk detector of depression. We also explain how CATENAE topologies can be connected to non-Python systems.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {18},
numpages = {8},
keywords = {Text Mining, Stream Processing, Social Media, Real-Time Processing, Python, Docker, Depression},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@phdthesis{10.5555/AAI29137969,
author = {Singadi, Srinivas Kenchappa},
advisor = {Mnauel, de Jesus Pereira, Antonio},
title = {Earth Observation},
year = {2017},
isbn = {9798835529124},
publisher = {Instituto Politecnico de Leiria (Portugal)},
abstract = {This is a system project report explaining Earth observation project undertaken during my internship with XSealence S.A. The project was proposed by the company but since it was a large project it was divided into small several units and assigned to various groups. The report has been divided into various chapters which discuss the software development life cycle of Earth observation system. the project used agile development model which allowed the project to be divided into various phases. The first phase is discussed in the first chapter which helped state the motivations behind the project. Buoyed by rising demand for marine weather forecast XSealence S.A company developed a marine weather monitoring web application to monitor and predict weather data based on data downloaded from Copernicus marine monitoring service. So as to have an installable system that can be installed on sea vessels XSealence S.A developed a desktop version of the same. This report discusses the development of earth observation system which is the part of the main desktop application.The aim of developing earth observation was to help predict the marine weather. The project initialization involved setting project aims and goals which were to guide through the development process. The main project goal was to deliver a desktop application that will help forecast marine weather. The application was linked to Copernicus marine monitoring service where it downloaded data in a .nc format and converted to .txt format.The system requirement phase involved the collection of functional requirements of the system. With the help of my supervisor, it was established my part of the main system only needed to predict marine weather including water and ice speed, water velocity, sea surface height, temperature among other marine weather options. These capabilities of the system helped in system design and development to ensure the final product meets the set targets. UML diagrams including use case, data flow diagrams helped illustrate the flow of information in the system. These designs together with class diagrams were converted into a working solution during system development phase as planned in the project planning phase of the agile methodology.The system was tested for the achievement of system goals and objectives as stated in system functional requirements. Functional requirements achievement was tested using test use case. The test case tested every functionality of the system to establish the non-achieved goals and objectives. These defects found were communicated to the developer for correction in the subsequent releases. The system has been handed to the XSealence S.A for integration with another main desktop application. The entire system maintenance and deployment will be done by the company. To close the project, documentation was done to cover all activities of the project.},
note = {AAI29137969}
}

@inproceedings{10.1145/3041021.3055136,
author = {Lecuyer, Mathias and Tucker, Max and Chaintreau, Augustin},
title = {Improving the Transparency of the Sharing Economy},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3055136},
doi = {10.1145/3041021.3055136},
abstract = {The idealistic beginnings of the sharing economy made ways to an entrenched battle to win over the public opinion and for law makers to appreciate its benefits and its risks. The stakes are high as the success of services like Airbnb reveals that under-utilized assets (e.g. spare rooms or apartments left vacant) can be efficiently matched to individual demands to generate a significant surplus to their owners. Rules and regulation, which are increasingly felt as necessary by many communities, also create friction over the best way to leverage these opportunities for growth. To make things worse, the sharing economy is complex and poorly documented: Three recent reports from public institutions and lobbying groups arrived at opposite conclusions with seemingly contradictory facts about the occupancy distribution.In this paper, we show how to overcome this opacity by offering the first large-scale, reproducible study of Airbnb's supply and transactions. We devised and deployed frequently repeated crawls using no proprietary data. We show that these can be used to accurately estimate not only the supply of available rooms, but the effective transactions, occupancy, and revenue of hosts. Our results provide the first complete view of the occupancy and the distribution of revenue, revealing important trends that generalize previous observations. In particular we found that previous observations that seemed at odds are all explained by a variant of the "inspection paradox". We also found from our detailed data that enforcing a maximum occupancy of 90 nights a year would greatly reduce most concerns raised by various advocacy groups, while affecting only marginally the justifying claims that Airbnb quotes to argue for its beneficial impact.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1043–1051},
numpages = {9},
keywords = {transparency, sharing economy, measurement, airbnb},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

