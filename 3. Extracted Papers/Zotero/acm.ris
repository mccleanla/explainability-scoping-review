TY  - JOUR
TI  - Cyber-physical systems with Human-in-the-Loop: A systematic review of socio-technical perspectives
AU  - Clemmensen, Torkil
AU  - Moghaddam, Mahyar Tourchi
AU  - Nørbjerg, Jacob
DA  - 2025/05//
PY  - 2025
DO  - 10.1016/j.jss.2025.112348
VL  - 226
IS  - C
J2  - J. Syst. Softw.
SN  - 0164-1212
UR  - https://doi.org/10.1016/j.jss.2025.112348
KW  - Cyber-physical systems
KW  - Human-in-the-Loop
KW  - Socio-technical
ER  - 

TY  - JOUR
TI  - Human in the loop automation: Ride-hailing with remote (tele-)drivers
AU  - Benjaafar, Saif
AU  - Wang, Zicheng
AU  - Yang, Xiaotang
T2  - Management Science Series A-theory
AB  - Tele-driving refers to a novel concept by which drivers can remotely operate vehicles (without being physically in the vehicle). By putting the human back in the loop, tele-driving has emerged recently as a more viable alternative to fully automated vehicles with ride-hailing (and other on-demand transportation-enabled services) being an important application. Because remote drivers can be operated as a shared resource (any driver can be assigned to any customer regardless of trip origin or destination), it may be possible for such services to deploy fewer drivers than vehicles without significantly reducing service quality. In this paper, we examine the extent to which this is possible. Using a spatial queueing model that captures the dynamics of both pickup and trip times, we show that the impact of reducing the number of drivers depends crucially on system workload relative to the number of vehicles. In particular, when workload is sufficiently high relative to the number of vehicles, we show that, perhaps surprisingly, reducing the number of drivers relative to the number of vehicles can actually improve service level (e.g., as measured by the amount of demand fulfilled in the case of impatient customers). Having fewer drivers than vehicles ensures that there are always idle vehicles; the fewer the drivers, the likelier it is for there to be more idle vehicles. Consequently, the fewer the drivers, the likelier it is for the pickup times to be shorter (making overall shorter service times likelier). The impact of shorter service time is particularly significant when the workload is high, and in this case, it is enough to overcome the loss in driver capacity. When workload is sufficiently low relative to the number of vehicles, we show that it is possible to significantly reduce the number of drivers without significantly reducing service level. In systems in which customers are patient and willing to queue up for the service, we show that reducing the number of drivers can also reduce delay, including stabilizing a system that may otherwise be unstable. In general, relative to a system in which the number of vehicles equals the number of drivers (as in a system with in-vehicle drivers), a system with remote drivers can result in savings in the number of drivers either without significantly degrading performance or actually improving performance. We discuss how these results can, in part, be explained by the interplay of two counteracting forces: (1) having fewer drivers increasing service rate and (2) having fewer drivers reducing the number of servers with the relative strength of these forces depending on system workload. This paper was accepted by Baris Ata, stochastic models and simulation. Funding: This work was supported by the US National Science Foundation [Grant SCC-1831140], and the Guangdong (China) Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence [2023B1212010001]. Supplemental Material: The online appendix and data files are available at .
DA  - 2025/03//
PY  - 2025
DO  - 10.1287/mnsc.2022.01687
VL  - 71
IS  - 3
SP  - 2527
EP  - 2543
J2  - Manage. Sci.
SN  - 0025-1909
UR  - https://doi.org/10.1287/mnsc.2022.01687
KW  - capacity optimization
KW  - ride hailing
KW  - spatial queueing systems
KW  - tele-driving
ER  - 

TY  - CONF
TI  - Explainable human-in-the-loop dynamic data-driven digital twins
AU  - Zhang, Nan
AU  - Bahsoon, Rami
AU  - Tziritas, Nikos
AU  - Theodoropoulos, Georgios
AB  - Digital Twins (DT) are essentially dynamic data-driven models that serve as real-time symbiotic “virtual replicas” of real-world systems. DT can leverage fundamentals of Dynamic Data-Driven Applications Systems (DDDAS) bidirectional symbiotic sensing feedback loops for its continuous updates. Sensing loops can consequently steer measurement, analysis and reconfiguration aimed at more accurate modelling and analysis in DT. The reconfiguration decisions can be autonomous or interactive, keeping human-in-the-loop. The trustworthiness of these decisions can be hindered by inadequate explainability of the rationale, and utility gained in implementing the decision for the given situation among alternatives. Additionally, different decision-making algorithms and models have varying complexity, quality and can result in different utility gained for the model. The inadequacy of explainability can limit the extent to which humans can evaluate the decisions, often leading to updates which are unfit for the given situation, erroneous, compromising the overall accuracy of the model. The novel contribution of this paper is an approach to harnessing explainability in human-in-the-loop DDDAS and DT systems, leveraging bidirectional symbiotic sensing feedback. The approach utilises interpretable machine learning and goal modelling to explainability, and considers trade-off analysis of utility gained. We use examples from smart warehousing to demonstrate the approach.
C1  - Cambridge, MA, USA and Berlin, Heidelberg
C3  - Dynamic data driven applications systems: 4th international conference, DDDAS 2022, cambridge, MA, USA, october 6–10, 2022, proceedings
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-52670-1_23
SP  - 233
EP  - 243
PB  - Springer-Verlag
SN  - 978-3-031-52669-5
UR  - https://doi.org/10.1007/978-3-031-52670-1_23
KW  - DDDAS
KW  - Digital Twins
KW  - Explainability
KW  - Human-in-the-loop
ER  - 

TY  - JOUR
TI  - HEX: Human-in-the-loop explainability via deep reinforcement learning
AU  - Lash, Michael T.
T2  - Decision Support Systems
DA  - 2024/12//
PY  - 2024
DO  - 10.1016/j.dss.2024.114304
VL  - 187
IS  - C
J2  - Decis. Support Syst.
SN  - 0167-9236
UR  - https://doi.org/10.1016/j.dss.2024.114304
KW  - Machine learning
KW  - Explainability
KW  - Human-in-the-loop
KW  - Behavioral machine learning
KW  - Decision support
KW  - Deep reinforcement learning
KW  - Interpretability
ER  - 

TY  - JOUR
TI  - Ethical management of human-AI interaction: Theory development review
AU  - Heyder, Teresa
AU  - Passlack, Nina
AU  - Posegga, Oliver
DA  - 2023/09//
PY  - 2023
DO  - 10.1016/j.jsis.2023.101772
VL  - 32
IS  - 3
J2  - J. Strateg. Inf. Syst.
SN  - 0963-8687
UR  - https://doi.org/10.1016/j.jsis.2023.101772
KW  - Artificial intelligence
KW  - Ethics
KW  - Human-AI interaction
KW  - Sociomateriality
KW  - Theoretical review
ER  - 

TY  - CONF
TI  - Autonomous policy explanations for effective human-machine teaming
AU  - Tabrez, Aaquib
T3  - AAAI'24/IAAI'24/EAAI'24
AB  - Policy explanation, a process for describing the behavior of an autonomous system, plays a crucial role in effectively conveying an agent's decision-making rationale to human collaborators and is essential for safe real-world deployments. It becomes even more critical in effective human-robot teaming, where good communication allows teams to adapt and improvise successfully during uncertain situations by enabling value alignment within the teams. This thesis proposal focuses on improving human-machine teaming by developing novel human-centered explainable AI (xAI) techniques that empower autonomous agents to communicate their capabilities and limitations via multiple modalities, teach and influence human teammates' behavior as decision-support systems, and effectively build and manage trust in HRI systems.
C3  - Proceedings of the thirty-eighth AAAI conference on artificial intelligence and thirty-sixth conference on innovative applications of artificial intelligence and fourteenth symposium on educational advances in artificial intelligence
DA  - 2024///
PY  - 2024
DO  - 10.1609/aaai.v38i21.30412
PB  - AAAI Press
SN  - 978-1-57735-887-9
UR  - https://doi.org/10.1609/aaai.v38i21.30412
ER  - 

TY  - JOUR
TI  - A taxonomy of human–machine collaboration: capturing automation and technical autonomy
AU  - Simmler, Monika
AU  - Frischknecht, Ruth
T2  - AI Soc.
AB  - Due to the ongoing advancements in technology, socio-technical collaboration has become increasingly prevalent. This poses challenges in terms of governance and accountability, as well as issues in various other fields. Therefore, it is crucial to familiarize decision-makers&nbsp;and researchers with the core of human–machine collaboration. This study introduces a taxonomy that enables identification of the very nature of human–machine interaction. A literature review has revealed that automation and technical autonomy are main parameters for describing and understanding such interaction. Both aspects must be carefully evaluated, as their increase has potentially far-reaching consequences. Hence, these two concepts comprise the taxonomy’s axes. Five levels of automation and five levels of technical autonomy are introduced below, based on the assumption that both automation and autonomy are gradual. The levels of automation were developed from existing approaches; those of autonomy were carefully derived from a review of the literature. The taxonomy’s use is also explained, as are its limitations and avenues for further research.
DA  - 2021/03//
PY  - 2021
DO  - 10.1007/s00146-020-01004-z
VL  - 36
IS  - 1
SP  - 239
EP  - 250
SN  - 0951-5666
UR  - https://doi.org/10.1007/s00146-020-01004-z
KW  - Autonomy
KW  - Automation
KW  - Human–machine collaboration
KW  - Taxonomy
ER  - 

TY  - JOUR
TI  - Real-time decision support for human–machine interaction in digital railway control rooms
AU  - Sobrie, Léon
AU  - Verschelde, Marijn
T2  - Decision Support Systems
DA  - 2024/06//
PY  - 2024
DO  - 10.1016/j.dss.2024.114216
VL  - 181
IS  - C
J2  - Decis. Support Syst.
SN  - 0167-9236
UR  - https://doi.org/10.1016/j.dss.2024.114216
KW  - Behavioral prescription
KW  - Decision support systems
KW  - End-user feedback
KW  - Explainable prediction
KW  - Human–machine interaction
KW  - Real-time railway traffic management
ER  - 

TY  - CONF
TI  - An uncertainty-based human-in-the-loop system for industrial tool wear analysis
AU  - Treiss, Alexander
AU  - Walk, Jannis
AU  - Kühl, Niklas
AB  - Convolutional neural networks have shown to achieve superior performance on image segmentation tasks. However, convolutional neural networks, operating as black-box systems, generally do not provide a reliable measure about the confidence of their decisions. This leads to various problems in industrial settings, amongst others, inadequate levels of trust from users in the model’s outputs as well as a non-compliance with current policy guidelines (e.g., EU AI Strategy). To address these issues, we use uncertainty measures based on Monte-Carlo dropout in the context of a human-in-the-loop system to increase the system’s transparency and performance. In particular, we demonstrate the benefits described above on a real-world multi-class image segmentation task of wear analysis in the machining industry. Following previous work, we show that the quality of a prediction correlates with the model’s uncertainty. Additionally, we demonstrate that a multiple linear regression using the model’s uncertainties as independent variables significantly explains the quality of a prediction (R2=0.718). Within the uncertainty-based human-in-the-loop system, the multiple regression aims at identifying failed predictions on an image-level. The system utilizes a human expert to label these failed predictions manually. A simulation study demonstrates that the uncertainty-based human-in-the-loop system increases performance for different levels of human involvement in comparison to a random-based human-in-the-loop system. To ensure generalizability, we show that the presented approach achieves similar results on the publicly available Cityscapes dataset.
C1  - Ghent, Belgium and Berlin, Heidelberg
C3  - Machine learning and knowledge discovery in databases. Applied data science and demo track: European conference, ECML PKDD 2020, ghent, belgium, september 14–18, 2020, proceedings, part V
DA  - 2020///
PY  - 2020
DO  - 10.1007/978-3-030-67670-4_6
SP  - 85
EP  - 100
PB  - Springer-Verlag
SN  - 978-3-030-67669-8
UR  - https://doi.org/10.1007/978-3-030-67670-4_6
KW  - Deep learning
KW  - Uncertainty
KW  - Human-in-the-loop
KW  - Image segmentation
ER  - 

TY  - CONF
TI  - Understanding contestability on the margins: Implications for the design of algorithmic decision-making in public services
AU  - Karusala, Naveena
AU  - Upadhyay, Sohini
AU  - Veeraraghavan, Rajesh
AU  - Gajos, Krzysztof Z.
T3  - Chi '24
AB  - Policymakers have established that the ability to contest decisions made by or with algorithms is core to responsible artificial intelligence (AI). However, there has been a disconnect between research on contestability of algorithms, and what the situated practice of contestation looks like in contexts across the world, especially amongst communities on the margins. We address this gap through a qualitative study of follow-up and contestation in accessing public services for land ownership in rural India and affordable housing in the urban United States. We find there are significant barriers to exercising rights and contesting decisions, which intermediaries like NGO workers or lawyers work with communities to address. We draw on the notion of accompaniment in global health to highlight the open-ended work required to support people in navigating violent social systems. We discuss the implications of our findings for key aspects of contestability, including building capacity for contestation, human review, and the role of explanations. We also discuss how sociotechnical systems of algorithmic decision-making can embody accompaniment by taking on a higher burden of preventing denials and enabling contestation.
C1  - Honolulu, HI, USA and New York, NY, USA
C3  - Proceedings of the 2024 CHI conference on human factors in computing systems
DA  - 2024///
PY  - 2024
DO  - 10.1145/3613904.3641898
PB  - Association for Computing Machinery
SN  - 979-8-4007-0330-0
UR  - https://doi.org/10.1145/3613904.3641898
KW  - algorithmic decision-making
KW  - contestability
KW  - India
KW  - public services
KW  - United States
ER  - 

TY  - CONF
TI  - The impact of explanations on fairness in human-AI decision-making: Protected vs proxy features
AU  - Goyal, Navita
AU  - Baumler, Connor
AU  - Nguyen, Tin
AU  - Daumé III, Hal
T3  - Iui '24
AB  - AI systems have been known to amplify biases in real-world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants’ perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments—explanations, model bias disclosure and proxy correlation disclosure—affect fairness perception and parity. We find that explanations help people detect direct but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model biases. Disclosures can help mitigate this effect for indirect biases, improving both unfairness recognition and decision-making fairness. We hope that our findings can help guide further research into advancing explanations in support of fair human-AI decision-making.
C1  - Greenville, SC, USA and New York, NY, USA
C3  - Proceedings of the 29th international conference on intelligent user interfaces
DA  - 2024///
PY  - 2024
DO  - 10.1145/3640543.3645210
SP  - 155
EP  - 180
PB  - Association for Computing Machinery
SN  - 979-8-4007-0508-3
UR  - https://doi.org/10.1145/3640543.3645210
KW  - explanations
KW  - fairness
KW  - human-AI decision-making
KW  - indirect biases
ER  - 

TY  - CONF
TI  - New frontiers of human-centered explainable AI (HCXAI): Participatory civic AI, benchmarking llms, XAI hallucinations, and responsible AI audits
AU  - Ehsan, Upol
AU  - Watkins, Elizabeth A
AU  - Wintersberger, Philipp
AU  - Manger, Carina
AU  - Hubig, Nina
AU  - Savage, Saiph
AU  - Weisz, Justin D.
AU  - Riener, Andreas
T3  - Chi ea '25
AB  - Explainable AI (XAI) is more than just “opening” the black box — who opens it matters just as much, if not more, as the ways of opening it. Human-centered XAI (HCXAI) advocates that algorithmic transparency alone is not sufficient for making AI explainable. In our fifth CHI workshop on Human-Centered XAI (HCXAI), we shift our focus to new, emerging frontiers of explainability: (1) participatory approaches toward explainability in civic AI applications; (2) addressing hallucinations in LLMs using explainability benchmarks; (3) connecting HCXAI research with Responsible AI practices, algorithmic auditing, and public policy; and (4) improving representation of XAI issues from the Global South. We have built a strong community of HCXAI researchers through our workshop series whose work has made important conceptual, methodological, and technical impact on the field. In this installment, we will push the frontiers of work in HCXAI with an emphasis on operationalizing perspectives sociotechnically.
C1  - New York, NY, USA
C3  - Proceedings of the extended abstracts of the CHI conference on human factors in computing systems
DA  - 2025///
PY  - 2025
DO  - 10.1145/3706599.3706713
PB  - Association for Computing Machinery
SN  - 979-8-4007-1395-8
UR  - https://doi.org/10.1145/3706599.3706713
ER  - 

TY  - THES
TI  - Interactive hybrid intelligence systems for human-AI/robot collaboration: Improving the practices of physical stroke rehabilitation
AU  - Lee, Min Hun
AU  - Alexandre, Bernardino
AU  - K., Anind, Dey
AU  - Asim, Smailagic
AU  - M., Kris, Kitani
AU  - Hugo, Nicolau
AU  - Sergi, Bermúdez i Badia
AB  - Rapid advances in machine learning (ML) have made it applicable to healthcare practices. However, the deployment of these ML models remains a challenge due to the lack of user-centered designs and model interpretability and adaptability. This thesis introduces an interactive hybrid approach that combines an ML model with a rule-based model from experts to support transparent interactions with a user, but also iteratively be tuned with user inputs (e.g. therapist's feedback or patient's motions) to improve AI and robotic systems. Through iterative engagements with therapists and post-stroke patients, we explore how humans and artificial intelligence (AI) and robotic systems can collaborate to improve practices of physical stroke rehabilitation therapy: 1) human-AI collaborative decision-making on rehabilitation assessment for therapists and (2) human-robot collaborative rehabilitation therapy for post-stroke survivors. For human-AI collaborative decision-making, we found that our interactive system with transparent, patient-specific analysis significantly reduces therapists' efforts and improves their agreement level on rehabilitation assessment. In addition, therapists can provide feedback on our system for a more personalized prediction and a more efficient development of ML models. For human-robot collaborative rehabilitation therapy, we found that our system can be tuned with post-stroke survivor's data to generate personalized corrective feedback. Both therapists and post-stroke survivors appreciated the potential benefits of our system to achieve more systematic management and improve post-stroke survivors' self-efficacy and motivation in rehabilitation. Overall, this thesis discusses the value of iterative engagement with stakeholders and making a system explainable and interactive to create effective human-AI/robot collaboration on a complex task.
CY  - USA
DA  - 2021///
PY  - 2021
M3  - phd
PB  - Carnegie Mellon University
N1  - AAI28648408
ER  - 

TY  - CONF
TI  - Understanding uncertainty: How lay decision-makers perceive and interpret uncertainty in human-AI decision making
AU  - Prabhudesai, Snehal
AU  - Yang, Leyao
AU  - Asthana, Sumit
AU  - Huan, Xun
AU  - Liao, Q. Vera
AU  - Banovic, Nikola
T3  - Iui '23
AB  - Decision Support Systems (DSS) based on Machine Learning (ML) often aim to assist lay decision-makers, who are not math-savvy, in making high-stakes decisions. However, existing ML-based DSS are not always transparent about the probabilistic nature of ML predictions and how uncertain each prediction is. This lack of transparency could give lay decision-makers a false sense of reliability. Growing calls for AI transparency have led to increasing efforts to quantify and communicate model uncertainty. However, there are still gaps in knowledge regarding how and why the decision-makers utilize ML uncertainty information in their decision process. Here, we conducted a qualitative, think-aloud user study with 17 lay decision-makers who interacted with three different DSS: 1) interactive visualization, 2) DSS based on an ML model that provides predictions without uncertainty information, and 3) the same DSS with uncertainty information. Our qualitative analysis found that communicating uncertainty about ML predictions forced participants to slow down and think analytically about their decisions. This in turn made participants more vigilant, resulting in reduction in over-reliance on ML-based DSS. Our work contributes empirical knowledge on how lay decision-makers perceive, interpret, and make use of uncertainty information when interacting with DSS. Such foundational knowledge informs the design of future ML-based DSS that embrace transparent uncertainty communication.
C1  - Sydney, NSW, Australia and New York, NY, USA
C3  - Proceedings of the 28th international conference on intelligent user interfaces
DA  - 2023///
PY  - 2023
DO  - 10.1145/3581641.3584033
SP  - 379
EP  - 396
PB  - Association for Computing Machinery
SN  - 979-8-4007-0106-1
UR  - https://doi.org/10.1145/3581641.3584033
KW  - Machine Learning
KW  - Decision-making
KW  - Uncertainty.
ER  - 

TY  - THES
TI  - Human-machine collaboration in real-world machine-learning applications
AU  - Roberts, Claudia Veronica
AU  - Adji, Bousso Dieng
AU  - Barbara, Engelhardt
AU  - Andrés, Monroy-Hernández
AU  - Matt, Salganik
AB  - Automation tools like machine learning are a necessity in our big data world. Thanks to the Internet and advancements in all facets of computer and storage technology, almost everyone has a voice in the Internet connected world. However, there are still very real physical limits in our physical world. This dichotomy—the seemingly limitless nature of technology enabled data colliding with the physical limits of the real world—has made automation tools a necessity, and predictive models powered by machine learning algorithms are one such tool.The promise of machine learning to accurately predict future human behavior and human preferences has lead practitioners and researchers alike to apply machine learning automation tools to tasks such as product recommendations and speculatory activities such as long term job applicant success. However, due to the mercurial nature of humans, developing mathematical intermediaries to attempt to model and predict human behavior is challenging and not a straight-forward task. One way of harnessing the power of machine-learning backed automation to help reduce the scale of many real-world applications in more challenging domain settings is by having humans and machines collaborating in non-trivial ways. In this dissertation, we delineate the various ways in which humans and machines collaborate in challenging real-world applications. Moreover, we highlight three specific ways in which we can use human-machine collaboration to keep or increase utility and reduce real-world harm when using these systems in the wild: (i) humans enabling computers with domain specific knowledge, (ii) computers providing humans with algorithmic explanations, (iii) humans and computers working together in decision making.
CY  - USA
DA  - 2023///
PY  - 2023
M3  - phd
PB  - Princeton University
N1  - AAI30244191
ER  - 

TY  - JOUR
TI  - Artificial intelligence for decision-making and the future of work
AU  - Dennehy, Denis
AU  - Griva, Anastasia
AU  - Pouloudi, Nancy
AU  - Mäntymäki, Matti
AU  - Pappas, Ilias
DA  - 2023/04//
PY  - 2023
DO  - 10.1016/j.ijinfomgt.2022.102574
VL  - 69
IS  - C
J2  - Int. J. Inf. Manag.
SN  - 0268-4012
UR  - https://doi.org/10.1016/j.ijinfomgt.2022.102574
KW  - Artificial intelligence
KW  - Future of work
ER  - 

TY  - JOUR
TI  - The right to transparency in public governance: Freedom of information and the use of artificial intelligence by public agencies
AU  - Olsen, Henrik Palmer
AU  - Hildebrandt, Thomas Troels
AU  - Wiesener, Cornelius
AU  - Larsen, Matthias Smed
AU  - Flügge, Asbjørn William Ammitzbøll
T2  - Digit. Gov.: Res. Pract.
AB  - What information should and can be transparent for artificial intelligence (AI) algorithms? This article examines the socio-technical and legal perspectives of transparency in relation to algorithmic decision-making in public administration. We show how transparency in AI can be understood in light of the various technologies and the challenges one may encounter. Despite some first steps in that direction, there exists so far no mature standard for documenting AI models. From a legal perspective, this article examined the applicable freedom of information (FOI) regimes across different jurisdictions, with a particular focus on Denmark and other Scandinavian countries. Despite notable differences, our findings show that the FOI regimes generally only grant access to existing documents, and that access can be denied on the basis of the wide proprietary interests and internal documents exemptions. This is why we ultimately conclude that the European data-protection framework and the proposed EU AI Act — with their far-reaching duties to document the functioning of AI systems — provide promising new avenues for research and insights into transparency in AI.
DA  - 2024/03//
PY  - 2024
DO  - 10.1145/3632753
VL  - 5
IS  - 1
UR  - https://doi.org/10.1145/3632753
KW  - algorithm
KW  - Transparency
KW  - administrative decision-making
KW  - freedom of information
ER  - 

TY  - JOUR
TI  - A metaphysical account of agency for technology governance
AU  - Soltanzadeh, Sadjad
T2  - AI Soc.
AB  - The way in which agency is conceptualised has implications for understanding human–machine interactions and the governance of technology, especially artificial intelligence (AI)&nbsp;systems. Traditionally, agency is conceptualised as a capacity, defined by intrinsic properties, such as cognitive or volitional facilities. I argue that the capacity-based account of agency is inadequate to explain the dynamics of human–machine interactions and guide technology governance. Instead, I propose to conceptualise agency as impact. Agents as impactful entities can be identified at different levels: from the low level of individual entities to the high level of complex socio-technical systems. Entities can impact their surroundings through different channels, and more influential channels of impact lead to higher degrees of agency. Technology governance must take into account different channels of impact in the contexts of use, design and regulation.
DA  - 2024/04//
PY  - 2024
DO  - 10.1007/s00146-024-01941-z
VL  - 40
IS  - 3
SP  - 1723
EP  - 1734
SN  - 0951-5666
UR  - https://doi.org/10.1007/s00146-024-01941-z
KW  - Agency as impact
KW  - Capacity-based theories of agency
KW  - Channels of impact
KW  - Design and regulation
KW  - Human-machine interaction
ER  - 

TY  - CONF
TI  - Human perceptions on moral responsibility of AI: a case study in AI-assisted bail decision-making
AU  - Lima, Gabriel
AU  - Grgić-Hlača, Nina
AU  - Cha, Meeyoung
T3  - Chi '21
AB  - How to attribute responsibility for autonomous artificial intelligence (AI) systems’ actions has been widely debated across the humanities and social science disciplines. This work presents two experiments (N=200 each) that measure people’s perceptions of eight different notions of moral responsibility concerning AI and human agents in the context of bail decision-making. Using real-life adapted vignettes, our experiments show that AI agents are held causally responsible and blamed similarly to human agents for an identical task. However, there was a meaningful difference in how people perceived these agents’ moral responsibility; human agents were ascribed to a higher degree of present-looking and forward-looking notions of responsibility than AI agents. We also found that people expect both AI and human decision-makers and advisors to justify their decisions regardless of their nature. We discuss policy and HCI implications of these findings, such as the need for explainable AI in high-stakes scenarios.
C1  - Yokohama, Japan and New York, NY, USA
C3  - Proceedings of the 2021 CHI conference on human factors in computing systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3411764.3445260
PB  - Association for Computing Machinery
SN  - 978-1-4503-8096-6
UR  - https://doi.org/10.1145/3411764.3445260
KW  - AI
KW  - Responsibility
KW  - Moral Responsibility
KW  - Bail Decision-Making
KW  - Blame
KW  - COMPAS
KW  - Liability
KW  - Moral Judgment
ER  - 

TY  - THES
TI  - Empowering humans in human-AI decision making
AU  - Lai, Vivian
AU  - Chenhao, Tan
AU  - Vera, Q., Liao
AU  - Tamara, Sumner
AU  - Tom, Yeh
AB  - Due to recent advances in Artificial Intelligence (AI), AI models are able to surpass human performance in various tasks unprecedentedly and are rapidly integrated into systems that assist humans in making decisions. However, deploying such systems into the real world requires an understanding of the potential risks and challenges we might face. How do we interpret and explain AI models' predictions while being aware of their biases and weaknesses? In this thesis, I discuss my work that empowers humans to make better decisions with AI models through AI-backed interactive systems. I describe (1) how humans make decisions with models (Chapter 2), (2) how explanations differ across models and methods (Chapter 3), (3) how humans learn counterintuitive patterns from models (Chapter 4), and (4) how humans and imperfect models could collaborate effectively (Chapter 5). I conclude by discussing future research perspectives on making human-AI collaborations better and more accessible.
CY  - USA
DA  - 2022///
PY  - 2022
M3  - phd
PB  - University of Colorado at Boulder
N1  - AAI29322003
ER  - 

TY  - CONF
TI  - From human-human collaboration to human-AI collaboration: Designing AI systems that can work together with people
AU  - Wang, Dakuo
AU  - Churchill, Elizabeth
AU  - Maes, Pattie
AU  - Fan, Xiangmin
AU  - Shneiderman, Ben
AU  - Shi, Yuanchun
AU  - Wang, Qianying
T3  - Chi ea '20
AB  - Artificial Intelligent (AI) and Machine Learning (ML) algorithms are coming out of research labs into the real-world applications, and recent research has focused a lot on Human-AI Interaction (HAI) and Explainable AI (XAI). However, Interaction is not the same as Collaboration. Collaboration involves mutual goal understanding, preemptive task co-management and shared progress tracking. Most of human activities today are done collaboratively, thus, to integrate AI into the already-complicated human workflow, it is critical to bring the Computer-Supported Cooperative Work (CSCW) perspective into the root of the algorithmic research and plan for a Human-AI Collaboration future of work. In this panel we ask: Can this future for trusted human-AI collaboration be realized? If so, what will it take? This panel will bring together HCI experts who work on human collaboration and AI applications in various application contexts, from industry and academia and from both the U.S. and China. Panelists will engage the audience through discussion of their shared and diverging visions, and through suggestions for opportunities and challenges for the future of human-AI collaboration.
C1  - Honolulu, HI, USA and New York, NY, USA
C3  - Extended abstracts of the 2020 CHI conference on human factors in computing systems
DA  - 2020///
PY  - 2020
DO  - 10.1145/3334480.3381069
SP  - 1
EP  - 6
PB  - Association for Computing Machinery
SN  - 978-1-4503-6819-3
UR  - https://doi.org/10.1145/3334480.3381069
KW  - ai partner
KW  - ai-powered healthcare
KW  - computer-supported corporative work
KW  - explainable ai
KW  - group collaboration
KW  - human-ai collaboration
KW  - trusted ai
ER  - 

TY  - CONF
TI  - Collaborative appropriation of AI in the context of interacting with AI
AU  - Herrmann, Thomas
AB  - In the context of maintaining technical equipment, AI is used to detect possible problems. Human specialists check whether a real problem is addressed, and, in this case, try to solve it. Furthermore, they go on trying to translate the problem notification into an improvement of other software components into which the AI system is embedded. Thus, every AI result is not only the cause of immediate action but is also a trigger within the process of continuous appropriation of the technical infrastructure that includes AI. The whole socio-technical system is a subject of AI-related improvement as a collaborative task that requires continuous advancement of human competences and skills. This has to be supported by a type of explainable AI by which the process of understanding the reasons driving AI output is not a task for a single end-user but rather the result of combining different specialists’ viewpoints and competences.
C1  - Copenhagen, Denmark and Berlin, Heidelberg
C3  - Artificial intelligence in HCI: 4th international conference, AI-HCI 2023, held as part of the 25th HCI international conference, HCII 2023, copenhagen, denmark, july 23–28, 2023, proceedings, part II
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-35894-4_18
SP  - 249
EP  - 260
PB  - Springer-Verlag
SN  - 978-3-031-35893-7
UR  - https://doi.org/10.1007/978-3-031-35894-4_18
KW  - appropriation
KW  - human-centered artificial intelligence
KW  - keeping the organization in the loop
KW  - organizational practices
KW  - rule extraction
KW  - socio-technical design
ER  - 

TY  - CONF
TI  - Identifying explanation needs of end-users: Applying and extending the XAI question bank
AU  - Sipos, Lars
AU  - Schäfer, Ulrike
AU  - Glinka, Katrin
AU  - Müller-Birn, Claudia
T3  - MuC '23
AB  - Explainable Artificial Intelligence (XAI) is concerned with making the decisions of AI systems interpretable to humans. Explanations are typically developed by AI experts and focus on algorithmic transparency and the inner workings of AI systems. Research has shown that such explanations do not meet the needs of users who do not have AI expertise. As a result, explanations are often ineffective in making system decisions interpretable and understandable. We aim to strengthen a socio-technical view of AI by following a Human-Centered Explainable Artificial Intelligence (HC-XAI) approach, which investigates the explanation needs of end-users (i.e., subject matter experts and lay users) in specific usage contexts. One of the most influential works in this area is the XAI Question Bank (XAIQB) by Liao et al. The authors propose a set of questions that end-users might ask when using an AI system, which in turn is intended to help developers and designers identify and address explanation needs. Although the XAIQB is widely referenced, there are few reports of its use in practice. In particular, it is unclear to what extent the XAIQB sufficiently captures the explanation needs of end-users and what potential problems exist in the practical application of the XAIQB. To explore these open questions, we used the XAIQB as the basis for analyzing 12 think-aloud software explorations with subject matter experts, i.e., art historians. We investigated the suitability of the XAIQB as a tool for identifying explanation needs in a specific usage context. Our analysis revealed a number of explanation needs that were missing from the question bank, but that emerged repeatedly as our study participants interacted with an AI system. We also found that some of the XAIQB questions were difficult to distinguish and required interpretation during use. Our contribution is an extension of the XAIQB with 11 new questions. In addition, we have expanded the descriptions of all new and existing questions to facilitate their use. We hope that this extension will enable HCI researchers and practitioners to use the XAIQB in practice and may provide a basis for future studies on the identification of explanation needs in different contexts.
C1  - Rapperswil, Switzerland and New York, NY, USA
C3  - Proceedings of mensch und computer 2023
DA  - 2023///
PY  - 2023
DO  - 10.1145/3603555.3608551
SP  - 492
EP  - 497
PB  - Association for Computing Machinery
SN  - 979-8-4007-0771-1
UR  - https://doi.org/10.1145/3603555.3608551
KW  - Explainable AI
KW  - Explanation needs
KW  - Human-AI collaboration
KW  - User study
ER  - 

TY  - CONF
TI  - AI said, she said - how users perceive consumer scoring in practice
AU  - Recki, Lena
AU  - Esau-Held, Margarita
AU  - Lawo, Dennis
AU  - Stevens, Gunnar
T3  - MuC '23
AB  - As digitization continues, consumers are increasingly exposed to AI scoring decisions. However, currently lacking is a thorough understanding of how users’ misjudgments of an AI-supported system lead to it being rejected. Therefore, investigations are needed into the appropriation of such socio-technical systems in practice and how users describe their experience with algorithm-based scoring. To address this issue, we evaluated 1,003 user reviews of an app on car insurance that calculates premiums based on the consumers’ individual driving behavior. We find evidence that users develop their own folk theories to explain the algorithms with the help of situation-related experiences and that insufficient explanations lead to power asymmetries between consumers, the system, and the company. In particular, as a result of the different needs of the stakeholders, we uncover a fundamental conflict between computational risk assessment and the perceived agency to influence the score.
C1  - Rapperswil, Switzerland and New York, NY, USA
C3  - Proceedings of mensch und computer 2023
DA  - 2023///
PY  - 2023
DO  - 10.1145/3603555.3603562
SP  - 149
EP  - 160
PB  - Association for Computing Machinery
SN  - 979-8-4007-0771-1
UR  - https://doi.org/10.1145/3603555.3603562
KW  - Fairness
KW  - Explainable AI
KW  - Algorithmic Decision Making
KW  - Empirical study
KW  - Perception
ER  - 

TY  - CONF
TI  - STAIDCC20: 1st international workshop on socio-technical AI systems for defence, cybercrime and cybersecurity
AU  - Middleton, Stuart E.
AU  - Lavorgna, Anita
AU  - McAlister, Ruth
T3  - WebSci '20 companion
AB  - The purpose of STAIDCC20 workshop is to bring together a mixture of inter-disciplinary researchers and practitioners working in defence, cybercrime and cybersecurity application areas to discuss and explore the challenges and future research directions around socio-technical AI systems. The workshop will showcase where the state of the art is in socio-technical AI, charting a path around issues including transparency, trustworthiness, explaining bias and error, incorporating human judgment and ethical frameworks for deployment of socio-technical AI in the future.
C1  - Southampton, United Kingdom and New York, NY, USA
C3  - Companion publication of the 12th ACM conference on web science
DA  - 2020///
PY  - 2020
DO  - 10.1145/3394332.3402897
SP  - 78
EP  - 79
PB  - Association for Computing Machinery
SN  - 978-1-4503-7994-6
UR  - https://doi.org/10.1145/3394332.3402897
KW  - Artificial Intelligence
KW  - Cybercrime
KW  - Cybersecurity
KW  - Socio-technical
KW  - Criminology
KW  - Defence
ER  - 

TY  - JOUR
TI  - Power dynamics and value conflicts in designing and maintaining socio-technical algorithmic processes
AU  - Park, Joon Sung
AU  - Karahalios, Karrie
AU  - Salehi, Niloufar
AU  - Eslami, Motahhare
T2  - Proc. ACM Hum.-Comput. Interact.
AB  - How do power dynamics and value conflicts affect our ability to design and maintain socio-technical algorithmic processes? In this paper, we study the SIGCHI student volunteer (SV) selection process that uses a weighted semi-randomized algorithm to recruit a desired pool of volunteers. Our interviews with the community members showed that the process is complex and socio-technical; the algorithm's outputs are interpreted and adjusted by the conference organizers to reflect the community values while ensuring the selection of effective volunteers to help with organizing the conference. This provides a stage in which the power dynamics and value conflicts among the stakeholders play salient roles in determining how the process was perceived and envisioned. For instance, non-organizers of the conference found the algorithm used in the selection process to be a power-balancer that places a check on the organizers who oversee the process. However, even with a participatory process to elicit the algorithm's weights, the power dynamics and value conflicts between the participants made it difficult to reach a consensus on what the SV selection process should consider and prioritize. Our findings highlight the importance of value transparency – the type of transparency that focuses on explaining why a decision was made rather than how it was made – as a mechanism for resolving such conflicts. Based on our findings, we lay out design recommendations that can guide communities to better design and maintain algorithmic socio-technical processes over time in the face of power dynamics and value conflicts.
DA  - 2022/04//
PY  - 2022
DO  - 10.1145/3512957
VL  - 6
IS  - CSCW1
UR  - https://doi.org/10.1145/3512957
KW  - algorithmic governance
KW  - fairness
KW  - collective participation
KW  - participatory algorithm design
ER  - 

TY  - CONF
TI  - AI is entering regulated territory: Understanding the supervisors' perspective for model justifiability in financial crime detection
AU  - Bertrand, Astrid
AU  - Eagan, James R.
AU  - Maxwell, Winston
AU  - Brand, Joshua
T3  - Chi '24
AB  - Artificial intelligence (AI) has the potential to bring significant benefits to highly regulated industries such as healthcare or banking. Adoption, however, remains low. AI’s entry into complex socio-techno-legal systems raises issues of transparency, specifically for regulators. However, the perspective of supervisors, regulators who monitor compliance with applicable financial regulations, has rarely been studied. This paper focuses on understanding the needs of supervisors in anti-money laundering (AML) to better inform the design of AI justifications and explanations in highly regulated fields. Through scenario-based workshops with 13 supervisors and 6 banking professionals, we outline the auditing practices and socio-technical context of the supervisor. By combining the workshops’ insights with an analysis of compliance requirements, we identify the AML obligations that conflict with AI opacity. We then formulate seven needs that supervisors have for model justifiability. We discuss the role of explanations as reliable evidence on which to base justifications.
C1  - Honolulu, HI, USA and New York, NY, USA
C3  - Proceedings of the 2024 CHI conference on human factors in computing systems
DA  - 2024///
PY  - 2024
DO  - 10.1145/3613904.3642326
PB  - Association for Computing Machinery
SN  - 979-8-4007-0330-0
UR  - https://doi.org/10.1145/3613904.3642326
KW  - AI regulation
KW  - anti-money laundering
KW  - explainability
KW  - highly-regulated environment
KW  - justifiability
ER  - 

TY  - CONF
TI  - A missing piece in the puzzle: Considering the role of task complexity in human-AI decision making
AU  - Salimzadeh, Sara
AU  - He, Gaole
AU  - Gadiraju, Ujwal
T3  - Umap '23
AB  - Recent advances in the performance of machine learning algorithms have led to the adoption of AI models in decision making contexts across various domains such as healthcare, finance, and education. Different research communities have attempted to optimize and evaluate human-AI team performance through empirical studies by increasing transparency of AI systems, or providing explanations to aid human understanding of such systems. However, the variety in decision making tasks considered and their operationalization in prior empirical work, has led to an opacity around how findings from one task or domain carry forward to another. The lack of a standardized means of considering task attributes prevents straightforward comparisons across decision tasks, thereby limiting the generalizability of findings. We argue that the lens of ‘task complexity’ can be used to tackle this problem of under-specification and facilitate comparison across empirical research in this area. To retrospectively explore how different HCI communities have considered the influence of task complexity in designing experiments in the realm of human-AI decision making, we survey literature and provide an overview of empirical studies on this topic. We found a serious dearth in the consideration of task complexity across various studies in this realm of research. Inspired by Robert Wood’s seminal work on the construct, we operationalized task complexity with respect to three dimensions (component, coordinative, and dynamic) and quantified the complexity of decision tasks in existing work accordingly. We then summarized current trends and proposed research directions for the future. Our study highlights the need to account for task complexity as an important design choice. This is a first step to help the scientific community in drawing meaningful comparisons across empirical studies in human-AI decision making and to provide opportunities to generalize findings across diverse domains and experimental settings.
C1  - Limassol, Cyprus and New York, NY, USA
C3  - Proceedings of the 31st ACM conference on user modeling, adaptation and personalization
DA  - 2023///
PY  - 2023
DO  - 10.1145/3565472.3592959
SP  - 215
EP  - 227
PB  - Association for Computing Machinery
SN  - 978-1-4503-9932-6
UR  - https://doi.org/10.1145/3565472.3592959
ER  - 

TY  - CONF
TI  - Human-AI interactive and continuous sensemaking: a case study of image classification using scribble attention maps
AU  - Shen, Haifeng
AU  - Liao, Kewen
AU  - Liao, Zhibin
AU  - Doornberg, Job
AU  - Qiao, Maoying
AU  - van den Hengel, Anton
AU  - Verjans, Johan W.
T3  - Chi ea '21
AB  - Advances in Artificial Intelligence (AI), especially the stunning achievements of Deep Learning (DL) in recent years, have shown AI/DL models possess remarkable understanding towards the logic reasoning behind the solved tasks. However, human understanding towards what knowledge is captured by deep neural networks is still elementary and this has a detrimental effect on human’s trust in the decisions made by AI systems. Explainable AI (XAI) is a hot topic in both AI and HCI communities in order to open up the blackbox to elucidate the reasoning processes of AI algorithms in such a way that makes sense to humans. However, XAI is only half of human-AI interaction and research on the other half - human’s feedback on AI explanations together with AI making sense of the feedback - is generally lacking. Human cognition is also a blackbox to AI and effective human-AI interaction requires unveiling both blackboxes to each other for mutual sensemaking. The main contribution of this paper is a conceptual framework for supporting effective human-AI interaction, referred to as interactive and continuous sensemaking (HAICS). We further implement this framework in an image classification application using deep Convolutional Neural Network (CNN) classifiers as a browser-based tool that displays network attention maps to the human for explainability and collects human’s feedback in the form of scribble annotations overlaid onto the maps. Experimental results using a real-world dataset has shown significant improvement of classification accuracy (the AI performance) with the HAICS framework.
C1  - Yokohama, Japan and New York, NY, USA
C3  - Extended abstracts of the 2021 CHI conference on human factors in computing systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3411763.3451798
PB  - Association for Computing Machinery
SN  - 978-1-4503-8095-9
UR  - https://doi.org/10.1145/3411763.3451798
KW  - attention map
KW  - explainable AI
KW  - image classification
KW  - interactive sensemaking
KW  - scribble interaction
ER  - 

TY  - CONF
TI  - History-aware explanations: towards enabling human-in-the-loop in self-adaptive systems
AU  - Parra-Ullauri, Juan
AU  - Garcı́a-Domı́nguez, Antonio
AU  - Bencomo, Nelly
AU  - Garcia-Paucar, Luis
T3  - Models '22
AB  - The complexity of real-world problems requires modern software systems to autonomously adapt and modify their behaviour at run time to deal with internal and external challenges and contexts. Consequently, these self-adaptive systems (SAS) can show unexpected and surprising behaviours to users, who may not understand or agree with them. This is exacerbated due to the ubiquity and complexity of AI-based systems which are often considered as "black-boxes". Users may feel that the decision-making process of SAS is oblivious to the user's own decision-making criteria and priorities. Inevitably, users may mistrust or even avoid using the system. Furthermore, SAS could benefit from the human involvement in satisfying stakeholders' requirements. Accordingly, it is argued that a system should be able to explain its behaviour and how it has reached its current state. A history-aware, human-in-the-loop approach to address these issues is presented in this paper. For this approach, the system should i) offer access and retrieval of historic data about the past behaviour of the system, ii) track over time the reasons for its decisions to show and explain them to the users, and iii) provide capabilities, called effectors, to empower users by allowing them to steer the decision-making based on the information provided by i) and ii). This paper looks into enabling a human-in-the-loop approach into the decision-making of SAS based on the MAPE-K architecture. We present a feedback layer based on temporal graph databases (TGDB) that has been added to the MAPE-K architecture to provide a two-way communication between the human and the SAS. Collaboration, communication and trustworthiness between the human and SAS is promoted by the provision of history-based explanations extracted from the TGDB, and a set of effectors allow human users to influence the system based on the received information. The encouraging results of an application of the approach to a network management case study and a validation from a SAS expert are shown.
C1  - Montreal, Quebec, Canada and New York, NY, USA
C3  - Proceedings of the 25th international conference on model driven engineering languages and systems: Companion proceedings
DA  - 2022///
PY  - 2022
DO  - 10.1145/3550356.3561538
SP  - 286
EP  - 295
PB  - Association for Computing Machinery
SN  - 978-1-4503-9467-3
UR  - https://doi.org/10.1145/3550356.3561538
ER  - 

TY  - THES
TI  - Towards trustworthy decision-making in human-machine symbiosis
AU  - Lyu, Daoming
AU  - Levent, Yilmaz
AU  - Shiwen, Mao
AU  - Anh, Nguyen
AU  - Shiqi, Zhang
AB  - As artificial intelligence (AI) evolves, it becomes an integral part of our daily lives. To augment our effectiveness, human-machine symbiosis enables both humans and AI systems to offer different yet complementary capabilities. However, one of the significant concerns in human-machine symbiosis is the lack of human trust due to the potential ramifications, risks, or even dangers caused by AI. The critical question here is no longer whether AI will have an impact but by whom, how, where, and when this positive or negative impact will be felt. Trust is a prerequisite for humans to develop, deploy and use AI. Without AI being demonstrably worthy of trust, its uptake by humans might be hindered, hence undermining the realization of AI's vast economic and social benefits. This dissertation centers on building human trust in AI approaches to sequential decision problems, i.e., trustworthy decision-making. Specifically, there are three significant issues in current approaches.(i.)The first issue regards robustness where the brittleness in the planning indicates its inherent weaknesses. This identifies the potential risk that the AI system is unreliable and may lead to a blind trust that an AI system stays prone to errors even with high performances. To address the issue, I developed a framework to equip planning with the ability to learn so that the representation used for planning can be improved through the learned experience. Experimental results on benchmark domains demonstrate that the proposed approach can adapt to the domain uncertainties and changes and improve reliability.(ii.)The second issue regards interpretability where the learning behavior of deep reinforcement learning based on black-box neural networks is nontransparent and hard to explain and understand. This is identified as one of the main barriers to building human trust in the outcomes produced by the AI system. I developed a framework to address the issue by leveraging task decomposition and causal reasoning. Therefore, the task-level system behaviors can be interpreted in terms of causality – causal relations among different sub-tasks. Experimental results on the challenging domain with high-dimensional sensory inputs empirically validate the interpretability of sub-tasks, along with improved data efficiency compared with state-of-the-art approaches.(iii.)The third issue regards adaptive autonomy where the concern is to what degree of autonomy should be granted to an AI system. Furthermore, keeping humans in a supervisory role is key to striking a balance between machine-led and human-led decision-making. Therefore, I developed a human-machine collaborative decision-making framework to empower the machine agent to make decisions, with humans maintaining oversights. In addition, the openness supported by this paradigm, i.e., the willingness to give and receive ideas, can also increase human trust. Experiments with human evaluative feedback in different scenarios also demonstrate the effectiveness of the proposed approach.
CY  - USA
DA  - 2022///
PY  - 2022
M3  - phd
PB  - Auburn University
N1  - AAI29288743
ER  - 

TY  - CHAP
TI  - AI failure loops in feminized labor: Understanding the interplay of workplace AI and occupational devaluation
AU  - Kawakami, Anna
AU  - Taylor, Jordan
AU  - Fox, Sarah
AU  - Zhu, Haiyi
AU  - Holstein, Kenneth
T2  - Proceedings of the 2024 AAAI/ACM conference on AI, ethics, and society
AB  - Workplace AI systems often fail in practice. For example, in social services, AI-based decision support tools have been introduced across high-stakes settings, only to be dropped following backlash from workers or the public (Samant et al. 2021). Similarly, in healthcare, researchers have spent decades innovating on AI-based tools to support clinical decision-making, only to find that clinicians ignore them in practice (Yang, Steinfeld, and Zimmerman 2019).Past research has described a range of challenges that help explain these AI failures. For example, human-computer interaction (HCI) and science and technology studies (STS) literature has identified problems of poor contextual fit, where an AI system's design clashes with existing worker practices (e.g., (Suchman 1987; Forsythe 1993; Kawakami et al. 2022)). To address these challenges, prior work has proposed a range of new resources to support more responsible or participatory design. Yet, these resources are rarely used in practice, and AI teams continue to design flawed AI systems for the workplace (Delgado et al. 2023).Existing efforts to improve the design and evaluation of workplace AI tools has overlooked a critical factor: the role of occupational devaluation. How might the devaluation of worker expertise interplay with AI design, evaluation, and deployment practices? In our paper, we examine this through the case of feminized labor, a particularly extreme form of occupational devaluation (Balka and Wagner 2021). In the U.S., feminized labor often involves care-oriented work, like teaching and nursing. Historically misnomered as "women's work," feminized labor is still predominantly performed by women and people of color. While impacts on and of feminized labor have been studied by other disciplines, it remains under-examined in responsible AI research.Drawing together past scholarship on AI deployments in feminized occupations, we argue that workers in societally devalued occupations are particularly vulnerable to flawed AI deployments, rooted in impoverished understandings of workers' tasks and expertise. Moreover, we argue that these AI deployments further obscure the visibility of workers' expertise, triggering a negative feedback loop that further entrenches workers' devaluation.To understand this dynamic, we formalize the concept of AI Failure Loops: a set of interwoven, socio-technical failures across the development lifecycle of workplace AI systems that can amplify the harms from existing occupational devaluation. AI Failure Loops arise at the confluence of overclaims about the capabilities of workplace AI systems and under-recognition of the complexity of the work that workers perform. We ground an understanding of the driving factors, properties, and impacts of AI Failure Loops through three case studies of AI deployments in feminized labor contexts: AI-based risk assessment tools in social work, AI for home healthcare, and AI tutoring systems in K-12 teaching. We conclude with a discussion on future work towards proworker AI practice, policy, and organizing.
DA  - 2025///
PY  - 2025
SP  - 683
PB  - AAAI Press
ER  - 

TY  - JOUR
TI  - Accountability and control over autonomous weapon systems: a framework for comprehensive human oversight
AU  - Verdiesen, Ilse
AU  - Santoni de Sio, Filippo
AU  - Dignum, Virginia
T2  - Minds Mach.
AB  - Accountability and responsibility are key concepts in the academic and societal debate on Autonomous Weapon Systems, but these notions are often used as high-level overarching constructs and are not operationalised to be useful in practice. “Meaningful Human Control” is often mentioned as a requirement for the deployment of Autonomous Weapon Systems, but a common definition of what this notion means in practice, and a clear understanding of its relation with responsibility and accountability is also lacking. In this paper, we present a definition of these concepts and describe the relations between accountability, responsibility, control and oversight in order to show how these notions are distinct but also connected. We focus on accountability as a particular form of responsibility—the obligation to explain one’s action to a forum—and we present three ways in which the introduction of Autonomous Weapon Systems may create “accountability gaps”. We propose a Framework for Comprehensive Human Oversight based on an engineering, socio-technical and governance perspective on control. Our main claim is that combining the control mechanisms at technical, socio-technical and governance levels will lead to comprehensive human oversight over Autonomous Weapon Systems which may ensure solid controllability and accountability for the behaviour of Autonomous Weapon Systems. Finally, we give an overview of the military control instruments that are currently used in the Netherlands and show the applicability of the comprehensive human oversight Framework to Autonomous Weapon Systems. Our analysis reveals two main gaps in the current control mechanisms as applied to Autonomous Weapon Systems. We have identified three first options as future work for the design of a control mechanism, one in the technological layer, one in the socio-technical layer and one the governance layer, in order to achieve comprehensive human oversight and ensure accountability over Autonomous Weapon Systems.
DA  - 2021/03//
PY  - 2021
DO  - 10.1007/s11023-020-09532-9
VL  - 31
IS  - 1
SP  - 137
EP  - 163
SN  - 0924-6495
UR  - https://doi.org/10.1007/s11023-020-09532-9
KW  - Responsibility
KW  - Accountability
KW  - Accountability gap
KW  - Autonomous Weapon Systems
KW  - Comprehensive human oversight framework
KW  - Human oversight
KW  - Meaningful human control
ER  - 

TY  - CONF
TI  - Time for an explanation: a mini-review of explainable physio-behavioural time-series classification
AU  - Schneider, Jordan
AU  - Cheruvalath, Swathy Satheesan
AU  - Hassan, Teena
T3  - UbiComp '24
AB  - Time-series classification is seeing growing importance as device proliferation has lead to the collection of an abundance of sensor data. Although black-box models, whose internal workings are difficult to understand, are a common choice for this task, their use in safety-critical domains has raised calls for greater transparency. In response, researchers have begun employing explainable artificial intelligence together with physio-behavioural signals in the context of real-world problems. Hence, this paper examines the current literature in this area and contributes principles for future research to overcome the limitations of the reviewed works.
C1  - Melbourne VIC, Australia and New York, NY, USA
C3  - Companion of the 2024 on ACM international joint conference on pervasive and ubiquitous computing
DA  - 2024///
PY  - 2024
DO  - 10.1145/3675094.3679001
SP  - 885
EP  - 889
PB  - Association for Computing Machinery
SN  - 979-8-4007-1058-2
UR  - https://doi.org/10.1145/3675094.3679001
KW  - design principles
KW  - explainable artificial intelligence
KW  - physio-behavioural signals
KW  - time-series classification
KW  - ubiquitous computing
ER  - 

TY  - CONF
TI  - Establishing appropriate trust in AI through transparency and explainability
AU  - Kim, Sunnie S. Y.
T3  - Chi ea '24
AB  - As AI systems are increasingly transforming our society, it is critical to support relevant stakeholders to have appropriate understanding and trust in these systems. My dissertation research explores how AI transparency and explainability can help with this goal. I begin with human-centered evaluations of current AI explanation techniques, focusing on their usefulness for people in understanding model behavior and calibrating trust. Next, I identify what explainability needs real AI end-users have and what factors influence their trust through an in-depth case study of a real-world AI application. Finally, I describe two studies, one ongoing and one proposed, that investigate transparency and explainability approaches for Generative AI, such as large language models, to enable safe and successful interactions with this new and powerful technology. My dissertation contributes to both HCI and AI fields by elucidating mechanisms and factors of trust in AI and detailing design considerations for AI transparency and explainability approaches.
C1  - Honolulu, HI, USA and New York, NY, USA
C3  - Extended abstracts of the CHI conference on human factors in computing systems
DA  - 2024///
PY  - 2024
DO  - 10.1145/3613905.3638184
PB  - Association for Computing Machinery
SN  - 979-8-4007-0331-7
UR  - https://doi.org/10.1145/3613905.3638184
KW  - Explainable AI
KW  - Human-AI collaboration
KW  - AI transparency and explainability
KW  - Trust and reliance
ER  - 

TY  - THES
TI  - Role of human self-confidence and their confidence in artificial intelligence in AI-assisted decision-making in engineering design
AU  - Chong, Leah Myong
AU  - Kosa, Goucher-Lambert
AU  - B, Levent, Kara
AB  - As artificial intelligence (AI) systems are proving their usefulness in engineering design, they are increasingly deployed to assist human designers' decision-making by providing design suggestions. However, such AI assistance is only effective when human designers properly utilize AI input. Unfortunately, designers often misjudge the AI's and/or their own ability, leading to inappropriate trust and reliance on AI, and therefore bad designs. To avoid such outcomes, it is critical to understand how human designers' self-confidence and their confidence in the AI teammate(s) (two types of human confidence that are related to trust) change and impact AI-assisted decision-making in engineering design. Therefore, this dissertation conducts three cognitive experiments to investigate the influence of trial-by-trial experiences and AI performance during AI-assisted decision-making on human confidences and consequently the decision to accept or reject AI's design suggestions. A dynamic model of human confidence that calculates the change in confidence based on individual experience, accumulated confidence, and bias at any given trial is also developed and used for analysis.The first experiment examines the evolution of human self-confidence and their confidence in AI, as well as the impact of these confidences on their reliance on AI, in a one AI-assisted decision-making context with a non-design task. A non-design task is investigated to understand general problem-solving scenarios as well as to allow identification of design-specific results in the second experiment. The second experiment, then, resembles the first experiment but with an engineering design task. The results of this experiment reveal not only some similarities with the non-design task but also significant discrepancies that lead to a discussion of the differences in the two tasks. Next, the data from the second experiment is utilized to dig deeper into human designers' self-confidence and its role in AI-assisted decision-making. Having discovered in the two initial experiments that human designers' self-confidence significantly impacts their decision-making in individual trials, the dissertation then examines what self-confidence values at various points of collaboration reveal about designers' overall performance as a teammate, specifically in terms of their overall competence, reliance on AI, and team performance. Finally, the last experiment of this dissertation studies an AI-assisted decision-making context with two AIs. This final study offers insights into the effect of an additional AI and its suggestions on human designers' confidence in AI and self-confidence and consequently on the decision-making outcome. Altogether, this dissertation yields valuable information about how human designers' confidences change and impact AI-assisted decision-making in various contexts (i.e., non-design vs. design tasks, one AI vs. two AI teams), enabling detection and explanation of inappropriate confidence levels and presenting opportunities to effectively reduce mis-reliance on AI in engineering design.
CY  - USA
DA  - 2022///
PY  - 2022
M3  - phd
PB  - Carnegie Mellon University
N1  - AAI29210226
ER  - 

TY  - JOUR
TI  - The quest of parsimonious XAI: A human-agent architecture for explanation formulation
AU  - Mualla, Yazan
AU  - Tchappi, Igor
AU  - Kampik, Timotheus
AU  - Najjar, Amro
AU  - Calvaresi, Davide
AU  - Abbas-Turki, Abdeljalil
AU  - Galland, Stéphane
AU  - Nicolle, Christophe
T2  - Artificial Intelligence
DA  - 2022/01//
PY  - 2022
DO  - 10.1016/j.artint.2021.103573
VL  - 302
IS  - C
J2  - Artif. Intell.
SN  - 0004-3702
UR  - https://doi.org/10.1016/j.artint.2021.103573
KW  - Human-computer interaction
KW  - Explainable artificial intelligence
KW  - Empirical user studies
KW  - Multi-agent systems
KW  - Statistical testing
ER  - 

TY  - JOUR
TI  - Conceptualization and cases of study on cyber operations against the sustainability of the tactical edge
AU  - Sotelo Monge, Marco Antonio
AU  - Maestre Vidal, Jorge
T2  - Future Gener. Comput. Syst.
DA  - 2021/12//
PY  - 2021
DO  - 10.1016/j.future.2021.07.016
VL  - 125
IS  - C
SP  - 869
EP  - 890
SN  - 0167-739X
UR  - https://doi.org/10.1016/j.future.2021.07.016
KW  - Cyber defense
KW  - Economical Denial of Sustainability
KW  - Military operations
KW  - Situational Awareness
KW  - Tactical Denial of Sustainability
ER  - 

TY  - JOUR
TI  - SystemER: a human-in-the-loop system for explainable entity resolution
AU  - Qian, Kun
AU  - Popa, Lucian
AU  - Sen, Prithviraj
T2  - Proc. VLDB Endow.
AB  - Entity Resolution (ER) is the task of identifying different representations of the same real-world object. To achieve scalability and the desired level of quality, the typical ER pipeline includes multiple steps that may involve low-level coding and extensive human labor. We present SystemER, a tool for learning explainable ER models that reduces the human labor all throughout the stages of the ER pipeline. SystemER achieves explainability by learning rules that not only perform a given ER task but are human-comprehensible; this provides transparency into the learning process, and further enables verification and customization of the learned model by the domain experts. By leveraging a human in the loop and active learning, SystemER also ensures that a small number of labeled examples is sufficient to learn high-quality ER models. SystemER is a full-fledged tool that includes an easy to use interface, support for both flat files and semi-structured data, and scale-out capabilities by distributing computation via Apache Spark.
DA  - 2019/08//
PY  - 2019
DO  - 10.14778/3352063.3352068
VL  - 12
IS  - 12
SP  - 1794
EP  - 1797
SN  - 2150-8097
UR  - https://doi.org/10.14778/3352063.3352068
ER  - 

TY  - JOUR
TI  - Explainable AI and stakes in medicine: A user study
AU  - Baron, Sam
AU  - Latham, Andrew J.
AU  - Varga, Somogy
T2  - Artificial Intelligence
DA  - 2025/03//
PY  - 2025
DO  - 10.1016/j.artint.2025.104282
VL  - 340
IS  - C
J2  - Artif. Intell.
SN  - 0004-3702
UR  - https://doi.org/10.1016/j.artint.2025.104282
KW  - Human-AI interaction
KW  - User study
KW  - Causation
KW  - Explainable AI (XAI)
KW  - Explainable ML
KW  - Human-centered XAI
ER  - 

TY  - CONF
TI  - The importance of human factors for trusted human-robot collaborations
AU  - Ramirez-Amaro, Karinne
AU  - Torre, Ilaria
AU  - Diehl, Maximilian
AU  - Dean, Emmanuel
T3  - Hai '23
AB  - The next generation of robots is expected to work collaboratively with humans in natural (dynamic) settings. For this, it is important to properly study and model human factors, so that the AI and Robotic models can include them to enable robust Human-Robot Collaborations. This will enable safe and trustworthy hybrid decision-making approaches – Responsible AI – thereby streamlining robust collaborations (as per human-centred expectations). This interdisciplinary workshop will focus on the intersection of Cognitive Human Factors, Interpretable &amp; Explainable AI methods, Social Interaction, and Human-Centred Robotics to stimulate novel long-range avenues for innovative human-centred collaborative methods in real-world contexts.
C1  - Gothenburg, Sweden and New York, NY, USA
C3  - Proceedings of the 11th international conference on human-agent interaction
DA  - 2023///
PY  - 2023
DO  - 10.1145/3623809.3623981
SP  - 502
EP  - 503
PB  - Association for Computing Machinery
SN  - 979-8-4007-0824-4
UR  - https://doi.org/10.1145/3623809.3623981
KW  - Safety
KW  - Human factors
KW  - Explainable AI methods
KW  - Human-Robot Interaction
KW  - Interpretable &amp
ER  - 

TY  - CONF
TI  - Counterfactual explanations may not be the best algorithmic recourse approach
AU  - Upadhyay, Sohini
AU  - Lakkaraju, Himabindu
AU  - Gajos, Krzysztof Z.
T3  - Iui '25
AB  - Algorithmic recourse is a rapidly developing subfield in explainable AI (XAI) concerned with providing individuals subject to adverse high-stakes algorithmic outcomes with explanations indicating how to reverse said outcomes. While XAI research in the machine learning community doesn’t confine itself to counterfactual explanations, its algorithmic recourse subfield does, adopting the assumption that the optimal way to provide recourse is through counterfactual explanations. Though there has been extensive human-AI interaction research on explanations, translating these findings to the algorithmic recourse setting is non-obvious due to meaningful problem setting differences, leaving the question of whether counterfactuals are the most optimal explanation paradigm for recourse unanswered. While intuitively satisfying, the prescriptive nature of counterfactuals makes them vulnerable to poor outcomes when circumstances unknown to the decision-making and explanation generating algorithms affect re-application strategies. With these concerns in mind, we designed a series of experiments comparing different explanation methods in the recourse setting, explicitly incorporating scenarios where circumstances unknown to the decision-making and explanation algorithms affect re-application strategies. In Experiment 1, we compared counterfactuals with reason codes, a simple feature-based explanation, finding that they both yield comparable re-application success, and that reason codes led to better user outcomes when unknown circumstances had a high impact on re-application strategies. In Experiment 2, we sought to improve on reason code outcomes, comparing them to feature attributions, a more informative feature-based explanation, but found no improvements. Finally, in Experiment 3, we aimed to improve on reason code outcomes with a multiple counterfactual explanation condition, finding that multiple counterfactuals led to higher re-application success but still resulted in comparatively worse user outcomes in the face of high impact unknown circumstances. Taken together, these findings call into question whether the standard counterfactual paradigm is the best approach for the algorithmic recourse problem setting.
C1  - New York, NY, USA
C3  - Proceedings of the 30th international conference on intelligent user interfaces
DA  - 2025///
PY  - 2025
DO  - 10.1145/3708359.3712095
SP  - 446
EP  - 462
PB  - Association for Computing Machinery
SN  - 979-8-4007-1306-4
UR  - https://doi.org/10.1145/3708359.3712095
KW  - AI explanations
KW  - algorithmic recourse
KW  - counterfactual explanations
ER  - 

TY  - CONF
TI  - Reviewable automated decision-making: a framework for accountable algorithmic systems
AU  - Cobbe, Jennifer
AU  - Lee, Michelle Seng Ah
AU  - Singh, Jatinder
T3  - FAccT '21
AB  - This paper introduces reviewability as a framework for improving the accountability of automated and algorithmic decisionmaking (ADM) involving machine learning. We draw on an understanding of ADM as a socio-technical process involving both human and technical elements, beginning before a decision is made and extending beyond the decision itself. While explanations and other model-centric mechanisms may assist some accountability concerns, they often provide insufficient information of these broader ADM processes for regulatory oversight and assessments of legal compliance. Reviewability involves breaking down the ADM process into technical and organisational elements to provide a systematic framework for determining the contextually appropriate record-keeping mechanisms to facilitate meaningful review - both of individual decisions and of the process as a whole. We argue that a reviewability framework, drawing on administrative law's approach to reviewing human decision-making, offers a practical way forward towards more a more holistic and legally-relevant form of accountability for ADM.
C1  - Virtual Event, Canada and New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445921
SP  - 598
EP  - 609
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445921
KW  - machine learning
KW  - artificial intelligence
KW  - accountability
KW  - Algorithmic systems
KW  - audit
KW  - automated decision-making
ER  - 

TY  - JOUR
TI  - Artificial intelligence, autonomy, and human‐machine teams: Interdependence, context, and explainable AI: Editorial introduction to the special articles on context
AU  - Lawless, W. F.
AU  - Mittu, Ranjeev
AU  - Sofge, Donald
AU  - Hiatt, Laura
T2  - Ai Magazine
AB  - Because in military situations, as well as for self‐driving cars, information must be processed faster than humans can achieve, determination of context computationally, also known as situational assessment, is increasingly important. In this article, we introduce the topic of context, and we discuss what is known about the heretofore intractable research problem on the effects of interdependence, present in the best of human teams; we close by proposing that interdependence must be mastered mathematically to operate human‐machine teams efficiently, to advance theory, and to make the machine actions directed by AI explainable to team members and society. The special topic articles in this issue and a subsequent issue of AI Magazine review ongoing mature research and operational programs that address context for human‐machine teams.
DA  - 2019/09//
PY  - 2019
DO  - 10.1609/aimag.v40i3.2866
VL  - 40
IS  - 3
SP  - 5
EP  - 13
J2  - AI Mag.
SN  - 0738-4602
UR  - https://doi.org/10.1609/aimag.v40i3.2866
ER  - 

TY  - JOUR
TI  - A sociotechnical perspective for the future of AI: narratives, inequalities, and human control
AU  - Sartori, Laura
AU  - Theodorou, Andreas
T2  - Ethics and Inf. Technol.
AB  - Different people have different perceptions about artificial intelligence (AI). It is extremely important to bring together all the alternative frames of thinking—from the various communities of developers, researchers, business leaders, policymakers, and citizens—to properly start acknowledging AI. This article highlights the ‘fruitful collaboration’ that sociology and AI could develop in both social and technical terms. We discuss how biases and unfairness are among the major challenges to be addressed in such a sociotechnical perspective. First, as intelligent machines reveal their nature of ‘magnifying glasses’ in the automation of existing inequalities, we show how the AI technical community is calling for transparency and explainability, accountability and contestability. Not to be considered as panaceas, they all contribute to ensuring human control in novel practices that include requirement, design and development methodologies for a fairer AI. Second, we elaborate on the mounting attention for technological narratives as technology is recognized as a social practice within a specific institutional context. Not only do narratives reflect organizing visions for society, but they also are a tangible sign of the traditional lines of social, economic, and political inequalities. We conclude with a call for a diverse approach within the AI community and a richer knowledge about narratives as they help in better addressing future technical developments, public debate, and policy. AI practice is interdisciplinary by nature and it will benefit from a socio-technical perspective.
DA  - 2022/03//
PY  - 2022
DO  - 10.1007/s10676-022-09624-3
VL  - 24
IS  - 1
SN  - 1388-1957
UR  - https://doi.org/10.1007/s10676-022-09624-3
KW  - Artificial intelligence
KW  - Sociology
KW  - Transparency
KW  - Inequalities
KW  - Accountability
KW  - Human control
KW  - Narratives
ER  - 

TY  - THES
TI  - Design and evaluation of a conversational agent for mental health support: Forming human-agent sociotechnical and therapeutic relationships
AU  - Liao, Yuting
AU  - Beth, St. Jean
AU  - Amanda, Lazar
AU  - Kyoung, Eun, Choe
AU  - Dennis, Kivlighan
AB  - Many people with mental health disorders face significant challenges getting the help they need, including the costs of obtaining psychological counseling or psychiatry services, as well as fear of being stigmatized. As a way of addressing these barriers, text-based conversational agents (chatbots) have gained traction as a new form of e-therapy. Powered by artificial intelligence (AI) and natural language processing techniques, this technology offers more natural interactions and a "judgment-free zone" for clients concerned about stigma. However, literature on psychotherapeutic chatbots is sparse in both the psychology and human computer interaction (HCI) fields. While recent studies indicate that chatbots provide an affordable and effective therapy delivery method, this research has not thoroughly explained the underlying mechanisms for increasing acceptance of chatbots and making them more engaging. Don Norman (1994) has argued the main difficulties of utilizing intelligent agents are social—not technical—and particularly center around people's perceptions of agents. In exploring the use of chatbots in psychotherapy, we must investigate how this technology is conceptually understood, and the thoughts and feelings they evoke when people interact with them. This dissertation focuses on two types of relationships critical to the success of utilizing chatbots for mental health interventions: sociotechnical relationships and therapeutic relationships. A sociotechnical relationship concerns technology adoption, usability, and the compatibility between humans and chatbots. A therapeutic relationship encompasses people's feelings and attitudes toward a chatbot therapist. Therefore, this dissertation asks: What are the optimal design principles for a conversational agent that facilitates the development of both sociotechnical and therapeutic relationships to help people manage their mental health? To investigate this question, I designed an original conversational system with eight gendered and racially heterogeneous personas, and one neutral robot-like persona. Using a mixed-method approach (online experiment and interviews), I evaluated factors related to the adoption and use of conversational agents for psychotherapeutic purposes. I also unpacked the human-agent relational dynamics and evaluated how anthropomorphism and perceived racial similarity impact people's perceptions of and interactions with the chatbot. These findings contributed to the wider understanding of conversational AI application in mental health support and provided actionable design recommendations.
DA  - 2021///
PY  - 2021
M3  - phd
PB  - University of Maryland, College Park
N1  - AAI28717215
ER  - 

TY  - CONF
TI  - Human behaviour centered design: developing a software system for cultural heritage
AU  - Dugdale, Julie
AU  - Moghaddam, Mahyar T.
AU  - Muccini, Henry
T3  - Icse-seis '20
AB  - This paper introduces an integrated framework for sustainability and urban security socio-technical systems. The focus is to design and develop a hardware/software system based on human expected and real behaviour.The paper explains the steps taken through developing the Uffizi Museum crowd monitoring and queue management system. The goal of implementing such system was to remove queues outside the Museum which is in line with urban security and visitors comfort. We took advantage of a data-driven approach mapped on sustainability framework. Such approach which was fed with both real-time sensory data and prediction models, successfully eliminated long queues to access the museum. We took into consideration performance of software system as well to reduce the response time to a threshold that is compliant with real-time requirements. We started our experiments from fall 2016 and operationalized it in October, 2018. During this experimentation period, we learned a lot of lessons that we report in this paper.
C1  - Seoul, South Korea and New York, NY, USA
C3  - Proceedings of the ACM/IEEE 42nd international conference on software engineering: Software engineering in society
DA  - 2020///
PY  - 2020
DO  - 10.1145/3377815.3381375
SP  - 85
EP  - 94
PB  - Association for Computing Machinery
SN  - 978-1-4503-7125-4
UR  - https://doi.org/10.1145/3377815.3381375
KW  - crowd monitoring
KW  - cultural heritage
KW  - empirical study
KW  - queue management
KW  - software engineering
KW  - sustainability
ER  - 

TY  - JOUR
TI  - Seamful XAI: Operationalizing seamful design in explainable AI
AU  - Ehsan, Upol
AU  - Liao, Q. Vera
AU  - Passi, Samir
AU  - Riedl, Mark O.
AU  - Daumé, Hal
T2  - Proc. ACM Hum.-Comput. Interact.
AB  - Mistakes in AI systems are inevitable, arising from both technical limitations and sociotechnical gaps. While black-boxing AI systems can make the user experience seamless, hiding the seams risks disempowering users to mitigate fallouts from AI mistakes. Instead of hiding these AI imperfections, can we leverage them to help the user? While Explainable AI (XAI) has predominantly tackled algorithmic opaqueness, we propose that seamful design can foster AI explainability by revealing and leveraging sociotechnical and infrastructural mismatches. We introduce the concept of Seamful XAI by (1) conceptually transferring "seams" to the AI context and (2) developing a design process that helps stakeholders anticipate and design with seams. We explore this process with 43 AI practitioners and real end-users, using a scenario-based co-design activity informed by real-world use cases. We found that the Seamful XAI design process helped users foresee AI harms, identify underlying reasons (seams), locate them in the AI's lifecycle, learn how to leverage seamful information to improve XAI and user agency. We share empirical insights, implications, and reflections on how this process can help practitioners anticipate and craft seams in AI, how seamfulness can improve explainability, empower end-users, and facilitate Responsible AI.
DA  - 2024/04//
PY  - 2024
DO  - 10.1145/3637396
VL  - 8
IS  - CSCW1
UR  - https://doi.org/10.1145/3637396
KW  - explainable ai
KW  - human-ai interaction
KW  - responsible ai
KW  - seamful design
ER  - 

TY  - JOUR
TI  - How does cost leadership strategy suppress the performance benefits of explainability of AI applications in organizations?
AU  - Yang, Alan
AU  - Talaei, Jason
AU  - Takishova, Tolkyn
AU  - Masialeti, Masialeti
AB  - This study examines how explainable artificial intelligence-enabled agility, encompassing operational and market agility, influences performance. We examine the potentially negative effect of a cost-leadership strategy on the performance benefits of explainable AI. Data from a survey of IT executives was analyzed. Findings suggest that the explainability of artificial intelligence applications does not directly relate to performance. Instead, its benefits are actualized through operational and market agility. However, our study finds that in organizations that prioritize cost leadership, the positive link between explainability and performance becomes negative and significant. Additionally, the mediating roles of operational and market agility become insignificant in such organizations.
DA  - 2024/09//
PY  - 2024
DO  - 10.4018/JGIM.354062
VL  - 32
IS  - 1
SP  - 1
EP  - 23
J2  - J. Glob. Inf. Manage.
SN  - 1062-7375
UR  - https://doi.org/10.4018/JGIM.354062
KW  - Cost Leadership
KW  - Explainable Artificial Intelligence
KW  - Market Agility
KW  - Operational Agility
KW  - Performance
ER  - 

TY  - CONF
TI  - Training towards critical use: Learning to situate AI predictions relative to human knowledge
AU  - Kawakami, Anna
AU  - Guerdan, Luke
AU  - Cheng, Yanghuidi
AU  - Glazko, Kate
AU  - Lee, Matthew
AU  - Carter, Scott
AU  - Arechiga, Nikos
AU  - Zhu, Haiyi
AU  - Holstein, Kenneth
T3  - Ci '23
AB  - A growing body of research has explored how to support humans in making better use of AI-based decision support, including via training and onboarding. Existing research has focused on decision-making tasks where it is possible to evaluate “appropriate reliance” by comparing each decision against a ground truth label that cleanly maps to both the AI’s predictive target and the human decision-maker’s goals. However, this assumption does not hold in many real-world settings where AI tools are deployed today (e.g., social work, criminal justice, and healthcare). In this paper, we introduce a process-oriented notion of appropriate reliance called critical use that centers the human’s ability to situate AI predictions against knowledge that is uniquely available to them but unavailable to the AI model. To explore how training can support critical use, we conduct a randomized online experiment in a complex social decision-making setting: child maltreatment screening. We find that, by providing participants with accelerated, low-stakes opportunities to practice AI-assisted decision-making in this setting, novices came to exhibit patterns of disagreement with AI that resemble those of experienced workers. A qualitative examination of participants’ explanations for their AI-assisted decisions revealed that they drew upon qualitative case narratives, to which the AI model did not have access, to learn when (not) to rely on AI predictions. Our findings open new questions for the study and design of training for real-world AI-assisted decision-making.
C1  - Delft, Netherlands and New York, NY, USA
C3  - Proceedings of the ACM collective intelligence conference
DA  - 2023///
PY  - 2023
DO  - 10.1145/3582269.3615595
SP  - 63
EP  - 78
PB  - Association for Computing Machinery
SN  - 979-8-4007-0113-9
UR  - https://doi.org/10.1145/3582269.3615595
KW  - AI onboarding and training
KW  - algorithm-assisted decision-making
KW  - augmented intelligence
KW  - human-AI complementarity
ER  - 

TY  - CONF
TI  - Robust relatable explanations of machine learning with disentangled cue-specific saliency
AU  - Abichandani, Harshavardhan Sunil
AU  - Zhang, Wencan
AU  - Lim, Brian Y
T3  - Iui '25
AB  - Concept-based explanations help users understand the relation between model predictions and meaningful cues. However, under noisy real-world conditions, data perturbations lead to distorted and deviated explanations. We hypothesize that these corruptions affect specific cues rather than all, so disentangling them may help reduce model dependency on degraded cues. For the application of explaining emotional speech recognition, we propose RobustRexNet to explain with disentangled and discretized saliency maps for separate speech cues (e.g., loudness, pitch) to improve robustness against noise. Modeling evaluations show that RobustRexNet improved both model performance and explanation faithfulness in noisy and privacy-preserving distortions. User studies further indicate that the robust explanations aligned better with human intuition and improved user emotion labeling under noise. This work contributes toward robust explainable AI to improve user trust under real-world conditions.
C1  - New York, NY, USA
C3  - Proceedings of the 30th international conference on intelligent user interfaces
DA  - 2025///
PY  - 2025
DO  - 10.1145/3708359.3712105
SP  - 1203
EP  - 1231
PB  - Association for Computing Machinery
SN  - 979-8-4007-1306-4
UR  - https://doi.org/10.1145/3708359.3712105
KW  - Explainable AI
KW  - misleading explanations
KW  - robust machine learning
KW  - vocal emotion
ER  - 

TY  - CONF
TI  - The AI-DEC: a card-based design method for user-centered AI explanations
AU  - Lee, Christine P
AU  - Lee, Min Kyung
AU  - Mutlu, Bilge
T3  - Dis '24
AB  - Increasing evidence suggests that many deployed AI systems do not sufficiently support end-user interaction and information needs. Engaging end-users in the design of these systems can reveal user needs and expectations, yet effective ways of engaging end-users in the AI explanation design remain under-explored. To address this gap, we developed a design method, called AI-DEC, that defines four dimensions of AI explanations that are critical for the integration of AI systems—communication content, modality, frequency, and direction—and offers design examples for end-users to design AI explanations that meet their needs. We evaluated this method through co-design sessions with workers in healthcare, finance, and management industries who regularly use AI systems in their daily work. Findings indicate that the AI-DEC effectively supported workers in designing explanations that accommodated diverse levels of performance and autonomy needs, which varied depending on the AI system’s workplace role and worker values. We discuss the implications of using the AI-DEC for the user-centered design of AI explanations in real-world systems.
C1  - Copenhagen, Denmark and New York, NY, USA
C3  - Proceedings of the 2024 ACM designing interactive systems conference
DA  - 2024///
PY  - 2024
DO  - 10.1145/3643834.3661576
SP  - 1010
EP  - 1028
PB  - Association for Computing Machinery
SN  - 979-8-4007-0583-0
UR  - https://doi.org/10.1145/3643834.3661576
KW  - user-centered design
KW  - human-AI interaction
KW  - Design cards
ER  - 

TY  - CONF
TI  - Making transparency influencers: a case study of an educational approach to improve responsible AI practices in news and media
AU  - Bell, Andrew
AU  - Stoyanovich, Julia
T3  - Chi ea '24
AB  - Concerns about the risks posed by artificial intelligence (AI) have resulted in growing interest in algorithmic transparency. While algorithmic transparency is well-studied, there is evidence that many organizations do not value implementing transparency. In this case study, we test a ground-up approach to ensuring better real-world algorithmic transparency by creating transparency influencers — motivated individuals within organizations who advocate for transparency. We held an interactive online workshop on algorithmic transparency and advocacy for 15 professionals from news, media, and journalism. We reflect on workshop design choices and presents insights from participant interviews. We found positive evidence for our approach: In the days following the workshop, three participants had done pro-transparency advocacy. Notably, one of them advocated for algorithmic transparency at an organization-wide AI strategy meeting. In the words of a participant: “if you are questioning whether or not you need to tell people [about AI], you need to tell people.”
C1  - Honolulu, HI, USA and New York, NY, USA
C3  - Extended abstracts of the CHI conference on human factors in computing systems
DA  - 2024///
PY  - 2024
DO  - 10.1145/3613905.3637113
PB  - Association for Computing Machinery
SN  - 979-8-4007-0331-7
UR  - https://doi.org/10.1145/3613905.3637113
KW  - machine learning
KW  - artificial intelligence
KW  - Transparency
KW  - explainability
KW  - tempered radicals
ER  - 

TY  - CONF
TI  - Through the&nbsp;eyes of&nbsp;the&nbsp;expert: Aligning human and&nbsp;machine attention for&nbsp;industrial AI
AU  - Koebler, Alexander
AU  - Greisinger, Christian
AU  - Paulus, Jan
AU  - Thon, Ingo
AU  - Buettner, Florian
AB  - Human expertise and intuition are crucial in solving many tasks in expert-driven domains such as industrial manufacturing or medical diagnosis. In this work, we use the human expert’s gaze information to take a step towards transferring this knowledge to a machine learning model. In this way, we are aligning the attention the machine and the human pay to solve the task. Previous works in the medical field have shown that privileged gaze information during training can increase predictive performance and reduce the label requirement of a machine learning model. We extend on the aim of those works and quantitatively evaluate the benefit of aligning human and machine attention on the quality of the model’s explanations as well as its robustness - thus, its trustworthiness. We demonstrate our approach on a real-world visual quality inspection task in the multi-label setting, which is common in industrial applications. Our work illustrates the importance of incorporating human knowledge more explicitly in training machine learning models and takes a step towards enabling machine learning based systems in high-stakes applications.
C1  - Washington DC, USA and Berlin, Heidelberg
C3  - Artificial intelligence in HCI: 5th international conference, AI-HCI 2024, held as part of the 26th HCI international conference, HCII 2024, washington, DC, USA, june 29–july 4, 2024, proceedings, part II
DA  - 2024///
PY  - 2024
DO  - 10.1007/978-3-031-60611-3_28
SP  - 407
EP  - 423
PB  - Springer-Verlag
SN  - 978-3-031-60613-7
UR  - https://doi.org/10.1007/978-3-031-60611-3_28
KW  - Explainable AI
KW  - Human Gaze
KW  - Human-Centered AI
KW  - Model Robustness
ER  - 

TY  - JOUR
TI  - Enhancing trust in automated 3D point cloud data interpretation through explainable counterfactuals
AU  - Holzinger, Andreas
AU  - Lukač, Niko
AU  - Rozajac, Dzemail
AU  - Johnston, Emile
AU  - Kocic, Veljka
AU  - Hoerl, Bernhard
AU  - Gollob, Christoph
AU  - Nothdurft, Arne
AU  - Stampfer, Karl
AU  - Schweng, Stefan
AU  - Del Ser, Javier
T2  - Inf. Fusion
DA  - 2025/04//
PY  - 2025
DO  - 10.1016/j.inffus.2025.103032
VL  - 119
IS  - C
SN  - 1566-2535
UR  - https://doi.org/10.1016/j.inffus.2025.103032
KW  - Interpretability
KW  - Explainable AI
KW  - Counterfactual reasoning
KW  - Human-centered AI
KW  - Information fusion
KW  - Point cloud data
ER  - 

TY  - CONF
TI  - 3rd workshop on explainability in human-robot collaboration: Real-world concerns
AU  - Yadollahi, Elmira
AU  - Dogan, Fethiye Irmak
AU  - Romeo, Marta
AU  - Kontogiorgos, Dimosthenis
AU  - Qian, Peizhu
AU  - Zhang, Yan
T3  - Hri '25
AB  - Robots powered by AI and machine learning are increasingly capable of collaboration and social interaction with humans, leading to a demand to develop new approaches to ensure their transparency and explainable behaviour. As explainable AI (XAI) seeks to clarify AI decisions, its integration into physical robots often creates an illusion of explainability-raising questions about whether current approaches truly enhance understanding. The 3rd Workshop on Explainability in Human-Robot Collaboration aims to address the real-world concerns associated with developing explainable and transparent robots through a focused, multi-faceted panel discussion and a series of paper presentations. In this workshop, we will focus on refining when and how explanations should be provided, integrating human communication principles to enhance trust and transparency in human-robot collaboration through both technical and user-centred solutions.
C1  - Melbourne, Australia
C3  - Proceedings of the 2025 ACM/IEEE international conference on human-robot interaction
DA  - 2025///
PY  - 2025
SP  - 1994
EP  - 1996
PB  - IEEE Press
KW  - explainable robotics
KW  - human-centered robot explanations
KW  - xai
ER  - 

TY  - CONF
TI  - Advancing interactive explainable AI via belief change theory
AU  - Rago, Antonio
AU  - Martinez, Maria Vanina
T3  - Kr '24
AB  - As AI models become ever more complex and intertwined in humans' daily lives, greater levels of interactivity of explainable AI (XAI) methods are needed. In this paper, we propose the use of belief change theory as a formal foundation for operators that model the incorporation of new information, i.e. user feedback in interactive XAI, to logical representations of data-driven classifiers. We argue that this type of formalisation provides a framework and a methodology to develop interactive explanations in a principled manner, providing warranted behaviour and favouring transparency and accountability of such interactions. Concretely, we first define a novel, logic-based formalism to represent explanatory information shared between humans and machines. We then consider real world scenarios for interactive XAI, with different prioritisations of new and existing knowledge, where our formalism may be instantiated. Finally, we analyse a core set of belief change postulates, discussing their suitability for our real world settings and pointing to particular challenges that may require the relaxation or reinterpretation of some of the theoretical assumptions underlying existing operators.
C1  - Hanoi, Vietnam
C3  - Proceedings of the 21st international conference on principles of knowledge representation and reasoning
DA  - 2024///
PY  - 2024
DO  - 10.24963/kr.2024/87
SN  - 978-1-956792-05-8
UR  - https://doi.org/10.24963/kr.2024/87
ER  - 

TY  - CONF
TI  - Designing for AI transparency in public services: a user-centred study of citizens’ preferences
AU  - Schmager, Stefan
AU  - Gupta, Samrat
AU  - Pappas, Ilias
AU  - Vassilakopoulou, Polyxeni
AB  - Enhancing transparency in AI enabled public services has the potential to improve their adoption and service delivery. Hence, it is important to identify effective design strategies for AI transparency in public services. To this end, we conduct this empirical qualitative study providing insights for responsible deployment of AI in practice by public organizations. We design an interactive prototype for a Norwegian public welfare service organization which aims to use AI to support sick leaves related services. Qualitative analysis of citizens’ data collected through survey, think-aloud interactions with the prototype, and open-ended questions revealed three key themes related to: articulating information in written form, representing information in graphical form, and establishing the appropriate level of information detail for improving AI transparency in public service delivery. This study advances research pertaining to design of public service portals and has implications for AI implementation in the public sector.
C1  - Washington DC, USA and Berlin, Heidelberg
C3  - HCI in business, government and organizations: 11th international conference, HCIBGO 2024, held as part of the 26th HCI international conference, HCII 2024, washington, DC, USA, june 29 – july 4, 2024, proceedings, part I
DA  - 2024///
PY  - 2024
DO  - 10.1007/978-3-031-61315-9_17
SP  - 237
EP  - 253
PB  - Springer-Verlag
SN  - 978-3-031-61314-2
UR  - https://doi.org/10.1007/978-3-031-61315-9_17
KW  - Transparency
KW  - Human-Centered AI
KW  - Action Design Research
KW  - Design Preferences
ER  - 

TY  - CONF
TI  - Designing AI interfaces for transparent decision-making and ethical reflection
AU  - Bhat, Maalvika
T3  - IUI '25 companion
AB  - As artificial intelligence (AI) systems increasingly mediate high-stakes decisions in domains such as healthcare, finance, and education, ensuring transparency and ethical accountability in AI interfaces is critical. However, existing interfaces often obscure algorithmic processes, leading to overtrust, disengagement, or misinterpretation of AI-generated outputs. My research explores how interface design—including presentation modes, interactive explainability tools, and speculative design interventions—can shape user perceptions, foster critical reflection, and enhance AI literacy. By integrating controlled experiments, participatory design, and qualitative analysis, my work aims to develop AI interfaces that not only communicate algorithmic decisions effectively but also encourage users to critically assess the ethical implications of AI technologies. I examine the trade-offs between transparency, usability, and engagement, investigating how interfaces can balance cognitive load while making ethical considerations more salient. Through this doctoral consortium, I seek mentorship and feedback on designing adaptive transparency mechanisms and mitigating overtrust in engaging AI interfaces.
C1  - New York, NY, USA
C3  - Companion proceedings of the 30th international conference on intelligent user interfaces
DA  - 2025///
PY  - 2025
DO  - 10.1145/3708557.3716150
SP  - 211
EP  - 214
PB  - Association for Computing Machinery
SN  - 979-8-4007-1409-2
UR  - https://doi.org/10.1145/3708557.3716150
KW  - Control
KW  - Trust
KW  - Explainable AI
KW  - Agency
KW  - AI Literacy
KW  - AI Transparency
KW  - Cognitive Interaction
KW  - Design Theory
KW  - Interaction Design
KW  - Technology-Mediated Learning
KW  - User Research
ER  - 

TY  - CONF
TI  - Explaining recommendations in e-learning: Effects on adolescents' trust
AU  - Ooge, Jeroen
AU  - Kato, Shotallo
AU  - Verbert, Katrien
T3  - Iui '22
AB  - In the scope of explainable artificial intelligence, explanation techniques are heavily studied to increase trust in recommender systems. However, studies on explaining recommendations typically target adults in e-commerce or media contexts; e-learning has received less research attention. To address these limits, we investigated how explanations affect adolescents’ initial trust in an e-learning platform that recommends mathematics exercises with collaborative filtering. In a randomized controlled experiment with 37&nbsp;adolescents, we compared real explanations with placebo and no explanations. Our results show that real explanations significantly increased initial trust when trust was measured as a multidimensional construct of competence, benevolence, integrity, intention to return, and perceived transparency. Yet, this result did not hold when trust was measured one-dimensionally. Furthermore, not all adolescents attached equal importance to explanations and trust scores were high overall. These findings underline the need to tailor explanations and suggest that dynamically learned factors may be more important than explanations for building initial trust. To conclude, we thus reflect upon the need for explanations and recommendations in e-learning in low-stakes and high-stakes situations.
C1  - Helsinki, Finland and New York, NY, USA
C3  - Proceedings of the 27th international conference on intelligent user interfaces
DA  - 2022///
PY  - 2022
DO  - 10.1145/3490099.3511140
SP  - 93
EP  - 105
PB  - Association for Computing Machinery
SN  - 978-1-4503-9144-3
UR  - https://doi.org/10.1145/3490099.3511140
KW  - XAI
KW  - education
KW  - explainability
KW  - interpretability
KW  - teenagers
ER  - 

TY  - CONF
TI  - GPT-4 as a moral reasoner for robot command rejection
AU  - Wen, Ruchen
AU  - Ferraro, Francis
AU  - Matuszek, Cynthia
T3  - Hai '24
AB  - To support positive, ethical human-robot interactions, robots need to be able to respond to unexpected situations in which societal norms are violated, including rejecting unethical commands. Implementing robust communication for robots is inherently difficult due to the variability of context in real-world settings and the risks of unintended influence during robots’ communication. HRI researchers have begun exploring the potential use of LLMs as a solution for language-based communication, which will require an in-depth understanding and evaluation of LLM applications in different contexts. In this work, we explore how an existing LLM responds to and reasons about a set of norm-violating requests in HRI contexts. We ask human participants to assess the performance of a hypothetical GPT-4-based robot on moral reasoning and explanatory language selection as it compares to human intuitions. Our findings suggest that while GPT-4 performs well at identifying norm violation requests and suggesting non-compliant responses, its flaws in not matching the linguistic preferences and context sensitivity of humans prevent it from being a comprehensive solution for moral communication between humans and robots. Based on our results, we provide a four-point recommendation for the community in incorporating LLMs into HRI systems.
C1  - Swansea, United Kingdom and New York, NY, USA
C3  - Proceedings of the 12th international conference on human-agent interaction
DA  - 2024///
PY  - 2024
DO  - 10.1145/3687272.3688319
SP  - 54
EP  - 63
PB  - Association for Computing Machinery
SN  - 979-8-4007-1178-7
UR  - https://doi.org/10.1145/3687272.3688319
KW  - command rejection
KW  - large language models in HRI
KW  - moral communication
KW  - moral reasoning
KW  - robot explanation
ER  - 

TY  - JOUR
TI  - Personalized explanations for clinician-AI interaction in breast imaging diagnosis by adapting communication to expertise levels
AU  - Calisto, Francisco Maria
AU  - Abrantes, João Maria
AU  - Santiago, Carlos
AU  - Nunes, Nuno J.
AU  - Nascimento, Jacinto C.
T2  - Int. J. Hum.-Comput. Stud.
DA  - 2025/04//
PY  - 2025
DO  - 10.1016/j.ijhcs.2025.103444
VL  - 197
IS  - C
SN  - 1071-5819
UR  - https://doi.org/10.1016/j.ijhcs.2025.103444
KW  - Personalized medicine
KW  - Human–computer interaction
KW  - Breast cancer
KW  - Intelligent agents
KW  - Medical imaging
ER  - 

TY  - CONF
TI  - Tripartite intelligence: Synergizing deep neural network, large language model, and human intelligence for public health misinformation detection (archival full paper)
AU  - Zhang, Yang
AU  - Zong, Ruohan
AU  - Shang, Lanyu
AU  - Yue, Zhenrui
AU  - Zeng, Huimin
AU  - Liu, Yifan
AU  - Wang, Dong
T3  - Ci '24
AB  - The threat of rapidly spreading health misinformation through social media during crises like COVID-19 emphasizes the importance of addressing both clear falsehoods and complex misinformation, including conspiracy theories and subtle distortions. This paper designs a novel tripartite collective intelligence approach that integrates deep neural networks (DNNs), large language models (LLMs), and crowdsourced human intelligence (HI) to collaboratively detect complex forms of public health misinformation on social media. Our design is inspired by the collaborative strengths of DNNs, LLMs, and HI, which complement each other. We observe that DNNs efficiently handle large datasets for initial misinformation screening but struggle with complex content and rely on high-quality training data. LLMs enhance misinformation detection with improved language understanding but may sometimes provide eloquent yet factually incorrect explanations, risking misinformation mislabeling. HI provides critical thinking and ethical judgment superior to DNNs and LLMs but is slower and more costly in misinformation detection. In particular, we develop TriIntel , a tripartite collaborative intelligence framework that leverages the collective intelligence of DNNs, LLMs, and HI to tackle the public health information detection problem under a novel few-shot and uncertainty-aware maximum likelihood estimation framework. Evaluation results on a real-world public health misinformation detection application related to COVID-19 show that TriIntel outperforms representative DNNs, LLMs, and human-AI collaboration baselines in accurately detecting public health misinformation under a diverse set of evaluation scenarios.
C1  - Boston, MA, USA and New York, NY, USA
C3  - Proceedings of the ACM collective intelligence conference
DA  - 2024///
PY  - 2024
DO  - 10.1145/3643562.3672613
SP  - 63
EP  - 75
PB  - Association for Computing Machinery
SN  - 979-8-4007-0554-0
UR  - https://doi.org/10.1145/3643562.3672613
KW  - Collective Intelligence
KW  - Human-AI Collaboration
KW  - Large Language Model
KW  - Misinformation
ER  - 

TY  - CONF
TI  - Medical science in wikipedia: The construction of scientific knowledge in open science projects
AU  - Tamime, Reham Al
AU  - Hall, Wendy
AU  - Giordano, Richard
T3  - OpenSym '16
AB  - Wikipedia has challenged the way traditional encyclopedia knowledge is built and contested by creating an open socio-technical environment that allows non-domain experts to contribute to scientific and medical knowledge. The open nature of Wikipedia has been successful, but there are concerns about the quality and trustworthiness of its articles. The goal of my research is to build a theoretical framework to explain the dynamic of knowledge building in crowd-sourcing based environments like Wikipedia and judge the trustworthiness of the medical articles based on the dynamic network data. By applying Actor Network Theory and Social Network Analysis, the contribution of my research is theoretical and practical as to build a theory on the dynamics of knowledge building in Wikipedia across times and to offer insights for developing citizen science crowd-sourcing platforms by better understanding how editors interact to build health science content.
C1  - Berlin, Germany and New York, NY, USA
C3  - Proceedings of the 12th international symposium on open collaboration companion
DA  - 2016///
PY  - 2016
DO  - 10.1145/2962132.2962141
PB  - Association for Computing Machinery
SN  - 978-1-4503-4481-4
UR  - https://doi.org/10.1145/2962132.2962141
KW  - trust
KW  - citizen science
KW  - facts' dynamics
KW  - Open collaboration
KW  - wikipedia
ER  - 

TY  - CONF
TI  - DisDiff: unsupervised disentanglement of diffusion probabilistic models
AU  - Yang, Tao
AU  - Wang, Yuwang
AU  - Lu, Yan
AU  - Zheng, Nanning
T3  - Nips '23
AB  - Targeting to understand the underlying explainable factors behind observations and modeling the conditional generation process on these factors, we connect disentangled representation learning to Diffusion Probabilistic Models (DPMs) to take advantage of the remarkable modeling ability of DPMs. We propose a new task, disentanglement of (DPMs): given a pre-trained DPM, without any annotations of the factors, the task is to automatically discover the inherent factors behind the observations and disentangle the gradient fields of DPM into sub-gradient fields, each conditioned on the representation of each discovered factor. With disentangled DPMs, those inherent factors can be automatically discovered, explicitly represented, and clearly injected into the diffusion process via the sub-gradient fields. To tackle this task, we devise an unsupervised approach named DisDiff, achieving disentangled representation learning in the framework of DPMs. Extensive experiments on synthetic and real-world datasets demonstrate the effectiveness of DisDiff.
C1  - New Orleans, LA, USA and Red Hook, NY, USA
C3  - Proceedings of the 37th international conference on neural information processing systems
DA  - 2023///
PY  - 2023
PB  - Curran Associates Inc.
ER  - 

TY  - CONF
TI  - Tutorial on landing generative AI in industrial social and e-commerce recsys
AU  - Xu, Da
AU  - Zhang, Danqing
AU  - Ruan, Chuanwei
AU  - Zheng, Lingling
AU  - Yang, Bo
AU  - Yang, Guangyu
AU  - Xu, Shuyuan
AU  - Wang, Haixun
T3  - Www '25
AB  - Over the past two years, generative AI (GenAI) has evolved rapidly, influencing interdisciplinary fields including social and e-commerce Recsys. Despite several exciting research advances, landing GenAI innovations in real-world Recsys remains challenging due to the sophistication of modern industrial product and systems. Our tutorial begins with a brief overview of industrial Recsys and GenAI fundamentals (including LLMOps), followed by the ongoing efforts and opportunities to enhance existing Recsys data and model with foundation models. We then explore how GenAI's curation and reasoning capabilities can be integrated into Recsys-for example, by repurposing raw content, incorporating external knowledge for display and creative optimization, and generating personalized insights/explanations to foster transparency and trust. Following this, the tutorial highlights how AI agents can reshape Recsys by introducing interactive reasoning and action loops, moving beyond traditional passive feedback models. Lastly, we shed insights on real-world solutions for human-AI alignment and responsible GenAI practices. A key feature of the tutorial is the presentation of the holistic AI, Infrastructure, LLMOps, and Product roadmap, including evaluation and responsible AI practices, based on production solutions from LinkedIn, Amazon, Meta, TikTok, Microsoft, and Instacart. Real-world case studies and demos are further provided for illustration. While GenRecsys is still in its early stages, this tutorial provides valuable insights and practical strategies for the Recsys and GenAI communities, bridging scientific research and applied solutions in this novel and rapidly growing field.
C1  - Sydney NSW, Australia and New York, NY, USA
C3  - Companion proceedings of the ACM on web conference 2025
DA  - 2025///
PY  - 2025
DO  - 10.1145/3701716.3715871
SP  - 57
EP  - 60
PB  - Association for Computing Machinery
SN  - 979-8-4007-1331-6
UR  - https://doi.org/10.1145/3701716.3715871
KW  - generative ai
KW  - industrial system
KW  - information retrieval
KW  - large language model
KW  - recommender system
ER  - 

TY  - CONF
TI  - Understanding and being understood: User strategies for identifying and recovering from mistranslations in machine translation-mediated chat
AU  - Robertson, Samantha
AU  - Dı́az, Mark
T3  - FAccT '22
AB  - Machine translation (MT) is now widely and freely available, and has the potential to greatly improve cross-lingual communication. In order to use MT reliably and safely, end users must be able to assess the quality of system outputs and determine how much they can rely on them to guide their decisions and actions. However, it can be difficult for users to detect and recover from mistranslations due to limited language skills. In this work we collected 19 MT-mediated role-play conversations in housing and employment scenarios, and conducted in-depth interviews to understand how users identify and recover from translation errors. Participants communicated using four language pairs: English, and one of Spanish, Farsi, Igbo, or Tagalog. We conducted qualitative analysis to understand user challenges in light of limited system transparency, strategies for recovery, and the kinds of translation errors that proved more or less difficult for users to overcome. We found that users broadly lacked relevant and helpful information to guide their assessments of translation quality. Instances where a user erroneously thought they had understood a translation correctly were rare but held the potential for serious consequences in the real world. Finally, inaccurate and disfluent translations had social consequences for participants, because it was difficult to discern when a disfluent message was reflective of the other person’s intentions, or an artifact of imperfect MT. We draw on theories of grounding and repair in communication to contextualize these findings, and propose design implications for explainable AI (XAI) researchers, MT researchers, as well as collaboration among them to support transparency and explainability in MT. These directions include handling typos and non-standard grammar common in interpersonal communication, making MT in interfaces more visible to help users evaluate errors, supporting collaborative repair of conversation breakdowns, and communicating model strengths and weaknesses to users.
C1  - Seoul, Republic of Korea and New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534638
SP  - 2223
EP  - 2238
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534638
KW  - human-AI interaction
KW  - computer-mediated communication
KW  - explainable machine learning
KW  - machine translation
ER  - 

TY  - CONF
TI  - Demo: SectorX, an en-route ATC simulator for AI-based decision support to air traffic controllers: a case study in the MAHALO project
AU  - Cavagnetto, Nicola
AU  - Venditti, Roberto
AU  - Cocchioni, Matteo
AU  - Bonelli, Stefano
T3  - CHItaly '23
AB  - The EU funded MAHALO Project investigated the effects of transparent and conformal Machine Learning models in support of the safety-critical time-pressured task of Conflict Detection and Resolution for en-route Air Traffic Controllers. The experimental phase was conducted in Italy and Sweden, involving 36 Air Traffic Controllers playing simulated en-route Air Traffic Control scenarios. These scenarios were presented through Sector X, an en-route Air Traffic Control simulator with an ecological User Interface based on the Maastricht Upper Area Control's Controller Working Position. The researchers investigated transparency by introducing three different explainability levels through three visual explanations, and conformance by training with three different conflict resolution styles in the Machine Learning model. The researchers will present the UI designed to achieve transparency to the participants of the demo sessions, who will be able to play Air Traffic Control scenarios.
C1  - Torino, Italy and New York, NY, USA
C3  - Proceedings of the 15th biannual conference of the italian SIGCHI chapter
DA  - 2023///
PY  - 2023
DO  - 10.1145/3605390.3610822
PB  - Association for Computing Machinery
SN  - 979-8-4007-0806-0
UR  - https://doi.org/10.1145/3605390.3610822
KW  - Transparency
KW  - Human-Computer Interaction
KW  - Explainable AI
KW  - Conformance
KW  - Human-AI Teaming
ER  - 

TY  - CONF
TI  - "You can’t build what you don’t understand": Practitioner Perspectives on Explainable AI in the Global South
AU  - Okolo, Chinasa T.
AU  - Lin, Hongjin
T3  - Chi ea '24
AB  - AI for Social Good (AI4SG) has been advocated as a way to address social impact problems using emerging technologies, but little research has examined practitioner motivations behind building these tools and how practitioners make such tools understandable to stakeholders and end users, e.g., through leveraging techniques such as explainable AI (XAI). In this study, we interviewed 12 AI4SG practitioners to understand their experiences developing social impact technologies and their perceptions of XAI, focusing on projects in the Global South. While most of our participants were aware of XAI, many did not incorporate these techniques due to a lack of domain expertise, difficulty incorporating XAI into their existing workflows, and perceiving XAI as less valuable for end users with low levels of AI and digital literacy. Our work reflects on the shortcomings of XAI for real-world use and advocates for a reimagined agenda for human-centered explainability research.
C1  - Honolulu, HI, USA and New York, NY, USA
C3  - Extended abstracts of the CHI conference on human factors in computing systems
DA  - 2024///
PY  - 2024
DO  - 10.1145/3613905.3651080
PB  - Association for Computing Machinery
SN  - 979-8-4007-0331-7
UR  - https://doi.org/10.1145/3613905.3651080
KW  - Artificial Intelligence
KW  - Explainable AI
KW  - AI for Social Good
KW  - Human-Centered Design
KW  - Social Impact
ER  - 

TY  - CONF
TI  - SQLucid: Grounding natural language database queries with interactive explanations
AU  - Tian, Yuan
AU  - Kummerfeld, Jonathan K.
AU  - Li, Toby Jia-Jun
AU  - Zhang, Tianyi
T3  - Uist '24
AB  - Though recent advances in machine learning have led to significant improvements in natural language interfaces for databases, the accuracy and reliability of these systems remain limited, especially in high-stakes domains. This paper introduces SQLucid, a novel user interface that bridges the gap between non-expert users and complex database querying processes. SQLucid addresses existing limitations by integrating visual correspondence, intermediate query results, and editable step-by-step SQL explanations in natural language to facilitate user understanding and engagement. This unique blend of features empowers users to understand and refine SQL queries easily and precisely. Two user studies and one quantitative experiment were conducted to validate SQLucid’s effectiveness, showing significant improvement in task completion accuracy and user confidence compared to existing interfaces. Our code is available at https://github.com/magic-YuanTian/SQLucid.
C1  - Pittsburgh, PA, USA and New York, NY, USA
C3  - Proceedings of the 37th annual ACM symposium on user interface software and technology
DA  - 2024///
PY  - 2024
DO  - 10.1145/3654777.3676368
PB  - Association for Computing Machinery
SN  - 979-8-4007-0628-8
UR  - https://doi.org/10.1145/3654777.3676368
KW  - Databases
KW  - Explanations
KW  - Natural Language Interfaces
ER  - 

TY  - JOUR
TI  - Explainable AI in the military domain
AU  - Wood, Nathan Gabriel
T2  - Ethics and Inf. Technol.
AB  - Artificial intelligence (AI) has become nearly ubiquitous in modern society, from components of mobile applications to medical support systems, and everything in between. In societally impactful systems imbued with AI, there has been increasing concern related to opaque AI, that is, artificial intelligence where it is unclear how or why certain decisions are reached. This has led to a recent boom in research on “explainable AI” (XAI), or approaches to making AI more explainable and understandable to human users. In the military domain, numerous bodies have argued that autonomous and AI-enabled weapon systems ought not incorporate unexplainable AI, with the International Committee of the Red Cross and the United States Department of Defense both explicitly including explainability as a relevant factor in the development and use of such systems. In this article, I present a cautiously critical assessment of this view, arguing that explainability will be irrelevant for many current and near-future autonomous systems in the military (which do not incorporate any AI), that it will be trivially incorporated into most military systems which do possess AI (as these generally possess simpler AI systems), and that for those systems with genuinely opaque AI, explainability will prove to be of more limited value than one might imagine. In particular, I argue that explainability, while indeed a virtue in design, is a virtue aimed primarily at designers and troubleshooters of AI-enabled systems, but is far less relevant for users and handlers actually deploying these systems. I further argue that human–machine teaming is a far more important element of responsibly using AI for military purposes, adding that explainability may undermine efforts to improve human–machine teamings by creating a prima facie sense that the AI, due to its explainability, may be utilized with little (or less) potential for mistakes. I conclude by clarifying that the arguments are not against XAI in the military, but are instead intended as a caution against over-inflating the value of XAI in this domain, or ignoring the limitations and potential pitfalls of this approach.
DA  - 2024/04//
PY  - 2024
DO  - 10.1007/s10676-024-09762-w
VL  - 26
IS  - 2
SN  - 1388-1957
UR  - https://doi.org/10.1007/s10676-024-09762-w
KW  - Artificial intelligence
KW  - AI
KW  - Explainability
KW  - Human–machine interaction
KW  - Autonomous weapon systems
ER  - 

TY  - CONF
TI  - Building a socio-technical theory of coordination: why and how (outstanding research award)
AU  - Herbsleb, James
T3  - Fse 2016
AB  - Research aimed at understanding and addressing coordination breakdowns experienced in global software development (GSD) projects at Lucent Technologies took a path from open-ended qualitative exploratory studies to quantitative studies with a tight focus on a key problem – delay – and its causes. Rather than being directly associated with delay, multi-site work items involved more people than comparable same-site work items, and the number of people was a powerful predictor of delay. To counteract this, we developed and deployed tools and practices to support more effective communication and expertise location. After conducting two case studies of open source development, an extreme form of GSD, we realized that many tools and practices could be effective for multi-site work, but none seemed to work under all conditions. To achieve deeper insight, we developed and tested our Socio-Technical Theory of Coordination (STTC) in which the dependencies among engineering decisions are seen as defining a constraint satisfaction problem that the organization can solve in a variety of ways. I conclude by explaining how we applied these ideas to transparent development environments, then sketch important open research questions.
C1  - Seattle, WA, USA and New York, NY, USA
C3  - Proceedings of the 2016 24th ACM SIGSOFT international symposium on foundations of software engineering
DA  - 2016///
PY  - 2016
DO  - 10.1145/2950290.2994160
SP  - 2
EP  - 10
PB  - Association for Computing Machinery
SN  - 978-1-4503-4218-6
UR  - https://doi.org/10.1145/2950290.2994160
KW  - collaboration
KW  - Coordination
KW  - empirical studies
KW  - global software development
KW  - open source
KW  - socio-technical theory of coordination
KW  - transparent environments
ER  - 

TY  - CONF
TI  - Learning explainable entity resolution algorithms for small business data using SystemER
AU  - Qian, Kun
AU  - Burdick, Douglas
AU  - Gurajada, Sairam
AU  - Popa, Lucian
T3  - DSMM'19
AB  - The 2019 FEIII CALI data challenge aims at linking different representations of the same real-world entities across multiple public datasets that collect identification and activity data about small to medium enterprises (SMEs) in California. We formalize this challenge as a learning-based entity resolution (ER) task, the goal of which is to learn a high-precision and high-recall pair-wise ER model that classifies small business entity pairs into matches and non-matches. Realistic ER tasks usually involve a pipeline of laborintensive and error-prone tasks, such as data preprocesing, gathering of training data, feature engineering, and model tuning. In this task, we apply an advanced human-in-the-loop system, named SystemER, to learn ER algorithms for SME entities. Powered by active learning and via a carefully designed user interface, SystemER can learn high-quality explainable ER algorithms with low human effort, while achieving high-accuracy on the datasets provided by the FEIII CALI data challenge.
C1  - Amsterdam, Netherlands and New York, NY, USA
C3  - Proceedings of the 5th workshop on data science for macro-modeling with financial and economic datasets
DA  - 2019///
PY  - 2019
DO  - 10.1145/3336499.3338010
PB  - Association for Computing Machinery
SN  - 978-1-4503-6823-0
UR  - https://doi.org/10.1145/3336499.3338010
KW  - Entity resolution
KW  - human-in-the-loop
KW  - small business
KW  - SystemER
ER  - 

TY  - CONF
TI  - Evaluating the role of interactivity on improving transparency in autonomous agents
AU  - Qian, Peizhu
AU  - Unhelkar, Vaibhav
T3  - Aamas '22
AB  - Autonomous agents are increasingly being deployed amongst human end-users. Yet, human users often have little knowledge of how these agents work or what they will do next. This lack of transparency has already resulted in unintended consequences during AI use: a concerning trend which is projected to increase with the proliferation of autonomous agents. To curb this trend and ensure safe use of AI, assisting users in establishing an accurate understanding of agents that they work with is essential. In this work, we present AI teacher, a user-centered Explainable AI framework to address this need for autonomous agents that follow a Markovian policy. Our framework first computes salient instructions of agent behavior by estimating a user's mental model and utilizing algorithms for sequential decision-making. Next, in contrast to existing solutions, these instructions are presented interactively to the end-users, thereby enabling a personalized approach to improving AI transparency. We evaluate our framework, with emphasis on its interactive features, through experiments with human participants. The experiment results suggest that, relative to non-interactive approaches, interactive teaching can both reduce the amount of time it takes for humans to create accurate mental models of these agents and is subjectively preferred by human users.
C1  - Virtual Event, New Zealand and Richland, SC
C3  - Proceedings of the 21st international conference on autonomous agents and multiagent systems
DA  - 2022///
PY  - 2022
SP  - 1083
EP  - 1091
PB  - International Foundation for Autonomous Agents and Multiagent Systems
SN  - 978-1-4503-9213-6
KW  - human-AI collaboration
KW  - explainable AI
KW  - machine teaching
KW  - Monte-Carlo tree search
KW  - shared mental models
ER  - 

TY  - CONF
TI  - CIP-ES: Causal input perturbation for explanation surrogates
AU  - Steindl, Sebastian
AU  - Surner, Martin
T3  - Cacml '23
AB  - With current advances in Machine Learning and its growing use in high-impact scenarios, the demand for interpretable and explainable models becomes crucial. Causality research tries to go beyond statistical correlations by focusing on causal relationships, which is fundamental for Interpretable and Explainable Artificial Intelligence. In this paper, we perturb the input for explanation surrogates based on causal graphs. We present an approach to combine surrogate-based explanations with causal knowledge. We apply the perturbed data to the Local Interpretable Model-agnostic Explanations (LIME) approach to showcase how causal graphs improve explanations of surrogate models. We thus integrate features from both domains by adding a causal component to local explanations. The proposed approach enables explanations that suit the expectations of the user by having the user define an appropriate causal graph. Accordingly, these expectations are true to the user. We demonstrate the suitability of our method using real world data.
C1  - Shanghai, China and New York, NY, USA
C3  - Proceedings of the 2023 2nd asia conference on algorithms, computing and machine learning
DA  - 2023///
PY  - 2023
DO  - 10.1145/3590003.3590107
SP  - 569
EP  - 574
PB  - Association for Computing Machinery
SN  - 978-1-4503-9944-9
UR  - https://doi.org/10.1145/3590003.3590107
KW  - Interpretability
KW  - Explainable AI
KW  - Causability
KW  - Causality
KW  - Surrogate Models
ER  - 

TY  - CONF
TI  - Design, development, and deployment of context-adaptive AI systems for enhanced user adoption
AU  - Lee, Christine P.
T3  - Chi ea '24
AB  - My research centers on the development of context-adaptive AI systems to improve end-user adoption through the integration of technical methods. I deploy these AI systems across various interaction modalities, including user interfaces and embodied agents like robots, to expand their practical applicability. My research unfolds in three key stages: design, development, and deployment. In the design phase, user-centered approaches were used to understand user experiences with AI systems and create design tools for user participation in crafting AI explanations. In the ongoing development stage, a safety-guaranteed AI system for a robot agent was created to automatically provide adaptive solutions and explanations for unforeseen scenarios. The next steps will involve the implementation and evaluation of context-adaptive AI systems in various interaction forms. I seek to prioritize human needs in technology development, creating AI systems that tangibly benefit end-users in real-world applications and enhance interaction experiences.
C1  - Honolulu, HI, USA and New York, NY, USA
C3  - Extended abstracts of the CHI conference on human factors in computing systems
DA  - 2024///
PY  - 2024
DO  - 10.1145/3613905.3638195
PB  - Association for Computing Machinery
SN  - 979-8-4007-0331-7
UR  - https://doi.org/10.1145/3613905.3638195
KW  - user-centered design
KW  - human-robot interaction
KW  - Human-AI interaction
ER  - 

TY  - CONF
TI  - Harmonizing ethical principles: Feedback generation approaches in modeling human factors for assisted psychomotor systems
AU  - Portaz, Miguel
AU  - Manjarres, Angeles
AU  - Santos, Olga C.
T3  - UMAP adjunct '24
AB  - As the demand for personalized and adaptive learning experiences increase, there is a urgent need for providing effective feedback mechanisms within critical systems, such as in psychomotor learning systems. This proposal introduces an approach for the integration of retrieval-augmented generation tools to provide comprehensive and insightful feedback to users. By combining the strengths of retrieval-based techniques and generative models, these tools offer the potential to enhance learning outcomes by delivering tailored feedback that is both informative and engaging. The proposal also emphasises the importance of incorporating explainability and transparency concepts. Following the hybrid intelligence paradigm it is possible to ensure that the feedback provided by these tools is not only accurate but also understandable to humans. This approach fosters trust and promotes a deeper understanding of the psychomotor learning process, empowering users and facilitators to make informed decisions about the psychomotor learning path. The hybrid intelligence paradigm, which combines the strengths of both human and artificial intelligence, plays a crucial role in the deployment of these solutions. By taking advantage of the cognitive capabilities of human experts alongside the computational power of artificial intelligence algorithms, it is possible to offer personalised feedback that takes into account both technical accuracy and pedagogical effectiveness. Through these collaborative efforts it is also possible to create learning environments that are inclusive, adaptable, and beneficial to lifelong learning. In conclusion, this proposal introduces retrieval-augmented generation tools for providing feedback in psychomotor learning systems, which represents a significant step towards in its personalization, and whose ethical implications align with the new regulations on the implementation of intelligent technologies in critical systems.
C1  - Cagliari, Italy and New York, NY, USA
C3  - Adjunct proceedings of the 32nd ACM conference on user modeling, adaptation and personalization
DA  - 2024///
PY  - 2024
DO  - 10.1145/3631700.3664900
SP  - 380
EP  - 385
PB  - Association for Computing Machinery
SN  - 979-8-4007-0466-6
UR  - https://doi.org/10.1145/3631700.3664900
KW  - Ethics
KW  - XAI
KW  - Collaborative Learning
KW  - Human-Centered
KW  - Hybrid Intelligence
KW  - Intelligent Psychomotor Systems
KW  - Retrieval Augmented Generation
ER  - 

TY  - JOUR
TI  - Integrating knowledge graphs with symbolic AI: The path to interpretable hybrid AI systems in medicine
AU  - Vidal, Maria-Esther
AU  - Chudasama, Yashrajsinh
AU  - Huang, Hao
AU  - Purohit, Disha
AU  - Torrente, Maria
T2  - Web Semant.
DA  - 2025/01//
PY  - 2025
DO  - 10.1016/j.websem.2024.100856
VL  - 84
IS  - C
SN  - 1570-8268
UR  - https://doi.org/10.1016/j.websem.2024.100856
KW  - Counterfactual prediction
KW  - KG-based applications
KW  - Knowledge Graphs
KW  - Neuro-symbolic systems
KW  - Semantic Data Management
KW  - Valid link prediction
ER  - 

TY  - JOUR
TI  - Fine-tuning language model embeddings to reveal domain knowledge: An explainable artificial intelligence perspective on medical decision making
AU  - Kraišniković, Ceca
AU  - Harb, Robert
AU  - Plass, Markus
AU  - Zoughbi, Wael Al
AU  - Holzinger, Andreas
AU  - Müller, Heimo
T2  - Engineering Applications of Artificial Intelligence
DA  - 2025/01//
PY  - 2025
DO  - 10.1016/j.engappai.2024.109561
VL  - 139
IS  - PB
J2  - Eng. Appl. Artif. Intell.
SN  - 0952-1976
UR  - https://doi.org/10.1016/j.engappai.2024.109561
KW  - Analysis of embeddings
KW  - Bidirectional Encoder Representations from Transformers model
KW  - Digital pathology
KW  - Domain-language adaptation
KW  - Fine-tuning
KW  - Interpretable medical decision scores
KW  - Language model for German
KW  - Large language models in pathology
KW  - Pathology reports
KW  - Pathology-specific tasks
ER  - 

TY  - THES
TI  - Human-interpretable explanations for black-box machine learning models: An application to fraud detection
AU  - Balayan, Vladimir
AB  - Machine Learning (ML) has been increasingly used to aid humans making high-stakes decisions in a wide range of areas, from public policy to criminal justice, education, healthcare, or financial services. However, it is very hard for humans to grasp the rationale behind every ML model's prediction, hindering trust in the system. The field of Explainable Artificial Intelligence (XAI) emerged to tackle this problem, aiming to research and develop methods to make those "black-boxes" more interpretable, but there is still no major breakthrough. Additionally, the most popular explanation methods — LIME and SHAP — produce very low-level feature attribution explanations, being of limited usefulness to personas without any ML knowledge.This work was developed at Feedzai, a fintech company that uses ML to prevent financial crime. One of the main Feedzai products is a case management application used by fraud analysts to review suspicious financial transactions flagged by the ML models. Fraud analysts are domain experts trained to look for suspicious evidence in transactions but they do not have ML knowledge, and consequently, current XAI methods do not suit their information needs. To address this, we present JOEL, a neural network-based framework to jointly learn a decision-making task and associated domain knowledge explanations. JOEL is tailored to human-in-the-loop domain experts that lack deep technical ML knowledge, providing high-level insights about the model's predictions that very much resemble the experts' own reasoning. Moreover, by collecting the domain feedback from a pool of certified experts (human teaching), we promote seamless and better quality explanations. Lastly, we resort to semantic mappings between legacy expert systems and domain taxonomies to automatically annotate a bootstrap training set, overcoming the absence of concept-based human annotations. We validate JOEL empirically on a real-world fraud detection dataset, at Feedzai. We show that JOEL can generalize the explanations from the bootstrap dataset. Furthermore, obtained results indicate that human teaching is able to further improve the explanations prediction quality.
DA  - 2020///
PY  - 2020
M3  - master
PB  - Universidade NOVA de Lisboa (Portugal)
N1  - AAI29097887
ER  - 

TY  - JOUR
TI  - Physician adoption of AI assistant
AU  - Hou, Ting
AU  - Li, Meng
AU  - Tan, Yinliang (Ricky)
AU  - Zhao, Huazhong
T2  - Manufacturing &amp; Service Operations Management
AB  - Problem definition: Artificial intelligence (AI) assistants—software agents that can perform tasks or services for individuals—are among the most promising AI applications. However, little is known about the adoption of AI assistants by service providers (i.e., physicians) in a real-world healthcare setting. In this paper, we investigate the impact of the AI smartness (i.e., whether the AI assistant is powered by machine learning intelligence) and the impact of AI transparency (i.e., whether physicians are informed of the AI assistant). Methodology/results: We collaborate with a leading healthcare platform to run a field experiment in which we compare physicians’ adoption behavior, that is, adoption rate and adoption timing, of smart and automated AI assistants under transparent and non-transparent conditions. We find that the smartness can increase the adoption rate and shorten the adoption timing, whereas the transparency can only shorten the adoption timing. Moreover, the impact of AI transparency on the adoption rate is contingent on the smartness level of the AI assistant: the transparency increases the adoption rate only when the AI assistant is not equipped with smart algorithms and fails to do so when the AI assistant is smart. Managerial implications: Our study can guide platforms in designing their AI strategies. Platforms should improve the smartness of AI assistants. If such an improvement is too costly, the platform should transparentize the AI assistant, especially when it is not smart.Funding: This research was supported by a Behavioral Research Assistance Grant from the C. T. Bauer College of Business, University of Houston. H. Zhao acknowledges support from Hong Kong General Research Fund [9043593]. Y. (R.) Tan acknowledges generous support from CEIBS Research [Grant AG24QCS].Supplemental Material: The online appendix is available at .
DA  - 2024/09//
PY  - 2024
DO  - 10.1287/msom.2023.0093
VL  - 26
IS  - 5
SP  - 1639
EP  - 1655
SN  - 1526-5498
UR  - https://doi.org/10.1287/msom.2023.0093
KW  - chatbot
KW  - field experiment
KW  - generative AI
KW  - health intelligence
KW  - medical platform
KW  - operational transparency
ER  - 

TY  - CONF
TI  - Exploring tangible explainable AI (TangXAI): a user study of two XAI approaches
AU  - Colley, Ashley
AU  - Kalving, Matilda
AU  - Häkkilä, Jonna
AU  - Väänänen, Kaisa
T3  - OzCHI '23
AB  - Explainable AI (XAI) has garnered significant attention as a theoretical subject in the research community. However, the practical application of XAI, particularly in the realm of user interfaces, remains limited. Moreover, evaluations of these interfaces from the perspective of end-users are scarce. In this paper, we introduce and evaluate two innovative tangible XAI interface concepts. The tangible interfaces capitalize on the widely recognized advantages of data physicalization, offering users a more intuitive and hands-on experience. We implemented two distinct XAI approaches within this tangible framework: feature relevance and local explanations. These approaches were applied to real-world use cases: recommending recipes and selecting jogging routes, respectively. The findings of our Wizard of Oz study indicate that participants had some challenges in distinguishing between the primary objectives of the XAI interface and the typical interactions associated with an AI recommender system. However, tangibility seems to support users’ understanding of AI’s explanations and enables users to reflect on their trust in the AI model.
C1  - Wellington, New Zealand and New York, NY, USA
C3  - Proceedings of the 35th australian computer-human interaction conference
DA  - 2024///
PY  - 2024
DO  - 10.1145/3638380.3638426
SP  - 679
EP  - 683
PB  - Association for Computing Machinery
SN  - 979-8-4007-1707-9
UR  - https://doi.org/10.1145/3638380.3638426
KW  - XAI
KW  - Explainable AI
KW  - HCI
KW  - human centered AI
KW  - tangible interaction
KW  - TangXAI
KW  - TUI
ER  - 

TY  - CONF
TI  - Toward robust policy summarization
AU  - Lage, Isaac
AU  - Lifschitz, Daphna
AU  - Doshi-Velez, Finale
AU  - Amir, Ofra
T3  - Aamas '19
AB  - AI agents are being developed to help people with high stakes decision-making processes from driving cars to prescribing drugs. It is therefore becoming increasingly important to develop "explainable AI" methods that help people understand the behavior of such agents. Summaries of agent policies can help human users anticipate agent behavior and facilitate more effective collaboration. Prior work has framed agent summarization as a machine teaching problem where examples of agent behavior are chosen to maximize reconstruction quality under the assumption that people do inverse reinforcement learning to infer an agent's policy from demonstrations. We compare summaries generated under this assumption to summaries generated under the assumption that people use imitation learning. We show through simulations that in some domains, there exist summaries that produce high-quality reconstructions under different models, but in other domains, only matching the summary extraction model to the reconstruction model produces high-quality reconstructions. These results highlight the importance of assuming correct computational models for how humans extrapolate from a summary, suggesting human-in-the-loop approaches to summary extraction.
C1  - Montreal QC, Canada and Richland, SC
C3  - Proceedings of the 18th international conference on autonomous agents and MultiAgent systems
DA  - 2019///
PY  - 2019
SP  - 2081
EP  - 2083
PB  - International Foundation for Autonomous Agents and Multiagent Systems
SN  - 978-1-4503-6309-9
KW  - explainable AI
KW  - policy summarization
ER  - 

TY  - CONF
TI  - Tutorial on landing generative AI in industrial social and e-commerce recsys
AU  - Xu, Da
AU  - Zhang, Danqing
AU  - Zheng, Lingling
AU  - Yang, Bo
AU  - Yang, Guangyu
AU  - Xu, Shuyuan
AU  - Liang, Cindy
T3  - Cikm '24
AB  - Over the past two years, GAI has evolved rapidly, influencing various fields including social and e-commerce Recsys. Despite exciting advances, landing these innovations in real-world Recsys remains challenging due to the sophistication of modern industrial product and systems. Our tutorial begins with a brief overview of building industrial Recsys and GAI fundamentals, followed by the ongoing efforts and opportunities to enhance personalized recommendations with foundation models.We then explore the integration of curation capabilities into Recsys, such as repurposing raw content, incorporating external knowledge, and generating personalized insights/explanations to foster transparency and trust. Next, the tutorial illustrates how AI agents can transform Recsys through interactive reasoning and action loops, shifting away from traditional passive feedback models. Finally, we shed insights on real-world solutions for human-AI alignment and responsible GAI practices.A critical component of the tutorial is detailing the AI, Infrastructure, LLMOps, and Product roadmap (including the evaluation and responsible AI practices) derived from the production solutions in LinkedIn, Amazon, TikTok, and Microsoft. While GAI in Recsys is still in its early stages, this tutorial provides valuable insights and practical solutions for the Recsys and GAI communities.
C1  - Boise, ID, USA and New York, NY, USA
C3  - Proceedings of the 33rd ACM international conference on information and knowledge management
DA  - 2024///
PY  - 2024
DO  - 10.1145/3627673.3679099
SP  - 5538
EP  - 5542
PB  - Association for Computing Machinery
SN  - 979-8-4007-0436-9
UR  - https://doi.org/10.1145/3627673.3679099
KW  - industrial system
KW  - information retrieval
KW  - large language model
KW  - recommender system
KW  - generative AI
ER  - 

TY  - CONF
TI  - Invisible AI-driven HCI systems – when, why and how
AU  - Alm, Cecilia Ovesdotter
AU  - Alvarez, Alberto
AU  - Font, José
AU  - Liapis, Antonios
AU  - Pederson, Thomas
AU  - Salo, Johan
T3  - NordiCHI '20
AB  - The InvisibleAI (InvAI’20) workshop aims to systematically discuss a growing class of interactive systems that invisibly remove some decision-making tasks away from humans to machines, based on recent advances in artificial intelligence (AI), data science, and sensor or actuation technology. While the interest in the affordances as well as the risks of hidden pervasive AI are high on the agenda in public debate, discussion on the topic is needed within the human-computer interaction (HCI) community. In particular, we want to gather insights, ideas, and models for approaching the use of barely noticeable AI decision-making in systems design from a human-centered perspective, so as to make the most out of the automated systems and algorithms that support human activity both as designers and users. Concurrently, these systems should safeguard that humans remain in charge when it counts (high stakes decisions, privacy, monitoring lack of explainability and fairness, etc.). What to automate and what not to automate is often a system designer’s choice &nbsp;[8]. By taking the established concept of explicit interaction between a system and its user as a point of departure, and inviting authors to provide examples from their own research, we aim to stimulate dynamic discussion while keeping the workshop concrete and system design-focused. The workshop especially directs itself to participants from the interaction design, AI, and HCI communities. The targeted scientific outcome of the workshop is an up-to-date ontology of invisible AI-HCI systems and hybrid human-AI collaboration mechanisms, and approaches. Additionally, we expect that the workgroups and the roundtables will provide starting points shaping continued discussions, new collaborations, and innovative scientific contributions that springboard from the workgroups’ findings. The focus of the proposed workshop involves the bridging of two spaces of computational research that impact user experiences and societal domains (HCI and AI). Thus, the proposed workshop topic aligns well with the theme of this year’s NordiCHI conference which is Shaping Experiences, Shaping Society.
C1  - Tallinn, Estonia and New York, NY, USA
C3  - Proceedings of the 11th nordic conference on human-computer interaction: Shaping experiences, shaping society
DA  - 2020///
PY  - 2020
DO  - 10.1145/3419249.3420099
PB  - Association for Computing Machinery
SN  - 978-1-4503-7579-5
UR  - https://doi.org/10.1145/3419249.3420099
KW  - artificial intelligence
KW  - Human-computer interaction
KW  - human-machine collaboration
ER  - 

TY  - CHAP
TI  - Habemus a right to an explanation: So what? - A framework on transparency-explainability functionality and tensions in the EU AI act
AU  - Nannini, Luca
T2  - Proceedings of the 2024 AAAI/ACM conference on AI, ethics, and society
AB  - The European Union's Artificial Intelligence Act (AI Act), finalized in February 2024, mandates transparency and explainability requirements for AI systems to enable effective oversight and safeguard fundamental rights. Yet the practical implementation of these requirements faces challenges due to tensions between the need for meaningful explanations and their potential risks. This research proposes the Transparency-Explainability Functionality and Tensions (TEFT) framework to analyze the interplay of legal, technical, and socio-ethical factors shaping the realization of algorithmic transparency and explainability in the EU context. Through a two-pronged approach combining a focused literature review and an in-depth examination of the AI Act's provisions, we identify key friction points and challenges in operationalizing the right to explanation. The TEFT framework maps the interests and incentives of various stakeholders, including AI providers &amp; deployers, oversight bodies, and affected individuals, while considering their goals, expected benefits, risks, possible negative impacts, and context to algorithmic explainability.
DA  - 2025///
PY  - 2025
SP  - 1023
EP  - 1035
PB  - AAAI Press
ER  - 

TY  - CONF
TI  - Dialectical reconciliation via structured argumentative dialogues
AU  - Vasileiou, Stylianos Loukas
AU  - Kumar, Ashwin
AU  - Yeoh, William
AU  - Son, Tran Cao
AU  - Toni, Francesca
T3  - Kr '24
AB  - We present a novel framework designed to extend model reconciliation approaches, commonly used in human-aware planning, for enhanced human-AI interaction. By adopting a structured argumentation-based dialogue paradigm, our framework enables dialectical reconciliation to address knowledge discrepancies between an explainer (AI agent) and an explainee (human user), where the goal is for the explainee to understand the explainer's decision. We formally describe the operational semantics of our proposed framework, providing theoretical guarantees. We then evaluate the framework's efficacy "in the wild" via computational and human-subject experiments. Our findings suggest that our framework offers a promising direction for fostering effective human-AI interactions in domains where explainability is important.
C1  - Hanoi, Vietnam
C3  - Proceedings of the 21st international conference on principles of knowledge representation and reasoning
DA  - 2024///
PY  - 2024
DO  - 10.24963/kr.2024/73
SN  - 978-1-956792-05-8
UR  - https://doi.org/10.24963/kr.2024/73
ER  - 

TY  - JOUR
TI  - A cloud-based robot system for long-term interaction: Principles, implementation, lessons learned
AU  - Kaptein, Frank
AU  - Kiefer, Bernd
AU  - Cully, Antoine
AU  - Celiktutan, Oya
AU  - Bierman, Bert
AU  - Rijgersberg-peters, Rifca
AU  - Broekens, Joost
AU  - Van Vught, Willeke
AU  - Van Bekkum, Michael
AU  - Demiris, Yiannis
AU  - Neerincx, Mark A.
T2  - J. Hum.-Robot Interact.
AB  - Making the transition to long-term interaction with social-robot systems has been identified as one of the main challenges in human-robot interaction. This article identifies four design principles to address this challenge and applies them in a real-world implementation: cloud-based robot control, a modular design, one common knowledge base for all applications, and hybrid artificial intelligence for decision making and reasoning. The control architecture for this robot includes a common Knowledge-base (ontologies), Data-base, “Hybrid Artificial Brain” (dialogue manager, action selection and explainable AI), Activities Centre (Timeline, Quiz, Break and Sort, Memory, Tip of the Day, ( ldots ) ), Embodied Conversational Agent (ECA, i.e., robot and avatar), and Dashboards (for authoring and monitoring the interaction). Further, the ECA is integrated with an expandable set of (mobile) health applications. The resulting system is a Personal Assistant for a healthy Lifestyle (PAL), which supports diabetic children with self-management and educates them on health-related issues (48 children, aged 6–14, recruited via hospitals in the Netherlands and in Italy). It is capable of autonomous interaction “in the wild” for prolonged periods of time without the need for a “Wizard-of-Oz” (up until 6 months online). PAL is an exemplary system that provides personalised, stable and diverse, long-term human-robot interaction.
DA  - 2021/10//
PY  - 2021
DO  - 10.1145/3481585
VL  - 11
IS  - 1
UR  - https://doi.org/10.1145/3481585
KW  - Cloud-based robots
KW  - conversational agents
KW  - long-term human-robot interaction
KW  - pervasive lifestyle support
ER  - 

TY  - CONF
TI  - Evaluating human-centered AI explanations: Introduction of an XAI evaluation framework for fact-checking
AU  - Schmitt, Vera
AU  - Csomor, Balázs Patrik
AU  - Meyer, Joachim
AU  - Villa-Areas, Luis-Felipe
AU  - Jakob, Charlott
AU  - Polzehl, Tim
AU  - Möller, Sebastian
T3  - Mad '24
AB  - The rapidly increasing amount of online information and the advent of Generative Artificial Intelligence (GenAI) make the manual verification of information impractical. Consequently, AI systems are deployed to detect disinformation and deepfakes. Prior studies have indicated that combining AI and human capabilities yields enhanced performance in detecting disinformation. Furthermore, the European Union (EU) AI Act mandates human supervision for AI applications in areas impacting essential human rights, like freedom of speech, necessitating that AI systems be transparent and provide adequate explanations to ensure comprehensibility. Extensive research has been conducted on incorporating explainability (XAI) attributes to augment AI transparency, yet these often miss a human-centric assessment. The effectiveness of such explanations also varies with the user’s prior knowledge and personal attributes. Therefore, we developed a framework for validating XAI features for the collaborative human-AI fact-checking task. The framework allows the testing of XAI features with objective and subjective evaluation dimensions and follows human-centric design principles when displaying information about the AI system to the users. The framework was tested in a crowdsourcing experiment with 433 participants, including 406 crowdworkers and 27 journalists for the collaborative disinformation detection task. The tested XAI features increase the AI system’s perceived usefulness, understandability, and trust. With this publication, the XAI evaluation framework is made open source.
C1  - Phuket, Thailand and New York, NY, USA
C3  - Proceedings of the 3rd ACM international workshop on multimedia AI against disinformation
DA  - 2024///
PY  - 2024
DO  - 10.1145/3643491.3660283
SP  - 91
EP  - 100
PB  - Association for Computing Machinery
SN  - 979-8-4007-0552-6
UR  - https://doi.org/10.1145/3643491.3660283
KW  - blind trust in AI systems
KW  - Human-centered eXplanations
KW  - objective and subjective evaluation of eXplanations
ER  - 

TY  - CONF
TI  - Towards bridging the gaps between the right to explanation and the right to be forgotten
AU  - Krishna, Satyapriya
AU  - Ma, Jiaqi
AU  - Lakkaraju, Himabindu
T3  - ICML'23
AB  - The Right to Explanation and the Right to be Forgotten are two important principles outlined to regulate algorithmic decision making and data usage in real-world applications. While the right to explanation allows individuals to request an actionable explanation for an algorithmic decision, the right to be forgotten grants them the right to ask for their data to be deleted from all the databases and models of an organization. Intuitively, enforcing the right to be forgotten may trigger model updates which in turn invalidate previously provided explanations, thus violating the right to explanation. In this work, we investigate the technical implications arising due to the interference between the two aforementioned regulatory principles, and propose the first algorithmic framework to resolve the tension between them. To this end, we formulate a novel optimization problem to generate explanations that are robust to model updates due to the removal of training data instances by data deletion requests. We then derive an efficient approximation algorithm to handle the combinatorial complexity of this optimization problem. We theoretically demonstrate that our method generates explanations that are provably robust to worst-case data deletion requests with bounded costs in case of linear models and certain classes of non-linear models. Extensive experimentation with real-world datasets demonstrates the efficacy of the proposed framework
C1  - Honolulu, Hawaii, USA
C3  - Proceedings of the 40th international conference on machine learning
DA  - 2023///
PY  - 2023
PB  - JMLR.org
ER  - 

TY  - THES
TI  - Explanation-driven learning-based models for visual recognition tasks
AU  - Daniels, Zachary Alan
AU  - Michmizos, Konstantinos
AU  - Moustakides, George
AU  - Li, Fuxin
AB  - Safety-critical applications (e.g., autonomous vehicles, human-machine teaming, and automated medical diagnosis) often require the use of computational agents that are capable of understanding and reasoning about the high-level content of real-world scene images in order to make rational and grounded decisions that can be trusted by humans. Many of these agents rely on machine learning-based models which are increasingly being treated as black-boxes. One way to increase model interpretability is to make explainability a core principle of the model, e.g., by forcing deep neural networks to explicitly learn grounded and interpretable features. In this thesis, I provide a high-level overview of the field of explainable/interpretable machine learning and review some existing approaches for interpreting neural networks used for computer vision tasks. I also introduce four novel approaches for making convolutional neural networks (CNNs) more interpretable by utilizing explainability as a guiding principle when designing the model architecture. Finally, I discuss some possible future research directions involving explanation-driven machine learning.
DA  - 2020///
PY  - 2020
M3  - phd
PB  - Rutgers The State University of New Jersey, School of Graduate Studies
N1  - AAI27998733
ER  - 

TY  - CONF
TI  - Tutorial on explainable deep reinforcement learning: One framework, three paradigms and many challenges.
AU  - Vouros, George
T3  - Setn '22
AB  - Interpretability, explainability and transparency are key issues to introducing Artificial Intelligence closed—box methods in many critical domains: This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability and fairness, and has important consequences towards keeping the human in the loop in high levels of automation, especially in critical cases for decision making. Reinforcement learning methods, and especially their deep versions, are closed-box methods that support agents to act autonomously in the real world. This tutorial will provide a formal specification of the deep reinforcement learning explainability problems, and will present the necessary components of a general explainable reinforcement learning framework. Based on this framework will provide distinct explainability paradigms towards solving explainability problems, with examples from state-of-the-art methods and real-world cases. The tutorial will conclude identifying open questions and important challenges. The tutorial is based on the survey paper on “Explainable Deep Reinforcement Learning” State of the Art and Challenges” [1].
C1  - Corfu, Greece and New York, NY, USA
C3  - Proceedings of the 12th hellenic conference on artificial intelligence
DA  - 2022///
PY  - 2022
DO  - 10.1145/3549737.3549808
PB  - Association for Computing Machinery
SN  - 978-1-4503-9597-7
UR  - https://doi.org/10.1145/3549737.3549808
KW  - Transparency
KW  - Explainability
KW  - Interpretability
KW  - Deep Learning
KW  - Deep Reinforcement Learning
ER  - 

TY  - CONF
TI  - Towards design principles for user-centric explainable AI in fraud detection
AU  - Cirqueira, Douglas
AU  - Helfert, Markus
AU  - Bezbradica, Marija
AB  - Experts rely on fraud detection and decision support systems to analyze fraud cases, a growing problem in digital retailing and banking. With the advent of Artificial Intelligence (AI) for decision support, those experts face the black-box problem and lack trust in AI predictions for fraud. Such an issue has been tackled by employing Explainable AI (XAI) to provide experts with explained AI predictions through various explanation methods. However, fraud detection studies supported by XAI lack a user-centric perspective and discussion on how principles are deployed, both important requirements for experts to choose an appropriate explanation method. On the other hand, recent research in Information Systems (IS) and Human-Computer Interaction highlights the need for understanding user requirements to develop tailored design principles for decision support systems. In this research, we adopt a design science research methodology and IS theoretical lens to develop and evaluate design principles, which align fraud expert’s tasks with explanation methods for Explainable AI decision support. We evaluate the utility of these principles using an information quality framework to interview experts in banking fraud, plus a simulation. The results show that the principles are an useful tool for designing decision support systems for fraud detection with embedded user-centric Explainable AI.
C1  - Berlin, Heidelberg
C3  - Artificial intelligence in HCI: Second international conference, AI-HCI 2021, held as part of the 23rd HCI international conference, HCII 2021, virtual event, july 24–29, 2021, proceedings
DA  - 2021///
PY  - 2021
DO  - 10.1007/978-3-030-77772-2_2
SP  - 21
EP  - 40
PB  - Springer-Verlag
SN  - 978-3-030-77771-5
UR  - https://doi.org/10.1007/978-3-030-77772-2_2
KW  - Artificial intelligence
KW  - Human-AI interaction
KW  - Decision support systems
KW  - Explainable AI
KW  - Human-centered AI
KW  - HCI
KW  - Design principles
KW  - Fraud detection
ER  - 

TY  - CONF
TI  - Explaining entity resolution predictions: Where are we and what needs to be done?
AU  - Thirumuruganathan, Saravanan
AU  - Ouzzani, Mourad
AU  - Tang, Nan
T3  - Hilda '19
AB  - Entity resolution (ER) seeks to identify the set of tuples in a dataset that refer to the same real-world entity. It is one of the fundamental and well studied problems in data integration with applications in diverse domains such as banking, insurance, e-commerce, and so on. Machine Learning and Deep Learning based methods provide the state-of-the-art results. For practitioners, it is often challenging to understand why the classifier made a particular prediction. While there has been extensive work in the ML community on explaining classifier predictions, we found that a direct application of those techniques is not appropriate for ER. There is a huge gap between the needs of lay ER practitioners and the explanation community. In this paper, we provide a comprehensive taxonomy of these challenges, discuss research opportunities and propose preliminary solutions.
C1  - Amsterdam, Netherlands and New York, NY, USA
C3  - Proceedings of the workshop on human-in-the-loop data analytics
DA  - 2019///
PY  - 2019
DO  - 10.1145/3328519.3329130
PB  - Association for Computing Machinery
SN  - 978-1-4503-6791-2
UR  - https://doi.org/10.1145/3328519.3329130
ER  - 

TY  - JOUR
TI  - Evaluating explanations needs in blockchain smart contracts to reconcile surprises
AU  - Al Ghanmi, Hanouf
AU  - Ahmadjee, Sabreen
AU  - Bahsoon, Rami
T2  - ACM Trans. Softw. Eng. Methodol.
AB  - Smart contracts on the blockchain play an important role in decentralised systems by automating and executing agreements without the need for intermediaries. As these contracts become integral to various domains, ensuring users’ understanding of their functioning is paramount. This article investigates the need for explanations in smart contracts, drawing inspiration from contract law principles and established practices in Explainable Artificial intelligence (XAI). It introduces key purposes—justification, clarification, compliance and consent to design explainability. Additionally, the study proposes a novel assessment framework informed by the Metacognitive Explanation-Based (MEB) theory to systematically evaluate surprise potential in smart contracts lacking explanations. We use surprise as a guiding factor to systematically identify areas requiring improvement in terms of justification, clarification, compliance and consent. To demonstrate the utility of the assessment approach, we evaluate two decentralised lending projects, uncovering potential surprises. One of the key observations is the lack of setting information, especially concerning compliance, consent and decision justification. This absence of information has heightened the potential for surprises. In the process of validating the explanation purposes, we implement techniques to improve the design of the assessed smart contracts. Further, the research explores the trade-offs involved in integrating explanations, providing nuanced insights into economic implications such as increased deployment and execution costs. This work contributes to the broader comprehension of smart contract explainability requirements and lays out a theoretical foundation for a generic evaluation method. It aims to facilitate the development of more human-centric and comprehensible smart contracts.
DA  - 2025/03//
PY  - 2025
DO  - 10.1145/3721283
SN  - 1049-331X
UR  - https://doi.org/10.1145/3721283
N1  - Just Accepted
KW  - explainability
KW  - Blockchain
KW  - requirements
KW  - smart contracts
ER  - 

TY  - CONF
TI  - Designing AI for trust and collaboration in time-constrained medical decisions: a sociotechnical lens
AU  - Jacobs, Maia
AU  - He, Jeffrey
AU  - F. Pradier, Melanie
AU  - Lam, Barbara
AU  - Ahn, Andrew C.
AU  - McCoy, Thomas H.
AU  - Perlis, Roy H.
AU  - Doshi-Velez, Finale
AU  - Gajos, Krzysztof Z.
T3  - Chi '21
AB  - Major depressive disorder is a debilitating disease affecting 264 million people worldwide. While many antidepressant medications are available, few clinical guidelines support choosing among them. Decision support tools (DSTs) embodying machine learning models may help improve the treatment selection process, but often fail in clinical practice due to poor system integration. We use an iterative, co-design process to investigate clinicians’ perceptions of using DSTs in antidepressant treatment decisions. We identify ways in which DSTs need to engage with the healthcare sociotechnical system, including clinical processes, patient preferences, resource constraints, and domain knowledge. Our results suggest that clinical DSTs should be designed as multi-user systems that support patient-provider collaboration and offer on-demand explanations that address discrepancies between predictions and current standards of care. Through this work, we demonstrate how current trends in explainable AI may be inappropriate for clinical environments and consider paths towards designing these tools for real-world medical systems.
C1  - Yokohama, Japan and New York, NY, USA
C3  - Proceedings of the 2021 CHI conference on human factors in computing systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3411764.3445385
PB  - Association for Computing Machinery
SN  - 978-1-4503-8096-6
UR  - https://doi.org/10.1145/3411764.3445385
KW  - healthcare
KW  - decision support tools
KW  - major depressive disorder
ER  - 

TY  - JOUR
TI  - The decoupling concept bottleneck model
AU  - Zhang, Rui
AU  - Du, Xingbo
AU  - Yan, Junchi
AU  - Zhang, Shihua
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
AB  - The Concept Bottleneck Model (CBM) is an interpretable neural network that leverages high-level concepts to explain model decisions and conduct human-machine interaction. However, in real-world scenarios, the deficiency of informative concepts can impede the model's interpretability and subsequent interventions. This paper proves that insufficient concept information can lead to an inherent dilemma of concept and label distortions in CBM. To address this challenge, we propose the Decoupling Concept Bottleneck Model (DCBM), which comprises two phases: 1) DCBM for prediction and interpretation, which decouples heterogeneous information into explicit and implicit concepts while maintaining high label and concept accuracy, and 2) DCBM for human-machine interaction, which automatically corrects labels and traces wrong concepts via mutual information estimation. The construction of the interaction system can be formulated as a light min-max optimization problem. Extensive experiments expose the success of alleviating concept/label distortions, especially when concepts are insufficient. In particular, we propose the Concept Contribution Score (CCS) to quantify the interpretability of DCBM. Numerical results demonstrate that CCS can be guaranteed by the Jensen-Shannon divergence constraint in DCBM. Moreover, DCBM expresses two effective human-machine interactions, including forward intervention and backward rectification, to further promote concept/label accuracy via interaction with human experts.
DA  - 2025/02//
PY  - 2025
DO  - 10.1109/TPAMI.2024.3489597
VL  - 47
IS  - 2
SP  - 1250
EP  - 1265
J2  - IEEE Trans. Pattern Anal. Mach. Intell.
SN  - 0162-8828
UR  - https://doi.org/10.1109/TPAMI.2024.3489597
ER  - 

TY  - CONF
TI  - Under co-construction: Toward the social design of explainable AI systems
AU  - Rohlfing, Katharina J.
T3  - ITiCSE '21
AB  - Technological advancements in machine learning affecting humans' lives on the one hand and also regulatory initiatives fostering transparency in algorithmic decision making on the other hand drive a recent surge of interest in explainable AI (XAI). Explainability is discussed as a solution to sociotechnical challenges such as intelligent software providing incomprehensible decisions or big data enabling fast learning but becoming too complex to fully comprehend and judge its achievements. With explainable AI, more insights into the functions, decisions, and usefulness of algorithms are expected.If an explanation is successful, it results in an understanding. Current XAI research is centering around one-way interaction from which solutions to achieve understanding are derived. In the presentation, I will point to an important resource for achieving understanding that has been overlooked so far: the interaction with the addressee [1].Whereas in current XAI research, the addressee (explainee) is mostly seen as a passive receiver, I argue that the explainee can provide an active and crucial contribution to the process of understanding resulting in the explanation being tailored to a particular form of understanding and thus gaining on relevance. Within the co-constructive view [1], both partners scaffold and monitor each other-perpetuating, thus, the process of explaining as a joint endeavor toward a goal. I argue that such an endeavor can be implemented in explainable and interactive AI.
C1  - Virtual Event, Germany and New York, NY, USA
C3  - Proceedings of the 26th ACM conference on innovation and technology in computer science education V. 1
DA  - 2021///
PY  - 2021
DO  - 10.1145/3430665.3460419
SP  - 1
PB  - Association for Computing Machinery
SN  - 978-1-4503-8214-4
UR  - https://doi.org/10.1145/3430665.3460419
KW  - explainability
KW  - co-construction and interaction
ER  - 

TY  - JOUR
TI  - HC-COVID: a hierarchical crowdsource knowledge graph approach to explainable COVID-19 misinformation detection
AU  - Kou, Ziyi
AU  - Shang, Lanyu
AU  - Zhang, Yang
AU  - Wang, Dong
T2  - Proc. ACM Hum.-Comput. Interact.
AB  - The proliferation of social media has promoted the spread of misinformation that raises many concerns in our society. This paper focuses on a critical problem of explainable COVID-19 misinformation detection that aims to accurately identify and explain misleading COVID-19 claims on social media. Motivated by the lack of COVID-19 relevant knowledge in existing solutions, we construct a novel crowdsource knowledge graph based approach to incorporate the COVID-19 knowledge facts by leveraging the collaborative efforts of expert and non-expert crowd workers. Two important challenges exist in developing our solution: i) how to effectively coordinate the crowd efforts from both expert and non-expert workers to generate the relevant knowledge facts for detecting COVID-19 misinformation; ii) How to leverage the knowledge facts from the constructed knowledge graph to accurately explain the detected COVID-19 misinformation. To address the above challenges, we develop HC-COVID, a hierarchical crowdsource knowledge graph based framework that explicitly models the COVID-19 knowledge facts contributed by crowd workers with different levels of expertise and accurately identifies the related knowledge facts to explain the detection results. We evaluate HC-COVID using two public real-world datasets on social media. Evaluation results demonstrate that HC-COVID significantly outperforms state-of-the-art baselines in terms of the detection accuracy of misleading COVID-19 claims and the quality of the explanations.
DA  - 2022/01//
PY  - 2022
DO  - 10.1145/3492855
VL  - 6
IS  - GROUP
UR  - https://doi.org/10.1145/3492855
KW  - human-ai collaboration
KW  - covid19
KW  - explainable misinformation detection
ER  - 

TY  - JOUR
TI  - 'I'm categorizing LLM as a productivity tool': Examining ethics of LLM use in HCI research practices
AU  - Kapania, Shivani
AU  - Wang, Ruiyi
AU  - Li, Toby Jia-Jun
AU  - Li, Tianshi
AU  - Shen, Hong
T2  - Proc. ACM Hum.-Comput. Interact.
AB  - Large language models are increasingly applied in real-world scenarios, including research and education. These models, however, come with well-known ethical issues, which may manifest in unexpected ways in human-computer interaction research due to the extensive engagement with human subjects. This paper reports on research practices related to LLM use, drawing on 16 semi-structured interviews and a survey with 50 HCI researchers. We discuss the ways in which LLMs are already being utilized throughout the entire HCI research pipeline, from ideation to system development and paper writing. While researchers described nuanced understandings of ethical issues, they were rarely or only partially able to identify and address those ethical concerns in their own projects. This lack of action and reliance on workarounds was explained through the perceived lack of control and distributed responsibility in the LLM supply chain, the conditional nature of engaging with ethics, and competing priorities. Finally, we reflect on the implications of our findings and present opportunities to shape emerging norms of engaging with large language models in HCI research.
DA  - 2025/05//
PY  - 2025
DO  - 10.1145/3711000
VL  - 9
IS  - 2
UR  - https://doi.org/10.1145/3711000
KW  - large language models
KW  - hci research
KW  - research ethics
KW  - research practices
ER  - 

TY  - CONF
TI  - GEDI: Generating event data with&nbsp;intentional features for&nbsp;benchmarking process mining
AU  - Maldonado, Andrea
AU  - Frey, Christian M. M.
AU  - Tavares, Gabriel Marques
AU  - Rehwald, Nikolina
AU  - Seidl, Thomas
AB  - Process mining solutions include enhancing performance, conserving resources, and alleviating bottlenecks in organizational contexts. However, as in other data mining fields, success hinges on data quality and availability. Existing analyses for process mining solutions lack diverse and ample data for rigorous testing, hindering insights’ generalization. To address this, we propose Generating Event Data with Intentional features, a framework producing event data sets satisfying specific meta-features. Considering the meta-feature space that defines feasible event logs, we observe that existing real-world datasets describe only local areas within the overall space. Hence, our framework aims at providing the capability to generate an event data benchmark, which covers unexplored regions. Therefore, our approach leverages a discretization of the meta-feature space to steer generated data towards regions, where a combination of meta-features is not met yet by existing benchmark datasets. Providing a comprehensive data pool enriches process mining analyses, enables methods to capture a wider range of real-world scenarios, and improves evaluation quality. Moreover, it empowers analysts to uncover correlations between meta-features and evaluation metrics, enhancing explainability and solution effectiveness. Experiments demonstrate GEDI’s ability to produce a benchmark of intentional event data sets and robust analyses for process mining tasks.
C1  - Krakow, Poland and Berlin, Heidelberg
C3  - Business process management: 22nd international conference, BPM 2024, krakow, poland, september 1–6, 2024, proceedings
DA  - 2024///
PY  - 2024
DO  - 10.1007/978-3-031-70396-6_13
SP  - 221
EP  - 237
PB  - Springer-Verlag
SN  - 978-3-031-70395-9
UR  - https://doi.org/10.1007/978-3-031-70396-6_13
KW  - Benchmarking
KW  - Data Generation
KW  - Event Log Features
KW  - Hyperparameter Optimization
ER  - 

TY  - CONF
TI  - Explainability in process mining: a framework for improved decision-making
AU  - Nannini, Luca
T3  - Aies '23
AB  - This research project aims to develop and validate explanatory facilities to enhance information reception of process mining solutions, which could inform and be translated to other business intelligence platforms. Process mining, a nascent field for analyzing event data stored in information systems, faces challenges in adoption, engagement, and comprehensive explainability frameworks. The research problem lies in the difficulties organizations face when understanding the return on investment and integration requirements associated with process mining operationalization. Furthermore, users often struggle to comprehend the elaboration and representation of process outputs. This issue is compounded by the limited application of Explainable AI (XAI) in process mining, which so far has been predominantly focused on prediction and monitoring activities without a holistic view of explainability trade-offs.
C1  - Montréal, QC, Canada and New York, NY, USA
C3  - Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society
DA  - 2023///
PY  - 2023
DO  - 10.1145/3600211.3604729
SP  - 975
EP  - 976
PB  - Association for Computing Machinery
SN  - 979-8-4007-0231-0
UR  - https://doi.org/10.1145/3600211.3604729
KW  - AI Ethics
KW  - Human-Computer Interaction
KW  - Explainable AI
KW  - AI policy
KW  - Responsible Process Mining
ER  - 

TY  - CONF
TI  - "It depends": Configuring AI to improve clinical usefulness across contexts
AU  - Zając, Hubert Dariusz
AU  - Ribeiro, Jorge Miguel Neves
AU  - Ingala, Silvia
AU  - Gentile, Simona
AU  - Wanjohi, Ruth
AU  - Gitau, Samuel Nguku
AU  - Carlsen, Jonathan Frederik
AU  - Nielsen, Michael Bachmann
AU  - Andersen, Tariq Osman
T3  - Dis '24
AB  - Artificial Intelligence (AI) repeatedly match or outperform radiologists in lab experiments. However, real-world implementations of radiological AI-based systems are found to provide little to no clinical value. This paper explores how to design AI for clinical usefulness in different contexts. We conducted 19 design sessions and design interventions with 13 radiologists from 7 clinical sites in Denmark and Kenya, based on three iterations of a functional AI-based prototype. Ten sociotechnical dependencies were identified as crucial for the design of AI in radiology. We conceptualised four technical dimensions that must be configured to the intended clinical context of use: AI functionality, AI medical focus, AI decision threshold, and AI Explainability. We present four design recommendations on how to address dependencies pertaining to the medical knowledge, clinic type, user expertise level, patient context, and user situation that condition the configuration of these technical dimensions.
C1  - Copenhagen, Denmark and New York, NY, USA
C3  - Proceedings of the 2024 ACM designing interactive systems conference
DA  - 2024///
PY  - 2024
DO  - 10.1145/3643834.3660707
SP  - 874
EP  - 889
PB  - Association for Computing Machinery
SN  - 979-8-4007-0583-0
UR  - https://doi.org/10.1145/3643834.3660707
KW  - Machine Learning
KW  - AI Interaction
KW  - Human-Centred AI
KW  - Performance Optimisation
KW  - Transferability
KW  - Usability
KW  - User-Centred Design
ER  - 

TY  - CONF
TI  - Realizing AI in healthcare: Challenges appearing in the wild
AU  - Osman Andersen, Tariq
AU  - Nunes, Francisco
AU  - Wilcox, Lauren
AU  - Kaziunas, Elizabeth
AU  - Matthiesen, Stina
AU  - Magrabi, Farah
T3  - Chi ea '21
AB  - The last several years have shown a strong growth of Artificial Intelligence (AI) technologies with promising results for many areas of healthcare. HCI has contributed to these discussions, mainly with studies on explainability of advanced algorithms. However, there are only few AI-systems based on machine learning algorithms that make it to the real world and everyday care. This challenging move has been named the “last mile” of AI in healthcare, emphasizing the sociotechnical uncertainties and unforeseen learnings from involving users in the design or use of AI-based systems. The aim of this workshop is to set the stage for a new wave of HCI research that accounts for and begins to develop new insights, concepts, and methods, for transitioning from development to implementation and use of AI in healthcare. Participants are invited to collaboratively define an HCI research agenda focused on healthcare AI in the wild, which will require examining end-user engagements and questioning underlying concepts of AI in healthcare.
C1  - Yokohama, Japan and New York, NY, USA
C3  - Extended abstracts of the 2021 CHI conference on human factors in computing systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3411763.3441347
PB  - Association for Computing Machinery
SN  - 978-1-4503-8095-9
UR  - https://doi.org/10.1145/3411763.3441347
KW  - artificial intelligence
KW  - human computer interaction
ER  - 

TY  - JOUR
TI  - Enhancing trusted synchronization in open production logistics: A platform framework integrating blockchain and digital twin under social manufacturing
AU  - Zhang, Zhongfei
AU  - Qu, Ting
AU  - Zhao, Kuo
AU  - Zhang, Kai
AU  - Zhang, Yongheng
AU  - Guo, Wenyou
AU  - Liu, Lei
AU  - Chen, Zefeng
T2  - Advanced Engineering Informatics
DA  - 2024/08//
PY  - 2024
DO  - 10.1016/j.aei.2024.102404
VL  - 61
IS  - C
J2  - Adv. Eng. Inform.
SN  - 1474-0346
UR  - https://doi.org/10.1016/j.aei.2024.102404
KW  - Blockchain
KW  - Digital twin
KW  - Production logistics
KW  - Social manufacturing
KW  - Trusted synchronization
ER  - 

TY  - JOUR
TI  - When stakes are high: Balancing accuracy and transparency with Model-Agnostic Interpretable Data-driven suRRogates
AU  - Henckaerts, Roel
AU  - Antonio, Katrien
AU  - Côté, Marie-Pier
T2  - Expert Systems With Applications
DA  - 2022/09//
PY  - 2022
DO  - 10.1016/j.eswa.2022.117230
VL  - 202
IS  - C
J2  - Expert Syst. Appl.
SN  - 0957-4174
UR  - https://doi.org/10.1016/j.eswa.2022.117230
KW  - XAI
KW  - Feature selection
KW  - GLM
KW  - Global surrogate
KW  - Insurance
KW  - Segmentation
ER  - 

TY  - CONF
TI  - Towards explainable and trustworthy autonomous physical systems
AU  - Omeiza, Daniel
AU  - Anjomshoae, Sule
AU  - Kollnig, Konrad
AU  - Camburu, Oana-Maria
AU  - Främling, Kary
AU  - Kunze, Lars
T3  - Chi ea '21
AB  - The safe deployment of autonomous physical systems in real-world scenarios requires them to be explainable and trustworthy, especially in critical domains. In contrast with ‘black-box’ systems, explainable and trustworthy autonomous physical systems will lend themselves to easy assessments by system designers and regulators. This promises to pave ways for easy improvements that can lead to enhanced performance, and as well, increased public trust. In this one-day virtual workshop, we aim to gather a globally distributed group of researchers and practitioners to discuss the opportunities and social challenges in the design, implementation, and deployment of explainable and trustworthy autonomous physical systems, especially in a post-pandemic era. Interactions will be fostered through panel discussions and a series of spotlight talks. To ensure lasting impact of the workshop, we will conduct a pre-workshop survey which will examine the public perception of the trustworthiness of autonomous physical systems. Further, we will publish a summary report providing details about the survey as well as the identified challenges resulting from the workshop’s panel discussions.
C1  - Yokohama, Japan and New York, NY, USA
C3  - Extended abstracts of the 2021 CHI conference on human factors in computing systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3411763.3441338
PB  - Association for Computing Machinery
SN  - 978-1-4503-8095-9
UR  - https://doi.org/10.1145/3411763.3441338
KW  - trust
KW  - Explainability
KW  - collaboration
KW  - human-machine interaction
ER  - 

TY  - CONF
TI  - Interactive explanations by conflict resolution via argumentative exchanges
AU  - Rago, Antonio
AU  - Li, Hengzhi
AU  - Toni, Francesca
T3  - Kr '23
AB  - As the field of explainable AI (XAI) is maturing, calls for interactive explanations for (the outputs of) AI models are growing, but the state-of-the-art predominantly focuses on static explanations. In this paper, we focus instead on interactive explanations framed as conflict resolution between agents (i.e. AI models and/or humans) by leveraging on computational argumentation. Specifically, we define Argumentative eXchanges (AXs) for dynamically sharing, in multiagent systems, information harboured in individual agents' quantitative bipolar argumentation frameworks towards resolving conflicts amongst the agents. We then deploy AXs in the XAI setting in which a machine and a human interact about the machine's predictions. We identify and assess several theoretical properties characterising AXs that are suitable for XAI. Finally, we instantiate AXs for XAI by defining various agent behaviours, e.g. capturing counterfactual patterns of reasoning in machines and highlighting the effects of cognitive biases in humans. We show experimentally (in a simulated environment) the comparative advantages of these behaviours in terms of conflict resolution, and show that the strongest argument may not always be the most effective.
C1  - Rhodes, Greece
C3  - Proceedings of the 20th international conference on principles of knowledge representation and reasoning
DA  - 2023///
PY  - 2023
DO  - 10.24963/kr.2023/57
SN  - 978-1-956792-02-7
UR  - https://doi.org/10.24963/kr.2023/57
ER  - 

TY  - CONF
TI  - Why reliabilism is not enough: Epistemic and moral justification in machine learning
AU  - Smart, Andrew
AU  - James, Larry
AU  - Hutchinson, Ben
AU  - Wu, Simone
AU  - Vallor, Shannon
T3  - Aies '20
AB  - In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of em justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed wide adoption of machine learning? We argue that, in general, people implicitly adoptreliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method citegoldman2012reliabilism. We argue that, in cases where model deployments require em moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral "wrapper” around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification—moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.
C1  - New York, NY, USA and New York, NY, USA
C3  - Proceedings of the AAAI/ACM conference on AI, ethics, and society
DA  - 2020///
PY  - 2020
DO  - 10.1145/3375627.3375866
SP  - 372
EP  - 377
PB  - Association for Computing Machinery
SN  - 978-1-4503-7110-0
UR  - https://doi.org/10.1145/3375627.3375866
KW  - machine learning
KW  - neural networks
KW  - explainability
KW  - interpretability
KW  - epistemology
KW  - moral justification
ER  - 

TY  - JOUR
TI  - Explainable incremental learning for high-impedance fault detection in distribution networks
AU  - Bai, Hao
AU  - Gao, Jian-Hong
AU  - Liu, Tong
AU  - Guo, Zi-Yi
AU  - Guo, Mou-Fa
T2  - Computers & Electrical Engineering
DA  - 2025/04//
PY  - 2025
DO  - 10.1016/j.compeleceng.2024.110006
VL  - 122
IS  - C
J2  - Comput. Electr. Eng.
SN  - 0045-7906
UR  - https://doi.org/10.1016/j.compeleceng.2024.110006
KW  - Backpropagation neural network
KW  - Discrete wavelet transform
KW  - Distribution network
KW  - High impedance fault
KW  - Incremental learning
KW  - Model explainability
ER  - 

TY  - CONF
TI  - LightenDiffusion: Unsupervised low-light image enhancement with&nbsp;latent-retinex diffusion models
AU  - Jiang, Hai
AU  - Luo, Ao
AU  - Liu, Xiaohong
AU  - Han, Songchen
AU  - Liu, Shuaicheng
AB  - In this paper, we propose a diffusion-based unsupervised framework that incorporates physically explainable Retinex theory with diffusion models for low-light image enhancement, named LightenDiffusion. Specifically, we present a content-transfer decomposition network that performs Retinex decomposition within the latent space instead of image space as in previous approaches, enabling the encoded features of unpaired low-light and normal-light images to be decomposed into content-rich reflectance maps and content-free illumination maps. Subsequently, the reflectance map of the low-light image and the illumination map of the normal-light image are taken as input to the diffusion model for unsupervised restoration with the guidance of the low-light feature, where a self-constrained consistency loss is further proposed to eliminate the interference of normal-light content on the restored results to improve overall visual quality. Extensive experiments on publicly available real-world benchmarks show that the proposed LightenDiffusion outperforms state-of-the-art unsupervised competitors and is comparable to supervised methods while being more generalizable to various scenes. Our code is available at .
C1  - Milan, Italy and Berlin, Heidelberg
C3  - Computer vision – ECCV 2024: 18th european conference, milan, italy, september 29–october 4, 2024, proceedings, part XLVIII
DA  - 2024///
PY  - 2024
DO  - 10.1007/978-3-031-73195-2_10
SP  - 161
EP  - 179
PB  - Springer-Verlag
SN  - 978-3-031-73194-5
UR  - https://doi.org/10.1007/978-3-031-73195-2_10
KW  - Diffusion models
KW  - Image restoration
KW  - Low-light image enhancement
KW  - Retinex theory
ER  - 

TY  - CONF
TI  - Working alongside non-human agents
AU  - Duin, Ann Hill
AU  - Pedersen, Isabel
AB  - We coexist with non-human AI agents, and we now must plan for human and non-human-agent teaming, for cooperation and collaboration, as a means to expand collaborative intelligence in our ongoing quest for user advocacy. For practice and experimentation, we provide links to current non-human agents. We then distinguish automation and autonomy, and discuss humanness design, teaming. A deeper understanding of usability and ethical considerations for working alongside these systems, deploying robots and building bonds and trust with nonhuman agents, begins with differentiation of automation and autonomy, human-autonomy teaming, and a humanness design approach as a means to prevent undesirable autonomy. While TPC scholarship attends to privacy, accountability, safety and security, and transparency and explainability, we need additional vigilance regarding fairness and non-discrimination, human control of technology, TPC professional responsibility, and continued promotion of human values as we work alongside non-human agents.
C1  - Pittsburgh, PA, USA
C3  - 2021 IEEE international professional communication conference (ProComm)
DA  - 2021///
PY  - 2021
DO  - 10.1109/ProComm52174.2021.00005
SP  - 1
EP  - 5
PB  - IEEE Press
UR  - https://doi.org/10.1109/ProComm52174.2021.00005
ER  - 

TY  - JOUR
TI  - History and future of human-automation interaction
AU  - Janssen, Christian P.
AU  - Donker, Stella F.
AU  - Brumby, Duncan P.
AU  - Kun, Andrew L.
T2  - Int. J. Hum.-Comput. Stud.
DA  - 2019/11//
PY  - 2019
DO  - 10.1016/j.ijhcs.2019.05.006
VL  - 131
IS  - C
SP  - 99
EP  - 107
SN  - 1071-5819
UR  - https://doi.org/10.1016/j.ijhcs.2019.05.006
KW  - Ethics
KW  - Robotics
KW  - Automation
KW  - Automated vehicles
KW  - Autonomous agents
KW  - Divided attention
KW  - Embodied systems
KW  - Human-automation interaction
KW  - Safety-critical systems
KW  - Situated systems
ER  - 

TY  - CONF
TI  - How do different hmis of autonomous driving monitoring systems influence the perceived safety of robotaxis passengers? a field study
AU  - Gu, Yaoqin
AU  - Sheng, Youyu
AU  - Duan, Yujia
AU  - Zhang, Jingyu
AB  - Despite their significant potential to transform urban mobility, passengers still harbor safety concerns regarding robotaxis. Effective Human-Machine Interfaces (HMIs) may enhance users’ safety perceptions of these systems. Yet, strategies to improve such perceptions are not well-explored. To elucidate how system differences influence passenger perceptions, we conducted a field study involving a comparative analysis of two commercially operated robotaxi systems, System A and System B. Subsequently, we gathered data from passengers of both systems to assess their perceptions of safety. The results indicated that under normal operating conditions, passengers using both systems reported similar levels of safety. However, in the event of incidents, perceived safety significantly declined among passengers who used System B, whereas it remained stable for passengers using System A which provides more explanation on the nature of the incidents. These findings offer crucial insights for operators and provide a valuable resource for future transportation research.
C1  - Washington DC, USA and Berlin, Heidelberg
C3  - HCI international 2024 – late breaking papers: 26th international conference on human-computer interaction, HCII 2024, washington, DC, USA, june 29 – july 4, 2024, proceedings, part VIII
DA  - 2024///
PY  - 2024
DO  - 10.1007/978-3-031-76824-8_5
SP  - 51
EP  - 60
PB  - Springer-Verlag
SN  - 978-3-031-76823-1
UR  - https://doi.org/10.1007/978-3-031-76824-8_5
KW  - Autonomous driving
KW  - HMIs
KW  - Perceived safety
KW  - Robotaxis
ER  - 

TY  - JOUR
TI  - When immediate interactive feedback boosts optimization problem solving
AU  - Kefalidou, Genovefa
T2  - Computers in Human Behavior
AB  - In past, feedback in problem solving was found to improve human performance and focused mainly on learning applications. Interactive tools supporting decision-making and general problem-solving processes have long being developed to assist operations but not in optimization problem solving. Optimization problem solving is currently addressed within Operational Research (OR) through computational algorithms that aim to find the best solution in a problem (e.g. routing problem). Limited investigation there is on how computerized interactivity and metacognitive support (e.g. feedback and planning) can support optimization problem solving. This paper reports on human performance on Capacitated Vehicle Routing Problems (CVRPs) using paper-based problems and two different versions of an interactive computerized tool (one version with live explanatory and directive feedback alongside planning (strategy) support; one version without strategy support but with live explanatory feedback). Results suggest that human performance did not change when people were given paper-based post-problem feedback. On the contrary, participants' performance improved significantly when they used either version of the interactive tool that facilitated both live feedback support. No differences in performance across the two versions were observed. Implications on current theories and design implications for future optimization systems are discussed. Participants solved paper-based and computerized interactive CVRPs.Paper-based post-problem feedback did not affect human performance.Concurrent interactive feedback boosts human optimization performance.Concurrent explanatory feedback improves performance quality but reduces speed.Computerized metacognitive support improves human performance on CVRPs.
DA  - 2017/08//
PY  - 2017
DO  - 10.1016/j.chb.2017.03.019
VL  - 73
IS  - C
SP  - 110
EP  - 124
J2  - Comput. Hum. Behav.
SN  - 0747-5632
UR  - https://doi.org/10.1016/j.chb.2017.03.019
KW  - Human-in-the-loop
KW  - Concurrent feedback
KW  - Human performance
KW  - Interactive route optimization
KW  - Metacognition
ER  - 

TY  - JOUR
TI  - k-Best egalitarian stable marriages for task assignment
AU  - Wu, Siyuan
AU  - U, Leong Hou
AU  - Karras, Panagiotis
T2  - Proc. VLDB Endow.
AB  - In a two-sided market with each agent ranking individuals on the other side according to their preferences, such as location or incentive, the stable marriage problem calls to find a perfect matching among the two sides such that no pair of agents prefers each other to their assigned matches. Recent studies show that the number of solutions can be large in practice. Yet the classic solution by the Gale-Shapley (GS) algorithm is optimal for agents on the one side and pessimal for those on the other side. Some algorithms find a stable marriage that optimizes a measure of the cumulative satisfaction of all agents, such as egalitarian cost. However, in many real-world circumstances, a decision-maker needs to examine a set of solutions that are stable and attentive to both sides and choose among them based on expert knowledge. With such a disposition, it is necessary to identify a set of high-quality stable marriages and provide transparent explanations for any reassigned matches to the decision-maker. In this paper, we provide efficient algorithms that find the k-best stable marriages by egalitarian cost. Our exhaustive experimental study using real-world data and realistic preferences demonstrates the efficacy and efficiency of our solution.
DA  - 2023/07//
PY  - 2023
DO  - 10.14778/3611479.3611522
VL  - 16
IS  - 11
SP  - 3240
EP  - 3252
SN  - 2150-8097
UR  - https://doi.org/10.14778/3611479.3611522
ER  - 

TY  - JOUR
TI  - Information that matters: Exploring information needs of people affected by algorithmic decisions
AU  - Schmude, Timothée
AU  - Koesten, Laura
AU  - Möller, Torsten
AU  - Tschiatschek, Sebastian
T2  - Int. J. Hum.-Comput. Stud.
DA  - 2025/01//
PY  - 2025
DO  - 10.1016/j.ijhcs.2024.103380
VL  - 193
IS  - C
SN  - 1071-5819
UR  - https://doi.org/10.1016/j.ijhcs.2024.103380
KW  - Explainable AI
KW  - Affected stakeholders
KW  - Information needs
KW  - Qualitative methods
KW  - Question-driven explanations
KW  - Understanding
ER  - 

TY  - CONF
TI  - FIND: a function description benchmark for evaluating interpretability methods
AU  - Schwettmann, Sarah
AU  - Shaham, Tamar Rott
AU  - Materzynska, Joanna
AU  - Chowdhury, Neil
AU  - Li, Shuang
AU  - Andreas, Jacob
AU  - Bau, David
AU  - Torralba, Antonio
T3  - Nips '23
AB  - Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions are procedurally constructed across textual and numeric domains, and involve a range of real-world complexities, including noise, composition, approximation, and bias. We evaluate methods that use pretrained language models (LMs) to produce code-based and natural language descriptions of function behavior. Additionally, we introduce a new interactive method in which an Automated Interpretability Agent (AIA) generates function descriptions. We find that an AIA, built with an off-the-shelf LM augmented with black-box access to functions, can sometimes infer function structure—acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, FIND also reveals that LM-based descriptions capture global function behavior while missing local details. These results suggest that FIND will be useful for characterizing the performance of more sophisticated interpretability methods before they are applied to real-world models.
C1  - New Orleans, LA, USA and Red Hook, NY, USA
C3  - Proceedings of the 37th international conference on neural information processing systems
DA  - 2023///
PY  - 2023
PB  - Curran Associates Inc.
ER  - 

TY  - CONF
TI  - 3rd workshop on ethical artificial intelligence: Methods and applications (EAI)
AU  - Zhao, Chen
AU  - Chen, Feng
AU  - Wu, Xintao
AU  - Li, Jundong
AU  - Chen, Haifeng
T3  - Kdd '24
AB  - Ethical AI has become increasingly important, and it has been attracting attention from academia and industry, due to its increased popularity in real-world applications with fairness concerns. It also places fundamental importance on ethical considerations in determining legitimate and illegitimate uses of AI. Organizations that apply ethical AI have clearly stated well-defined review processes to ensure adherence to legal guidelines. Therefore, the wave of research at the intersection of ethical AI in data mining and machine learning has also influenced other fields of science, including computer vision, natural language processing, reinforcement learning, and social science. Despite these successes, ethical AI still faces many challenges, such as a lack of interpretable and explainable methods for fairness-aware deep learning models, etc. Consequently, there is an urgent need to bring experts and researchers together at prestigious venues to discuss ethical AI, which has been rarely seen in previous KDD conferences. This workshop will provide a premium platform for both research and industry from different backgrounds to exchange ideas on opportunities, challenges, and cutting-edge techniques in ethical AI.
C1  - Barcelona, Spain and New York, NY, USA
C3  - Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining
DA  - 2024///
PY  - 2024
DO  - 10.1145/3637528.3671482
SP  - 6751
EP  - 6752
PB  - Association for Computing Machinery
SN  - 979-8-4007-0490-1
UR  - https://doi.org/10.1145/3637528.3671482
KW  - machine learning
KW  - ethical artificial intelligence
KW  - fairness-aware
ER  - 

TY  - CONF
TI  - ExpressEdit: Video editing with natural language and sketching
AU  - Tilekbay, Bekzat
AU  - Yang, Saelyne
AU  - Lewkowicz, Michal Adam
AU  - Suryapranata, Alex
AU  - Kim, Juho
T3  - Iui '24
AB  - Informational videos serve as a crucial source for explaining conceptual and procedural knowledge to novices and experts alike. When producing informational videos, editors edit videos by overlaying text/images or trimming footage to enhance the video quality and make it more engaging. However, video editing can be difficult and time-consuming, especially for novice video editors who often struggle with expressing and implementing their editing ideas. To address this challenge, we first explored how multimodality—natural language (NL) and sketching, which are natural modalities humans use for expression—can be utilized to support video editors in expressing video editing ideas. We gathered 176 multimodal expressions of editing commands from 10 video editors, which revealed the patterns of use of NL and sketching in describing edit intents. Based on the findings, we present ExpressEdit, a system that enables editing videos via NL text and sketching on the video frame. Powered by LLM and vision models, the system interprets (1) temporal, (2) spatial, and (3) operational references in an NL command and spatial references from sketching. The system implements the interpreted edits, which then the user can iterate on. An observational study (N=10) showed that ExpressEdit enhanced the ability of novice video editors to express and implement their edit ideas. The system allowed participants to perform edits more efficiently and generate more ideas by generating edits based on user’s multimodal edit commands and supporting iterations on the editing commands. This work offers insights into the design of future multimodal interfaces and AI-based pipelines for video editing.
C1  - Greenville, SC, USA and New York, NY, USA
C3  - Proceedings of the 29th international conference on intelligent user interfaces
DA  - 2024///
PY  - 2024
DO  - 10.1145/3640543.3645164
SP  - 515
EP  - 536
PB  - Association for Computing Machinery
SN  - 979-8-4007-0508-3
UR  - https://doi.org/10.1145/3640543.3645164
KW  - human-AI interaction
KW  - multimodal input
KW  - video editing
ER  - 

TY  - CONF
TI  - Beyond-accuracy goals, again
AU  - de Rijke, Maarten
T3  - Wsdm '23
AB  - Improving the performance of information retrieval systems tends to be narrowly scoped. Often, better prediction performance is considered the only metric of improvement. As a result, work on improving information retrieval methods usually focuses on im- proving the methods' accuracy. Such a focus is myopic. Instead, as researchers and practitioners we should adopt a richer perspective measuring the performance of information retrieval systems. I am not the first to make this point (see, e.g., [4]), but I want to highlight dimensions that broaden the scope considered so far and offer a number of examples to illustrate what this would mean for our research agendas.First, trustworthiness is a prerequisite for people, organizations, and societies to use AI-based, and, especially, machine learning- based systems in general, and information retrieval systems in particular. Trust can be gained in an intrinsic manner by revealing the inner workings of an AI-based system, i.e., through explainability. Or it can be gained extrinsically by showing, in a principled or empirical manner, that a system upholds verifiable guarantees. Such guarantees should obtained for the following dimensions (at a minimum): (i) accuracy, including well-defined and explained contexts of usage; (ii) reliability, including exhibiting parity with respect to sensitive attributes; (iii) repeatable and reproducible results, including audit trails; (iv) resilience to adversarial examples, distributional shifts; and (v) safety, including privacy-preserving search and recommendation.Second, in information retrieval, our experiments are mostly conducted in controlled laboratory environments. Extrapolating this information to evaluate the real-world effects often remains a challenge. This is particularly true when measuring the impact of information retrieval systems across broader scales, both temporally and spatially. Conducting controlled experimental trials for evaluating real-world impacts of information retrieval systems can result in depicting a snapshot situation, where systems are tailored towards that specific environment. As society is constantly changing, the requirements set for information retrieval systems are changing as well, resulting in short-term and long-term feedback loops with interactions between society and information retrieval systems.
C1  - Singapore, Singapore and New York, NY, USA
C3  - Proceedings of the sixteenth ACM international conference on web search and data mining
DA  - 2023///
PY  - 2023
DO  - 10.1145/3539597.3572332
SP  - 2
EP  - 3
PB  - Association for Computing Machinery
SN  - 978-1-4503-9407-9
UR  - https://doi.org/10.1145/3539597.3572332
KW  - evaluation
KW  - measurement
KW  - performance
ER  - 

TY  - CONF
TI  - REX: Designing user-centered repair and explanations to address robot failures
AU  - Lee, Christine P
AU  - Praveena, Pragathi
AU  - Mutlu, Bilge
T3  - Dis '24
AB  - Robots in real-world environments continuously engage with multiple users and encounter changes that lead to unexpected conflicts in fulfilling user requests. Recent technical advancements (e.g., large-language models (LLMs), program synthesis) offer various methods for automatically generating repair plans that address such conflicts. In this work, we understand how automated repair and explanations can be designed to improve user experience with robot failures through two user studies. In our first, online study (n = 162), users expressed increased trust, satisfaction, and utility with the robot performing automated repair and explanations. However, we also identified risk factors—safety, privacy, and complexity—that require adaptive repair strategies. The second, in-person study (n = 24) elucidated distinct repair and explanation strategies depending on the level of risk severity and type. Using a design-based approach, we explore automated repair with explanations as a solution for robots to handle conflicts and failures, complemented by adaptive strategies for risk factors. Finally, we discuss the implications of incorporating such strategies into robot designs to achieve seamless operation among changing user needs and environments.
C1  - Copenhagen, Denmark and New York, NY, USA
C3  - Proceedings of the 2024 ACM designing interactive systems conference
DA  - 2024///
PY  - 2024
DO  - 10.1145/3643834.3661559
SP  - 2911
EP  - 2925
PB  - Association for Computing Machinery
SN  - 979-8-4007-0583-0
UR  - https://doi.org/10.1145/3643834.3661559
KW  - user-centered design
KW  - human-robot interaction
KW  - robot
KW  - failures
KW  - program repair
KW  - vignette study
ER  - 

TY  - CONF
TI  - The effect of explanations on trust in an assistance system for public transport users and the role of the propensity to trust
AU  - Faulhaber, Anja K.
AU  - Ni, Ina
AU  - Schmidt, Ludger
T3  - MuC '21
AB  - The present study aimed to investigate whether explanations increase trust in an assistance system. Moreover, we wanted to take the role of the individual propensity to trust in technology into account. We conducted an empirical study in a virtual reality environment where 40 participants interacted with a specific assistance system for public transport users. The study was in a 2x2 mixed design with the within-subject factor assistance system feature (trip planner and connection request) and the between-subject factor explanation (with or without). We measured trust as explicit trust via a questionnaire and as implicit trust via an operationalization of the participants’ behavior. The results showed that trust propensity predicted explicit trust, and explanations increased explicit trust significantly. This was not the case for implicit trust, though, suggesting that explicit and implicit trust do not necessarily coincide. In conclusion, our results complement the literature on explainable artificial intelligence and trust in automation and provide topics for future research regarding the effect of explanations on trust in assistance systems or other technologies.
C1  - Ingolstadt, Germany and New York, NY, USA
C3  - Proceedings of mensch und computer 2021
DA  - 2021///
PY  - 2021
DO  - 10.1145/3473856.3473886
SP  - 303
EP  - 310
PB  - Association for Computing Machinery
SN  - 978-1-4503-8645-6
UR  - https://doi.org/10.1145/3473856.3473886
ER  - 

TY  - JOUR
TI  - Creating meaningful work in the age of AI: explainable AI, explainability, and why it matters to organizational designers
AU  - Wulff, Kristin
AU  - Finnestrand, Hanne
T2  - AI Soc.
AB  - In this paper, we contribute to research on enterprise artificial intelligence (AI), specifically to organizations improving the customer experiences and their internal processes through using the type of AI called machine learning (ML). Many organizations are struggling to get enough value from their AI efforts, and part of this is related to the area of explainability. The need for explainability is especially high in what is called black-box ML models, where decisions are made without anyone understanding how an AI reached a particular decision. This opaqueness creates a user need for explanations. Therefore, researchers and designers create different versions of so-called eXplainable AI (XAI). However, the demands for XAI can reduce the accuracy of the predictions the AI makes, which can reduce the perceived usefulness of the AI solution, which, in turn, reduces the interest in designing the organizational task structure to benefit from the AI solution. Therefore, it is important to ensure that the need for XAI is as low as possible. In this paper, we demonstrate how to achieve this by optimizing the task structure according to sociotechnical systems design principles. Our theoretical contribution is to the underexplored field of the intersection of AI design and organizational design. We find that explainability goals can be divided into two groups, pattern goals and experience goals, and that this division is helpful when defining the design process and the task structure that the AI solution will be used in. Our practical contribution is for AI designers who include organizational designers in their teams, and for organizational designers who answer that challenge.
DA  - 2023/01//
PY  - 2023
DO  - 10.1007/s00146-023-01633-0
VL  - 39
IS  - 4
SP  - 1843
EP  - 1856
SN  - 0951-5666
UR  - https://doi.org/10.1007/s00146-023-01633-0
KW  - Machine learning
KW  - Artificial intelligence
KW  - Explainability
KW  - Organizational design
KW  - Sociotechnical systems (STS)
ER  - 

TY  - JOUR
TI  - Distributed and explainable GHSOM for anomaly detection in sensor networks
AU  - Mignone, Paolo
AU  - Corizzo, Roberto
AU  - Ceci, Michelangelo
T2  - Machine Learning
AB  - The identification of anomalous activities is a challenging and crucially important task in sensor networks. This task is becoming increasingly complex with the increasing volume of data generated in real-world domains, and greatly benefits from the use of predictive models to identify anomalies in real time. A key use case for this task is the identification of misbehavior that may be caused by involuntary faults or deliberate actions. However, currently adopted anomaly detection methods are often affected by limitations such as the inability to analyze large-scale data, a reduced effectiveness when data presents multiple densities, a strong dependence on user-defined threshold configurations, and a lack of explainability in the extracted predictions. In this paper, we propose a distributed deep learning method that extends growing hierarchical self-organizing maps, originally designed for clustering tasks, to address anomaly detection tasks. The SOM-based modeling capabilities of the method enable the analysis of data with multiple densities, by exploiting multiple SOMs organized as a hierarchy. Our map-reduce implementation under Apache Spark allows the method to process and analyze large-scale sensor network data. An automatic threshold-tuning strategy reduces user efforts and increases the robustness of the method with respect to noisy instances. Moreover, an explainability component resorting to instance-based feature ranking emphasizes the most salient features influencing the decisions of the anomaly detection model, supporting users in their understanding of raised alerts. Experiments are conducted on five real-world sensor network datasets, including wind and photovoltaic energy production, vehicular traffic, and pedestrian flows. Our results show that the proposed method outperforms state-of-the-art anomaly detection competitors. Furthermore, a scalability analysis reveals that the method is able to scale linearly as the data volume presented increases, leveraging multiple worker nodes in a distributed computing setting. Qualitative analyses on the level of anomalous pollen in the air further emphasize the effectiveness of our proposed method, and its potential in determining the level of danger in raised alerts.
DA  - 2024/01//
PY  - 2024
DO  - 10.1007/s10994-023-06501-y
VL  - 113
IS  - 7
SP  - 4445
EP  - 4486
J2  - Mach. Learn.
SN  - 0885-6125
UR  - https://doi.org/10.1007/s10994-023-06501-y
KW  - Explainable AI
KW  - Anomaly detection
KW  - Distributed learning
KW  - Self-organizing maps
KW  - Sensor networks
ER  - 

TY  - CONF
TI  - Unsupervised segmentation in&nbsp;real-world images via&nbsp;spelke object inference
AU  - Chen, Honglin
AU  - Venkatesh, Rahul
AU  - Friedman, Yoni
AU  - Wu, Jiajun
AU  - Tenenbaum, Joshua B.
AU  - Yamins, Daniel L. K.
AU  - Bear, Daniel M.
AB  - Self-supervised, category-agnostic segmentation of real-world images is a challenging open problem in computer vision. Here, we show how to learn static grouping priors from motion self-supervision by building on the cognitive science concept of a Spelke Object: a set of physical stuff that moves together. We introduce the Excitatory-Inhibitory Segment Extraction Network (EISEN), which learns to extract pairwise affinity graphs for static scenes from motion-based training signals. EISEN then produces segments from affinities using a novel graph propagation and competition network. During training, objects that undergo correlated motion (such as robot arms and the objects they move) are decoupled by a bootstrapping process: EISEN explains away the motion of objects it has already learned to segment. We show that EISEN achieves a substantial improvement in the state of the art for self-supervised image segmentation on challenging synthetic and real-world robotics datasets.
C1  - Tel Aviv, Israel and Berlin, Heidelberg
C3  - Computer vision – ECCV 2022: 17th european conference, tel aviv, israel, october 23–27, 2022, proceedings, part XXIX
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-19818-2_41
SP  - 719
EP  - 735
PB  - Springer-Verlag
SN  - 978-3-031-19817-5
UR  - https://doi.org/10.1007/978-3-031-19818-2_41
ER  - 

TY  - CONF
TI  - LimeAttack: local explainable method for textual hard-label adversarial attack
AU  - Zhu, Hai
AU  - Zhao, Qingyang
AU  - Shang, Weiwei
AU  - Wu, Yuren
AU  - Liu, Kai
T3  - AAAI'24/IAAI'24/EAAI'24
AB  - Natural language processing models are vulnerable to adversarial examples. Previous textual adversarial attacks adopt model internal information (gradients or confidence scores) to generate adversarial examples. However, this information is unavailable in the real world. Therefore, we focus on a more realistic and challenging setting, named hard-label attack, in which the attacker can only query the model and obtain a discrete prediction label. Existing hard-label attack algorithms tend to initialize adversarial examples by random substitution and then utilize complex heuristic algorithms to optimize the adversarial perturbation. These methods require a lot of model queries and the attack success rate is restricted by adversary initialization. In this paper, we propose a novel hard-label attack algorithm named LimeAttack, which leverages a local explainable method to approximate word importance ranking, and then adopts beam search to find the optimal solution. Extensive experiments show that LimeAttack achieves the better attacking performance compared with existing hard-label attack under the same query budget. In addition, we evaluate the effectiveness of LimeAttack on large language models and some defense methods, and results indicate that adversarial examples remain a significant threat to large language models. The adversarial examples crafted by LimeAttack are highly transferable and effectively improve model robustness in adversarial training.
C3  - Proceedings of the thirty-eighth AAAI conference on artificial intelligence and thirty-sixth conference on innovative applications of artificial intelligence and fourteenth symposium on educational advances in artificial intelligence
DA  - 2024///
PY  - 2024
DO  - 10.1609/aaai.v38i17.29950
PB  - AAAI Press
SN  - 978-1-57735-887-9
UR  - https://doi.org/10.1609/aaai.v38i17.29950
ER  - 

TY  - CONF
TI  - Building an embodied musicking dataset for co-creative music-making
AU  - Vear, Craig
AU  - Poltronieri, Fabrizio
AU  - DiDonato, Balandino
AU  - Zhang, Yawen
AU  - Benerradi, Johann
AU  - Hutchinson, Simon
AU  - Turowski, Paul
AU  - Shell, Jethro
AU  - Malekmohamadi, Hossein
AB  - In this paper, we present our findings of the design, development and deployment of a proof-of-concept dataset that captures some of the physiological, musicological, and psychological aspects of embodied musicking. After outlining the conceptual elements of this research, we explain the design of the dataset and the process of capturing the data. We then introduce two tests we used to evaluate the dataset: a) using data science techniques and b) a practice-based application in an AI-robot digital score. The results from these tests are conflicting: from a data science perspective the dataset could be considered questionable, but when applied to a real-world musicking situation performers reported it was transformative and felt to be ‘co-creative. We discuss this duality and pose some important questions for future study. However, we feel that the datatset contains a set of relationships that are useful to explore in the creation of music.
C1  - Aberystwyth, United Kingdom and Berlin, Heidelberg
C3  - Artificial intelligence in music, sound, art and design: 13th international conference, EvoMUSART 2024, held as part of EvoStar 2024, aberystwyth, UK, april 3–5, 2024, proceedings
DA  - 2024///
PY  - 2024
DO  - 10.1007/978-3-031-56992-0_24
SP  - 373
EP  - 388
PB  - Springer-Verlag
SN  - 978-3-031-56991-3
UR  - https://doi.org/10.1007/978-3-031-56992-0_24
KW  - dataset
KW  - embodied AI
KW  - music performance
ER  - 

TY  - CONF
TI  - Working with AI sound: Exploring the future of workplace AI sound technologies
AU  - Deacon, Thomas
AU  - Plumbley, Mark D.
T3  - Chiwork '24
AB  - The workplace is a site for the rapid development and deployment of Artificial Intelligence (AI) systems. However, our research suggests that their adoption could already be hindered by critical issues such as trust, privacy, and security. This paper examines the integration of AI-enabled sound technologies in the workplace, with a focus on enhancing well-being and productivity through a soundscape approach while addressing ethical concerns. To explore these concepts, we used scenario-based design and structured feedback sessions with knowledge workers from open-plan offices and those working from home. To do this, we present initial design concepts for AI sound analysis and control systems. Based on the perspectives gathered, we present user requirements and concerns, particularly regarding privacy and the potential for workplace surveillance, emphasising the need for user consent and levels of transparency in AI deployments. Navigating these ethical considerations is a key implication of the study. We advocate for novel ways to incorporate people’s involvement in the design process through co-design and serious games to shape the future of AI audio technologies in the workplace.
C1  - Newcastle upon Tyne, United Kingdom and New York, NY, USA
C3  - Proceedings of the 3rd annual meeting of the symposium on human-computer interaction for work
DA  - 2024///
PY  - 2024
DO  - 10.1145/3663384.3663391
PB  - Association for Computing Machinery
SN  - 979-8-4007-1017-9
UR  - https://doi.org/10.1145/3663384.3663391
KW  - privacy
KW  - AI sound systems
KW  - focus groups
KW  - knowledge workers
KW  - open-plan offices
KW  - personalised sound systems
KW  - sound monitoring
KW  - soundscape
KW  - thematic analysis
KW  - work-from-home
KW  - workplace acoustics
ER  - 

TY  - CONF
TI  - Jupybara: Operationalizing a design space for actionable data analysis and storytelling with llms
AU  - Wang, Huichen Will
AU  - Birnbaum, Larry
AU  - Setlur, Vidya
T3  - Chi '25
AB  - Mining and conveying actionable insights from complex data is a key challenge of exploratory data analysis (EDA) and storytelling. To address this challenge, we present a design space for actionable EDA and storytelling. Synthesizing theory and expert interviews, we highlight how semantic precision, rhetorical persuasion, and pragmatic relevance underpin effective EDA and storytelling. We also show how this design space subsumes common challenges in actionable EDA and storytelling, such as identifying appropriate analytical strategies and leveraging relevant domain knowledge. Building on the potential of LLMs to generate coherent narratives with commonsense reasoning, we contribute Jupybara, an AI-enabled assistant for actionable EDA and storytelling implemented as a Jupyter Notebook extension. Jupybara employs two strategies—design-space-aware prompting and multi-agent architectures—to operationalize our design space. An expert evaluation confirms Jupybara’s usability, steerability, explainability, and reparability, as well as the effectiveness of our strategies in operationalizing the design space framework with LLMs.
C1  - New York, NY, USA
C3  - Proceedings of the 2025 CHI conference on human factors in computing systems
DA  - 2025///
PY  - 2025
DO  - 10.1145/3706598.3713913
PB  - Association for Computing Machinery
SN  - 979-8-4007-1394-1
UR  - https://doi.org/10.1145/3706598.3713913
KW  - Data Science
KW  - Human-AI Collaboration
KW  - Large Language Model
KW  - Actionable Insights
KW  - Data Storytelling
KW  - Exploratory Data Analysis
KW  - Multi-Agent System
KW  - Pragmatics.
KW  - Rhetoric
KW  - Semantics
ER  - 

TY  - CONF
TI  - Reptile: Aggregation-level explanations for hierarchical data
AU  - Huang, Zezhou
AU  - Wu, Eugene
T3  - Sigmod '22
AB  - Users often can see from overview-level statistics that some results look "off", but are rarely able to characterize even the type of error. Reptile is an iterative human-in-the-loop explanation and cleaning system for errors in hierarchical data. Users specify an anomalous distributive aggregation result (a complaint), and Reptile recommends drill-down operations to help the user "zoom-in" on the underlying errors. Unlike prior explanation systems that intervene on raw records, Reptile intervenes by learning a group's expected statistics, and ranks drill-down sub-groups by how much the intervention fixes the complaint. This group-level formulation supports a wide range of error types (missing, duplicates, value errors) and uniquely leverages the distributive properties of the user complaint. Further, the learning-based intervention lets users provide domain expertise that Reptile learns from.In each drill-down iteration, Reptile must train a large number of predictive models. We thus extend factorized learning from count-join queries to aggregation-join queries, and develop a suite of optimizations that leverage the data's hierarchical structure. These optimizations reduce runtimes by &gt;6× compared to a Lapack-based implementation. When applied to real-world Covid-19 and African farmer survey data, Reptile correctly identifies 21/30 (vs 2 using existing explanation approaches) and 20/22 errors. Reptile has been deployed in Ethiopia and Zambia, and used to clean nation-wide farmer survey data; the clean data has been used to design national drought insurance policies.
C1  - Philadelphia, PA, USA and New York, NY, USA
C3  - Proceedings of the 2022 international conference on management of data
DA  - 2022///
PY  - 2022
DO  - 10.1145/3514221.3517854
SP  - 399
EP  - 413
PB  - Association for Computing Machinery
SN  - 978-1-4503-9249-5
UR  - https://doi.org/10.1145/3514221.3517854
KW  - explanations
KW  - data cleaning
KW  - data factorisation
KW  - data mining
KW  - data provenance
KW  - exploratory data analysis
KW  - functional dependencies
KW  - provenance
ER  - 

TY  - JOUR
TI  - Deep learning, explained: Fundamentals, explainability, and bridgeability to process-based modelling
AU  - Razavi, Saman
T2  - Environmental Modelling and Software
DA  - 2021/10//
PY  - 2021
DO  - 10.1016/j.envsoft.2021.105159
VL  - 144
IS  - C
J2  - Environ. Model. Softw.
SN  - 1364-8152
UR  - https://doi.org/10.1016/j.envsoft.2021.105159
KW  - Deep learning
KW  - Machine learning
KW  - Artificial intelligence
KW  - Artificial neural networks
KW  - Earth systems
KW  - Hydrology
KW  - Process-based modelling
ER  - 

TY  - CONF
TI  - Cultural considerations in AI systems for the global south: a systematic review
AU  - Anuyah, Oghenemaro
AU  - Wan, Ruyuan
AU  - Adejoro, Cornelius
AU  - Yeh, Tom
AU  - Metoyer, Ronald
AU  - Badillo-Urquiola, Karla
T3  - AfriCHI '23
AB  - The field of Artificial Intelligence (AI) is leading transformative impacts across different sectors. However, these advancements are often developed with a Western-centric focus, neglecting the cultural diversity in regions such as the Global South. In this paper, we synthesize twelve research papers focusing on cultural considerations in designing AI systems for the Global South. Our findings revealed a significant focus on domains like healthcare and intelligent assistants and challenges, including usability, AI transparency, and data availability, that can hinder the design and deployment of culturally sensitive AI systems. Moreover, the results demonstrated the need to integrate cultural values to increase the acceptance and usefulness of AI systems for regions in the Global South. This paper guides future research towards developing culturally and contextually relevant AI systems for Africa, highlighting the need for increased visibility and representation of African researchers in HCI and AI domains.
C1  - East London, South Africa and New York, NY, USA
C3  - Proceedings of the 4th african human computer interaction conference
DA  - 2024///
PY  - 2024
DO  - 10.1145/3628096.3629046
SP  - 125
EP  - 134
PB  - Association for Computing Machinery
SN  - 979-8-4007-0887-9
UR  - https://doi.org/10.1145/3628096.3629046
KW  - Artificial Intelligence
KW  - Culture
KW  - Design Research
KW  - Global South
KW  - Systematic Review
ER  - 

TY  - CONF
TI  - Sparsity-guided holistic explanation for LLMs with interpretable inference-time intervention
AU  - Tan, Zhen
AU  - Chen, Tianlong
AU  - Zhang, Zhenyu
AU  - Liu, Huan
T3  - AAAI'24/IAAI'24/EAAI'24
AB  - Large Language Models (LLMs) have achieved unprecedented breakthroughs in various natural language processing domains. However, the enigmatic "black-box" nature of LLMs remains a significant challenge for interpretability, hampering transparent and accountable applications. While past approaches, such as attention visualization, pivotal subnetwork extraction, and concept-based analyses, offer some insight, they often focus on either local or global explanations within a single dimension, occasionally falling short in providing comprehensive clarity. In response, we propose a novel methodology anchored in sparsity-guided techniques, aiming to provide a holistic interpretation of LLMs. Our framework, termed SparseCBM, innovatively integrates sparsity to elucidate three intertwined layers of interpretation: input, subnetwork, and concept levels. In addition, the newly introduced dimension of interpretable inference-time intervention facilitates dynamic adjustments to the model during deployment. Through rigorous empirical evaluations on real-world datasets, we demonstrate that SparseCBM delivers a profound understanding of LLM behaviors, setting it apart in both interpreting and ameliorating model inaccuracies. Codes are provided in supplements.
C3  - Proceedings of the thirty-eighth AAAI conference on artificial intelligence and thirty-sixth conference on innovative applications of artificial intelligence and fourteenth symposium on educational advances in artificial intelligence
DA  - 2024///
PY  - 2024
DO  - 10.1609/aaai.v38i19.30160
PB  - AAAI Press
SN  - 978-1-57735-887-9
UR  - https://doi.org/10.1609/aaai.v38i19.30160
ER  - 

TY  - CONF
TI  - Designing proactive safety systems for industrial workers using intelligent mechanisms
AU  - Schobesberger, Martin
AU  - Huber, Jaroslava
AU  - Grünberger, Stefan
AU  - Haslgrübler, Michael
AU  - Ferscha, Alois
T3  - Petra '22
AB  - In the process of increasing industrial productivity, the aspect of worker safety plays an important but often neglected part. The following workshop paper discusses potential implementations of a priorly designed concept of a HumanAI based assistive and proactive safety system in a specific industrial use case. The goal is to address safety threats which occur during a polymer recycling process (shredding, melting, and producing plastic granulate) during human-machine interaction by encouraging a meaningful transition from the more traditional view of safety (Safety-I), to a more modern and flexible approach to safety (Safety-II), potentially applying intelligent systems. The principles of Safety-I and Safety-II as well as the STOP principle are explained alongside general considerations for safety and assistance systems. This introduction is followed by a detailed description and safety analysis of the chosen representative real-world use case before a transition in the STOP principle from Safety-I to Safety-II is proposed and illustrated in an in depth example. The contribution of this workshop paper is an introduction of Safety-II mechanisms to replace or enhance established Safety-I mechanisms in the STOP principle to increase worker safety.
C1  - Corfu, Greece and New York, NY, USA
C3  - Proceedings of the 15th international conference on pervasive technologies related to assistive environments
DA  - 2022///
PY  - 2022
DO  - 10.1145/3529190.3534775
SP  - 480
EP  - 485
PB  - Association for Computing Machinery
SN  - 978-1-4503-9631-8
UR  - https://doi.org/10.1145/3529190.3534775
KW  - accident prevention
KW  - activity recognition
KW  - assistance systems
KW  - emergency break assistant
KW  - humanAI
KW  - polymer recycling
KW  - Safety-II
KW  - worker safety
ER  - 

TY  - JOUR
TI  - A probabilistic generative model for tracking multi-knowledge concept mastery probability
AU  - Liu, Hengyu
AU  - Zhang, Tiancheng
AU  - Li, Fan
AU  - Yu, Minghe
AU  - Yu, Ge
AB  - Knowledge tracing aims to track students’ knowledge status over time to predict students’ future performance accurately. In a real environment, teachers expect knowledge tracing models to provide the interpretable result of knowledge status. Markov chain-based knowledge tracing (MCKT) models, such as Bayesian Knowledge Tracing, can track knowledge concept mastery probability over time. However, as the number of tracked knowledge concepts increases, the time complexity of MCKT predicting student performance increases exponentially (also called explaining away problem). When the number of tracked knowledge concepts is large, we cannot utilize MCKT to track knowledge concept mastery probability over time. In addition, the existing MCKT models only consider the relationship between students’ knowledge status and problems when modeling students’ responses but ignore the relationship between knowledge concepts in the same problem. To address these challenges, we propose an inTerpretable pRobAbilistiC gEnerative moDel (TRACED), which can track students’ numerous knowledge concepts mastery probabilities over time. To solve explain away problem, we design long and short-term memory (LSTM)-based networks to approximate the posterior distribution, predict students’ future performance, and propose a heuristic algorithm to train LSTMs and probabilistic graphical model jointly. To better model students’ exercise responses, we proposed a logarithmic linear model with three interactive strategies, which models students’ exercise responses by considering the relationship among students’ knowledge status, knowledge concept, and problems. We conduct experiments with four real-world datasets in three knowledge-driven tasks. The experimental results show that TRACED outperforms existing knowledge tracing methods in predicting students’ future performance and can learn the relationship among students, knowledge concepts, and problems from students’ exercise sequences. We also conduct several case studies. The case studies show that TRACED exhibits excellent interpretability and thus has the potential for personalized automatic feedback in the real-world educational environment.
DA  - 2024/01//
PY  - 2024
DO  - 10.1007/s11704-023-3008-x
VL  - 18
IS  - 3
J2  - Front. Comput. Sci.
SN  - 2095-2228
UR  - https://doi.org/10.1007/s11704-023-3008-x
KW  - deep learning
KW  - knowledge tracing
KW  - learner modeling
KW  - probabilistic graphical model
ER  - 

TY  - JOUR
TI  - Achieving adaptive tasks from human instructions for robots using large language models and behavior trees
AU  - Zhou, Haotian
AU  - Lin, Yunhan
AU  - Yan, Longwu
AU  - Min, Huasong
T2  - Robotics and Autonomous Systems
DA  - 2025/04//
PY  - 2025
DO  - 10.1016/j.robot.2025.104937
VL  - 187
IS  - C
J2  - Robot. Auton. Syst.
SN  - 0921-8890
UR  - https://doi.org/10.1016/j.robot.2025.104937
KW  - Adaptive tasks
KW  - Behavior tree generation
KW  - Behavior trees
KW  - Large language models
KW  - Reactive policy
ER  - 

TY  - JOUR
TI  - Self-supervised bot play for transcript-free conversational critiquing with rationales
AU  - Li, Shuyang
AU  - Prasad Majumder, Bodhisattwa
AU  - McAuley, Julian
T2  - ACM Trans. Recomm. Syst.
AB  - Conversational critiquing in recommender systems offers a way for users to engage in multi-turn conversations to find items they enjoy. For users to trust an agent and give effective feedback, the recommender system must be able to explain its suggestions and rationales. We develop a two-part framework for training multi-turn conversational critiquing in recommender systems that provide recommendation rationales that users can effectively interact with to receive better recommendations. First, we train a recommender system to jointly suggest items and explain its reasoning via subjective rationales. We then fine-tune this model to incorporate iterative user feedback via self-supervised bot-play. Experiments on three real-world datasets demonstrate that our system can be applied to different recommendation models across diverse domains to achieve state-of-the-art performance in multi-turn recommendation. Human studies show that systems trained with our framework provide more useful, helpful, and knowledgeable suggestions in warm- and cold-start settings. Our framework allows us to use only product reviews during training, avoiding the need for expensive dialog transcript datasets that limit the applicability of previous conversational recommender agents.
DA  - 2024/08//
PY  - 2024
DO  - 10.1145/3665502
VL  - 3
IS  - 1
UR  - https://doi.org/10.1145/3665502
KW  - Conversational recommendation
KW  - critiquing
ER  - 

TY  - CONF
TI  - A multimodal automated interpretability agent
AU  - Shaham, Tamar Rott
AU  - Schwettmann, Sarah
AU  - Wang, Franklin
AU  - Rajaram, Achyuta
AU  - Hernandez, Evan
AU  - Andreas, Jacob
AU  - Torralba, Antonio
T3  - ICML'24
AB  - This paper describes MAIA, a Multimodal Automated Interpretability Agent. MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery. It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior. These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results. Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior. We evaluate applications of MAIA to computer vision models. We first characterize MAIA's ability to describe (neuron-level) features in learned representations of images. Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters. We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified.
C1  - Vienna, Austria
C3  - Proceedings of the 41st international conference on machine learning
DA  - 2024///
PY  - 2024
PB  - JMLR.org
ER  - 

TY  - CONF
TI  - MAPS-x: Explainable multi-robot motion planning via segmentation
AU  - Kottinger, Justin
AU  - Almagor, Shaull
AU  - Lahijanian, Morteza
AB  - Traditional multi-robot motion planning (MMP) focuses on computing trajectories for multiple robots acting in an environment, such that the robots do not collide when the trajectories are taken simultaneously. In safety-critical applications, a human supervisor may want to verify that the plan is indeed collision-free. In this work, we propose a notion of explanation for a plan of MMP, based on visualization of the plan as a short sequence of images representing time segments, where in each time segment the trajectories of the agents are disjoint, clearly illustrating the safety of the plan. We show that standard notions of optimality (e.g., makespan) may create conflict with short explanations. Thus, we propose meta-algorithms, namely multi-agent plan segmenting-X (MAPS-X) and its lazy variant, that can be plugged on existing centralized sampling-based tree planners X to produce plans with good explanations using a desirable number of images. We demonstrate the efficacy of this explanation-planning scheme and extensively evaluate the performance of MAPS-X and its lazy variant in various environments and agent dynamics.
C1  - Xi'an, China
C3  - 2021 IEEE international conference on robotics and automation (ICRA)
DA  - 2021///
PY  - 2021
DO  - 10.1109/ICRA48506.2021.9561893
SP  - 7994
EP  - 8000
PB  - IEEE Press
UR  - https://doi.org/10.1109/ICRA48506.2021.9561893
ER  - 

TY  - JOUR
TI  - A personality-guided preference aggregator for ephemeral group recommendation
AU  - Ye, Guangze
AU  - Wu, Wen
AU  - Shi, Liye
AU  - Hu, Wenxin
AU  - Chen, Xi
AU  - He, Liang
T2  - Applied Soft Computing
DA  - 2024/12//
PY  - 2024
DO  - 10.1016/j.asoc.2024.112274
VL  - 167
IS  - PA
J2  - Appl. Soft Comput.
SN  - 1568-4946
UR  - https://doi.org/10.1016/j.asoc.2024.112274
KW  - Data sparsity
KW  - Group recommendation
KW  - Personality traits
ER  - 

TY  - CONF
TI  - How platform-user power relations shape algorithmic accountability: a case study of instant loan platforms and financially stressed users in india
AU  - Ramesh, Divya
AU  - Kameswaran, Vaishnav
AU  - Wang, Ding
AU  - Sambasivan, Nithya
T3  - FAccT '22
AB  - Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‘high-risk’ AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‘boon’ of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.
C1  - Seoul, Republic of Korea and New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533237
SP  - 1917
EP  - 1928
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533237
KW  - socio-technical systems
KW  - human-ai interaction
KW  - algorithmic accountability
KW  - algorithmic fairness
KW  - instant loans
ER  - 

TY  - JOUR
TI  - HTSE: hierarchical time-surface model for temporal knowledge graph embedding
AU  - Jin, Langjunqing
AU  - Zhao, Feng
AU  - Jin, Hai
T2  - World Wide Web-internet and Web Information Systems
AB  - Representation learning based on temporal knowledge graphs (TKGs) has attracted widespread interest, and temporal knowledge graph embedding (TKGE) expresses time entity and relation tokens and exhibit strong dynamics. Despite the significance of the dynamics and the persistent updates in TKGs, most studies have been devoted to static knowledge graphs. Moreover, previous temporal works ignored the semantic hierarchies observed in knowledge modelling cases, which are common in real-world applications. Inaccurate semantic expressions caused by incomplete projections might not capture complex topological structures very well. To solve this problem, a novel hierarchicaltime-surfaceembedding (HTSE) model is proposed for the representation learning of entities, relations and time. Specifically, a unified relation-oriented hierarchical space aims to distinguish relations at different semantic levels of a hierarchy, and entities can naturally reflect the corresponding hierarchy. Then, a time surface aims to enhance the temporal characteristics, and quadruples are learned through exponential mapping and tangent planes in the time surface. According to extensive experiments, HTSE can achieve remarkable performance on five benchmark datasets, outperforming baseline models for time scope prediction, temporal link prediction and hierarchical relation embedding tasks.Furthermore, the qualitative analysis is used to demonstrate the explainable strategy for hierarchical embeddings and their significance in TKGs.
DA  - 2023/05//
PY  - 2023
DO  - 10.1007/s11280-023-01170-2
VL  - 26
IS  - 5
SP  - 2947
EP  - 2967
J2  - World Wide Web
SN  - 1386-145X
UR  - https://doi.org/10.1007/s11280-023-01170-2
KW  - Knowledge graph embedding
KW  - Semantic hierarchy
KW  - Temporal prediction
KW  - Time surface
ER  - 

TY  - CONF
TI  - DISCO: Comprehensive and explainable disinformation detection
AU  - Fu, Dongqi
AU  - Ban, Yikun
AU  - Tong, Hanghang
AU  - Maciejewski, Ross
AU  - He, Jingrui
T3  - Cikm '22
AB  - Disinformation refers to false information deliberately spread to influence the general public, and the negative impact of disinformation on society can be observed in numerous issues, such as political agendas and manipulating financial markets. In this paper, we identify prevalent challenges and advances related to automated disinformation detection from multiple aspects and propose a comprehensive and explainable disinformation detection framework called DISCO. It leverages the heterogeneity of disinformation and addresses the opaqueness of prediction. Then we provide a demonstration of DISCO on a real-world fake news detection task with satisfactory detection accuracy and explanation. The demo video and source code of DISCO is now publicly available https://github.com/DongqiFu/DISCO. We expect that our demo could pave the way for addressing the limitations of identification, comprehension, and explainability as a whole.
C1  - Atlanta, GA, USA and New York, NY, USA
C3  - Proceedings of the 31st ACM international conference on information &amp; knowledge management
DA  - 2022///
PY  - 2022
DO  - 10.1145/3511808.3557202
SP  - 4848
EP  - 4852
PB  - Association for Computing Machinery
SN  - 978-1-4503-9236-5
UR  - https://doi.org/10.1145/3511808.3557202
KW  - disinformation detection
KW  - explanation
KW  - graph augmentation
ER  - 

TY  - CONF
TI  - SafeML: Safety monitoring of machine learning classifiers through statistical difference measures
AU  - Aslansefat, Koorosh
AU  - Sorokos, Ioannis
AU  - Whiting, Declan
AU  - Tavakoli Kolagari, Ramin
AU  - Papadopoulos, Yiannis
AB  - Ensuring safety and explainability of machine learning (ML) is a topic of increasing relevance as data-driven applications venture into safety-critical application domains, traditionally committed to high safety standards that are not satisfied with an exclusive testing approach of otherwise inaccessible black-box systems. Especially the interaction between safety and security is a central challenge, as security violations can lead to compromised safety. The contribution of this paper to addressing both safety and security within a single concept of protection applicable during the operation of ML systems is active monitoring of the behavior and the operational context of the data-driven system based on distance measures of the Empirical Cumulative Distribution Function (ECDF). We investigate abstract datasets (XOR, Spiral, Circle) and current security-specific datasets for intrusion detection (CICIDS2017) of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures. Our preliminary findings indicate that there is a meaningful correlation between ML decisions and the ECDF-based distances measures of the input features. Thus, they can provide a confidence level that can be used for a) analyzing the applicability of the ML system in a given field (safety/security) and b) analyzing if the field data was maliciously manipulated. (Our preliminary code and results are available at .)
C1  - Lisbon, Portugal and Berlin, Heidelberg
C3  - Model-based safety and assessment: 7th international symposium, IMBSA 2020, lisbon, portugal, september 14–16, 2020, proceedings
DA  - 2020///
PY  - 2020
DO  - 10.1007/978-3-030-58920-2_13
SP  - 197
EP  - 211
PB  - Springer-Verlag
SN  - 978-3-030-58919-6
UR  - https://doi.org/10.1007/978-3-030-58920-2_13
KW  - Safety
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Deep Learning
KW  - Domain adaptation
KW  - SafeML
KW  - Statistical difference
ER  - 

TY  - CONF
TI  - 2nd workshop on ethical artificial intelligence: Methods and applications (EAI)
AU  - Zhao, Chen
AU  - Chen, Feng
AU  - Wu, Xintao
AU  - Chen, Haifeng
AU  - Zhou, Jiayu
T3  - Kdd '23
AB  - Ethical AI has become increasingly important, and it has been attracting attention from academia and industry, due to its increased popularity in real-world applications with fairness concerns. It also places fundamental importance on ethical considerations in determining legitimate and illegitimate uses of AI. Organizations that apply ethical AI have clearly stated well-defined review processes to ensure adherence to legal guidelines. Therefore, the wave of research at the intersection of ethical AI in data mining and machine learning has also influenced other fields of science, including computer vision, natural language processing, reinforcement learning, and social science. Despite these successes, ethical AI still faces many challenges, such as a lack of interpretable and explainable methods for fairness-aware deep learning models, etc. Consequently, there is an urgent need to bring experts and researchers together at prestigious venues to discuss ethical AI, which has been rarely seen in previous KDD conferences. This workshop will provide a premium platform for both research and industry from different backgrounds to exchange ideas on opportunities, challenges, and cutting-edge techniques in ethical AI.
C1  - Long Beach, CA, USA and New York, NY, USA
C3  - Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining
DA  - 2023///
PY  - 2023
DO  - 10.1145/3580305.3599215
SP  - 5903
EP  - 5904
PB  - Association for Computing Machinery
SN  - 979-8-4007-0103-0
UR  - https://doi.org/10.1145/3580305.3599215
KW  - machine learning
KW  - ethical artificial intelligence
KW  - fairness-aware
ER  - 

TY  - CONF
TI  - Digital twin modelling for human-centered ergonomic design
AU  - George, Micah Wilson
AU  - Gaikwad, Nandini
AU  - Duffy, Vincent G.
AU  - Greenwood, Allen G.
AB  - Simulation turns out to be one of the most important aspects of ergonomics, as it allows testing the interaction of the people and the system before they are brought into effect to improve the design process. Digital Twin is a concept that utilizes simulation to implement the representation of the model of a system or product in a virtual manner so that its purpose and working can be analyzed and evaluated before bringing it into the real world to serve its purpose as expected. This report focuses on how Digital Twin is a powerful industrial tool and explains its distinct properties along with its applications through case studies that provide explanations for the required methodology to implement Digital Twin in a simulation software called FlexSim. The results show how important it is in today’s world to carry out proper simulations, i.e., implementing the Digital Twin to provide critical solutions in workplace ergonomics and other areas.
C1  - Copenhagen, Denmark and Berlin, Heidelberg
C3  - Digital human modeling and applications in health, safety, ergonomics and risk management: 14th international conference, DHM 2023, held as part of the 25th HCI international conference, HCII 2023, copenhagen, denmark, july 23–28, 2023, proceedings, part I
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-35741-1_6
SP  - 58
EP  - 69
PB  - Springer-Verlag
SN  - 978-3-031-35740-4
UR  - https://doi.org/10.1007/978-3-031-35741-1_6
KW  - Digital Twin
KW  - Ergonomics
KW  - Simulation
ER  - 

TY  - JOUR
TI  - Explainable, interactive content‐based image retrieval
AU  - Vasu, Bhavan
AU  - Hu, Brian
AU  - Dong, Bo
AU  - Collins, Roddy
AU  - Hoogs, Anthony
T2  - Applied AI Letters
AB  - Quantifying the value of explanations in a human‐in‐the‐loop (HITL) system is difficult. Previous methods either measure explanation‐specific values that do not correspond to user tasks and needs or poll users on how useful they find the explanations to be. In this work, we quantify how much explanations help the user through a utility‐based paradigm that measures change in task performance when using explanations vs not. Our chosen task is content‐based image retrieval (CBIR), which has well‐established baselines and performance metrics independent of explainability. We extend an existing HITL image retrieval system that incorporates user feedback with similarity‐based saliency maps (SBSM) that indicate to the user which parts of the retrieved images are most similar to the query image. The system helps the user understand what it is paying attention to through saliency maps, and the user helps the system understand their goal through saliency‐guided relevance feedback. Using the MS‐COCO dataset, a standard object detection and segmentation dataset, we conducted extensive, crowd‐sourced experiments validating that SBSM improves interactive image retrieval. Although the performance increase is modest in the general case, in more difficult cases such as cluttered scenes, using explanations yields an 6.5 image image
DA  - 2021/11//
PY  - 2021
DO  - 10.1002/ail2.41
VL  - 2
IS  - 4
UR  - https://doi.org/10.1002/ail2.41
KW  - explainable AI
KW  - image retrieval
KW  - saliency
KW  - user study
ER  - 

TY  - CONF
TI  - Compositional abilities emerge multiplicatively: exploring diffusion models on a synthetic task
AU  - Okawa, Maya
AU  - Lubana, Ekdeep Singh
AU  - Dick, Robert P.
AU  - Tanaka, Hidenori
T3  - Nips '23
AB  - Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhibits a sudden "emergence" due to multiplicative reliance on the performance of constituent tasks, partially explaining emergent phenomena seen in generative models; and (iii) composing concepts with lower frequency in the training data to generate out-of-distribution samples requires considerably more optimization steps compared to generating in-distribution samples. Overall, our study lays a foundation for understanding emergent capabilities and compositionality in generative models from a data-centric perspective.
C1  - New Orleans, LA, USA and Red Hook, NY, USA
C3  - Proceedings of the 37th international conference on neural information processing systems
DA  - 2023///
PY  - 2023
PB  - Curran Associates Inc.
ER  - 

TY  - CONF
TI  - An experimental study of quantitative evaluations on saliency methods
AU  - Li, Xiao-Hui
AU  - Shi, Yuhan
AU  - Li, Haoyang
AU  - Bai, Wei
AU  - Cao, Caleb Chen
AU  - Chen, Lei
T3  - Kdd '21
AB  - It has been long debated that eXplainable AI (XAI) is an important technology for model and data exploration, validation, and debugging. To deploy XAI into actual systems, an executable and comprehensive evaluation of the quality of generated explanation is highly in demand. In this paper, we briefly summarize the status quo of the quantitative metrics of different properties of XAI including evaluation on faithfulness, localization, sensitivity check, and stability. With an exhaustive experimental study based on them, we conclude that among all the typical methods we compare, no single explanation method dominates others in all metrics. Nonetheless, Gradient-weighted Class Activation Mapping (Grad-CAM) and Randomly Input Sampling for Explanation (RISE) perform fairly well in most of the metrics. We further present a novel utilization of the evaluation results to diagnose the classification bases for models. Hopefully, this valuable work could serve as a guide for future research.
C1  - Virtual Event, Singapore and New York, NY, USA
C3  - Proceedings of the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining
DA  - 2021///
PY  - 2021
DO  - 10.1145/3447548.3467148
SP  - 3200
EP  - 3208
PB  - Association for Computing Machinery
SN  - 978-1-4503-8332-5
UR  - https://doi.org/10.1145/3447548.3467148
KW  - explainable artificial intelligence
KW  - evaluation
KW  - metrics
KW  - model diagnosis
ER  - 

TY  - CONF
TI  - First workshop on user perspectives in human-centred artificial intelligence (HCAI4U)
AU  - De Luca, Ernesto William
AU  - Purificato, Erasmo
AU  - Boratto, Ludovico
AU  - Marrone, Stefano
AU  - Sansone, Carlo
T3  - CHItaly '23
AB  - The emerging concept of Human-Centred Artificial Intelligence (HCAI) involves the amplification, augmentation, empowerment, and enhancement of individuals. The goal of HCAI is to ensure that AI meets our needs while also operating transparently, delivering fair and equitable outcomes, and respecting privacy, all while preserving human control. This approach involves multiple stakeholders, such as researchers, developers, business leaders, policy makers, and users, who are affected in various ways by the implementation and evaluation of AI systems. The primary focus of the First Workshop on User Perspectives in Human-Centred Artificial Intelligence (HCAI4U) is to examine the potential positive and negative impacts of automated decision-making systems on end-users, as well as how their interaction with AI is influenced by human-centred aspects of reliability, safety, and fairness. The workshop aims to facilitate discussion and exchange of ideas among the community on advances in developing trustworthy, fair, and privacy-preserving systems, as well as user interfaces that are explainable, with a specific focus on the users’ perception in real-world scenarios rather than solely on the algorithmic and model performance. Additionally, HCAI4U aims to foster cross-disciplinary and interdisciplinary discussions between experts from various research fields, such as computer science, psychology, sociology, law, medicine, business, etc., to discuss problems and synergies in this exciting research topic.
C1  - Torino, Italy and New York, NY, USA
C3  - Proceedings of the 15th biannual conference of the italian SIGCHI chapter
DA  - 2023///
PY  - 2023
DO  - 10.1145/3605390.3610829
PB  - Association for Computing Machinery
SN  - 979-8-4007-0806-0
UR  - https://doi.org/10.1145/3605390.3610829
KW  - Reliability
KW  - Fairness
KW  - Artificial Intelligence
KW  - Trustworthiness
KW  - Human-Computer Interaction
KW  - Explainability
KW  - Human-Centred Artificial Intelligence
KW  - User Perspectives
ER  - 

TY  - JOUR
TI  - The 4C framework: Towards a holistic understanding of consumer engagement with augmented reality
AU  - Rauschnabel, Philipp A.
AU  - Felix, Reto
AU  - Heller, Jonas
AU  - Hinsch, Chris
T2  - Computers in Human Behavior
DA  - 2024/05//
PY  - 2024
DO  - 10.1016/j.chb.2023.108105
VL  - 154
IS  - C
J2  - Comput. Hum. Behav.
SN  - 0747-5632
UR  - https://doi.org/10.1016/j.chb.2023.108105
KW  - Context
KW  - Augmented Reality
KW  - Consumer
KW  - Content
KW  - Engagement
KW  - Metaverse
ER  - 

TY  - CONF
TI  - Vehicle-to-pedestrian communication feedback module: a study on&nbsp;increasing legibility, public acceptance and&nbsp;trust
AU  - Schmidt-Wolf, Melanie
AU  - Feil-Seifer, David
AB  - Vehicle pedestrian communication is extremely important when developing autonomy for an autonomous vehicle. Enabling bidirectional nonverbal communication between pedestrians and autonomous vehicles will lead to an improvement of pedestrians’ safety in autonomous driving. The autonomous vehicle should provide feedback to the human about what it is about to do. The user study presented in this paper investigated several possible options for an external vehicle display for effective nonverbal communication between an autonomous vehicle and a human. The result of this study will guide the development of the feedback module to optimize for public acceptance and trust in the autonomous vehicle’s decision while being legible to the widest range of potential users. The results of this study show that participants prefer symbols over text, lights and road projection. We plan to elaborate and focus on the selected interaction modes via Virtual Reality and in the real world in ongoing and future studies.
C1  - Florence, Italy and Berlin, Heidelberg
C3  - Social robotics: 14th international conference, ICSR 2022, florence, italy, december 13–16, 2022, proceedings, part I
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-24667-8_2
SP  - 14
EP  - 23
PB  - Springer-Verlag
SN  - 978-3-031-24666-1
UR  - https://doi.org/10.1007/978-3-031-24667-8_2
KW  - Trust
KW  - Autonomous vehicle
KW  - eHMI
KW  - Legibility
KW  - Public acceptance
KW  - V2P
ER  - 

TY  - CONF
TI  - Heterogeneous graph learning for explainable recommendation over academic networks
AU  - Chen, Xiangtai
AU  - Tang, Tao
AU  - Ren, Jing
AU  - Lee, Ivan
AU  - Chen, Honglong
AU  - Xia, Feng
T3  - Wi-iat '21
AB  - With the explosive growth of new graduates with research degrees every year, unprecedented challenges arise for early-career researchers to find a job at a suitable institution. This study aims to understand the behavior of academic job transition and hence recommend suitable institutions for PhD graduates. Specifically, we design a deep learning model to predict the career move of early-career researchers and provide suggestions. The design is built on top of scholarly/academic networks, which contains abundant information about scientific collaboration among scholars and institutions. We construct a heterogeneous scholarly network to facilitate the exploring of the behavior of career moves and the recommendation of institutions for scholars. We devise an unsupervised learning model called HAI (Heterogeneous graph Attention InfoMax) which aggregates attention mechanism and mutual information for institution recommendation. Moreover, we propose scholar attention and meta-path attention to discover the hidden relationships between several meta-paths. With these mechanisms, HAI provides ordered recommendations with explainability. We evaluate HAI upon a real-world dataset against baseline methods. Experimental results verify the effectiveness and efficiency of our approach.
C1  - Melbourne, VIC, Australia and New York, NY, USA
C3  - IEEE/WIC/ACM international conference on web intelligence and intelligent agent technology
DA  - 2022///
PY  - 2022
DO  - 10.1145/3498851.3498926
SP  - 29
EP  - 36
PB  - Association for Computing Machinery
SN  - 978-1-4503-9187-0
UR  - https://doi.org/10.1145/3498851.3498926
KW  - explainability
KW  - academic social networks
KW  - graph learning
KW  - heterogeneous networks
KW  - recommender systems
ER  - 

TY  - JOUR
TI  - An ontology-based framework for worker’s health reasoning enabled by machine learning
AU  - Bavaresco, Rodrigo
AU  - Ren, Yutian
AU  - Barbosa, Jorge
AU  - Li, G.P.
T2  - Computers & Industrial Engineering
DA  - 2024/07//
PY  - 2024
DO  - 10.1016/j.cie.2024.110310
VL  - 193
IS  - C
J2  - Comput. Ind. Eng.
SN  - 0360-8352
UR  - https://doi.org/10.1016/j.cie.2024.110310
KW  - Deep learning
KW  - Reasoning
KW  - Ontology
KW  - Knowledge representation
KW  - Occupational health and safety
ER  - 

TY  - JOUR
TI  - Explainable multi-instance and multi-task learning for COVID-19 diagnosis and lesion segmentation in CT images
AU  - Li, Minglei
AU  - Li, Xiang
AU  - Jiang, Yuchen
AU  - Zhang, Jiusi
AU  - Luo, Hao
AU  - Yin, Shen
T2  - Know.-Based Syst.
DA  - 2022/09//
PY  - 2022
DO  - 10.1016/j.knosys.2022.109278
VL  - 252
IS  - C
SN  - 0950-7051
UR  - https://doi.org/10.1016/j.knosys.2022.109278
KW  - COVID-19
KW  - Adaptive multi-task learning
KW  - Automated diagnosis
KW  - Explainable multi-instance learning
KW  - Lesion segmentation
ER  - 

TY  - CONF
TI  - Designing adversarial robust and explainable malware detection system for android based smartphones: PhD forum abstract
AU  - Rathore, Hemant
T3  - Ipsn '21
AB  - Android smartphones and malware have grown exponentially in the last decade. Literature suggests that the current malware detection systems cannot cope with the present security challenges. Thus researchers are developing next-generation malware detection systems/models using the machine and deep learning. However, the proposed systems/models have poor explainability and are vulnerable against adversarial attacks, which will jeopardize their adoption in the future security ecosystem. Thus, we aim to construct adversarial robust malware detection models by first acting as an adversary to find vulnerabilities in models and then proposing preventive countermeasures. We also aim to improve models' explain-ability to win security community confidence before real-world implementation.
C1  - Nashville, TN, USA and New York, NY, USA
C3  - Proceedings of the 20th international conference on information processing in sensor networks (co-located with CPS-IoT week 2021)
DA  - 2021///
PY  - 2021
DO  - 10.1145/3412382.3459209
SP  - 412
EP  - 413
PB  - Association for Computing Machinery
SN  - 978-1-4503-8098-0
UR  - https://doi.org/10.1145/3412382.3459209
ER  - 

TY  - BOOK
TI  - Real-time linked dataspaces: Enabling data ecosystems for intelligent systems
AU  - Curry, Edward
AB  - This open access book explores the dataspace paradigm as a best-effort approach to data management within data ecosystems. It establishes the theoretical foundations and principles of real-time linked dataspaces as a data platform for intelligent systems. The book introduces a set of specialized best-effort techniques and models to enable loose administrative proximity and semantic integration for managing and processing events and streams. The book is divided into five major parts: Part I "Fundamentals and Concepts" details the motivation behind and core concepts of real-time linked dataspaces, and establishes the need to evolve data management techniques in order to meet the challenges of enabling data ecosystems for intelligent systems within smart environments. Further, it explains the fundamental concepts of dataspaces and the need for specialization in the processing of dynamic real-time data. Part II "Data Support Services" explores the design and evaluation of critical services, including catalog, entity management, query and search, data service discovery, and human-in-the-loop. In turn, Part III "Stream and Event Processing Services" addresses the design and evaluation of the specialized techniques created for real-time support services including complex event processing, event service composition, stream dissemination, stream matching, and approximate semantic matching. Part IV "Intelligent Systems and Applications" explores the use of real-time linked dataspaces within real-world smart environments. In closing, Part V "Future Directions" outlines future research challenges for dataspaces, data ecosystems, and intelligent systems. Readers will gain a detailed understanding of how the dataspace paradigm is now being used to enable data ecosystems for intelligent systems within smart environments. The book covers the fundamental theory, the creation of new techniques needed for support services, and lessons learned from real-world intelligent systems and applications focused on sustainability. Accordingly, it will benefit not only researchers and graduate students in the fields of data management, big data, and IoT, but also professionals who need to create advanced data management platforms for intelligent systems, smart environments, and data ecosystems.
DA  - 2019///
PY  - 2019
ET  - 1
PB  - Springer Publishing Company, Incorporated
SN  - 3-030-29664-4
ER  - 

TY  - CONF
TI  - Interactional freedom and&nbsp;cybersecurity
AU  - Bella, Giampaolo
AB  - We have become accustomed to the news of more and more cunning attacks to real-world systems, and equally accustomed to try to fix them even though further attacks may come. I discuss how to tackle and ultimately resolve this tedious and infamous attack-fix-loop practice by distilling out five paradigms to achieve cybersecurity: democratic, dictatorial, beautiful, invisible and explainable security. While each of these has distinctive features, various combinations, at some rate, of them may coexist, with the final aim of improving the way security measures account for the human element. Towards the end of the paper, I conjecture how the paradigms could be used to improve the ultimate security measure of our times, a Security Operation Centre. May I remark that many of the observations made below derive from my personal and current understanding and would require a number of experiments to be fully confirmed.
C1  - Berlin, Heidelberg
C3  - Innovative security solutions for information technology and communications: 15th international conference, SecITC 2022, virtual event, december 8–9, 2022, revised selected papers
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-32636-3_1
SP  - 1
EP  - 16
PB  - Springer-Verlag
SN  - 978-3-031-32635-6
UR  - https://doi.org/10.1007/978-3-031-32636-3_1
KW  - Beautiful security
KW  - Democratic security
KW  - Dictatorial security
KW  - Explainable Security
KW  - Invisible Security
ER  - 

TY  - CONF
TI  - Learning citywide patterns of life from trajectory monitoring
AU  - Tenzer, Mark
AU  - Rasheed, Zeeshan
AU  - Shafique, Khurram
T3  - Sigspatial '22
AB  - The recent proliferation of real-world human mobility datasets has catalyzed geospatial and transportation research in trajectory prediction, demand forecasting, travel time estimation, and anomaly detection. However, these datasets also enable, more broadly, a descriptive analysis of intricate systems of human mobility. We formally define patterns of life analysis as a natural, explainable extension of online unsupervised anomaly detection, where we not only monitor a data stream for anomalies but also explicitly extract normal patterns over time. To learn patterns of life, we adapt Grow When Required (GWR) episodic memory from research in computational biology and neurorobotics to a new domain of geospatial analysis. This biologically-inspired neural network, related to self-organizing maps (SOM), constructs a set of "memories" or prototype traffic patterns incrementally as it iterates over the GPS stream. It then compares each new observation to its prior experiences, inducing an online, unsupervised clustering and anomaly detection on the data. We mine patterns-of-interest from the Porto taxi dataset, including both major public holidays and newly-discovered transportation anomalies, such as festivals and concerts which, to our knowledge, have not been previously acknowledged or reported in prior work. We anticipate that the capability to incrementally learn normal and abnormal road transportation behavior will be useful in many domains, including smart cities, autonomous vehicles, and urban planning and management.
C1  - Seattle, Washington and New York, NY, USA
C3  - Proceedings of the 30th international conference on advances in geographic information systems
DA  - 2022///
PY  - 2022
DO  - 10.1145/3557915.3560978
PB  - Association for Computing Machinery
SN  - 978-1-4503-9529-8
UR  - https://doi.org/10.1145/3557915.3560978
KW  - anomaly detection
KW  - biological neural networks
KW  - geospatial analysis
KW  - patterns of life
KW  - self-organizing feature maps
ER  - 

TY  - JOUR
TI  - Path-enhanced explainable recommendation with knowledge graphs
AU  - Huang, Yafan
AU  - Zhao, Feng
AU  - Gui, Xiangyu
AU  - Jin, Hai
T2  - World Wide Web-internet and Web Information Systems
AB  - Recommender systems, which are used to predict user requirements precisely, play a vital role in the modern internet industry. As an effective tool with rich semantics, knowledge graphs have recently attracted growing research attention in enhancing recommendation results. By mining multihop relations (i.e., paths) between user-item interactions within a knowledge graph, implicit user preferences and other side information can be clearly revealed. Nevertheless, existing knowledge graph-based recommendation methods have two fundamental limitations. First, the indiscriminate utilization of user-item path sets conveys unclear information and negatively influences explainability. Moreover, obtaining reliable recommendation results with these methods requires large amounts of prior knowledge, which indicates that they show poor performance in terms of accuracy and handling cold-start issues. To address these issues, we propose a novel model called the Path-enhanced Recurrent Network (PeRN). Specifically, PeRN integrates a recurrent neural network encoder with a metapath-based entropy encoder to increase explainability and accuracy and reduce cold-start costs. The recurrent network encoder has a strong ability to represent sequential path semantics in a knowledge graph, while the entropy encoder, as an efficient statistical analysis tool, leverages metapath information to differentiate paths in a single user-item interaction. A path extraction algorithm with a bidirectional scheme is also proposed to make PeRN more feasible. The experimental results on two real-world datasets demonstrate our significant improvements with reasonable explanations, promising accuracy and a minimal amount of prior knowledge compared with several state-of-the-art baselines.
DA  - 2021/09//
PY  - 2021
DO  - 10.1007/s11280-021-00912-4
VL  - 24
IS  - 5
SP  - 1769
EP  - 1789
J2  - World Wide Web
SN  - 1386-145X
UR  - https://doi.org/10.1007/s11280-021-00912-4
KW  - Knowledge graph
KW  - Metapath
KW  - Recommender system
KW  - Recurrent neural network
ER  - 

TY  - THES
TI  - Development of a behavior-based analysis framework for promoting RPV deployment in singapore
AU  - Nan, Zhang
AB  - Residential Photovoltaic (RPV) systems are one type of building-integrated photovoltaics (BIPV) specialized for residential buildings. The deployment of RPV holds great potential to solve energy poverty and reduce greenhouse gas emissions because residential buildings consume approximately 38
DA  - 2021///
PY  - 2021
M3  - phd
PB  - National University of Singapore (Singapore)
N1  - AAI29352355
ER  - 

TY  - CONF
TI  - Can an algorithmic system be a 'friend' to a police officer's discretion? ACM FAT 2020 translation tutorial
AU  - Oswald, Marion
AU  - Powell, David
T3  - Fat* '20
AB  - This tutorial aims to increase understanding of the importance of discretion in police decision-making. It will guide computer scientists, policy-makers, lawyers and others in considering practical and technical issues crucial to avoiding the prejudicial and instead develop algorithms that are supportive - a 'friend'- to legitimate discretionary decision-making. It combines explanation of the relevant law and related literature with discussion based upon deep operational experience in the area of preventative and protective policing work.Autonomy and discretion are fundamental to police work, not only in relation to strategy and policy but for day-to-day operational decisions taken by front line officers. Such discretion 'recognizes the fallibility of interfacing rules with their field of application.' (Hildebrandt 2016). This discretion is not unbounded however and English common law expects discretion to be exercised reasonably and fairly. Conversely, discretion must not be fettered unlawfully, by failing to take a relevant factor into account when making a decision, or by abdicating responsibility to another person, body or 'thing'. Algorithmic systems have the potential to contribute to factors relevant to the decision in question at the point of interaction between their outputs and the real-world outcome for the victim, offender and/or community.Algorithmic decision tools present a number of challenges to legitimate discretionary police decision-making. Unnuanced outputs could be highly influential on the human decision-maker (Cooke and Michie 2012) and may undermine discretionary power to deal with atypical cases and 'un-thought of' factors that rely upon uncodified knowledge (Oswald 2018).Practical and technical considerations will be crucial to developing MLA that are supportive to discretionary decision-making. These include the methodological approach, design of the humancomputer interface having regard the decision-maker's responsibility to give reasons for their decision, the avoidance of unnuanced or over-confident framing of results, understanding of the policing context in which the MLA will operate, and consideration of the implications of organisational culture and processes to the MLA's influence.
C1  - Barcelona, Spain and New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375673
SP  - 698
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375673
KW  - machine learning
KW  - algorithms
KW  - discretion
KW  - police
ER  - 

TY  - JOUR
TI  - CrowdGraph: a crowdsourcing multi-modal knowledge graph approach to explainable fauxtography detection
AU  - Kou, Ziyi
AU  - Zhang, Yang
AU  - Zhang, Daniel
AU  - Wang, Dong
T2  - Proc. ACM Hum.-Comput. Interact.
AB  - Human-centric fauxtography is a category of multi-modal posts that spread misleading information on online information distribution and sharing platforms such as online social media. The reason of a human-centric post being fauxtography is closely related to its multi-modal content that consists of diversified human and non-human subjects with complex and implicit relationships. In this paper, we focus on an explainable fauxtography detection problem where the goal is to accurately identify and explain why a human-centric social media post is fauxtography (or not). Our problem is motivated by the limitations of current fauxtography detection solutions that focus primarily on the detection task but ignore the important aspect of explaining their results (e.g., why a certain component of the post delivers the misinformation). Two important challenges exist in solving our problem: 1) it is difficult to capture the implicit relations and attributions of different subjects in a fauxtography post given the fact that many of such knowledge is shared between different crowd workers; 2) it is not a trivial task to create a multi-modal knowledge graph from crowd workers to identify and explain human-centric fauxtography posts with multi-modal contents. To address the above challenges, we develop CrowdGraph, a crowdsourcing based multi-modal knowledge graph approach to address the explainable fauxtography detection problem. We evaluate the performance of CrowdGraph by creating a real-world dataset that consists of human-centric fauxtography posts from Twitter and Reddit. The results show that CrowdGraph not only detects the fauxtography posts more accurately than the state-of-the-arts but also provides well-justified explanations to the detection results with convincing evidence.
DA  - 2022/11//
PY  - 2022
DO  - 10.1145/3555178
VL  - 6
IS  - CSCW2
UR  - https://doi.org/10.1145/3555178
KW  - crowdsourcing
KW  - multi modal information
KW  - social media
ER  - 

TY  - CONF
TI  - A coverage-based utility model for identifying unknown unknowns
AU  - Bansal, Gagan
AU  - Weld, Daniel S.
T3  - AAAI'18/IAAI'18/EAAI'18
AB  - A classifier's low confidence in prediction is often indicative of whether its prediction will be wrong; in this case, inputs are called known unknowns. In contrast, unknown unknowns (UUs) are inputs on which a classifier makes a high confidence mistake. Identifying UUs is especially important in safety-critical domains like medicine (diagnosis) and law (recidivism prediction). Previous work by Lakkaraju et al. (2017) on identifying unknown unknowns assumes that the utility of each revealed UU is independent of the others, rather than considering the set holistically. While this assumption yields an efficient discovery algorithm, we argue that it produces an incomplete understanding of the classifier's limitations. In response, this paper proposes a new class of utility models that rewards how well the discovered UUs cover (or "explain") a sample distribution of expected queries. Although choosing an optimal cover is intractable, even if the UUs were known, our utility model is monotone submodular, affording a greedy discovery strategy. Experimental results on four datasets show that our method outperforms bandit-based approaches and achieves within 60.9
C1  - New Orleans, Louisiana, USA
C3  - Proceedings of the thirty-second AAAI conference on artificial intelligence and thirtieth innovative applications of artificial intelligence conference and eighth AAAI symposium on educational advances in artificial intelligence
DA  - 2018///
PY  - 2018
PB  - AAAI Press
SN  - 978-1-57735-800-8
ER  - 

TY  - CONF
TI  - IoT security seminar: Raising awareness and sharing critical knowledge
AU  - Goeman, Victor
AU  - de Ruck, Dairo
AU  - Bohé, Ilse
AU  - Lapon, Jorn
AU  - Naessens, Vincent
T3  - Ares '23
AB  - The security of the Internet of Things (IoT) devices has become a major concern as the number of connected devices continues to increase. Despite this concern, there is a lack of training opportunities to educate IoT developers on security measures. While there are ample ICT and Network Management courses for developers, there is a lack of security courses scoped for this audience. One of the reasons is that raising cybersecurity awareness and increasing the security expertise of developers presents a significant challenge due to the complexity of IoT security. This work presents a cybersecurity seminar that tackles these challenges. It is aimed at various actors in the IoT device development cycle (e.g. software designers, developers and managers) to raise IoT security awareness and share critical knowledge. It cultivates the basics of both offensive and defensive security through a custom-built vulnerable IoT firmware image with vulnerabilities found in real-world IoT devices. This intentionally vulnerable image is accompanied by a detailed walkthrough explaining various exploitation and mitigation techniques. Our seminar has been held multiple times in both industry and academics and consistently received very positive feedback. It has been successful in educating participants about the importance of IoT security and providing them with additional knowledge and skills to take action in their own practices.
C1  - Benevento, Italy and New York, NY, USA
C3  - Proceedings of the 18th international conference on availability, reliability and security
DA  - 2023///
PY  - 2023
DO  - 10.1145/3600160.3604986
PB  - Association for Computing Machinery
SN  - 979-8-4007-0772-8
UR  - https://doi.org/10.1145/3600160.3604986
KW  - Education
KW  - Cybersecurity
KW  - Awareness
KW  - IoT
ER  - 

TY  - CHAP
TI  - Explanation-friendly query answering under uncertainty
AU  - Martinez, Maria Vanina
AU  - Simari, Gerardo I.
T2  - Reasoning web. Explainable artificial intelligence
AB  - Many tasks often regarded as requiring some form of intelligence to perform can be seen as instances of query answering over a semantically rich knowledge base. In this context, two of the main problems that arise are: (i) uncertainty, including both inherent uncertainty (such as events involving the weather) and uncertainty arising from lack of sufficient knowledge; and (ii) inconsistency, which involves dealing with conflicting knowledge. These unavoidable characteristics of real world knowledge often yield complex models of reasoning; assuming these models are mostly used by humans as decision-support systems, meaningful explainability of their results is a critical feature. These lecture notes are divided into two parts, one for each of these basic issues. In Part&nbsp;1, we present basic probabilistic graphical models and discuss how they can be incorporated into powerful ontological languages; in Part&nbsp;2, we discuss both classical inconsistency-tolerant semantics for ontological query answering based on the concept of repair and other semantics that aim towards more flexible yet principled ways to handle inconsistency. Finally, in both parts we ponder the issue of deriving different kinds of explanations that can be attached to query results.
CY  - Berlin, Heidelberg
DA  - 2022///
PY  - 2022
SP  - 65
EP  - 103
PB  - Springer-Verlag
SN  - 978-3-030-31422-4
UR  - https://doi.org/10.1007/978-3-030-31423-1_2
ER  - 

TY  - JOUR
TI  - Interpreting deep learning-based vulnerability detector predictions based on heuristic searching
AU  - Zou, Deqing
AU  - Zhu, Yawei
AU  - Xu, Shouhuai
AU  - Li, Zhen
AU  - Jin, Hai
AU  - Ye, Hengkai
T2  - ACM Trans. Softw. Eng. Methodol.
AB  - Detecting software vulnerabilities is an important problem and a recent development in tackling the problem is the use of deep learning models to detect software vulnerabilities. While effective, it is hard to explain why a deep learning model predicts a piece of code as vulnerable or not because of the black-box nature of deep learning models. Indeed, the interpretability of deep learning models is a daunting open problem. In this article, we make a significant step toward tackling the interpretability of deep learning model in vulnerability detection. Specifically, we introduce a high-fidelity explanation framework, which aims to identify a small number of tokens that make significant contributions to a detector’s prediction with respect to an example. Systematic experiments show that the framework indeed has a higher fidelity than existing methods, especially when features are not independent of each other (which often occurs in the real world). In particular, the framework can produce some vulnerability rules that can be understood by domain experts for accepting a detector’s outputs (i.e., true positives) or rejecting a detector’s outputs (i.e., false-positives and false-negatives). We also discuss limitations of the present study, which indicate interesting open problems for future research.
DA  - 2021/03//
PY  - 2021
DO  - 10.1145/3429444
VL  - 30
IS  - 2
SN  - 1049-331X
UR  - https://doi.org/10.1145/3429444
KW  - deep learning
KW  - Explainable AI
KW  - sensitivity analysis
KW  - vulnerability detection
ER  - 

TY  - CONF
TI  - Explainable demand forecasting: a data mining goldmine
AU  - Rozanec, Joze M.
T3  - Www '21
AB  - Demand forecasting is a crucial component of demand management. Value is provided to the organization through accurate forecasts and insights into the reasons driving the forecasts to increase confidence and assist decision-making. In this Ph.D., we aim to develop state-of-the-art demand forecasting models for irregular demand, develop explainability mechanisms to avoid exposing models fine-grained information regarding the model features, create a recommender system to assist users on decision-making and develop mechanisms to enrich knowledge graphs with feedback provided by the users through artificial intelligence-powered feedback modules. We have already developed models for accurate forecasts regarding steady and irregular demand and architecture to provide forecast explanations that preserve sensitive information regarding model features. These explanations highlighting real-world events that provide insights on the general context captured through the dataset features while highlighting actionable items and suggesting datasets for future data enrichment.
C1  - Ljubljana, Slovenia and New York, NY, USA
C3  - Companion proceedings of the web conference 2021
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442442.3453708
SP  - 723
EP  - 724
PB  - Association for Computing Machinery
SN  - 978-1-4503-8313-4
UR  - https://doi.org/10.1145/3442442.3453708
KW  - Machine learning
KW  - Decision support systems
KW  - Artificial Intelligence (AI)
KW  - Demand forecasting
KW  - Explainable Artificial Intelligence (XAI)
KW  - Lumpy demand
ER  - 

TY  - JOUR
TI  - Reinforcement learning for crop management support: Review, prospects and challenges
AU  - Gautron, Romain
AU  - Maillard, Odalric-Ambrym
AU  - Preux, Philippe
AU  - Corbeels, Marc
AU  - Sabbadin, Régis
T2  - Computers and Electronics in Agriculture
DA  - 2022/09//
PY  - 2022
DO  - 10.1016/j.compag.2022.107182
VL  - 200
IS  - C
J2  - Comput. Electron. Agric.
SN  - 0168-1699
UR  - https://doi.org/10.1016/j.compag.2022.107182
KW  - machine learning
KW  - reinforcement learning
KW  - crop management
KW  - decision support system
KW  - multi-armed bandit
ER  - 

TY  - JOUR
TI  - Developing ePartners for human-robot teams in space based on ontologies and formal abstraction hierarchies
T2  - International Journal of Agent-Oriented Software Engineering
AB  - Manned space missions are typically performed by teams composed of humans as well as technical systems and are situated in complex, dynamic and safety-critical domains. Intelligent electronic partners ePartners can play an important role here to support human-robot teams in their collaborative problem solving process when things do not go as planned. To engage in effective team collaboration, ePartners, humans and robots must align their communication at the right level of abstraction. In this paper, an approach is put forward to represent the functionality of human-robot teams in a formal manner using abstraction hierarchies. In this way, formal relations between domain knowledge at the system, functional and mission-oriented levels are established and reasoning rules are used to navigate through these relations. As a consequence, ePartners are equipped with the ability to reason about the status of a mission, propose solutions in non-nominal situations and provide explanations for the proposed solutions. The approach has been implemented within a mobile application on a tablet that can be used to support astronaut-robot teams during space missions. The application has been evaluated during an experiment at the European Space Research and Technology Centre ESTEC in the context of a Mars mission.
DA  - 2017/01//
PY  - 2017
DO  - 10.1504/IJAOSE.2017.087656
VL  - 5
IS  - 4
SP  - 366
EP  - 398
J2  - Int. J. Agent-Oriented Softw. Eng.
SN  - 1746-1375
UR  - https://doi.org/10.1504/IJAOSE.2017.087656
ER  - 

TY  - CONF
TI  - Generative causal interpretation model for spatio-temporal representation learning
AU  - Zhao, Yu
AU  - Deng, Pan
AU  - Liu, Junting
AU  - Jia, Xiaofeng
AU  - Zhang, Jianwei
T3  - Kdd '23
AB  - Learning, interpreting, and predicting from complex and high-dimensional spatio-temporal data is a natural ability of humans and other intelligent agents, and one of the most important and difficult challenges of AI. Although objects may present different observed phenomena under different situations, their causal mechanism and generation rules are stable and invariant. Different from most existing studies that focus on dynamic correlation, we explore the latent causal structure and mechanism of causal descriptors in the spatio-temporal dimension at the microscopic level, thus revealing the generation principle of observation. In this paper, we regard the causal mechanism as a spatio-temporal causal process modulated by non-stationary exogenous variables. To this end, we propose a theoretically-grounded Generative Causal Interpretation Model (GCIM), which infers explanatory-capable microscopic causal descriptors from observational data via spatio-temporal causal representations. The core of GCIM is to estimate the prior distribution of causal descriptors by using the spatio-temporal causal structure and transition process under the constraints of identifiable conditions, thus extending the Variational Auto Encoder (VAE). Furthermore, our method is able to automatically capture domain information from observations to model non-stationarity. We further analyze the model identifiability, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments on synthetic and real-world datasets show that GCIM can successfully identify latent causal descriptors and structures, and accurately predict future data.
C1  - Long Beach, CA, USA and New York, NY, USA
C3  - Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining
DA  - 2023///
PY  - 2023
DO  - 10.1145/3580305.3599363
SP  - 3537
EP  - 3548
PB  - Association for Computing Machinery
SN  - 979-8-4007-0103-0
UR  - https://doi.org/10.1145/3580305.3599363
KW  - generative causal model
KW  - spatio-temporal representation learning
ER  - 

TY  - CONF
TI  - The hidden assumptions behind counterfactual explanations and principal reasons
AU  - Barocas, Solon
AU  - Selbst, Andrew D.
AU  - Raghavan, Manish
T3  - Fat* '20
AB  - Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established "principal reason" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant—and withholding others.These "feature-highlighting explanations" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear.In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes.We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden.While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world—and the subjective choices necessary to compensate for this—must be understood before these techniques can be usefully implemented.
C1  - Barcelona, Spain and New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372830
SP  - 80
EP  - 89
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372830
ER  - 

TY  - JOUR
TI  - Towards algorithms and models that we can trust: A theoretical perspective
AU  - Oneto, Luca
AU  - Ridella, Sandro
AU  - Anguita, Davide
T2  - Neurocomput.
DA  - 2024/08//
PY  - 2024
DO  - 10.1016/j.neucom.2024.127798
VL  - 592
IS  - C
SN  - 0925-2312
UR  - https://doi.org/10.1016/j.neucom.2024.127798
KW  - Algorithmic distribution stability
KW  - Complexity-based methods
KW  - Deterministic algorithms and models
KW  - Differential privacy
KW  - Ethical metrics
KW  - Generalization
KW  - PAC-bayes
KW  - Randomized algorithms and models
KW  - Technical metrics
KW  - Trustworthy machine learning
ER  - 

TY  - JOUR
TI  - Imagining machine vision: Four visual registers from the Chinese AI industry
AU  - de Seta, Gabriele
AU  - Shchetvina, Anya
T2  - AI Soc.
AB  - Machine vision is one of the main applications of artificial intelligence. In China, the machine vision industry makes up more than a third of the national AI market, and technologies like face recognition, object tracking and automated driving play a central role in surveillance systems and social governance projects relying on the large-scale collection and processing of sensor data. Like other novel articulations of technology and society, machine vision is defined, developed and explained by different actors through the work of imagination. In this article, we draw on the concept of sociotechnical imaginaries to understand how Chinese companies represent machine vision. Through a qualitative multimodal analysis of the corporate websites of leading industry players, we identify a cohesive sociotechnical imaginary of machine vision, and explain how four distinct visual registers contribute to its articulation. These four registers, which we call computational abstraction, human–machine coordination, smooth everyday, and dashboard realism, allow Chinese tech companies to articulate their global ambitions and competitiveness through narrow and opaque representations of machine vision technologies.
DA  - 2023/08//
PY  - 2023
DO  - 10.1007/s00146-023-01733-x
VL  - 39
IS  - 5
SP  - 2267
EP  - 2284
SN  - 0951-5666
UR  - https://doi.org/10.1007/s00146-023-01733-x
KW  - Artificial intelligence
KW  - China
KW  - Machine vision
KW  - Sociotechnical imaginaries
KW  - Tech industry
KW  - Visual culture
ER  - 

TY  - CONF
TI  - “I’d like an explanation for that!”exploring reactions to unexpected autonomous driving
AU  - Wiegand, Gesa
AU  - Eiband, Malin
AU  - Haubelt, Maximilian
AU  - Hussmann, Heinrich
T3  - MobileHCI '20
AB  - Autonomous vehicles are complex systems that may behave in unexpected ways. From the drivers’ perspective, this can cause stress and lower trust and acceptance of autonomous driving. Prior work has shown that explanation of system behavior can mitigate these negative effects. Nevertheless, it remains unclear in which situations drivers actually need an explanation and what kind of interaction is relevant to them. Using thematic analysis of real-world experience reports, we first identified 17 situations in which a vehicle behaved unexpectedly. We then conducted a think-aloud study (N = 26) in a driving simulator to validate these situations and enrich them with qualitative insights about drivers’ need for explanation. We identified six categories to describe the main concerns and topics during unexpected driving behavior (emotion and evaluation, interpretation and reason, vehicle capability, interaction, future driving prediction and explanation request times). Based on these categories, we suggest design implications for autonomous vehicles, in particular related to collaboration insights, user mental models and explanation requests.
C1  - Oldenburg, Germany and New York, NY, USA
C3  - 22nd international conference on human-computer interaction with mobile devices and services
DA  - 2020///
PY  - 2020
DO  - 10.1145/3379503.3403554
PB  - Association for Computing Machinery
SN  - 978-1-4503-7516-0
UR  - https://doi.org/10.1145/3379503.3403554
KW  - Explainable AI
KW  - Autonomous driving.
KW  - Unexpected driving behavior
ER  - 

TY  - JOUR
TI  - When machines trade on corporate disclosures: Using text analytics for investment strategies
AU  - Schmitz, Hans Christian
AU  - Lutz, Bernhard
AU  - Wolff, Dominik
AU  - Neumann, Dirk
T2  - Decision Support Systems
DA  - 2023/02//
PY  - 2023
DO  - 10.1016/j.dss.2022.113892
VL  - 165
IS  - C
J2  - Decis. Support Syst.
SN  - 0167-9236
UR  - https://doi.org/10.1016/j.dss.2022.113892
KW  - Machine learning
KW  - Decision support
KW  - Corporate disclosures
KW  - Text mining
KW  - Trading strategies
ER  - 

TY  - JOUR
TI  - Tab2KG: Semantic table interpretation with lightweight semantic profiles
AU  - Alam, Mehwish
AU  - Buscaldi, Davide
AU  - Cochez, Michael
AU  - Osborne, Francesco
AU  - Recupero, Diego Reforgiato
AU  - Sack, Harald
AU  - Gottschalk, Simon
AU  - Demidova, Elena
AU  - Alam, Mehwish
AU  - Buscaldi, Davide
AU  - Cochez, Michael
AU  - Osborne, Francesco
AU  - Refogiato Recupero, Diego
AU  - Sack, Harald
T2  - Semant. Web
AB  - Tabular data plays an essential role in many data analytics and machine learning tasks. Typically, tabular data does not possess any machine-readable semantics. In this context, semantic table interpretation is crucial for making data analytics workflows more robust and explainable. This article proposes Tab2KG – a novel method that targets at the interpretation of tables with previously unseen data and automatically infers their semantics to transform them into semantic data graphs. We introduce original lightweight semantic profiles that enrich a domain ontology’s concepts and relations and represent domain and table characteristics. We propose a one-shot learning approach that relies on these profiles to map a tabular dataset containing previously unseen instances to a domain ontology. In contrast to the existing semantic table interpretation approaches, Tab2KG relies on the semantic profiles only and does not require any instance lookup. This property makes Tab2KG particularly suitable in the data analytics context, in which data tables typically contain new instances. Our experimental evaluation on several real-world datasets from different application domains demonstrates that Tab2KG outperforms state-of-the-art semantic table interpretation baselines.
DA  - 2022/01//
PY  - 2022
DO  - 10.3233/SW-222993
VL  - 13
IS  - 3
SP  - 571
EP  - 597
SN  - 1570-0844
UR  - https://doi.org/10.3233/SW-222993
KW  - domain knowledge graphs
KW  - one-shot learning
KW  - semantic profiles
KW  - Semantic table interpretation
ER  - 

TY  - JOUR
TI  - Reproducible experiments on three-dimensional entity resolution with JedAI
AU  - Mandilaras, George
AU  - Papadakis, George
AU  - Gagliardelli, Luca
AU  - Simonini, Giovanni
AU  - Thanos, Emmanouil
AU  - Giannakopoulos, George
AU  - Bergamaschi, Sonia
AU  - Palpanas, Themis
AU  - Koubarakis, Manolis
AU  - Lara-Clares, Alicia
AU  - Fariña, Antonio
DA  - 2021/12//
PY  - 2021
DO  - 10.1016/j.is.2021.101830
VL  - 102
IS  - C
J2  - Inf. Syst.
SN  - 0306-4379
UR  - https://doi.org/10.1016/j.is.2021.101830
KW  - Batch methods
KW  - Entity Resolution
KW  - Progressive methods
KW  - Reproducibility
ER  - 

TY  - CONF
TI  - Natural language interfaces with fine-grained user interaction: a case study on web apis
AU  - Su, Yu
AU  - Hassan Awadallah, Ahmed
AU  - Wang, Miaosen
AU  - White, Ryen W.
T3  - Sigir '18
AB  - The rapidly increasing ubiquity of computing puts a great demand on next-generation human-machine interfaces. Natural language interfaces, exemplified by virtual assistants like Apple Siri and Microsoft Cortana, are widely believed to be a promising direction. However, current natural language interfaces provide users with little help in case of incorrect interpretation of user commands. We hypothesize that the support of fine-grained user interaction can greatly improve the usability of natural language interfaces. In the specific setting of natural language interface to web APIs, we conduct a systematic study to verify our hypothesis. To facilitate this study, we propose a novel modular sequence-to-sequence model to create interactive natural language interfaces. By decomposing the complex prediction process of a typical sequence-to-sequence model into small, highly-specialized prediction units called modules, it becomes straightforward to explain the model prediction to the user, and solicit user feedback to correct possible prediction errors at a fine-grained level. We test our hypothesis by comparing an interactive natural language interface with its non-interactive version through both simulation and human subject experiments with real-world APIs. We show that with the interactive natural language interface, users can achieve a higher success rate and a lower task completion time, which lead to greatly improved user satisfaction.
C1  - Ann Arbor, MI, USA and New York, NY, USA
C3  - The 41st international ACM SIGIR conference on research &amp; development in information retrieval
DA  - 2018///
PY  - 2018
DO  - 10.1145/3209978.3210013
SP  - 855
EP  - 864
PB  - Association for Computing Machinery
SN  - 978-1-4503-5657-2
UR  - https://doi.org/10.1145/3209978.3210013
KW  - natural language interface
KW  - user feedback
KW  - web api
ER  - 

TY  - CONF
TI  - Toward explainable deep anomaly detection
AU  - Pang, Guansong
AU  - Aggarwal, Charu
T3  - Kdd '21
AB  - Anomaly explanation, also known as anomaly localization, is as important as, if not more than, anomaly detection in many real-world applications. However, it is challenging to build explainable detection models due to the lack of anomaly-supervisory information and the unbounded nature of anomaly; most existing studies exclusively focus on the detection task only, including the recently emerging deep learning-based anomaly detection that leverages neural networks to learn expressive low-dimensional representations or anomaly scores for the detection task. Deep learning models, including deep anomaly detection models, are often constructed as black boxes, which have been criticized for the lack of explainability of their prediction results. To tackle this explainability issue, there have been numerous techniques introduced over the years, many of which can be utilized or adapted to offer highly explainable detection results. This tutorial aims to present a comprehensive review of the advances in deep learning-based anomaly detection and explanation. We first review popular state-of-the-art deep anomaly detection methods from different categories of approaches, followed by the introduction of a number of principled approaches used to provide anomaly explanation for deep detection models. Through this tutorial, we aim to promote the development in algorithms, theories and evaluation of explainable deep anomaly detection in the machine learning and data mining community. The slides and other materials of the tutorial are made publicly available at https://tinyurl.com/explainableDeepAD.
C1  - Virtual Event, Singapore and New York, NY, USA
C3  - Proceedings of the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining
DA  - 2021///
PY  - 2021
DO  - 10.1145/3447548.3470794
SP  - 4056
EP  - 4057
PB  - Association for Computing Machinery
SN  - 978-1-4503-8332-5
UR  - https://doi.org/10.1145/3447548.3470794
KW  - deep learning
KW  - explainable machine learning
KW  - anomaly detection
KW  - anomaly explanation
KW  - anomaly localization
KW  - outlying feature selection
ER  - 

TY  - JOUR
TI  - MinMax fairness: from Rawlsian Theory of Justice to solution for algorithmic bias
AU  - Barsotti, Flavia
AU  - Koçer, Rüya Gökhan
T2  - AI Soc.
AB  - This paper presents an intuitive explanation about why and how Rawlsian Theory of Justice (Rawls in A theory of justice, Harvard University Press, Harvard, 1971) provides the foundations to a solution for algorithmic bias. The contribution of the paper is to discuss and show why Rawlsian ideas in their original form (e.g. the veil of ignorance, original position, and allowing inequalities that serve the worst-off) are relevant to operationalize fairness for algorithmic decision making. The paper also explains how this leads to a specific MinMaxfairness solution, which addresses the basic challenges of algorithmic justice. We combine substantive elements of Rawlsian perspective with an intuitive explanation in order to provide accessible and practical insights. The goal is to propose and motivate why and how the MinMaxfairness solution derived from Rawlsian principles overcomes some of the current challenges for algorithmic bias and highlight the benefits provided when compared to other approaches. The paper presents and discusses the solution by building a bridge between the qualitative theoretical aspects and the quantitative technical approach.
DA  - 2022/11//
PY  - 2022
DO  - 10.1007/s00146-022-01577-x
VL  - 39
IS  - 3
SP  - 961
EP  - 974
SN  - 0951-5666
UR  - https://doi.org/10.1007/s00146-022-01577-x
KW  - Ethics
KW  - Fairness
KW  - Justice
KW  - AI systems
KW  - Algorithmic bias
KW  - Rawlsian&nbsp
ER  - 

TY  - CONF
TI  - Gobo: a system for exploring user control of invisible algorithms in social media
AU  - Bhargava, Rahul
AU  - Chung, Anna
AU  - Gaikwad, Neil S.
AU  - Hope, Alexis
AU  - Jen, Dennis
AU  - Rubinovitz, Jasmin
AU  - Saldı́as-Fuentes, Belén
AU  - Zuckerman, Ethan
T3  - CSCW '19 companion
AB  - In recent years, there has been an unprecedented growth in content that is shared and presented on social media platforms. Along with this growth, however, there is an increasing concern over the lack of control social media users have on the content they are shown by invisible algorithms. In this paper, we introduce Gobo, an open-source social media browser system that enables users to manage and filter content from multiple platforms on their own. Gobo aims to help users control what's hidden from their feeds, add perspectives from outside their network to help them break filter bubbles, and explore why they see certain content on their feed. Through an iterative design process, we've built and deployed Gobo in the wild and conducted a pilot study in the form of a survey to understand how the users respond to the shift of control from invisible algorithms to themselves. Our initial findings suggest that Gobo has potential to provide an alternate design space to enhance control, transparency, and explainability in social media.
C1  - Austin, TX, USA and New York, NY, USA
C3  - Companion publication of the 2019 conference on computer supported cooperative work and social computing
DA  - 2019///
PY  - 2019
DO  - 10.1145/3311957.3359452
SP  - 151
EP  - 155
PB  - Association for Computing Machinery
SN  - 978-1-4503-6692-2
UR  - https://doi.org/10.1145/3311957.3359452
KW  - algorithmic accountability
KW  - algorithmic transparency
ER  - 

TY  - CONF
TI  - : Online spectral learning for single topic models
AU  - Yu, Tong
AU  - Kveton, Branislav
AU  - Wen, Zheng
AU  - Bui, Hung
AU  - Mengshoel, Ole J.
AB  - We study the problem of learning a latent variable model online from a stream of data. Latent variable models are popular because they can explain observed data through unobserved concepts. These models have traditionally been studied in the offline setting. In the online setting, online expectation maximization (EM) is arguably the most popular approach for learning latent variable models. Although online EM is computationally efficient, it typically converges to a local optimum. In this work, we develop a new online learning algorithm for latent variable models, which we call . converges to the global optimum, and we derive a sublinear upper bound on its n-step regret in a single topic model. In both synthetic and real-world experiments, we show that performs similarly to or better than online EM with tuned hyper-parameters.
C1  - Dublin, Ireland and Berlin, Heidelberg
C3  - Machine learning and knowledge discovery in databases: European conference, ECML PKDD 2018, dublin, ireland, september 10–14, 2018, proceedings, part II
DA  - 2018///
PY  - 2018
DO  - 10.1007/978-3-030-10928-8_23
SP  - 379
EP  - 395
PB  - Springer-Verlag
SN  - 978-3-030-10927-1
UR  - https://doi.org/10.1007/978-3-030-10928-8_23
KW  - Online learning
KW  - Spectral method
KW  - Topic models
ER  - 

TY  - JOUR
TI  - Less data, more knowledge: Building next-generation semantic communication networks
AU  - Chaccour, Christina
AU  - Saad, Walid
AU  - Debbah, Mérouane
AU  - Han, Zhu
AU  - Vincent Poor, H.
T2  - Communications Surveys and Tutorials
AB  - Semantic communication is viewed as a revolutionary paradigm that can potentially transform how we design and operate wireless communication systems. However, despite a recent surge of research activities in this area, remarkably, the research landscape is still limited in at least three ways. First and foremost, the definition of a “semantic communication system” is ambiguous and varies widely between different studies. This lack of consensus makes it challenging to develop rigorous and scalable frameworks for building semantic communication networks. Secondly, current approaches to building semantic communication networks are limited by their reliance on data-driven and information-driven AI-augmented networks. These networks remain “tied” to the data, which limits their ability to perform versatile logic. In contrast, knowledge-driven and reasoning-driven AI-native networks would allow for more flexible and powerful communication capabilities. However, there is currently a lack of technical foundations to support such networks. Thirdly, the concept of “semantic representation” is not well understood yet, and its role in embedding meaning and structure in data transferred across wireless network is still a subject of active research. The development of semantic representations that are minimalist, generalizable, and efficient is critical to enabling the transmitter and receiver to generate content via a minimally semantic representation. To address these limitations, in this tutorial, we propose the first rigorous and holistic vision of an end-to-end semantic communication network that is founded on novel concepts from artificial intelligence (AI), causal reasoning, transfer learning, and minimum description length theory. We first discuss how the design of semantic communication networks requires a move from data-driven AI-augmented networks, in which wireless networks remain “tied” to data, towards reasoning-driven AI-native networks which can perform versatile logic and generalizable intelligence. We then distinguish the concept of semantic communications from several other approaches that have been conflated with it. We opine that building effective and efficient semantic communication systems necessitates surpassing the creation of new encoder and decoder types at the transmitter/receiver side, or developing an “AI for wireless” framework that only extracts application features or fine-tunes wireless protocols/algorithms. Then, we identify the main tenets that are needed to build an end-to-end semantic communication network. Among those building blocks of a semantic communication network, we highlight the necessity of creating semantic representations of data that satisfy the key properties of minimalism, generalizability, and efficiency so as to faithfully represent the data and enable the transmitter and receiver to do more with less. We then explain how those representations can form the basis of a so-called semantic language that will allow a transmitter and receiver to communicate at a semantic level. We then concretely define the concept of reasoning by investigating the fundamentals of causal representation learning and their role in designing reasoning-driven semantic communication networks. For such reasoning-driven networks, we propose novel and essential semantic communication key performance indicators (KPIs) and metrics, including new “reasoning capacity” measures that could surpass Shannon’s bound to capture the imminent convergence of computing and communication resources. Finally, we explain how semantic communications can be scaled to large-scale networks such as 6G and beyond cellular networks. In a nutshell, we expect this tutorial to provide a unified and self-contained reference on how to properly build, design, analyze, and deploy next-generation semantic communication networks.
DA  - 2025/02//
PY  - 2025
DO  - 10.1109/COMST.2024.3412852
VL  - 27
IS  - 1
SP  - 37
EP  - 76
J2  - Commun. Surveys Tuts.
SN  - 1553-877X
UR  - https://doi.org/10.1109/COMST.2024.3412852
ER  - 

TY  - CONF
TI  - ConceptX: a framework for latent concept analysis
AU  - Alam, Firoj
AU  - Dalvi, Fahim
AU  - Durrani, Nadir
AU  - Sajjad, Hassan
AU  - Khan, Abdul Rafae
AU  - Xu, Jia
T3  - AAAI'23/IAAI'23/EAAI'23
AB  - The opacity of deep neural networks remains a challenge in deploying solutions where explanation is as important as precision. We present ConceptX, a human-in-the-loop framework for interpreting and annotating latent representational space in pre-trained Language Models (pLMs). We use an unsupervised method to discover concepts learned in these models and enable a graphical interface for humans to generate explanations for the concepts. To facilitate the process, we provide auto-annotations of the concepts (based on traditional linguistic ontologies). Such annotations enable development of a linguistic resource that directly represents latent concepts learned within deep NLP models. These include not just traditional linguistic concepts, but also task-specific or sensitive concepts (words grouped based on gender or religious connotation) that helps the annotators to mark bias in the model. The framework consists of two parts (i) concept discovery and (ii) annotation platform.
C3  - Proceedings of the thirty-seventh AAAI conference on artificial intelligence and thirty-fifth conference on innovative applications of artificial intelligence and thirteenth symposium on educational advances in artificial intelligence
DA  - 2023///
PY  - 2023
DO  - 10.1609/aaai.v37i13.27057
PB  - AAAI Press
SN  - 978-1-57735-880-0
UR  - https://doi.org/10.1609/aaai.v37i13.27057
ER  - 

TY  - THES
TI  - Towards human-centered recommender systems
AU  - Fu, Zuohui
AU  - Hao, Wang
AU  - Qingyao, Ai
AB  - Recently, there has been extensive interest in developing intelligent human-centered AI (artificial intelligence) systems that support human participation so as to facilitate cooperation between humans and machines. As one of the typical decision making paradigm in AI, recommender systems which have become an integral part of our lives, are a particularly pervasive form of AI system that can aid in decision-making in the face of ever-growing amounts of information. It now becomes imaginable and achievable with the help of advanced artificial intelligence, especially the modern deep learning based recommender systems that is known for its superior representation and predictive power, have made great strides in accuracy and effectiveness. Meanwhile, it also raises a number of important challenges:1) How can we actively incorporate human participation into the decision-making procedure of recommender systems? It aims to integrate human participation as guidance to keep the decision-making process consistent with human feedback to maintain the trustworthiness to human beings. 2) How can we ensure that explanations are provided such that users can better understand why particular items are being recommended? In this aspect, explainable recommendation can be leveraged to not only assist the agent to provide high-quality recommendation results but also offering personalized and intuitive explanations with better user engagement, which are important for several modern recommender systems such as e-commerce and social media platforms etc. 3) How can we alleviate biases in recommender systems? Seldom progress has been explored to mitigate the biases that arise in human-centered recommender systems so as to hurt user satisfaction and trust towards the recommendation service.In this thesis, we proposes several novel methods to fill these gaps. In particular, for improved human understanding, we introduce an adversarial semantic learning framework for cross-lingual settings understanding. For human integration, a human-in-the-loop conversational recommender system with external graph structure is introduced. To ensure fair explanations, we mitigate the unfairness within graph-based explainable reasoning in the recommender system. Finally, for human-system cooperation, we present a popularity debiasing framework to integrate user interaction and debiased dialogue stat management in a conversational recommender system.We not only extensively evaluate our proposed approaches on multiple real-world recommendation datasets, but also contribute open public datasets to the community. The experimental results demonstrate the effectiveness of the proposed methods in achieving satisfying prediction accuracy, mitigating bias, and providing users with understandable explanations.
DA  - 2021///
PY  - 2021
M3  - phd
PB  - Rutgers The State University of New Jersey, School of Graduate Studies
N1  - AAI28771459
ER  - 

TY  - CONF
TI  - Robot or&nbsp;employee? Exploring people’s choice for&nbsp;or&nbsp;against an&nbsp;interaction with&nbsp;a&nbsp;social robot
AU  - Finkel, Marcel
AU  - Timm, Lara
AU  - Erle, Lukas
AU  - Arntz, Alexander
AU  - Helgert, André
AU  - Straßmann, Carolin
AU  - Eimler, Sabrina C.
AB  - Employing social robots in public spaces to support employees at work is a frequently discussed scenario. However, the success of robotic systems often depends on people’s willingness to initiate interactions with them. This makes understanding people’s usage decisions crucial, yet only limited research has been done on why people select publicly accessible social robots over alternatives, such as human employees. Amongst various factors, people’s diversity characteristics are likely to influence this decision, such as people’s locus of control when using technology and their self-efficacy in human-robot interaction. To investigate this choice for or against using a robot, a field study (N = 65) was conducted in two public libraries in the Ruhr area (Germany). Participants had to decide to interact with a robot or an employee and were subsequently asked to explain their decision via a questionnaire and an interview. Results reveal that the decision could neither be explained by people’s locus of control when using technology nor by other diversity characteristics. Furthermore, no significant differences in self-efficacy in human-robot interaction between users who chose the robot instead of the human employee were found. Finally, the qualitative findings point to general interest in robots and people’s differences in dealing with novelty as reasons for their choice. Overall, our findings offer insights into the decision for or against the usage of a robot, which are relevant to both, research and the deployment of social robots in public spaces.
C1  - Odense, Denmark and Berlin, Heidelberg
C3  - Social robotics: 16th international conference, ICSR + AI 2024, odense, denmark, october 23–26, 2024, proceedings, part I
DA  - 2025///
PY  - 2025
DO  - 10.1007/978-981-96-3522-1_38
SP  - 446
EP  - 459
PB  - Springer-Verlag
SN  - 978-981-96-3521-4
UR  - https://doi.org/10.1007/978-981-96-3522-1_38
KW  - human-robot interaction
KW  - diversity characteristics
KW  - field study
KW  - locus of control
KW  - self-efficacy
KW  - use choice
ER  - 

TY  - CONF
TI  - The effects of example-based explanations in a machine learning interface
AU  - Cai, Carrie J.
AU  - Jongejan, Jonas
AU  - Holbrook, Jess
T3  - Iui '19
AB  - The black-box nature of machine learning algorithms can make their predictions difficult to understand and explain to end-users. In this paper, we propose and evaluate two kinds of example-based explanations in the visual domain, normative explanations and comparative explanations (Figure 1), which automatically surface examples from the training set of a deep neural net sketch-recognition algorithm. To investigate their effects, we deployed these explanations to 1150 users on QuickDraw, an online platform where users draw images and see whether a recognizer has correctly guessed the intended drawing. When the algorithm failed to recognize the drawing, those who received normative explanations felt they had a better understanding of the system, and perceived the system to have higher capability. However, comparative explanations did not always improve perceptions of the algorithm, possibly because they sometimes exposed limitations of the algorithm and may have led to surprise. These findings suggest that examples can serve as a vehicle for explaining algorithmic behavior, but point to relative advantages and disadvantages of using different kinds of examples, depending on the goal.
C1  - Marina del Ray, California and New York, NY, USA
C3  - Proceedings of the 24th international conference on intelligent user interfaces
DA  - 2019///
PY  - 2019
DO  - 10.1145/3301275.3302289
SP  - 258
EP  - 262
PB  - Association for Computing Machinery
SN  - 978-1-4503-6272-6
UR  - https://doi.org/10.1145/3301275.3302289
KW  - machine learning
KW  - human-AI interaction
KW  - explainable AI
KW  - example-based explanations
ER  - 

TY  - CONF
TI  - From undergraduate (software) capstone projects to start-ups: Challenges and opportunities in higher institutions of learning
AU  - Ogenrwot, Daniel
AU  - Tabo, Geoffrey Olok
AU  - Aber, Kevin
AU  - Nakatumba-Nabende, Joyce
T3  - Famecse '22
AB  - The capstone project is a fundamental part of almost all science and engineering degrees. It is not only a requirement for the partial fulfillment of an accredited university programme but also a method of assessing the students’ general mastery of concepts, critical thinking, problem-solving, and transferable skills. Annually, final-year undergraduate students offering computing programmes in Uganda build innovative software solutions to real-world problems within and outside their community. Anecdotal evidence indicates that most of those innovations have the potential for commercialization and transformation into technology-based businesses. However, limited progress has been made to commercialize students’ projects, and promising solutions are “buried” within academic reports. To this end, our research aims to explain the challenges and opportunities in the commercialization of students’ capstone projects across two (2) undergraduate computing programmes (Bachelor of Science in Computer Science and Bachelor of Information Technology) offered at Gulu University in Uganda. Using exploratory research design, we reviewed eighty-six (86) capstone projects, curricula, and a facilitated students &amp; stakeholders’ workshop report. This paper articulates factors hindering the commercialization of undergraduate software capstone projects and recommends mitigating measures. It also proposes a framework for extending capstone course design from a traditional curriculum structure to an inclusive industry and community-oriented approach capable of turning ideas into business start-ups. The findings from this research are expected to inform higher institutions of learning in Africa in developing novel pedagogical approaches for orchestrating (software) capstone project courses that are inclusive and profitable beyond the academic setting.
C1  - Cairo-Kampala, Egypt and New York, NY, USA
C3  - Proceedings of the federated africa and middle east conference on software engineering
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531056.3542775
SP  - 73
EP  - 82
PB  - Association for Computing Machinery
SN  - 978-1-4503-9663-9
UR  - https://doi.org/10.1145/3531056.3542775
KW  - Software Engineering
KW  - Capstone Projects
KW  - Commercialization
KW  - Start-ups
ER  - 

TY  - THES
TI  - Three maxims for developing human-centered AI for decision making
AU  - Bansal, Gagan
AU  - Jamie, Morgenstern
AU  - Besmira, Nushi
AB  - We focus on AI-advised decision making, where AI systems (e.g., classifiers) are deployed to assist users to make better decisions (e.g., in healthcare, finance, and criminal justice). While the dominant development practice deploys the most "accurate" autonomous AI to assist users, we argue that in order for AI to augment users, we should shift the focus of research to developing human-centered AI (HCAI). HCAI systems additional requirements atop those of autonomous AI. They are not just capable but also: trustworthy and dependable, they communicate and coordinate their reasoning with users, and complement users' expertise. We specifically develop and study three relevant maxims for developing HCAI systems: 1) help users understand when to trust AI recommendations, 2) preserve user's mental model of AI's trustworthiness, and 3) train AI to optimize for team performance.Through experiments on various tasks that involve AI-assisted decision making, we show that a) contrary to expectations, current XAI methods may be insufficient for helping users understand when to rely on AI recommendations. b) It is easier for users to create a mental model of AI's trustworthiness when its error boundary (i.e., regions where it errs) is simple and deterministic. c) The current practice of updates to AI systems (e.g., to improve its accuracy) can result in models that violate user trust, e.g., by introducing errors on examples on which the system was previously correct; however, we also show that its possible to create models that preserve trust by considering compatibility of updates during the training process. d) For a simple setting, we formally show that by accommodating the user's mental model in the AI's training process, we can train a model that results in higher a human-AI team performance than the team performance achieved with the most accurate AI. Finally, we discuss open problems and future work in developing HCAI including enabling explanatory dialogs (as opposed to static, one-shot explanations) and enabling user control of AI behavior. Overall, the problems and results in this thesis show the richness and interdisciplinary nature of the challenge of developing human-centered AI.
DA  - 2022///
PY  - 2022
M3  - phd
PB  - University of Washington
N1  - AAI28869734
ER  - 

TY  - CONF
TI  - KANDINSKY patterns as IQ-test for machine learning
AU  - Holzinger, Andreas
AU  - Kickmeier-Rust, Michael
AU  - Müller, Heimo
AB  - AI follows the notion of human intelligence which is unfortunately not a clearly defined term. The most common definition given by cognitive science as mental capability, includes, among others, the ability to think abstract, to reason, and to solve problems from the real world. A hot topic in current AI/machine learning research is to find out whether and to what extent algorithms are able to learn abstract thinking and reasoning similarly as humans can do – or whether the learning outcome remains on purely statistical correlation. In this paper we provide some background on testing intelligence, report some preliminary results from 271 participants of our online study on explainability, and propose to use our Kandinsky Patterns as an IQ-Test for machines. Kandinsky Patterns are mathematically describable, simple, self-contained hence controllable test data sets for the development, validation and training of explainability in AI. Kandinsky Patterns are at the same time easily distinguishable from human observers. Consequently, controlled patterns can be described by both humans and computers. The results of our study show that the majority of human explanations was made based on the properties of individual elements in an image (i.e., shape, color, size) and the appearance of individual objects (number). Comparisons of elements (e.g., more, less, bigger, smaller, etc.) were significantly less likely and the location of objects, interestingly, played almost no role in the explanation of the images. The next step is to compare these explanations with machine explanations.
C1  - Canterbury, United Kingdom and Berlin, Heidelberg
C3  - Machine learning and knowledge extraction: Third IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 international cross-domain conference, CD-MAKE 2019, canterbury, UK, august 26–29, 2019, proceedings
DA  - 2019///
PY  - 2019
DO  - 10.1007/978-3-030-29726-8_1
SP  - 1
EP  - 14
PB  - Springer-Verlag
SN  - 978-3-030-29725-1
UR  - https://doi.org/10.1007/978-3-030-29726-8_1
KW  - Artificial intelligence
KW  - Explainable-AI
KW  - Human intelligence
KW  - Intelligence testing
KW  - Interpretable machine learning
KW  - IQ-Test
ER  - 

TY  - CONF
TI  - Providing explanations for recommendations in reciprocal environments
AU  - Kleinerman, Akiva
AU  - Rosenfeld, Ariel
AU  - Kraus, Sarit
T3  - RecSys '18
AB  - Automated platforms which support users in finding a mutually beneficial match, such as online dating and job recruitment sites, are becoming increasingly popular. These platforms often include recommender systems that assist users in finding a suitable match. While recommender systems which provide explanations for their recommendations have shown many benefits, explanation methods have yet to be adapted and tested in recommending suitable matches. In this paper, we introduce and extensively evaluate the use of "reciprocal explanations" - explanations which provide reasoning as to why both parties are expected to benefit from the match. Through an extensive empirical evaluation, in both simulated and real-world dating platforms with 287 human participants, we find that when the acceptance of a recommendation involves a significant cost (e.g., monetary or emotional), reciprocal explanations outperform standard explanation methods, which consider the recommendation receiver alone. However, contrary to what one may expect, when the cost of accepting a recommendation is negligible, reciprocal explanations are shown to be less effective than the traditional explanation methods.
C1  - Vancouver, British Columbia, Canada and New York, NY, USA
C3  - Proceedings of the 12th ACM conference on recommender systems
DA  - 2018///
PY  - 2018
DO  - 10.1145/3240323.3240362
SP  - 22
EP  - 30
PB  - Association for Computing Machinery
SN  - 978-1-4503-5901-6
UR  - https://doi.org/10.1145/3240323.3240362
KW  - explanations
KW  - online-dating application
KW  - reciprocal recommender systems
ER  - 

TY  - CONF
TI  - Is there a trade-off between fairness and accuracy? a perspective using mismatched hypothesis testing
AU  - Dutta, Sanghamitra
AU  - Wei, Dennis
AU  - Yueksel, Hazar
AU  - Chen, Pin-Yu
AU  - Liu, Sijia
AU  - Varshney, Kush R.
T3  - ICML'20
AB  - A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.
C3  - Proceedings of the 37th international conference on machine learning
DA  - 2020///
PY  - 2020
PB  - JMLR.org
ER  - 

TY  - JOUR
TI  - Learning interpretable multi-class models by means of hierarchical decomposition: Threshold Control for Nested Dichotomies
AU  - Fdez-Sánchez, J.A.
AU  - Pascual-Triana, J.D.
AU  - Fernández, A.
AU  - Herrera, F.
T2  - Neurocomput.
DA  - 2021/11//
PY  - 2021
DO  - 10.1016/j.neucom.2021.07.097
VL  - 463
IS  - C
SP  - 514
EP  - 524
SN  - 0925-2312
UR  - https://doi.org/10.1016/j.neucom.2021.07.097
KW  - Transparency
KW  - Hierarchical decomposition
KW  - Interpretability and explainability
KW  - Multi-class classification
KW  - Nested Dichotomies
ER  - 

TY  - THES
TI  - The blessings of explainable ai in operations &amp; maintenance of wind turbines
AU  - Chatterjee, Joyjit
AB  - Wind turbines play an integral role in generating clean energy, but regularly suffer from operational inconsistencies and failures leading to unexpected downtimes and significant Operations &amp; Maintenance (O&amp;M) costs. Condition-Based Monitoring (CBM) has been utilised in the past to monitor operational inconsistencies in turbines by applying signal processing techniques to vibration data. The last decade has witnessed growing interest in leveraging Supervisory Control &amp; Acquisition (SCADA) data from turbine sensors towards CBM. Machine Learning (ML) techniques have been utilised to predict incipient faults in turbines and forecast vital operational parameters with high accuracy by leveraging SCADA data and alarm logs. More recently, Deep Learning (DL) methods have outperformed conventional ML techniques, particularly for anomaly prediction. Despite demonstrating immense promise in transitioning to Artificial Intelligence (AI), such models are generally black-boxes that cannot provide rationales behind their predictions, hampering the ability of turbine operators to rely on automated decision making. We aim to help combat this challenge by providing a novel perspective on Explainable AI (XAI) for trustworthy decision support. This thesis revolves around three key strands of XAI - DL, Natural Language Generation (NLG) and Knowledge Graphs (KGs), which are investigated by utilising data from an operational turbine. We leverage DL and NLG to predict incipient faults and alarm events in the turbine in natural language as well as generate human-intelligible O&amp;M strategies to assist engineers in fixing/averting the faults. We also propose specialised DL models which can predict causal relationships in SCADA features as well as quantify the importance of vital parameters leading to failures. The thesis finally culminates with an interactive Question- Answering (QA) system for automated reasoning that leverages multimodal domain-specific information from a KG, facilitating engineers to retrieve O&amp;M strategies with natural language questions. By helping make turbines more reliable, we envisage wider adoption of wind energy sources towards tackling climate change.
DA  - 2021///
PY  - 2021
M3  - phd
PB  - University of Hull (United Kingdom)
N1  - AAI29350071
ER  - 

TY  - JOUR
TI  - Discovering underlying plans based on shallow models
AU  - Zhuo, Hankz Hankui
AU  - Zha, Yantian
AU  - Kambhampati, Subbarao
AU  - Tian, Xin
T2  - ACM Transactions on Intelligent Systems and Technology
AB  - Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or action models in hand. Previous approaches either discover plans by maximally “matching” observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing action models to best explain the observed actions, assuming that complete action models are available. In real-world applications, however, target plans are often not from plan libraries, and complete action models are often not available, since building complete sets of plans and complete action models are often difficult or expensive. In this article, we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Specifically, we propose two approaches, DUP and RNNPlanner, to discover target plans based on vector representations of actions. DUP explores the EM-style (Expectation Maximization) framework to capture local contexts of actions and discover target plans by optimizing the probability of target plans, while RNNPlanner aims to leverage long-short term contexts of actions based on RNNs (Recurrent Neural Networks) framework to help recognize target plans. In the experiments, we empirically show that our approaches are capable of discovering underlying plans that are not from plan libraries without requiring action models provided. We demonstrate the effectiveness of our approaches by comparing its performance to traditional plan recognition approaches in three planning domains. We also compare DUP and RNNPlanner to see their advantages and disadvantages.
DA  - 2020/01//
PY  - 2020
DO  - 10.1145/3368270
VL  - 11
IS  - 2
J2  - ACM Trans. Intell. Syst. Technol.
SN  - 2157-6904
UR  - https://doi.org/10.1145/3368270
KW  - action representation
KW  - Plan recognition
KW  - recurrent neural networks
KW  - shallow model
ER  - 

TY  - CONF
TI  - Increasing accuracy of automated essay grading by grouping similar graders
AU  - Zupanc, Kaja
AU  - Bosnić, Zoran
T3  - Wims '18
AB  - Automated essay evaluation is a widely used practical solution for replacing time-consuming manual grading of student essays. Automated systems are used in combination with human graders in different high-stake assessments, where grading models are learned on essays datasets scored by different graders. Despite the unified grading rules, human graders can unintentionally introduce subjective bias into scores. Consequently, a grading model has to learn from a data that represents a noisy relationship between essay attributes and its grade. We propose an approach for separating a set of essays into subsets that represent similar graders, which uses an explanation methodology and clustering. The results confirm our assumption that learning from the ensemble of separated models can significantly improve the average prediction accuracy on artificial and real-world datasets.
C1  - Novi Sad, Serbia and New York, NY, USA
C3  - Proceedings of the 8th international conference on web intelligence, mining and semantics
DA  - 2018///
PY  - 2018
DO  - 10.1145/3227609.3227645
PB  - Association for Computing Machinery
SN  - 978-1-4503-5489-9
UR  - https://doi.org/10.1145/3227609.3227645
KW  - automated essay evaluation
KW  - clustering
KW  - explanations of predictions
KW  - prediction accuracy
ER  - 

TY  - JOUR
TI  - A framework for objective image quality measures based on intuitionistic fuzzy sets
AU  - Hassaballah, M.
AU  - Ghareeb, A.
T2  - Applied Soft Computing
AB  - Graphical abstractDisplay Omitted HighlightsThis paper introduces a framework for using intuitionistic fuzzy sets (IFSs) theory in image processing, specifically in image comparison.Existing similarity measures on the IFSs space are discussed and highlighted their properties.This paper also introduces an intuitionistic fuzzy based image quality index measure.Besides, construction of neighborhood-based similarity is also proposed for improving the perceived visual quality of these IFS-based similarity measures.The proposed framework is tested on real world images under various types of image distortions and the obtained results are encourages. Measuring the distance or similarity objectively between images is an essential and a challenging problem in various image processing and pattern recognition applications. As it is very difficult to find a certain measure that can be successfully applied to all kinds of images comparisons-related problems in the same time, it is appropriate to look for new approaches for measuring the similarity. Several similarity measures tested on numerical cases are developed in the literature based on intuitionistic fuzzy sets (IFSs) without evaluation on real data. This paper introduces a framework for using the similarity measures on IFSs in image processing field, specifically for image comparison. First, some existing similarity measures are discussed and highlighted their properties. Then, modeling digital images using IFSs is explained. Moreover, the paper introduces an intuitionistic fuzzy based image quality index measure. Second, for improving the perceived visual quality of these IFS-based similarity measures, construction of neighborhood-based similarity is proposed, which takes into consideration homogeneity of images. Finally, the proposed framework is verified on real world natural images under various types of image distortions. Experimental results confirm the effectiveness of the proposed framework in measuring the similarity between images.
DA  - 2017/08//
PY  - 2017
DO  - 10.1016/j.asoc.2017.03.046
VL  - 57
IS  - C
SP  - 48
EP  - 59
J2  - Appl. Soft Comput.
SN  - 1568-4946
UR  - https://doi.org/10.1016/j.asoc.2017.03.046
KW  - Image comparison
KW  - Image processing
KW  - Image similarity
KW  - Intuitionistic fuzzy sets
KW  - Similarity measures
ER  - 

TY  - JOUR
TI  - Value of algorithm-enabled process innovation: The case of sepsis
AU  - Adjerid, Idris
AU  - Ayvaci, Mehmet U. S.
AU  - Özer, Özalp
T2  - Manufacturing &amp; Service Operations Management
AB  - Problem definition: Algorithm-enabled decision support has an increasingly important role in supporting the day-to-day operations of healthcare organizations. Yet, fully realizing the value of algorithmic decision support lies critically in the opportunity to re-engineer the related processes and redefine roles in ways that make organizations more effective. We study how and when algorithm-enabled process innovation (AEPI) creates value in light of dynamic operational environments (i.e., workload) and behavioral responses to algorithmic predictions (i.e., algorithmic accuracy). Our context is an AEPI effort around a rule-based decision-support algorithm for early detection of sepsis—a costly condition that is the leading cause of death for hospitalized patients. We collaborated with a large U.S.-based hospital system and examined whether AEPI developed for sepsis care (sepsis AEPI) impacts patient mortality and when this impact is stronger or weaker. Methodology/results: We utilize a rich set of clinical and nonclinical data in empirically examining the impact of sepsis AEPI on patient mortality. We leverage the staggered implementation of sepsis AEPI across hospital units and conduct our estimation on a carefully matched sample. The matching utilizes data on patient vitals and the logic behind the algorithm to create a robust comparison group consisting of patient visits for which sepsis AEPI would have triggered an alert if it had been in place. Our empirical analysis shows that sepsis AEPI reduces the likelihood of death from sepsis (45
DA  - 2023/07//
PY  - 2023
DO  - 10.1287/msom.2023.1226
VL  - 25
IS  - 4
SP  - 1545
EP  - 1566
SN  - 1526-5498
UR  - https://doi.org/10.1287/msom.2023.1226
KW  - algorithms
KW  - compliance
KW  - data-driven decisions
KW  - healthcare operations
KW  - process innovation
KW  - sepsis
ER  - 

TY  - CONF
TI  - ProtoAI: Model-informed prototyping for AI-powered interfaces
AU  - Subramonyam, Hariharan
AU  - Seifert, Colleen
AU  - Adar, Eytan
T3  - Iui '21
AB  - When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. However, AI poses challenges to design due to its dynamic behavior in response to training data, end-user data, and feedback. Designers must consider AI’s uncertainties and offer adaptations such as explainability, error recovery, and automation vs. human task control. Unfortunately, current prototyping tools assume a black-box view of AI, forcing designers to work with separate tools to explore machine learning models, understand model performance, and align interface choices with model behavior. This introduces friction to rapid and iterative prototyping. We propose Model-Informed Prototyping (MIP), a workflow for AIX design that combines model exploration with UI prototyping tasks. Our system, ProtoAI, allows designers to directly incorporate model outputs into interface designs, evaluate design choices across different inputs, and iteratively revise designs by analyzing model breakdowns. We demonstrate how ProtoAI can readily operationalize human-AI design guidelines. Our user study finds that designers can effectively engage in MIP to create and evaluate AI-powered interfaces during AIX design.
C1  - College Station, TX, USA and New York, NY, USA
C3  - Proceedings of the 26th international conference on intelligent user interfaces
DA  - 2021///
PY  - 2021
DO  - 10.1145/3397481.3450640
SP  - 48
EP  - 58
PB  - Association for Computing Machinery
SN  - 978-1-4503-8017-1
UR  - https://doi.org/10.1145/3397481.3450640
KW  - Human-Centered AI
KW  - AI-Powered Interfaces
KW  - Design-by-Instance
ER  - 

TY  - JOUR
TI  - Property specification patterns at work: verification and inconsistency explanation
AU  - Narizzano, Massimo
AU  - Pulina, Luca
AU  - Tacchella, Armando
AU  - Vuotto, Simone
AB  - Property specification patterns (PSPs) have been proposed to ease the formalization of requirements, yet enable automated verification thereof. In particular, the internal consistency of specifications written with PSPs can be checked automatically with the use of, for example, linear temporal logic (LTL) satisfiability solvers. However, for most practical applications, the expressiveness of PSPs is too restricted to enable writing useful requirement specifications, and proving that a set of requirements is inconsistent can be worthless unless a minimal set of conflicting requirements is extracted to help designers to correct a wrong specification. In this paper, we extend PSPs by considering Boolean as well as atomic numerical assertions, we contribute an encoding from extended PSPs to LTL formulas, and we present an algorithm computing inconsistency explanations, i.e., irreducible inconsistent subsets of the original set of requirements. Our extension enables us to reason about the internal consistency of functional requirements which would not be captured by basic PSPs. Experimental results demonstrate that our approach can check and explain (in)consistencies in specifications with nearly two thousand requirements generated using a probabilistic model, and that it enables effective handling of real-world case studies.
DA  - 2019/09//
PY  - 2019
DO  - 10.1007/s11334-019-00339-1
VL  - 15
IS  - 3–4
SP  - 307
EP  - 323
J2  - Innov. Syst. Softw. Eng.
SN  - 1614-5046
UR  - https://doi.org/10.1007/s11334-019-00339-1
KW  - Consistency of requirements
KW  - Inconsistency explanation
KW  - LTL satisfiability checking
KW  - Property specifications patterns
ER  - 

TY  - CONF
TI  - Generic automated lead ranking in dynamics CRM
AU  - Ronen, Royi
AU  - Berezin, Hilik
AU  - Preizler, Rotem
AU  - Kasturi, Gopal
AU  - Ezzour, AJ
AU  - Bhanavase, Sayalee
AU  - Hauon, Edan
AU  - Nir, Oron
T3  - RecSys '21
AB  - We developed a generic framework which enables Customer Relationship Management (CRM) organizations to deploy an automated ranking system for leads (commonly known as ‘lead scoring’). Leads are records that represent non-customers who might become customers. Lead ranking is a fundamental CRM problem with many flavors. Ranking serves as a prioritization management tool for CRM organizations, with many characteristics similar to those of recommender systems.We present the system with its most recent developments, emphasizing challenges that go beyond the core of the learning algorithm, and that have played an instrumental role in maturing the system into a trustable feature, robust to different types of organizations and datasets. Particularly, we present features which enable Human in the Loop [1], a dominant concept in both configuration and result consumption. Another type of features demonstrates the addition of domain knowledge into the machine learning based process.We present the concepts of feature selection, with and without human help, prediction explanations, insights on model inputs, data quality issues, training for UX consistency, and actionability for each individual prediction.
C1  - Amsterdam, Netherlands and New York, NY, USA
C3  - Proceedings of the 15th ACM conference on recommender systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3460231.3478880
SP  - 757
EP  - 759
PB  - Association for Computing Machinery
SN  - 978-1-4503-8458-2
UR  - https://doi.org/10.1145/3460231.3478880
ER  - 

TY  - CONF
TI  - Discovering underlying plans based on distributed representations of actions
AU  - Tian, Xin
AU  - Zhuo, Hankz Hankui
AU  - Kambhampati, Subbarao
T3  - Aamas '16
AB  - Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or domain models in hand. Previous approaches either discover plans by maximally "matching" observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing domain models to best explain the observed actions, assuming complete domain models are available. In real world applications, however, target plans are often not from plan libraries and complete domain models are often not available, since building complete sets of plans and complete domain models are often difficult or expensive. In this paper we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Our approach is capable of discovering underlying plans that are not from plan libraries, without requiring domain models provided. We empirically demonstrate the effectiveness of our approach by comparing its performance to traditional plan recognition approaches in three planning domains.
C1  - Singapore, Singapore and Richland, SC
C3  - Proceedings of the 2016 international conference on autonomous agents &amp; multiagent systems
DA  - 2016///
PY  - 2016
SP  - 1135
EP  - 1143
PB  - International Foundation for Autonomous Agents and Multiagent Systems
SN  - 978-1-4503-4239-1
KW  - distributed representation
KW  - plan recognition
KW  - planning
ER  - 

TY  - JOUR
TI  - ERP implementation failures: a case study and analysis
AU  - Chakravorty, Satya S.
AU  - Dulaney, Ronald E.
AU  - Franza, Richard M.
AB  - Despite the pervasiveness of ERP systems, there is a serious concern regarding the failure of ERP implementations. One explanation for many ERP implementation failures may be 'escalation of commitment'. Escalation of commitment refers to the propensity of decision-makers to continue investing in a failing course of action. Using escalation of commitment as a framework, this research describes an ERP implementation failure in a packaging manufacturing company and makes two significant contributions. First, this research describes the dynamics of 'real world' ERP implementation failures. Second, in studying these dynamics, relevant insights for improving ERP implementation successes are provided for both academicians and practioners.
DA  - 2016/03//
PY  - 2016
DO  - 10.1504/IJBIS.2016.075256
VL  - 21
IS  - 4
SP  - 462
EP  - 476
J2  - Int. J. Bus. Inf. Syst.
SN  - 1746-0972
UR  - https://doi.org/10.1504/IJBIS.2016.075256
ER  - 

TY  - CONF
TI  - Establishment and validation of flight crew training cost model
AU  - Li, Xianxue
AU  - Song, Tingying
AB  - As one of the important part of civil aircraft’s life cycle cost, training cost is the key factor for manufacturer’s consideration of market competitiveness and for airline’s aircraft procurement. A mathematical model of flight crew training cost is established to study the impact of cockpit layout, human machine interface and operational procedure to different type of aircraft’s flight crew training cost. This model is validated by using typical aircraft’s training cost and can be used to explain and predict flight crew’s training time.
C1  - Berlin, Heidelberg
C3  - HCI in mobility, transport, and automotive systems: 4th international conference, MobiTAS 2022, held as part of the 24th HCI international conference, HCII 2022, virtual event, june 26 – july 1, 2022, proceedings
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-04987-3_4
SP  - 62
EP  - 71
PB  - Springer-Verlag
SN  - 978-3-031-04986-6
UR  - https://doi.org/10.1007/978-3-031-04987-3_4
KW  - Aircraft
KW  - Model
KW  - Training cost
ER  - 

TY  - JOUR
TI  - Comparative criteria for partially observable contingent planning
AU  - Shmaryahu, Dorin
AU  - Shani, Guy
AU  - Hoffmann, Jörg
T2  - Autonomous Agents and Multi-Agent Systems
AB  - In contingent planning under partial observability with sensing actions, agents actively use sensing to discover meaningful facts about the world. The solution can be represented as a plan tree or graph, branching on various possible observations. Typically in contingent planning one seeks a satisfying plan leading to a goal state at each leaf. In many applications, however, one may prefer some satisfying plans to others, such as plans that lead to the goal with a lower average cost. However, methods such as average cost make an implicit assumption concerning the probabilities of outcomes, which may not apply when the stochastic dynamics of the environment are unknown. We focus on the problem of providing valid comparative criteria for contingent plan trees and graphs, allowing us to compare two plans and decide which one is preferable. We suggest a set of such comparison criteria—plan simplicity, dominance, and best and worst plan costs.We also argue that in some cases certain branches of the plan correspond to an unlikely combination of mishaps, and can be ignored, and provide methods for pruning such unlikely branches before comparing the plan graphs. We explain these criteria, and discuss their validity, correlations, and application to real world problems. We also suggest efficient algorithms for computing the comparative criteria where needed. We provide experimental results, showing that existing contingent planners provide diverse plans, that can be compared using these criteria.
DA  - 2019/09//
PY  - 2019
DO  - 10.1007/s10458-019-09406-0
VL  - 33
IS  - 5
SP  - 481
EP  - 517
SN  - 1387-2532
UR  - https://doi.org/10.1007/s10458-019-09406-0
KW  - Comparative Criteria
KW  - Contingent planning
KW  - Partial observability
KW  - Plan tree
KW  - Planning
ER  - 

TY  - CONF
TI  - Study on the impact of service robot autonomy on customer satisfaction
AU  - Li, Keli
AU  - Li, Guoxin
AB  - The rapid development of artificial intelligence technology has accelerated the promotion and application of service robot in the market. Although technology has provided service robot with increasingly autonomous functions, more research is needed on how service robot with different levels of autonomy affects customer satisfaction in service scenarios. Guided by the theory of affordance, this study examined whether service robot operational and decisional autonomy would have effects on customer satisfaction and explored explanatory mechanism. Adopting an experimental vignette method (EVM), the study reveals that direct effect of service robot operational autonomy and indirect effect of decisional autonomy on customer satisfaction, and functional affordance played a positive mediating role in the impact of service robot autonomy on customer satisfaction. The results extend and enrich the relevant literature on human-machine interaction and customer satisfaction research. Our results also provide marketing insights for enterprises to improve autonomous robot design and enhance customer relationships.
C1  - Copenhagen, Denmark and Berlin, Heidelberg
C3  - HCI in business, government and organizations: 10th international conference, HCIBGO 2023, held as part of the 25th HCI international conference, HCII 2023, copenhagen, denmark, july 23–28, 2023, proceedings, part II
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-36049-7_3
SP  - 30
EP  - 40
PB  - Springer-Verlag
SN  - 978-3-031-36048-0
UR  - https://doi.org/10.1007/978-3-031-36049-7_3
KW  - Autonomy
KW  - Affordance Perception
KW  - Customer Satisfaction
KW  - Functional Affordance
KW  - Service Robot
ER  - 

TY  - JOUR
TI  - Forming a cognitive automation strategy for Operator 4.0 in complex assembly
AU  - Mattsson, Sandra
AU  - Fast-Berglund, Åsa
AU  - Li, Dan
AU  - Thorvald, Peter
T2  - Computers & Industrial Engineering
DA  - 2020/01//
PY  - 2020
DO  - 10.1016/j.cie.2018.08.011
VL  - 139
IS  - C
J2  - Comput. Ind. Eng.
SN  - 0360-8352
UR  - https://doi.org/10.1016/j.cie.2018.08.011
KW  - Industry 4.0
KW  - Complexity
KW  - Human factors
KW  - Human-automation interaction
KW  - Ergonomics
KW  - Operator 4.0
ER  - 

TY  - JOUR
TI  - Responsible guidelines for authorship attribution tasks in NLP: Responsible guidelines for authorship attribution tasks in NLP
AU  - Saxena, Vageesh
AU  - Tamò-Larrieux, Aurelia
AU  - Van Dijck, Gijs
AU  - Spanakis, Gerasimos
T2  - Ethics and Inf. Technol.
AB  - Authorship Attribution (AA) approaches in Natural Language Processing (NLP) are important in various domains, including forensic analysis and cybercrime. However, they pose Ethical, Legal, and Societal Implications/Aspects (ELSI/ELSA) challenges that remain underexplored. Inspired by foundational AI ethics guidelines and frameworks, this research introduces a comprehensive framework of responsible guidelines that focuses on AA tasks in NLP, which are tailored to different stakeholders and development phases. These guidelines are structured around four core principles: privacy and data protection, fairness and non-discrimination, transparency and explainability, and societal impact. Furthermore, to illustrate a practical application of our guidelines, we apply them to a recent AA study that targets identifying and linking potential human trafficking vendors. We believe the proposed guidelines can assist researchers and practitioners in justifying their decisions, assisting ethical committees in promoting responsible practices, and identifying ethical concerns related to NLP-based AA approaches. Our study aims to contribute to ensuring the responsible development and deployment of AA tools.
DA  - 2025/03//
PY  - 2025
DO  - 10.1007/s10676-025-09821-w
VL  - 27
IS  - 2
SN  - 1388-1957
UR  - https://doi.org/10.1007/s10676-025-09821-w
KW  - Explainability
KW  - Authorship attribution (AA)
KW  - data protection
KW  - Fairness &amp
KW  - Natural language processing (NLP)
KW  - non-discrimination
KW  - Privacy &amp
KW  - Responsible AI
KW  - Societal impact
KW  - Transparency &amp
ER  - 

TY  - JOUR
TI  - Embedding values in artificial intelligence (AI) systems
AU  - van de Poel, Ibo
T2  - Minds Mach.
AB  - Organizations such as the EU High-Level Expert Group on AI and the IEEE have recently formulated ethical principles and (moral) values that should be adhered to in the design and deployment of artificial intelligence (AI). These include respect for autonomy, non-maleficence, fairness, transparency, explainability, and accountability. But how can we ensure and verify that an AI system actually respects these values? To help answer this question, I propose an account for determining when an AI system can be said to embody certain values. This account understands embodied values as the result of design activities intended to embed those values in such systems. AI systems are here understood as a special kind of sociotechnical system that, like traditional sociotechnical systems, are composed of technical artifacts, human agents, and institutions but—in addition—contain artificial agents and certain technical norms that regulate interactions between artificial agents and other elements of the system. The specific challenges and opportunities of embedding values in AI systems are discussed, and some lessons for better embedding values in AI systems are drawn.
DA  - 2020/09//
PY  - 2020
DO  - 10.1007/s11023-020-09537-4
VL  - 30
IS  - 3
SP  - 385
EP  - 409
SN  - 0924-6495
UR  - https://doi.org/10.1007/s11023-020-09537-4
KW  - Artificial intelligence
KW  - Ethics
KW  - Values
KW  - Artificial agent
KW  - Institution
KW  - Multi-agent system
KW  - Norms
KW  - Sociotechnical system
KW  - Value embedding
ER  - 

TY  - CONF
TI  - SHRUBS: simulating influencing human behaviour in security
AU  - Carmichael, Peter
AU  - Morisset, Charles
AU  - Groß, Thomas
T3  - Stast '18
AB  - An organisational requirement of no unauthorised personnel permitted in a restricted area may have a security policy such as all employees must wear identification badges and employees must challenge people who are not wearing a badge. An employee's choice to wear/not wear their badge can be strongly related to how they perceive the security policy, which we call the compliance attitude. Peoples behaviour towards a security policy can influence other peoples compliance attitudes, such as challenging those not wearing a badge influences the compliance attitudes of the people who are challenged and those observing the challenge. The exchange of a challenge by social interaction between two or more people can create a social influence, whereby a persons choice to wear their badge is nudged. For the organisation, the problem is assessing how these social influences propagate throughout the compliance attitudes for particular security policies of all employees. We present SHRUBS which is a work in progress. It is a tool evaluating global security compliance attitudes of human agents. SHRUBS simulates behaviour for security policies where social interaction is present. It is built from conclusions in psychology, behavioural economics and human factors in security. SHRUBS takes as input, a list of behaviour parameters describing agent behaviour and returns the global compliance attitude from a set of traces formed through simulation. We demonstrate the application of SHRUBS with a running example to illustrate how one might mitigate against poor compliance attitudes amongst agents. We then go onto discuss the validation possibilities and explain this with a real world data set that we have collected. We envision that future versions of the tool would enable organisations to make more informed security policy decisions about employee behaviour, such as the best behavioural intervention to use.
C1  - San Juan, Puerto Rico and New York, NY, USA
C3  - Proceedings of the 8th workshop on socio-technical aspects in security and trust
DA  - 2020///
PY  - 2020
DO  - 10.1145/3361331.3361337
PB  - Association for Computing Machinery
SN  - 978-1-4503-7285-5
UR  - https://doi.org/10.1145/3361331.3361337
ER  - 

TY  - THES
TI  - Sensai+expanse: Prediction of emotional valence changes on humans in context by an artificial agent towards empathy
AU  - Henriques, Nuno Andrade da Cruz
AB  - The field of Cognitive Science is broader enough on the interdisciplinary study of the brain, mind, and intelligence with a scientific research community gaining momentum over the last few decades. Specifically, joining the two fields of psychology and artificial intelligence (AI) one may envision agents, embodied or not, human-like or wearable, with the ability to significantly change the way humans live. This research conceive the artificial agent as a non-anthropomorphic with adaptive empathy for human-agent interaction (HAI) synergy towards better companionship. Therefore, the main objectives of this research are (a) to build a predictive model for each human user on context-based emotional valence changes; and (b) to study the age, gender, and human behaviour neutrality and robustness of the artificial agent regarding the prediction ability. The context include geographically located data from sensors, text sentiment analysis, and human emotional valence self-report, all timestamped events, using a common mobile device such as a smartphone. Also, to analyse and discuss the results on how to leverage such a model to adapt interaction strategies in order to foster higher levels of empathy between a non-anthropomorphic agent and its interacting human. For these goals SensAI+Expanse is developed where SensAI acts as an embodied nearby agent and Expanse encompass the machine learning resources in efficient manner, i.e., a distributed, fault-tolerant, mobile and Cloud-based platform from scratch as a research tool to continuously, online, gather and process data towards automated machine learning (AutoML) and prediction.The study is designed with a methodology in place to avoid the Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies bias. This goal is accomplished by collecting data in the wild and worldwide by making use of the publicly accessible Google Play repository for the Android™ SensAI smartphone application. Eligible participants are diverse in age, gender, and behaviour on self-reporting emotional valence. In order to balance the gender distribution by age a dichotomy approach using age median (M = 34) is used. Regarding participation duration, two thirds (33/49) of the eligible individuals for analysis remain interacting for the required minimum of four weeks. The analysis of the results show evidence of significant behaviour differences between some age and gender combinations regarding self-reported emotional valence. Furthermore, the results from a comparison study between state-of-the-art algorithms revealed Extreme Gradient Boosting on average the best model for prediction (F1 = 0:91) with efficient energy use, and explainable using feature importance inspection. Moreover, the artificial agent remained neutral regarding human demographics and, simultaneously, able to reveal individual idiosyncrasies. Therefore, this research contributions include results with evidence, restricted to population and data samples available, of differences in behaviour amongst some combinations of age ranges versus gender. The main contribution is a novel platform for studies regarding human emotional valence changes in context. This system may complement and supersede (eventually) traditional long-list self-appraisal questionnaires. The SensAI+Expanse platform contributes with several parts such as a mobile device application (SensAI) able to adapt and learn in order to predict emotional valence states with high performance, a cloud computing (Cloud) service (SensAI Expanse) with ready-to-action analysis and processing modules towards AutoML. Additionally, smartphone sensing add a contribution for continuous, non-invasive and personalised health check. In the future, developments about human-agent relationships regarding affective interactions are foreseen. Further, the measurement of empathetic reactions and evaluating outcomes may be used to verify and validate health status thus improving care and significantly change the way humans live.
DA  - 2020///
PY  - 2020
M3  - phd
PB  - Universidade de Lisboa (Portugal)
N1  - AAI28753684
ER  - 

TY  - CONF
TI  - Creative design of gaussian sensor system with encoding and decoding
AU  - Huang, Yu-Hsiung
AU  - Chen, Wei-Chun
AU  - Hsu, Su-Chu
AB  - Stuart Hall proposed the “encoding/decoding model of communication” for the theoretical method of media information production and dissemination in 1973. In 1980, he further proposed the research of classic contemporary culture titled “Encoding/Decoding”, which explained how media producers can “encode” an object, feeling, and ideas. Message in the media to achieve the purpose of disseminating information. In addition, “decoding” is the process and method of how the media message can be perceived by the “receiver” after being transformed and translated. It has been explained that the concept of encoding and decoding has a great influence on the research of different cultural media communication from the analogy to the digital age through&nbsp;many&nbsp;kinds of research. However, with the rapid development of digital media technology, we are faced with the production methods of information in tangible and intangible media, and most of them are translated in virtual form in programming languages or digital symbols. Encoding and decoding of digital symbols and codes has gradually changed the way we understand perception.In this paper, we propose the “Gaussian Sensor System”, which consists of three parts: Gausstoys magnetic sensor module, video/audio encoding and decoding, and interactive installation art. We&nbsp;used damped oscillator magnetic balance and Gausstoys sensor as a tangible user interface (TUI), and integrated the Gaussian sensor into the interactive installation art. When the user intervenes with the floating magnet device and disturbs the magnetic field, the gaussian sensor will “encode” the human analogy behavior. Then the data of human behavior is transformed into visual&nbsp;video and sound feedback. The&nbsp;RGB color&nbsp;of visual&nbsp;video and frequency feedback&nbsp;of audio on the screen is the “decoding” of perception. Therefore, in our Gaussian Sensor System, “balance” is generated through the floating magnetic force in our artwork. After the user “intervenes”, the entire behavior is transformed into a digital reproduction of video/audio and then transmitted to the user the perception feedback of color and sound. Our creative design has been&nbsp;shown&nbsp;to “Tsing Hua Effects 2020: STEM with A” Technology and Art Festival of Tsing Hua University in Taiwan and “Art Gallery, 2016 SIGGRAPH Asia” in Macau. In the past, many applications of Gaussian Sensor were used in interactive games or interactive learning, but our application was in “interactive installation art”. We applied the damped oscillator magnetic balance as a tangible interface device, which is quite rare in HCI applications or interactive art. In the future, the media in the digital age that we are facing will gradually transform real-world cognitions through digital programming languages to produce new perceptions. At present, many kinds of research&nbsp;in HCI&nbsp;field have explained how to experience hearing, taste, and touch in digital media. The encoding and decoding of digital media will be one of the important topics in HCI in the future. Our&nbsp;creative&nbsp;design can be applied to more HCI or TUI research fields in the future&nbsp;and&nbsp;drive&nbsp;users to experience more diverse perceptions through digital media.
C1  - Berlin, Heidelberg
C3  - Human interface and the management of information. Information-rich and intelligent environments: Thematic area, HIMI 2021, held as part of the 23rd HCI international conference, HCII 2021, virtual event, july 24–29, 2021, proceedings, part II
DA  - 2021///
PY  - 2021
DO  - 10.1007/978-3-030-78361-7_29
SP  - 385
EP  - 395
PB  - Springer-Verlag
SN  - 978-3-030-78360-0
UR  - https://doi.org/10.1007/978-3-030-78361-7_29
KW  - Damped oscillation
KW  - Encoding and Decoding
KW  - Gaussian Sensor
KW  - Interactive installation art
KW  - Tangible User Interface
ER  - 

TY  - JOUR
TI  - Sharing workarounds in health IT-enabled patient-care work: Impact on clinicians
AU  - Raman, Roopa
AU  - Sullivan, Nicholas
T2  - SIGMIS Database
AB  - Healthcare providers engaged in health information technology (HIT)-enabled patient-care work tend to resort to workarounds when problems arise in prescribed practices. Hospital administration discourages using such informal workarounds. Yet, due to the interdependent collaborative nature of hospital work, there is a tendency for clinicians to share informally with their colleagues the workarounds that they find useful. In this study, we investigate how the sharing of workarounds impacts the clinicians involved. Our qualitative field study within an inpatient hospital system allowed us to perform rich case studies on a set of four instantiations of the sharing of workarounds in electronic medication administration record (eMAR)-enabled medication administration work. Our findings offer the counterintuitive insight that positive outcomes can be realized from sharing workarounds, even when they are problematic. Our exploratory inductive research model presents a set of five propositions that explain two different ways in which the sharing of workarounds creates positive value for the clinicians involved. We encourage future research to investigate other consequences, and routes leading to those consequences, from the sharing of workarounds in various other IT-enabled contexts. Cumulatively, such research would inform, enrich, and even challenge traditional notions that largely prefer avoiding informal workarounds in the IT-enabled workplace.
DA  - 2024/05//
PY  - 2024
DO  - 10.1145/3663682.3663686
VL  - 55
IS  - 2
SP  - 42
EP  - 71
SN  - 0095-0033
UR  - https://doi.org/10.1145/3663682.3663686
KW  - electronic medical records
KW  - health information technology
KW  - it workarounds
KW  - patient-care practices
KW  - qualitative research
ER  - 

TY  - CONF
TI  - SubjectBook: Hypothesis-driven ubiquitous visualization for affective studies
AU  - Taamneh, Salah
AU  - Dcosta, Malcolm
AU  - Kwon, Kyeong-An
AU  - Pavlidis, Ioannis
T3  - Chi ea '16
AB  - Analyzing affective studies is challenging because they feature multimodal data, such as psychometric scores, imaging sequences, and signals from wearable sensors, with the latter streaming continuously for hours on end. Meaningful visual representations of such data can greatly facilitate insights and qualitative analysis. Various tools that were proposed to tackle this problem provide visualizations of the original data only; they do not support higher level abstractions. In this paper, we introduce SubjectBook, an interactive web-based tool for synchronizing, visualizing, exploring, and analyzing affective datasets. Uniquely, SubjectBook operates at three levels of abstraction, mirroring the stages of quantitative analysis in hypothesis-driven research. The top level uses a grid visualization to show the study's significant outcomes across subjects. The middle level summarizes, for each subject, context information along with the explanatory and response measurements in a construct reminiscent of an ID card. This enables the analyst to appreciate within subject phenomena. Finally, the bottom level brings together detailed information concerning the inner and outer state of human subjects along with their real-world interactions - a visualization fusion that supports cause and effect reasoning at the experimental session level. SubjectBook was evaluated on a case study focused on driving behaviors.
C1  - San Jose, California, USA and New York, NY, USA
C3  - Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems
DA  - 2016///
PY  - 2016
DO  - 10.1145/2851581.2892338
SP  - 1483
EP  - 1489
PB  - Association for Computing Machinery
SN  - 978-1-4503-4082-3
UR  - https://doi.org/10.1145/2851581.2892338
KW  - affective computing
KW  - affective datasets
KW  - affective studies
KW  - data visualization
KW  - physiological signals
KW  - qualitative analysis
ER  - 

TY  - CONF
TI  - Explainable AI reloaded: Challenging the XAI status quo in the era of large language models
AU  - Ehsan, Upol
AU  - Riedl, Mark
T3  - HttF '24
AB  - When the initial vision of Explainable (XAI) was articulated, the most popular framing was to open the (proverbial) “black-box” of AI so that we could understand the inner workings. With the advent of Large Language Models (LLMs), the very ability to open the black-box is increasingly limited. Especially when it comes to non-technical end-users. In this paper, we challenge the assumption of “opening” the black-box in the LLM era and argue for a shift in our XAI expectations. Highlighting the epistemic blind spots of an algorithm-centered XAI view, we argue that a human-centered perspective can be a path forward. We operationalize the argument by synthesizing XAI research along three dimensions: explainability outside the black-box, explainability around the edges of the black box, and explainability that leverages infrastructural seams. We conclude with takeaways that reflexively inform XAI as a domain.
C1  - Santa Cruz, CA, USA and New York, NY, USA
C3  - Proceedings of the halfway to the future symposium
DA  - 2024///
PY  - 2024
DO  - 10.1145/3686169.3686185
PB  - Association for Computing Machinery
SN  - 979-8-4007-1042-1
UR  - https://doi.org/10.1145/3686169.3686185
KW  - Large Language Models
KW  - Explainable AI
KW  - Generative AI
ER  - 

TY  - JOUR
TI  - User feedback and remote supervision for assisted living with mobile robots: A field study in long-term autonomy
AU  - Luperto, Matteo
AU  - Romeo, Marta
AU  - Monroy, Javier
AU  - Renoux, Jennifer
AU  - Vuono, Alessandro
AU  - Moreno, Francisco-Angel
AU  - Gonzalez-Jimenez, Javier
AU  - Basilico, Nicola
AU  - Borghese, N. Alberto
T2  - Robotics and Autonomous Systems
DA  - 2022/09//
PY  - 2022
DO  - 10.1016/j.robot.2022.104170
VL  - 155
IS  - C
J2  - Robot. Auton. Syst.
SN  - 0921-8890
UR  - https://doi.org/10.1016/j.robot.2022.104170
KW  - Field study
KW  - Long-term autonomy
KW  - Socially Assistive Robots
ER  - 

TY  - JOUR
TI  - Scanflow: A multi-graph framework for Machine Learning workflow management, supervision, and debugging▪
AU  - Bravo-Rocca, Gusseppe
AU  - Liu, Peini
AU  - Guitart, Jordi
AU  - Dholakia, Ajay
AU  - Ellison, David
AU  - Falkanger, Jeffrey
AU  - Hodak, Miroslav
T2  - Expert Systems With Applications
DA  - 2022/09//
PY  - 2022
DO  - 10.1016/j.eswa.2022.117232
VL  - 202
IS  - C
J2  - Expert Syst. Appl.
SN  - 0957-4174
UR  - https://doi.org/10.1016/j.eswa.2022.117232
KW  - Machine Learning
KW  - Concept drift
KW  - Containerization
KW  - Graph
KW  - Robustness
KW  - Symbolic knowledge
ER  - 

TY  - JOUR
TI  - How do children acquire knowledge about voice assistants? A longitudinal field study on children’s knowledge about how voice assistants store and process data
AU  - Szczuka, Jessica M.
AU  - Strathmann, Clara
AU  - Szymczyk, Natalia
AU  - Mavrina, Lina
AU  - Krämer, Nicole C.
DA  - 2022/09//
PY  - 2022
DO  - 10.1016/j.ijcci.2022.100460
VL  - 33
IS  - C
J2  - Int. J. Child-Comp. Interact.
SN  - 2212-8689
UR  - https://doi.org/10.1016/j.ijcci.2022.100460
KW  - Child–Computer Interaction
KW  - Data processing
KW  - Data storage
KW  - Secrets
KW  - Voice assistants
ER  - 

TY  - JOUR
TI  - Data siloing as infrastructural activism
AU  - Darian, Shiva
AU  - Robledo Yamamoto, Fujiko
AU  - Voida, Amy
T2  - Proc. ACM Hum.-Comput. Interact.
AB  - The refugee support ecosystem in the U.S.-Mexico Borderplex is resource-scarce, dynamic, and transitional, requiring intensive information work. To address coordination challenges among the cross-sector sociotechnical community working at one of the largest ports of entry for asylum seekers, the county government of El Paso, TX, USA, has proposed a centralized information system for use by all refugee-serving organizations on the U.S. side of the border. However, stakeholder responses have varied, with some organizations approving, and others resisting the proposal. This research investigates the nuanced dynamics of information infrastructures among stakeholders from different refugee-serving organizations working in the Borderplex, explaining how they navigate pressures to centralize their information infrastructures amid myriad concerns while considering the costs of not doing so, particularly for the refugees. Through a combination of ethnographic fieldwork, semi-structured interviews, and thematic analysis, we explore why some of the organizations in the Borderplex are choosing to silo their data—in support of financial freedom, mission malleability, and maintaining privacy in a liminal context—as a form of infrastructural activism. Our findings contribute to discussions of non-use and deliberate disconnection, highlighting the complex political and practical dimensions of technology (non-)adoption.
DA  - 2025/05//
PY  - 2025
DO  - 10.1145/3710906
VL  - 9
IS  - 2
UR  - https://doi.org/10.1145/3710906
KW  - asylum seekers
KW  - database infrastructure
KW  - human service organization
KW  - immigration
KW  - migration
KW  - organizational networks
KW  - refugees
ER  - 

TY  - CONF
TI  - An explainable deep fusion network for affect recognition using physiological signals
AU  - Lin, Jionghao
AU  - Pan, Shirui
AU  - Lee, Cheng Siong
AU  - Oviatt, Sharon
T3  - Cikm '19
AB  - Affective computing is an emerging research area which provides insights on human's mental state through human-machine interaction. During the interaction process, bio-signal analysis is essential to detect human affective changes. Currently, machine learning methods to analyse bio-signals are the state of the art to detect the affective states, but most empirical works mainly deploy traditional machine learning methods rather than deep learning models due to the need for explainability. In this paper, we propose a deep learning model to process multimodal-multisensory bio-signals for affect recognition. It supports batch training for different sampling rate signals at the same time, and our results show significant improvement compared to the state of the art. Furthermore, the results are interpreted at the sensor- and signal- level to improve the explainaibility of our deep learning model.
C1  - Beijing, China and New York, NY, USA
C3  - Proceedings of the 28th ACM international conference on information and knowledge management
DA  - 2019///
PY  - 2019
DO  - 10.1145/3357384.3358160
SP  - 2069
EP  - 2072
PB  - Association for Computing Machinery
SN  - 978-1-4503-6976-3
UR  - https://doi.org/10.1145/3357384.3358160
KW  - deep learning
KW  - explainability
KW  - affect recognition
KW  - multimodal fusion
ER  - 

TY  - JOUR
TI  - Proposal and validation of an analytical generative model of SRP-PHAT power maps in reverberant scenarios
AU  - Velasco, Jose
AU  - Martı́n-Arguedas, Carlos J.
AU  - Macias-Guarasa, Javier
AU  - Pizarro, Daniel
AU  - Mazo, Manuel
T2  - Signal Processing
AB  - The algorithms for acoustic source localization based on PHAT filtering have been profusely used with good results in reverberant and noisy environments. However, there are very few studies that give a formal explanation of their robustness, most of them providing just an empirical validation or showing results on simulated data. In this work we present a novel analytical model for predicting the behavior of both the SRP-PHAT power maps and the GCC-PHAT functions. The results show that they are only affected by the signal bandwidth, the microphone array topology, and the room geometry, being independent of the spectral content of the received signal. The proposed model is shown to be valid in reverberant environments and under far and near field conditions. Using this result, an analysis study on how the aforementioned factors affect the SRP-PHAT power maps is presented providing well supported theoretical and practical considerations. The model validation is based on both synthetic and real data, obtaining in all cases a high accuracy of the model to reproduce the SRP-PHAT power maps, both in anechoic and non-anechoic scenarios, becoming thus an excellent tool to be exploited for the improvement of real world relevant applications related to acoustic localization. HighlightsA novel parametric analytical model to predict SRP-PHAT power maps is formulated.An exhaustive evaluation is done on both synthetic and real data.Results show high accuracy for very different acoustical and geometrical conditions.The paper also addresses practical issues in the model implementation.
DA  - 2016/02//
PY  - 2016
DO  - 10.1016/j.sigpro.2015.08.003
VL  - 119
IS  - C
SP  - 209
EP  - 228
J2  - Signal Process.
SN  - 0165-1684
UR  - https://doi.org/10.1016/j.sigpro.2015.08.003
KW  - Acoustic Source Location (ASL)
KW  - Generalized Cross-Correlation (GCC)
KW  - Microphone arrays
KW  - Phase Transform (PHAT)
KW  - Steered Response Power (SRP)
ER  - 

TY  - CONF
TI  - Explainability and&nbsp;transparency in&nbsp;practice: a comparison between corporate and&nbsp;national AI ethics guidelines in&nbsp;germany and&nbsp;china
AU  - Speith, Timo
AU  - Xu, Jing
AB  - As artificial intelligence (AI) continues to permeate various sectors, ethical concerns, particularly around transparency and explainability, have grown. While research has focused on making AI systems transparent or explainable and exploring the connection between these two principles and other ethical dimensions (e.g., fairness and responsibility), little is known about their practical applications in corporate settings. Against this background, this study conducts a comparative analysis of corporate and national AI ethics guidelines in Germany and China to uncover how transparency and explainability are operationalized in practice. The insights contribute not only to understanding corporate needs concerning transparency and explainability but also reveal distinct approaches taken by countries in fostering ethical AI. Overall, this study informs researchers, industry practitioners, and policymakers about the nuances of ethical AI implementation in global contexts.
C1  - Auckland, New Zealand and Berlin, Heidelberg
C3  - Explainable and transparent AI and multi-agent systems: 6th international workshop, EXTRAAMAS 2024, auckland, new zealand, may 6–10, 2024, revised selected papers
DA  - 2024///
PY  - 2024
DO  - 10.1007/978-3-031-70074-3_12
SP  - 205
EP  - 223
PB  - Springer-Verlag
SN  - 978-3-031-70073-6
UR  - https://doi.org/10.1007/978-3-031-70074-3_12
KW  - Transparency
KW  - AI Ethics
KW  - Explainability
KW  - Explainable AI
KW  - Comparative Analysis
KW  - Ethical AI
KW  - Ethics Guidelines
KW  - Trustworthy AI
ER  - 

TY  - JOUR
TI  - The political imaginary of national AI strategies
AU  - Paltieli, Guy
T2  - AI Soc.
AB  - In the past few years, several democratic governments have published their National AI Strategies (NASs). These documents outline how AI technology should be implemented in the public sector and explain the policies that will ensure the ethical use of personal data. In this article, I examine these documents as political texts and reconstruct the political imaginary that underlies them. I argue that these documents intervene in contemporary democratic politics by suggesting that AI can help democracies overcome some of the challenges they are facing. To achieve this, NASs use different kinds of imaginaries—democratic, sociotechnical and data—that help citizens envision how a future AI democracy might look like. As part of this collective effort, a new kind of relationship between citizens and governments is formed. Citizens are seen as autonomous data subjects, but at the same time, they are expected to share their personal data for the common good. As a result, I argue, a new kind of political imaginary is developed in these documents. One that maintains a human-centric approach while championing a vision of collective sovereignty over data. This kind of political imaginary can become useful in understanding the roles of citizens and governments in this technological age.
DA  - 2022/12//
PY  - 2022
DO  - 10.1007/s00146-021-01258-1
VL  - 37
IS  - 4
SP  - 1613
EP  - 1624
SN  - 0951-5666
UR  - https://doi.org/10.1007/s00146-021-01258-1
KW  - AI
KW  - Big data
KW  - Consent
KW  - Imaginaries
KW  - Political theory
ER  - 

TY  - JOUR
TI  - Ethical algorithm design
AU  - Kearns, Michael
AU  - Roth, Aaron
T2  - SIGecom Exch.
AB  - In this letter, we summarize the research agenda that we survey in our recent book The Ethical Algorithm, which is intended for a general, nontechnical audience. At a high level, this research agenda proposes formalizing the ethical and social values that we want our algorithms to maintain — values including privacy, fairness, and explainability — and then to embed these social values directly into our algorithms as part of their design. This broad research area is most mature in the area of privacy, specifically differential privacy. It is off to a good start in emerging areas like algorithmic fairness, and seems promising for more nebulous goals like explainability, if only we can find the right definitions. Most work in this area to date analyzes algorithms as isolated components, but game-theoretic and economic analysis will become increasingly important as we try and study the effects of algorithmic interventions in larger sociotechnical systems.
DA  - 2020/12//
PY  - 2020
DO  - 10.1145/3440959.3440966
VL  - 18
IS  - 1
SP  - 31
EP  - 36
UR  - https://doi.org/10.1145/3440959.3440966
KW  - privacy
KW  - fairness
KW  - explainability
KW  - game theory
ER  - 

TY  - CONF
TI  - The space of user aggression in Human-Robot Interaction
AU  - Rezzani, Andrea
AU  - De Angeli, Antonella
AU  - Menéndez Blanco, Marı́a
AU  - Dorfmann, Max
T3  - CHItaly '23
AB  - Aggression as a social behaviour towards technology has been highlighted in Human-Computer Interaction over a decade ago and recently found a renewed interest in the niche of robot abuse. However, a lack of operationalisation and definition of this behaviour resulted in specific-domain observations that hinder generalisations of the determinants that explain it. This paper presents a theoretically grounded workshop that aims to explore how university students imagine aggression towards robots through speculative design. A total of 69 participants, divided into seven workshops, took part in elaborating stories of aggression towards robots using the inspiration cards “Humans against Robots”. The results highlighted nuances of aggression that do not entirely fit the traditional definitions of psychological aggression, nor the term abuse, prompting a new operationalisation of this concept. Aggression towards robots is presented using four different behaviours: 1) Outburst against annoying tools; 2) Clash with conflicting companions; 3) Oppression of faithful tools; 4) Rebellion against unfriendly companions. These forms are represented in an orthogonal space according to the function of aggression (reactive vs proactive) and the identity attributed to the robot (subject vs object). This paper supports the importance of investigating sociotechnical imaginaries to understand the heterogeneity of behaviours and determinants that can explain aggression against robots. The space of user aggression in Human-Robot Interaction (HRI) allows for distinguishing between the different sociotechnical imaginaries that aggression against robots evokes and that designers and researchers might consider when exploring this topic.
C1  - Torino, Italy and New York, NY, USA
C3  - Proceedings of the 15th biannual conference of the italian SIGCHI chapter
DA  - 2023///
PY  - 2023
DO  - 10.1145/3605390.3605402
PB  - Association for Computing Machinery
SN  - 979-8-4007-0806-0
UR  - https://doi.org/10.1145/3605390.3605402
KW  - Human-Robot Interaction
KW  - Inspiration cards
KW  - Robot Abuse
KW  - Sociotechnical Imaginaries
KW  - Speculative design
ER  - 

TY  - JOUR
TI  - The role of resources in the success or failure of diverse teams: Resource scarcity activates negative performance-detracting resource dynamics in social category diverse teams
AU  - Yu, Siyu
AU  - Greer, Lindred L.
T2  - Organization Science
AB  - Increasing the social category diversity of work teams is top of mind for many organizations. However, such efforts may not always be sufficiently resourced, given the numerous resource demands facing organizations. In this paper, we offer a novel take on the relationship between social category diversity and team performance, seeking to understand the role resources may play in both altering and explaining the performance dynamics of diverse teams. Specifically, our resource framework explains how the effects of social category diversity on team performance can be explained by intrateam resource cognitions and behaviors and are dependent on team resource availability. We propose that in the face of scarcity in a focal resource (i.e., budget), diverse (but not homogenous) teams generalize this scarcity perception to fear that all resources (i.e., staff, time, etc.) are scarce, prompting performance-detracting power struggles over resources within the team. We find support for our model in three multimethod team-level studies, including two laboratory studies of interacting teams and a field study of work teams in research and development firms. Our resource framework provides a new lens to study the success or failure of diverse teams by illuminating a previously overlooked danger in diverse teams (negative resource cognitions (scarcity spillover bias) and behaviors (intrateam power struggles)), which offers enhanced explanatory power over prior explanations. This resource framework for the study of team diversity also yields insight into how to remove the roadblocks that may occur in diverse teams, highlighting the necessity of resource sufficiency for the success of diverse teams.
DA  - 2023/01//
PY  - 2023
DO  - 10.1287/orsc.2021.1560
VL  - 34
IS  - 1
SP  - 24
EP  - 50
SN  - 1526-5455
UR  - https://doi.org/10.1287/orsc.2021.1560
KW  - conflict
KW  - diversity
KW  - groups
KW  - power
KW  - resources
KW  - scarcity
KW  - teams
ER  - 

TY  - CONF
TI  - Enhancing government service delivery: a case study of ACQAR implementation and lessons learned from ChatGPT integration in a singapore government agency
AU  - Lee Hui Shan, Alvina
AU  - Shankararaman, Venky
AU  - Ouh, Eng Lieh
T3  - dg.o '24
AB  - This paper presents the pilot implementation of AI Based Citizen Question-Answer Recommender (ACQAR) as an attempt to enhance citizen service delivery within a Singaporean government agency. Drawing insights from previous studies on the Empath library's use in Service Level Agreement (SLA) prediction and the implementation of the Citizen Question-Answer system (CQAS), we redesigned the pilot system, ACQAR. ACQAR integrates the outputs from Empath X SLA predictor and CQAS as essential inputs to the ChatGPT engine, creating contextually aware responses for customer service officers to use as responses to the citizens.Empath X SLA predictor anticipates the expected service response time based on citizens' emotional states, while CQAS recommends answers for faster and more efficient officer responses. This paper provides a comprehensive blueprint for governments aiming to enhance citizen service delivery by fusing sentiment analysis, SLA prediction, question-answer models, and ChatGPT. The proposed system design aims to revolutionize government-citizen interactions, delivering empathetic, efficient, and tailored responses without violating SLAs.Although the full-scale deployment of ACQAR is pending, this paper outlines a foundational step towards the practical development and implementation of an intelligent system by sharing the trial outcomes of ACQAR. By leveraging ChatGPT, this system holds the potential to significantly enhance citizen satisfaction, foster trust in government services, and strengthen overall government-citizen relationships.Additionally, the paper addresses inherent challenges associated with ChatGPT, including data opacity, potential misinformation, and occasional errors, especially critical in government decision-making. Upholding public administration's core values of transparency and accountability, the paper emphasizes the importance of AI explainability in ChatGPT's adoption within government agencies. Strategies proposed include prompt engineering, data governance, and the adoption of interpretability tools such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) to enhance understanding and align ChatGPT's decision-making processes with these principles.
C1  - Taipei, Taiwan and New York, NY, USA
C3  - Proceedings of the 25th annual international conference on digital government research
DA  - 2024///
PY  - 2024
DO  - 10.1145/3657054.3657130
SP  - 645
EP  - 653
PB  - Association for Computing Machinery
SN  - 979-8-4007-0988-3
UR  - https://doi.org/10.1145/3657054.3657130
KW  - Citizen Services
KW  - Information Retrieval
KW  - Question Answering
KW  - Service Innovation
KW  - Text Analytics
ER  - 

TY  - JOUR
TI  - FairMask: Better fairness via model-based rebalancing of protected attributes
AU  - Peng, Kewen
AU  - Chakraborty, Joymallya
AU  - Menzies, Tim
T2  - IEEE Transactions on Software Engineering
AB  - &lt;italic&gt;Context&lt;/italic&gt;: Machine learning software can generate models that inappropriately discriminate against specific protected social groups (e.g., groups based on gender, ethnicity, etc.). Motivated by those results, software engineering researchers have proposed many methods for mitigating those discriminatory effects. While those methods are effective in mitigating bias, few of them can provide explanations on what is the root cause of bias. &lt;italic&gt;Objective&lt;/italic&gt;: We aim to better detect and mitigate algorithmic discrimination in machine learning software problems. &lt;italic&gt;Method&lt;/italic&gt;: Here we propose &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;sfFairMask&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant="sans-serif"&gt;FairMask&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="peng-ieq1-3220713.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, a &lt;italic&gt;model-based&lt;/italic&gt; extrapolation method that is capable of both mitigating bias and explaining the cause. In our &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;sfFairMask&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant="sans-serif"&gt;FairMask&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="peng-ieq2-3220713.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; approach, protected attributes are represented by models learned from the other independent variables (and these models offer extrapolations over the space between existing examples). We then use the extrapolation models to relabel protected attributes later seen in testing data or deployment time. Our approach aims to offset the biased predictions of the classification model by rebalancing the distribution of protected attributes. &lt;italic&gt;Results&lt;/italic&gt;: The experiments of this paper show that, without compromising (original) model performance, &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;sfFairMask&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant="sans-serif"&gt;FairMask&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="peng-ieq3-3220713.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; can achieve significantly better group and individual fairness (as measured in different metrics) than benchmark methods. Moreover, compared to another instance-based rebalancing method, our model-based approach shows faster runtime and thus better scalability. &lt;italic&gt;Conclusion&lt;/italic&gt;: Algorithmic decision bias can be removed via extrapolation that corrects the misleading latent correlation between the protected attributes and other non-protected ones. As evidence for this, our proposed &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;sfFairMask&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant="sans-serif"&gt;FairMask&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="peng-ieq4-3220713.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; is not only performance-wise better (measured by fairness and performance metrics) than two state-of-the-art fairness algorithms. &lt;italic&gt;Reproduction Package&lt;/italic&gt;: In order to better support open science, all scripts and data used in this study are available online at &lt;uri&gt;https://github.com/anonymous12138/biasmitigation&lt;/uri&gt;.
DA  - 2023/04//
PY  - 2023
DO  - 10.1109/TSE.2022.3220713
VL  - 49
IS  - 4
SP  - 2426
EP  - 2439
J2  - IEEE Trans. Softw. Eng.
SN  - 0098-5589
UR  - https://doi.org/10.1109/TSE.2022.3220713
ER  - 

TY  - JOUR
TI  - Assessing connected vehicle’s response to green light optimal speed advisory from field operational test and scaling up
AU  - Bhattacharyya, Kinjal
AU  - Laharotte, Pierre-Antoine
AU  - Burianne, Arthur
AU  - Faouzi, Nour-Eddin El
AB  - What are the main factors related to road or service configuration influencing the response behaviour of connected vehicles? How does it evolve with respect to the Market Penetration Rate (MPR) of connected vehicles? Here are some questions raised by this paper with a focus made on Green Light Optimal Speed Advisory (GLOSA) strategy. Such a system, based on V2I communication, aims at providing speed advice/recommendations when approaching an intersection to adjust speed and enhance fuel consumption. The message is displayed on the Human Machine Interface (HMI) of the connected vehicles and a response is expected from the driver. This paper derives its interest in the response behaviour of the driver to HMI. It develops a two-stage methodology based on (i) Field Operational Test to collect realistic inputs (e.g., response rate, delay, deceleration profile, etc.) and (ii) a simulated environment used for extending the findings to non-observed cases (e.g. higher MPR). Besides, the methodology that is well-fitted for generic evaluation and comparison of pilots sites’ conclusions, one further contribution lies in the process to select the explaining factors. Factors are targeted among features of (i) the road configuration (e.g. number of lanes), (ii) the service configuration (e.g. activation distance), or (iii) the individual route choice and traffic conditions. Among others, it is highlighted that the activation distance plays a significant role in the response behaviour and, depending on the cycle duration, a short activation distance might be completely inefficient, while a true environmental impact requires high MPR.
DA  - 2023/06//
PY  - 2023
DO  - 10.1109/TITS.2022.3187532
VL  - 24
IS  - 6
SP  - 6725
EP  - 6736
J2  - Trans. Intell. Transport. Sys.
SN  - 1524-9050
UR  - https://doi.org/10.1109/TITS.2022.3187532
ER  - 

TY  - CONF
TI  - Fostering flow experience in HCI to enhance and allocate human energy
AU  - Peifer, Corinna
AU  - Kluge, Annette
AU  - Rummel, Nikol
AU  - Kolossa, Dorothea
AB  - Motivation explains the direction, intensity and persistence of human behavior and thus plays a crucial role in the mobilization and allocation of available energy. An experience that occurs during motivated action is flow. Flow is perceived as highly rewarding for its own sake and, thus, in flow all attention is directed towards the task at hand, leading to an experience of absorption. At the same time, attention is shielded from irrelevant stimuli and the activity feels easy and effortless. This suggests that flow is a highly efficient state in terms of energy expenditure. Studies addressing the physiology of flow support this assumption. Accordingly, for an optimal use of energy, it is of interest to promote flow in relevant work processes. In HCI, for example, in production work, flow promotion could be enabled by a real-time measure of the operator’s flow state in combination with automated adjustments in the work system to achieve, sustain, or extend flow. Such a real-time measure should not interrupt a person, as traditional self-report measures do. A combination of physiological measures (e.g., heart rate variability, skin conductance, and blink rate) provides a promising starting point to find such a real-time measure. Automated adjustments first require the identification of design approaches that affect flow within the work system. Using the example of work in manufacturing, the concept of flow, its measurement, and potential design approaches for automated adaptation are presented, and their application in HCI processes is discussed.
C1  - Copenhagen, Denmark and Berlin, Heidelberg
C3  - Engineering psychology and cognitive ergonomics. Mental workload, human physiology, and human energy: 17th international conference, EPCE 2020, held as part of the 22nd HCI international conference, HCII 2020, copenhagen, denmark, july 19–24, 2020, proceedings, part I
DA  - 2020///
PY  - 2020
DO  - 10.1007/978-3-030-49044-7_18
SP  - 204
EP  - 220
PB  - Springer-Verlag
SN  - 978-3-030-49043-0
UR  - https://doi.org/10.1007/978-3-030-49044-7_18
KW  - Automated adaptation
KW  - Flow experience
KW  - Human energy
ER  - 

TY  - JOUR
TI  - Mobile shopping apps adoption: a systematic review of theories and future research directions
AU  - Srivastava, Arpita
AU  - Srivastava, Nidhi
AU  - Chadha, Priyanka
AU  - Gera, Rajat
AB  - In this study, 22 research papers published on consumer behavior and electronic commerce journals were selected and reviewed based on inclusion criteria of high-quality journals; the aim of this exercise was to conduct an analysis of the state-of the-art research in theories used for studies of MSA. Through this exercise, 22 consumer-level theories were grouped into seven categories. For each theory, its definition, current application in m-shopping apps (MSA) adoption research and suggestions for future research are presented. The review indicates the lack of a universal model of consumer and the predominant effect of cognitive factors i.e., perceived ease of use and utilitarian benefits in explaining usage and adoption intentions. The effects of hedonic benefits, experiential constructs and social influence are contextual and not homogenous. Most of the studies have adopted the behavioral theories of technology adoption by extending or modifying them or as components of multi theoretic approach. A relative lack of theories from socio-psychological, relationship marketing, experiential and emotion dominant disciplines is noted. There is an over-reliance on theories related to information systems, consumer behavior, and predominance of quantitative research-based techniques. Most of the studies focus on consumers pre-adoption intentions compared to usage behavior and consumer loyalty and fail to address the challenges of consumer retention, in-app purchase and consumer loyalty. A relative lack of cross-cultural and cross-country comparative studies is noted. A summary of theories applied and proposed for this study are presented along with proposed research questions. This review provides practitioners and policy makers with a framework of antecedent factors which can be applied for effective marketing strategies according to the stage of consumer adoption of MSA. While keeping in mind specific findings related to this literature review as well as the necessity to guide research in the future that deals with mobile and technology utilization in MSA, a research agenda is introduced.
DA  - 2024/07//
PY  - 2024
DO  - 10.4018/IJEBR.349930
VL  - 20
IS  - 1
SP  - 1
EP  - 26
J2  - Int. J. E-Bus. Res.
SN  - 1548-1131
UR  - https://doi.org/10.4018/IJEBR.349930
KW  - Adoption Behavior
KW  - Consumer Behavior and Continuous Usage
KW  - Mobile Shopping Apps
ER  - 

TY  - JOUR
TI  - Using a service lens to better understand practices –and vice versa
AU  - Farshchian, Babak A.
AU  - Mikalsen, Marius
T2  - Comput. Supported Coop. Work
AB  - Many studies of practices involve service exchange, and many service researchers have discovered the central role that sociotechnical practices play in service –in particular, within the service-dominant logic school of thought. In this paper, we propose an analytical lens that builds on this mutual interest to understand complex practices involving service exchange. Practice researchers can gain new insights regarding practices embedded in service ecosystems. At the same time, service researchers can better explain actor behavior by looking deeper at sociotechnical practices. We develop a concept toolbox based on practice and service-dominant logic research literature. We illustrate the usefulness of the toolbox through an interpretative case study of public service to include children with disabilities in leisure activities. Seeing practices as parts of larger multi-stakeholder service ecosystems 1) can help us better explain behavior in those practices and understand how they are affected by other overlapping practices, 2) brings forward the importance of value and how multiple actors need to interact in order to create value for each other, and 3) enriches service-dominant logic with a focus on sociotechnical aspects that are central to many practice studies.
DA  - 2023/08//
PY  - 2023
DO  - 10.1007/s10606-023-09478-3
VL  - 33
IS  - 3
SP  - 499
EP  - 551
SN  - 0925-9724
UR  - https://doi.org/10.1007/s10606-023-09478-3
KW  - Children
KW  - Boundary resource
KW  - Disability
KW  - Ecosystem
KW  - Practice
KW  - Practice-centered computing
KW  - Resource integration
KW  - S-D logic
KW  - Service
KW  - Service ecosystem
KW  - Service-dominant logic
KW  - Social inclusion
KW  - Value
KW  - Value co-creation
ER  - 

TY  - CONF
TI  - Steps towards value-aligned systems
AU  - Osoba, Osonde A.
AU  - Boudreaux, Benjamin
AU  - Yeung, Douglas
T3  - Aies '20
AB  - Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are now indispensable tools that help us manage the flood of information we use to try to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the body of research focused on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment so far has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.
C1  - New York, NY, USA and New York, NY, USA
C3  - Proceedings of the AAAI/ACM conference on AI, ethics, and society
DA  - 2020///
PY  - 2020
DO  - 10.1145/3375627.3375872
SP  - 332
EP  - 336
PB  - Association for Computing Machinery
SN  - 978-1-4503-7110-0
UR  - https://doi.org/10.1145/3375627.3375872
KW  - sociotechnical systems
KW  - ml fairness
KW  - systems analysis
KW  - value alignment
ER  - 

TY  - CONF
TI  - Where to go and what to do: Towards understanding task-based information behavior at transitional spaces
AU  - Kilian, Melanie A.
T3  - Chiir '19
AB  - In public transitional spaces, such as airports, users are faced with diverse challenges regarding information interaction and use. These challenges arise due to the scheduled and/or location-dependent procedures users are required to perform. Understanding what these users need or desire in the context of such spaces, what information is on offer, both online and in situ, and how these aspects interrelate is important to facilitate the design of systems that are accepted by the users concerned. However, very little is known about human information behavior (HIB) in public transitional spaces. As a starting point to understand how behavior in such spaces relates to or differs from information behavior in other contexts, holistically, I will create an explanatory model of airport information behavior by conducting an exploratory grounded theory based field study and relating my findings to those of existing models.
C1  - Glasgow, Scotland UK and New York, NY, USA
C3  - Proceedings of the 2019 conference on human information interaction and retrieval
DA  - 2019///
PY  - 2019
DO  - 10.1145/3295750.3298972
SP  - 413
EP  - 416
PB  - Association for Computing Machinery
SN  - 978-1-4503-6025-8
UR  - https://doi.org/10.1145/3295750.3298972
KW  - field study
KW  - grounded theory
KW  - information behavior
KW  - information needs
KW  - information tasks
KW  - models of information behavior
ER  - 

TY  - JOUR
TI  - Navigating the Nexus of ethical standards and moral values
AU  - Hammerschmidt, Teresa
T2  - Ethics and Inf. Technol.
AB  - This study examines how ethical standards established by stakeholders such as developers and policymakers provide top-down guidance aligned with deontological ethics or utilitarian goals. It also highlights a complementary bottom-up approach, rooted in virtue ethics, in which individuals engage in ethical deliberations shaped by their moral values. Both approaches have limitations, and, at times, ethical standards can clash with moral values, thus blurring lines of responsibilities. Deontological principles may offer a structured framework, but often lack adaptability to diverse cultural contexts; bottom-up approaches foster intrinsic moral intentions, but universal applicability may be challenging, thus raising moral dilemmas. Through a theoretical literature review, this study explains how different ontological and normative ethical perceptions lead to moral dilemmas in various AI application scenarios (e.g., algorithmically managed platforms, crime detection systems, medical AI assistants). It addresses top-down and bottom-up approaches that may help account for moral dilemmas ethically. The study discusses the balance between top-down regulatory frameworks and bottom-up community-driven ethics to navigate the complex ethical landscape of AI applications, whose increasing capabilities alter expectations of AI’s agency and morality. This study calls for holistic and multi-objective ethical frameworks that incorporate diverse normative ethical perspectives and recognizes context-specific ontologies throughout the AI lifecycle. It emphasizes a nuanced and context-specific combination of top-down standards (e.g., regulatory oversight, clear guidelines) and bottom-up fostering of moral values (e.g., by improving ethical knowledge). This tailored and ongoing reflection of ethical standards and moral values accounts for an ethical development, deployment, and utilization of AI technologies.
DA  - 2025/03//
PY  - 2025
DO  - 10.1007/s10676-025-09826-5
VL  - 27
IS  - 2
SN  - 1388-1957
UR  - https://doi.org/10.1007/s10676-025-09826-5
KW  - AI ethics
KW  - AI governance
KW  - Literature review
KW  - Normative ethics
ER  - 

TY  - CONF
TI  - Applying human cognition to assured autonomy
AU  - López-González, Mónica
AB  - The scaled deployment of semi- and fully autonomous systems undeniably depends on assured autonomy. This reality, however, has become far more complex than expected because it necessarily demands an integrated tripartite solution not yet achieved: consensus-based standards and compliance across industry, scientific innovation within artificial intelligence R&amp;D of explainability, and robust end-user education. In this is paper I present my human-centered approach to the design, development, and deployment of autonomous systems and break down how human factors such as cognitive and behavioral insights into how we think, feel, act, plan, make decisions, and problem-solve are foundational to assuring autonomy.
C1  - Berlin, Heidelberg
C3  - HCI international 2021 - late breaking papers: Multimodality, extended reality, and artificial intelligence: 23rd HCI international conference, HCII 2021, virtual event, july 24–29, 2021, proceedings
DA  - 2021///
PY  - 2021
DO  - 10.1007/978-3-030-90963-5_36
SP  - 474
EP  - 488
PB  - Springer-Verlag
SN  - 978-3-030-90962-8
UR  - https://doi.org/10.1007/978-3-030-90963-5_36
KW  - Artificial intelligence
KW  - Trust
KW  - Autonomous vehicles
KW  - Human factors
KW  - Explainability
KW  - Assured autonomy
ER  - 

TY  - JOUR
TI  - Accountability for the use of algorithms in a big data environment
AU  - Vedder, Anton
AU  - Naudts, Laurens
T2  - Int. Rev. Law Comput. Technol.
AB  - Accountability is the ability to provide good reasons in order to explain and to justify actions, decisions and policies for a hypothetical forum of persons or organisations. Since decision-makers, both in the private and in the public sphere, increasingly rely on algorithms operating on Big Data for their decision-making, special mechanisms of accountability concerning the making and deployment of algorithms in that setting become gradually more urgent. In the upcoming General Data Protection Regulation, the importance of accountability and closely related concepts, such as transparency, as guiding protection principles, is emphasised. Yet, the accountability mechanisms inherent in the regulation cannot be appropriately applied to algorithms operating on Big Data and their societal impact. First, algorithms are complex. Second, algorithms often operate on a random group level, which may pose additional difficulties when interpreting and articulating the risks of algorithmic decision-making processes. In light of the possible significance of the impact on human beings, the complexities and the broader scope of algorithms in a big data setting call for accountability mechanisms that transcend the mechanisms that are now inherent in the regulation.
DA  - 2017/05//
PY  - 2017
DO  - 10.1080/13600869.2017.1298547
VL  - 31
IS  - 2
SP  - 206
EP  - 224
SN  - 1360-0869
UR  - https://doi.org/10.1080/13600869.2017.1298547
ER  - 

TY  - JOUR
TI  - The survey on multi-source data fusion in cyber-physical-social systems: Foundational infrastructure for industrial metaverses and industries 5.0
AU  - Wang, Xiao
AU  - Wang, Yutong
AU  - Yang, Jing
AU  - Jia, Xiaofeng
AU  - Li, Lijun
AU  - Ding, Weiping
AU  - Wang, Fei-Yue
T2  - Inf. Fusion
DA  - 2024/07//
PY  - 2024
DO  - 10.1016/j.inffus.2024.102321
VL  - 107
IS  - C
SN  - 1566-2535
UR  - https://doi.org/10.1016/j.inffus.2024.102321
KW  - Social manufacturing
KW  - CPSS
KW  - Industrial metaverses
KW  - Multi-source data fusion
KW  - Parallel manufacturing
ER  - 

TY  - CONF
TI  - Harms from increasingly agentic algorithmic systems
AU  - Chan, Alan
AU  - Salganik, Rebecca
AU  - Markelius, Alva
AU  - Pang, Chris
AU  - Rajkumar, Nitarshan
AU  - Krasheninnikov, Dmitrii
AU  - Langosco, Lauro
AU  - He, Zhonghao
AU  - Duan, Yawen
AU  - Carroll, Micah
AU  - Lin, Michelle
AU  - Mayhew, Alex
AU  - Collins, Katherine
AU  - Molamohammadi, Maryam
AU  - Burden, John
AU  - Zhao, Wanru
AU  - Rismani, Shalaleh
AU  - Voudouris, Konstantinos
AU  - Bhatt, Umang
AU  - Weller, Adrian
AU  - Krueger, David
AU  - Maharaj, Tegan
T3  - FAccT '23
AB  - Research in Fairness, Accountability, Transparency, and Ethics (FATE)1 has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed, typically without strong regulatory barriers, threatening the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms, rather than just responding to them. Anticipation of harms is especially important given the rapid pace of developments in machine learning (ML). Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency – notably, these include systemic and/or long-range impacts, often on marginalized or unconsidered stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems.
C1  - Chicago, IL, USA and New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594033
SP  - 651
EP  - 666
PB  - Association for Computing Machinery
SN  - 979-8-4007-0192-4
UR  - https://doi.org/10.1145/3593013.3594033
KW  - safety
KW  - ethics
KW  - autonomy
KW  - sociotechnical systems
KW  - power
KW  - agency
KW  - algorithmic systems
KW  - delayed impacts
KW  - FATE
KW  - harms
KW  - negative externalities
ER  - 

TY  - CONF
TI  - Autistic children's use of technology and media: a fieldwork study
AU  - Alarcon-Licona, Susana
AU  - Loke, Lian
T3  - Idc '17
AB  - This qualitative field study conducted at a school for autistic children aimed to explain how low and medium functioning autistic children use technology at school. Additionally, it explored education professionals and parents' attitudes and concerns regarding technology use by children. Data-collection methods included focus groups with nine education professionals, naturalistic observations of fifteen children with autism in therapy sessions, and interviews with parents of four of the children. Results revealed that (1) children are regular users of digital technology and media, (2) there are different patterns of use of technologies with possible correlations to accessibility and behavioural manifestations of autism, (3) education professionals and parents have positive attitude towards the use of technology by the children, but also recognize disadvantages to its use.
C1  - Stanford, California, USA and New York, NY, USA
C3  - Proceedings of the 2017 conference on interaction design and children
DA  - 2017///
PY  - 2017
DO  - 10.1145/3078072.3084338
SP  - 651
EP  - 658
PB  - Association for Computing Machinery
SN  - 978-1-4503-4921-5
UR  - https://doi.org/10.1145/3078072.3084338
KW  - assistive technology
KW  - field study
KW  - autism
KW  - children
KW  - digital technology
KW  - interactive technology
KW  - media
ER  - 

TY  - JOUR
TI  - Operationalizing the ethics of connected and automated vehicles: An engineering perspective
AU  - Matteucci, Matteo
AU  - Fossa, Fabio
AU  - Arrigoni, Stefano
AU  - Caruso, Giandomenico
AU  - Cholakkal, Hafeez Husain
AU  - Dahal, Pragyan
AU  - Cheli, Federico
AB  - In response to the many social impacts of automated mobility, in September 2020 the European Commission published Ethics of Connected and Automated Vehicles, a report in which recommendations on road safety, privacy, fairness, explainability, and responsibility are drawn from a set of eight overarching principles. This paper presents the results of an interdisciplinary research where philosophers and engineers joined efforts to operationalize the guidelines advanced in the report. To this aim, we endorse a function-based working approach to support the implementation of values and recommendations into the design of automated vehicle technologies. Based on this, we develop methodological tools to tackle issues related to personal autonomy, explainability, and privacy as domains that most urgently require fine-grained guidance due to the associated ethical risks. Even though each tool still requires further inquiry, we believe that our work might already prove the productivity of the function-based approach and foster its adoption in the CAV scientific community.
DA  - 2022/02//
PY  - 2022
DO  - 10.4018/IJT.291553
VL  - 13
IS  - 1
SP  - 1
EP  - 20
J2  - Int. J. Technoethics
SN  - 1947-3451
UR  - https://doi.org/10.4018/IJT.291553
KW  - Ethics
KW  - Autonomy
KW  - Privacy
KW  - Principles
KW  - Explainability
KW  - Application
KW  - Connected and Automated Vehicles
ER  - 

TY  - CONF
TI  - Category-independent visual explanation for&nbsp;medical deep network understanding
AU  - Qian, Yiming
AU  - Li, Liangzhi
AU  - Fu, Huazhu
AU  - Wang, Meng
AU  - Peng, Qingsheng
AU  - Tham, Yih Chung
AU  - Cheng, Chingyu
AU  - Liu, Yong
AU  - Goh, Rick Siow Mong
AU  - Xu, Xinxing
AB  - Visual explanations have the potential to improve our understanding of deep learning models and their decision-making process, which is critical for building transparent, reliable, and trustworthy AI systems. However, existing visualization methods have limitations, including their reliance on categorical labels to identify regions of interest, which may be inaccessible during model deployment and lead to incorrect diagnoses if an incorrect label is provided. To address this issue, we propose a novel category-independent visual explanation method called Hessian-CIAM. Our algorithm uses the Hessian matrix, which is the second-order derivative of the activation function, to weigh the activation weight in the last convolutional layer and generate a region of interest heatmap at inference time. We then apply an SVD-based post-process to create a smoothed version of the heatmap. By doing so, our algorithm eliminates the need for categorical labels and modifications to the deep learning model. To evaluate the effectiveness of our proposed method, we compared it to seven state-of-the-art algorithms using the Chestx-ray8 dataset. Our approach achieved a 55
C1  - Vancouver, BC, Canada and Berlin, Heidelberg
C3  - Medical image computing and computer assisted intervention – MICCAI 2023: 26th international conference, vancouver, BC, canada, october 8–12, 2023, proceedings, part II
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-43895-0_17
SP  - 181
EP  - 191
PB  - Springer-Verlag
SN  - 978-3-031-43894-3
UR  - https://doi.org/10.1007/978-3-031-43895-0_17
ER  - 

TY  - THES
TI  - Human-centric machine learning: Enabling machine learning for high-stakes decision-making
AU  - Lakkaraju, Katyaini
AB  - Domains such as law, healthcare, and public policy often involve highly consequential decisions which are predominantly made by human decision-makers. The growing availability of data pertaining to such decisions offers an unprecedented opportunity to develop machine learning models which can help humans in making better decisions. However, the applicability of machine learning to such scenarios is limited by certain fundamental challenges: a) The data is selectively labeled i.e., we only observe the outcomes of the decisions made by human decision-makers and not the counterfactuals. b) The data is prone to a variety of selection biases and confounding effects. c) The successful adoption of the models that we develop depends on how well decision-makers can understand and trust their functionality, however, most of the existing machine learning models are primarily optimized for predictive accuracy and are not very interpretable. In this dissertation, we develop novel computational frameworks which address the aforementioned challenges, thus, paving the way for large-scale deployment of machine learning models and algorithms to address problems of significant societal impact. We first discuss how to build interpretable predictive models and explanations of complex black box models which can be readily understood and consequently trusted by human decision-makers. We then outline novel evaluation strategies which allow us to reliably compare the quality of human and algorithmic decision-making while accounting for challenges such as selective labels and confounding effects. Lastly, we present approaches which can diagnose and characterize biases (systematic errors) in human decisions and algorithmic predictions.
CY  - Stanford, CA, USA
DA  - 2018///
PY  - 2018
M3  - phd
PB  - Stanford University
N1  - AAI28114747
ER  - 

TY  - JOUR
TI  - Responsible data management
AU  - Stoyanovich, Julia
AU  - Howe, Bill
AU  - Jagadish, H. V.
T2  - Proc. VLDB Endow.
AB  - The need for responsible data management intensifies with the growing impact of data on society. One central locus of the societal impact of data are Automated Decision Systems (ADS), socio-legal-technical systems that are used broadly in industry, non-profits, and government. ADS process data about people, help make decisions that are consequential to people's lives, are designed with the stated goals of improving efficiency and promoting equitable access to opportunity, involve a combination of human and automated decision making, and are subject to auditing for legal compliance and to public disclosure. They may or may not use AI, and may or may not operate with a high degree of autonomy, but they rely heavily on data.In this article, we argue that the data management community is uniquely positioned to lead the responsible design, development, use, and oversight of ADS. We outline a technical research agenda that requires that we step outside our comfort zone of engineering for efficiency and accuracy, to also incorporate reasoning about values and beliefs. This seems high-risk, but one of the upsides is being able to explain to our children what we do and why it matters.
DA  - 2020/08//
PY  - 2020
DO  - 10.14778/3415478.3415570
VL  - 13
IS  - 12
SP  - 3474
EP  - 3488
SN  - 2150-8097
UR  - https://doi.org/10.14778/3415478.3415570
ER  - 

TY  - JOUR
TI  - The adoption of artificial intelligence in bureaucratic decision-making: a weberian perspective
AU  - Cetina Presuel, Rodrigo
AU  - Martinez Sierra, Jose M.
T2  - Digit. Gov.: Res. Pract.
AB  - This work questions AI´s role in bureaucratic decision-making. The Weberian conception of bureaucracy, based around the concept of an ideal bureaucracy in which authority is distributed, delegated, clearly delimited and hierarchical and that enshrines the following of formal rules, task specialization through division of labor, legal certainty and a predilection for efficiency in recordable and accountable decisions can serve as a framework to orient how governments should approach the adoption of artificial intelligence (AI) given the many problems associated with its careless deployment. Using theoretical analysis, this work explains Weberian ideas of bureaucracy and contrasts them with real-life cases of implementation of AI in bureaucratic decision-making, often with detrimental results for society. After identifying and framing issues related to AI, e.g., lack of transparency, attempts to shift accountability from humans to technology, the exacerbation of bias and potential for systemic discrimination, the paper proposes Weberian prescriptions that should help public administration make careful decisions about the adoption of AI and the consequences of its implementation. The article also engages with Weber critically, rejecting the notion that public administrators do not engage in politics and asserting that AI decision-making is necessarily political as well, as it entails exercising power over citizens.
DA  - 2024/03//
PY  - 2024
DO  - 10.1145/3609861
VL  - 5
IS  - 1
UR  - https://doi.org/10.1145/3609861
KW  - artificial intelligence
KW  - automated decision-making
KW  - Al bias
KW  - Digital Government
KW  - Weberian bureaucracy
ER  - 

TY  - CONF
TI  - Social robots in&nbsp;the&nbsp;wild and&nbsp;the&nbsp;novelty effect
AU  - Reimann, Merle
AU  - van de Graaf, Jesper
AU  - van Gulik, Nina
AU  - van de Sanden, Stephanie
AU  - Verhagen, Tibert
AU  - Hindriks, Koen
AB  - We designed a wine recommendation robot and deployed it in a small supermarket. In a study aimed to evaluate our design we found that people with no intent to buy wine were interacting with the robot rather than the intended audience of wine-buying customers. Behavioural data, moreover, suggests a very different evaluation of the robot than the surveys that were completed. We also found that groups were interacting more with the robot than individuals, a finding that has been reported more often in the literature. All of these findings taken together suggest that a novelty effect may have been at play. It also suggests that field studies should take this effect more seriously. The main contribution of our work is in identifying and proposing a set of indicators and thresholds that can be used to identify that a novelty effect is present. We argue that it is important to focus more on measuring attitudes towards robots that may explain behaviour due to novelty effects. Our findings also suggest research should focus more on verifying whether real user needs are met.
C1  - Doha, Qatar and Berlin, Heidelberg
C3  - Social robotics: 15th international conference, ICSR 2023, doha, qatar, december 3–7, 2023, proceedings, part II
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-981-99-8718-4_4
SP  - 38
EP  - 48
PB  - Springer-Verlag
SN  - 978-981-99-8717-7
UR  - https://doi.org/10.1007/978-981-99-8718-4_4
KW  - Socially Assistive Robots
KW  - Novelty effect indicators
KW  - Retail
ER  - 

TY  - JOUR
TI  - A typology of explanations to support Explainability-by-Design
AU  - Tsakalakis, Niko
AU  - Stalla-Bourdillon, Sophie
AU  - Huynh, Dong
AU  - Moreau, Luc
T2  - ACM J. Responsib. Comput.
AB  - As automated decision-making permeates almost all aspects of everyday life, capabilities to generate meaningful explanations for various stakeholders (i.e., decision-makers, addressees of decisions including individuals, auditors, and regulators) should be carefully deployed. This article presents a typology of explanations intended to support the first pillar of an explainability-by-design strategy. Its production has been achieved by pursuing a responsible innovation approach and introducing a new persona within the research and innovation process, i.e., a legal engineer, whose role is to work at the interface of two teams, the compliance and the engineering teams, and to oversee the process of requirement elicitation, which is often opinionated and narrowing. Once explanation requirements have been derived from applicable regulatory requirements, compliance rules, or business policies, they have been mapped to the dimensions of the typology to produce fine-grained explanation requirements, forming computable building blocks that can then be translated into system requirements during the technical design phase. The typology has been co-created with industry partners operating in two sectors: finance and education. Two pilot studies have thus been conducted to test both the feasibility of the generation and computation of explanations on the basis of the typology and the usefulness of the outputs in the light of the state-of-the-art. The typology comprises nine hierarchical dimensions. It can be leveraged to operate a stand-alone classifier of explanations that acts as detective controls within a broader partially automated compliance strategy. A machine-readable format of the typology is provided in the form of a light ontology.
DA  - 2025/02//
PY  - 2025
DO  - 10.1145/3708504
VL  - 2
IS  - 1
UR  - https://doi.org/10.1145/3708504
KW  - Artificial intelligence
KW  - explainability
KW  - data protection
KW  - automated decisions
KW  - typology
ER  - 

TY  - JOUR
TI  - Artificial Intelligence risk measurement
AU  - Giudici, Paolo
AU  - Centurelli, Mattia
AU  - Turchetta, Stefano
T2  - Expert Systems With Applications
DA  - 2024/01//
PY  - 2024
DO  - 10.1016/j.eswa.2023.121220
VL  - 235
IS  - C
J2  - Expert Syst. Appl.
SN  - 0957-4174
UR  - https://doi.org/10.1016/j.eswa.2023.121220
KW  - Machine learning
KW  - Sustainability
KW  - Fairness
KW  - Explainability
KW  - Accuracy
KW  - Financial risk management
ER  - 

TY  - JOUR
TI  - Prioritising national healthcare service issues from free text feedback – A computational text analysis &amp; predictive modelling approach
AU  - Ojo, Adegboyega
AU  - Rizun, Nina
AU  - Walsh, Grace
AU  - Mashinchi, Mona Isazad
AU  - Venosa, Maria
AU  - Rao, Manohar Narayana
T2  - Decision Support Systems
DA  - 2024/06//
PY  - 2024
DO  - 10.1016/j.dss.2024.114215
VL  - 181
IS  - C
J2  - Decis. Support Syst.
SN  - 0167-9236
UR  - https://doi.org/10.1016/j.dss.2024.114215
KW  - Computational grounded theory
KW  - Issue valence and salience
KW  - Maternity service experience
KW  - Policy &amp
KW  - Policy analytics
KW  - programme monitoring
KW  - Service quality
KW  - Theory of change
ER  - 

TY  - JOUR
TI  - Designing for self-organisation in sociotechnical systems: resilience engineering, cognitive work analysis, and the diagram of work organisation possibilities
AU  - Naikar, Neelam
AU  - Elix, Ben
AB  - In designing sociotechnical systems, accounting for the phenomenon of self-organisation is critical. Empirical studies show that workers in these systems adapt not just their individual behaviours, but also their collective structures to deal with complex work environments. The concept of self-organisation can explain how such adaptations can be achieved spontaneously, continuously, and relatively seamlessly, and why this phenomenon is important for dealing with instability, uncertainty, and unpredictability in the task demands. However, existing design approaches such as resilience engineering and cognitive work analysis are limited in their capacity to design for self-organisation. This paper demonstrates that the diagram of work organisation possibilities, a recent addition to cognitive work analysis, provides a sound theoretical basis for designing for self-organisation. That is, it shows how essential components of the diagram are aligned with the concept of self-organisation and are well-grounded in empirical observations of adaptation in a variety of sociotechnical systems, specifically emergency management, military, and healthcare systems. Consequently, designs based on this diagram should have the potential to facilitate the emergence of new spatial, temporal, and functional organisational structures from the flexible actions of individual, interacting actors, thereby enhancing a system’s capacity for dealing with a dynamic, ambiguous work environment. Future research should focus on validating these ideas and demonstrating their value in industrial settings.
DA  - 2021/02//
PY  - 2021
DO  - 10.1007/s10111-019-00595-y
VL  - 23
IS  - 1
SP  - 23
EP  - 37
J2  - Cogn. Technol. Work
SN  - 1435-5558
UR  - https://doi.org/10.1007/s10111-019-00595-y
KW  - Adaptation
KW  - Emergence
KW  - Flexibility
KW  - Organisational structure
KW  - Work design
ER  - 

TY  - CONF
TI  - Chatbot arena: an open platform for evaluating LLMs by human preference
AU  - Chiang, Wei-Lin
AU  - Zheng, Lianmin
AU  - Sheng, Ying
AU  - Angelopoulos, Anastasios N.
AU  - Li, Tianle
AU  - Li, Dacheng
AU  - Zhu, Banghua
AU  - Zhang, Hao
AU  - Jordan, Michael I.
AU  - Gonzalez, Joseph E.
AU  - Stoica, Ion
T3  - ICML'24
AB  - Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowd-sourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. The platform is publicly available at https://chat.lmsys.org.
C1  - Vienna, Austria
C3  - Proceedings of the 41st international conference on machine learning
DA  - 2024///
PY  - 2024
PB  - JMLR.org
ER  - 

TY  - CONF
TI  - Insecure by design in the backbone of critical infrastructure
AU  - Wetzels, Jos
AU  - Dos Santos, Daniel
AU  - Ghafari, Mohammad
T3  - CPS-IoT week '23
AB  - We inspected 45 actively deployed Operational Technology (OT) product families from ten major vendors and found that every system suffers from at least one trivial vulnerability. We reported a total of 53 weaknesses, stemming from insecure by design practices or basic security design failures. They enable attackers to take a device offline, manipulate its operational parameters, and execute arbitrary code without any constraint. We discuss why vulnerable products are often security certified and appear to be more secure than they actually are, and we explain complicating factors of OT risk management.
C1  - San Antonio, TX, USA and New York, NY, USA
C3  - Proceedings of cyber-physical systems and internet of things week 2023
DA  - 2023///
PY  - 2023
DO  - 10.1145/3576914.3587485
SP  - 7
EP  - 12
PB  - Association for Computing Machinery
SN  - 979-8-4007-0049-1
UR  - https://doi.org/10.1145/3576914.3587485
KW  - operational technology
KW  - secure by design
KW  - Vulnerability
ER  - 

TY  - CONF
TI  - Optimizing SaaS solutions for enhanced sustainability and predictive management of cloud assets
AU  - Poghosyan, Arnak
AU  - Harutyunyan, Ashot
AU  - Bunarjyan, Tigran
AU  - Baloian, Nelson
T3  - Aiccc '23
AB  - Identifying waste resources in modern-scale cloud infrastructures is a critical sustainability issue since it helps free up additional capacities for extra tasks and improves the performance of the systems while optimizing their costs. It is already well-recognized as a challenging task for human operators regarding manual and massive action efforts. At the same time, the problem is quite complicated – a complete and satisfactory solution is yet to be achieved. The paper proposes a novel and AI-driven approach to the problem. Applying rule induction learning across the history of service deployment instances to the log event data of the underlying entities, we extract conditions that lead to specific patterns, such as Resource Termination, thus providing a predictive mechanism for detecting objects subject to such actions in a real-time fashion. This explainable recommender system (called Cloud Sweeper) serves as an AI operations assistant for cloud users and Site Reliability Engineers (SRE) in their administrative duties.
C1  - Kyoto, Japan and New York, NY, USA
C3  - Proceedings of the 2023 6th artificial intelligence and cloud computing conference
DA  - 2024///
PY  - 2024
DO  - 10.1145/3639592.3639620
SP  - 204
EP  - 210
PB  - Association for Computing Machinery
SN  - 979-8-4007-1622-5
UR  - https://doi.org/10.1145/3639592.3639620
KW  - explainable AI
KW  - AI Ops
KW  - Automated SaaS management
KW  - cloud waste management
KW  - log event data
KW  - rule induction learning
ER  - 

TY  - JOUR
TI  - IMAGE: An open-source, extensible framework for deploying accessible audio and haptic renderings of web graphics
AU  - Regimbal, Juliette
AU  - Blum, Jeffrey R.
AU  - Kuo, Cyan
AU  - Cooperstock, Jeremy R.
T2  - ACM Trans. Access. Comput.
AB  - For accessibility practitioners, creating and deploying novel multimedia interactions for people with disabilities is a nontrivial task. As a result, many projects aiming to support such accessibility needs come and go or never make it to a public release. To reduce the overhead involved in deploying and maintaining a system that transforms web content into multimodal renderings, we created an open source, modular microservices architecture as part of the IMAGE project. This project aims to design richer means of interacting with web graphics than is afforded by a screen reader and text descriptions alone. To benefit the community of accessibility software developers, we discuss this architecture and explain how it provides support for several multimodal processing pipelines. Beyond illustrating the initial use case that motivated this effort, we further describe two use cases outside the scope of our project to explain how a team could use the architecture to develop and deploy accessible solutions for their own work. We then discuss our team’s experience working with the IMAGE architecture, informed by discussions with six project members, and provide recommendations to other practitioners considering applying the framework to their own accessibility projects.
DA  - 2024/07//
PY  - 2024
DO  - 10.1145/3665223
VL  - 17
IS  - 2
SN  - 1936-7228
UR  - https://doi.org/10.1145/3665223
KW  - Blind
KW  - Low vision
KW  - Multimodal interaction
KW  - Systems and architecture
KW  - Web accessibility
ER  - 

TY  - JOUR
TI  - APPL: Adaptive planner parameter learning
AU  - Xiao, Xuesu
AU  - Wang, Zizhao
AU  - Xu, Zifan
AU  - Liu, Bo
AU  - Warnell, Garrett
AU  - Dhamankar, Gauraang
AU  - Nair, Anirudh
AU  - Stone, Peter
T2  - Robotics and Autonomous Systems
DA  - 2022/08//
PY  - 2022
DO  - 10.1016/j.robot.2022.104132
VL  - 154
IS  - C
J2  - Robot. Auton. Syst.
SN  - 0921-8890
UR  - https://doi.org/10.1016/j.robot.2022.104132
KW  - Machine learning
KW  - Mobile robot navigation
KW  - Motion planning
ER  - 

TY  - CONF
TI  - CodeAid: Evaluating a classroom deployment of an LLM-based programming assistant that balances student and educator needs
AU  - Kazemitabaar, Majeed
AU  - Ye, Runlong
AU  - Wang, Xiaoning
AU  - Henley, Austin Zachary
AU  - Denny, Paul
AU  - Craig, Michelle
AU  - Grossman, Tovi
T3  - Chi '24
AB  - Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student’s incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI’s unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.
C1  - Honolulu, HI, USA and New York, NY, USA
C3  - Proceedings of the 2024 CHI conference on human factors in computing systems
DA  - 2024///
PY  - 2024
DO  - 10.1145/3613904.3642773
PB  - Association for Computing Machinery
SN  - 979-8-4007-0330-0
UR  - https://doi.org/10.1145/3613904.3642773
KW  - large language models
KW  - generative AI
KW  - AI assistants
KW  - AI tutoring
KW  - class deployment
KW  - design guidelines
KW  - educational technology
KW  - intelligent tutoring systems
KW  - programming education
ER  - 

TY  - CONF
TI  - Laban head-motions convey robot state: A call for robot body language
AU  - Knight, Heather
AU  - Simmons, Reid
AB  - Functional robots are an increasing presence in shared human-machine environments. Humans efficiently parse motion expressions, gaining an immediate impression of an agent's current action and state. Past work has shown that motion can effectively reveal a robot's current task objective to bystanders and collaborators, however, the layering of expression on pre-existing robot task motions has yet to be explored. Rather than showing us what the robot is doing, these layered motion characteristics leverage the how of the task motions to convey additional robot attitudes, e.g., confidence, adherence to deadline or flexibility of attention. To lay the foundations for this objective, we adapt the Laban Efforts, a system from dance and acting training in use for over 50 years. We operationalize features representing the four Laban Efforts (Time, Space, Weight, and Flow) to the movements of a 2-DOF Nao head and a 4-DOF Keepon robot during simple dance and look-for-someone behaviors. Using online survey, we collect 1028 motion ratings for 72 robot motion videos depicting contrasting Effort motion examples. We achieve statistically significant legibility results for all four Effort implementations. Even without human degrees of freedom, we find that robot motion patterns can convey complex expressions to people.
C1  - Stockholm, Sweden
C3  - 2016 IEEE international conference on robotics and automation (ICRA)
DA  - 2016///
PY  - 2016
DO  - 10.1109/ICRA.2016.7487451
SP  - 2881
EP  - 2888
PB  - IEEE Press
UR  - https://doi.org/10.1109/ICRA.2016.7487451
ER  - 

TY  - JOUR
TI  - Social influence for societal interest: a pro-ethical framework for improving human decision making through multi-stakeholder recommender systems
AU  - Fabbri, Matteo
T2  - AI Soc.
AB  - In the contemporary digital age, recommender systems (RSs) play a fundamental role in managing information on online platforms: from social media to e-commerce, from travels to cultural consumptions, automated recommendations influence the everyday choices of users at an unprecedented scale. RSs are trained on users’ data to make targeted suggestions to individuals according to their expected preference, but their ultimate impact concerns all the multiple stakeholders involved in the recommendation process. Therefore, whilst RSs are useful to reduce information overload, their deployment comes with significant ethical challenges, which are still largely unaddressed because of proprietary constraints and regulatory gaps that limit the effects of standard approaches to explainability and transparency. In this context, I address the ethical and social implications of automated recommendations by proposing a pro-ethical design framework aimed at reorienting the influence of RSs towards societal interest. In particular, after highlighting the problem of explanation for RSs, I discuss the application of beneficent informational nudging to the case of conversational recommender systems (CRSs), which rely on user-system dialogic interactions. Subsequently, through a comparison with standard recommendations, I outline the incentives for platforms and providers in adopting this approach and its benefits for both individual users and society.
DA  - 2022/05//
PY  - 2022
DO  - 10.1007/s00146-022-01467-2
VL  - 38
IS  - 2
SP  - 995
EP  - 1002
SN  - 0951-5666
UR  - https://doi.org/10.1007/s00146-022-01467-2
KW  - AI ethics
KW  - Explainability
KW  - Recommender systems
KW  - Social influence
ER  - 

TY  - CONF
TI  - FairRover: explorative model building for fair and responsible machine learning
AU  - Zhang, Hantian
AU  - Shahbazi, Nima
AU  - Chu, Xu
AU  - Asudeh, Abolfazl
T3  - Deem '21
AB  - The potential harms and drawbacks of automated decision making has become a challenge as data science blends into our lives. In particular, fairness issues with deployed machine learning models have drawn significant attention from the research community. Despite the myriad of algorithmic fairness work in various research communities, in practice data scientists still face many roadblocks in ensuring the fairness of their machine learning models. This is primarily because there does not exist an end-to-end system that guides the users in building a fair machine learning model in a responsible way from model auditing, to model explanation, to bias mitigation.We propose a explorative model building system FairRover for responsible fair model building. FairRover guides users in (1) discovering the potential biases in the model; (2) providing explanation to the discovered biases so as to help users in understanding potential causes of the biases; and (3) mitigating the most important biases selected by the users. Because of the impossibility theorem of fairness, and the well-known trade-off between fairness and accuracy, it is generally impossible to achieve a completely fair and accurate machine learning model. Therefore, this responsible model building process is naturally performed iteratively until a satisfying trade-off is reached. Human users are involved in the loop to make various decisions guided by FairRover.We demonstrate a case study on the Adult Census dataset, which shows how FairRover guides users in iteratively building a fair income prediction model in a responsible way. We discuss the current limitations of FairRover and future work.
C1  - Virtual Event, China and New York, NY, USA
C3  - Proceedings of the fifth workshop on data management for end-to-end machine learning
DA  - 2021///
PY  - 2021
DO  - 10.1145/3462462.3468882
PB  - Association for Computing Machinery
SN  - 978-1-4503-8486-5
UR  - https://doi.org/10.1145/3462462.3468882
ER  - 

TY  - JOUR
TI  - Meaningful human control as reason-responsiveness: the case of dual-mode vehicles
AU  - Mecacci, Giulio
AU  - Santoni de Sio, Filippo
T2  - Ethics and Inf. Technol.
AB  - In this paper, in line with the general framework of value-sensitive design, we aim to operationalize the general concept of “Meaningful Human Control” (MHC) in order to pave the way for its translation into more specific design&nbsp;requirements. In particular, we focus on the operationalization of the first of the two conditions (Santoni de Sio and Van den Hoven 2018) investigated: the so-called ‘tracking’ condition. Our investigation is led in relation to one specific subcase of automated system: dual-mode driving systems (e.g. Tesla ‘autopilot’). First, we connect and compare meaningful human control with a concept of control very popular in engineering and traffic psychology (Michon 1985), and we explain to what extent tracking resembles and differs from it. This will help clarifying the extent to which the idea of meaningful human control is connected to, but also goes beyond, current notions of control in engineering and psychology. Second, we take the systematic analysis of practical reasoning as&nbsp;traditionally presented in the philosophy of human action (Anscombe, Bratman, Mele)&nbsp;and we adapt it to offer a general framework where different types of reasons and&nbsp;agents are identified according to their relation to an automated system’s behaviour.&nbsp;This framework is meant to help explaining what reasons and what agents (should) play a role in controlling a given system, thereby enabling policy makers to produce&nbsp;usable guidelines and engineers to design systems that properly respond to selected&nbsp;human reasons. In the final part, we discuss a practical example of how our framework&nbsp;could be employed in designing automated driving systems.
DA  - 2020/06//
PY  - 2020
DO  - 10.1007/s10676-019-09519-w
VL  - 22
IS  - 2
SP  - 103
EP  - 115
SN  - 1388-1957
UR  - https://doi.org/10.1007/s10676-019-09519-w
KW  - Meaningful human control
KW  - Accountability for autonomous systems
KW  - Ethics of human–robot interaction
KW  - Ethics of self-driving cars
KW  - Proximity scale of reasons
KW  - Responsible innovation in self-driving cars
ER  - 

TY  - CONF
TI  - Resilient chatbots: Repair strategy preferences for conversational breakdowns
AU  - Ashktorab, Zahra
AU  - Jain, Mohit
AU  - Liao, Q. Vera
AU  - Weisz, Justin D.
T3  - Chi '19
AB  - Text-based conversational systems, also referred to as chatbots, have grown widely popular. Current natural language understanding technologies are not yet ready to tackle the complexities in conversational interactions. Breakdowns are common, leading to negative user experiences. Guided by communication theories, we explore user preferences for eight repair strategies, including ones that are common in commercially-deployed chatbots (e.g., confirmation, providing options), as well as novel strategies that explain characteristics of the underlying machine learning algorithms. We conducted a scenario-based study to compare repair strategies with Mechanical Turk workers (N=203). We found that providing options and explanations were generally favored, as they manifest initiative from the chatbot and are actionable to recover from breakdowns. Through detailed analysis of participants' responses, we provide a nuanced understanding on the strengths and weaknesses of each repair strategy.
C1  - Glasgow, Scotland Uk and New York, NY, USA
C3  - Proceedings of the 2019 CHI conference on human factors in computing systems
DA  - 2019///
PY  - 2019
DO  - 10.1145/3290605.3300484
SP  - 1
EP  - 12
PB  - Association for Computing Machinery
SN  - 978-1-4503-5970-2
UR  - https://doi.org/10.1145/3290605.3300484
KW  - conversational agents
KW  - chatbots
KW  - conversational breakdown
KW  - grounding
KW  - repair
ER  - 

TY  - CONF
TI  - Mind the gap: Autonomous systems, the responsibility gap, and moral entanglement
AU  - Goetze, Trystan S.
T3  - FAccT '22
AB  - When a computer system causes harm, who is responsible? This question has renewed significance given the proliferation of autonomous systems enabled by modern artificial intelligence techniques. At the root of this problem is a philosophical difficulty known in the literature as the responsibility gap. That is to say, because of the causal distance between the designers of autonomous systems and the eventual outcomes of those systems, the dilution of agency within the large and complex teams that design autonomous systems, and the impossibility of fully predicting how autonomous systems will behave once deployed, determining who is morally responsible for harms caused by autonomous systems is unclear at a conceptual level. I review past work on this topic, criticizing prior works for suggesting workarounds rather than philosophical answers to the conceptual problem presented by the responsibility gap. The view I develop, drawing on my earlier work on vicarious moral responsibility, explains why computing professionals are ethically required to take responsibility for the systems they design, despite not being blameworthy for the harms these systems may cause.
C1  - Seoul, Republic of Korea and New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533106
SP  - 390
EP  - 400
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533106
KW  - autonomous systems
KW  - accountability
KW  - computer ethics
KW  - ethics of artificial intelligence
KW  - lethal autonomous weapons systems (LAWS)
KW  - moral responsibility
KW  - professional responsibility
ER  - 

TY  - CONF
TI  - IAI MovieBot 2.0: An enhanced research platform with trainable neural components and transparent user modeling
AU  - Bernard, Nolwenn
AU  - Kostric, Ivica
AU  - Balog, Krisztian
T3  - Wsdm '24
AB  - While interest in conversational recommender systems has been on the rise, operational systems suitable for serving as research platforms for comprehensive studies are currently lacking. This paper introduces an enhanced version of the IAI MovieBot conversational movie recommender system, aiming to evolve it into a robust and adaptable platform for conducting user-facing experiments. The key highlights of this enhancement include the addition of trainable neural components for natural language understanding and dialogue policy, transparent and explainable modeling of user preferences, along with improvements in the user interface and research infrastructure.
C1  - Merida, Mexico and New York, NY, USA
C3  - Proceedings of the 17th ACM international conference on web search and data mining
DA  - 2024///
PY  - 2024
DO  - 10.1145/3616855.3635699
SP  - 1042
EP  - 1045
PB  - Association for Computing Machinery
SN  - 979-8-4007-0371-3
UR  - https://doi.org/10.1145/3616855.3635699
KW  - conversational ai
KW  - conversational recommender systems
ER  - 

TY  - JOUR
TI  - Autonomous weapons systems and the moral equality of combatants
AU  - Skerker, Michael
AU  - Purves, Duncan
AU  - Jenkins, Ryan
T2  - Ethics and Inf. Technol.
AB  - To many, the idea of autonomous weapons systems (AWS) killing human beings is grotesque. Yet critics have had difficulty explaining why it should make a significant moral difference if a human combatant is killed by an AWS as opposed to being killed by a human combatant. The purpose of this paper is to explore the roots of various deontological concerns with AWS and to consider whether these concerns are distinct from any concerns that also apply to long-distance, human-guided weaponry. We suggest that at least one major driver of the intuitive moral aversion to lethal AWS is that their use disrespects their human targets by violating the martial contract between human combatants. On our understanding of this doctrine, service personnel cede a right not to be directly targeted with lethal violence to other human agents alone. Artificial agents, of which AWS are one example, cannot understand the value of human life. A human combatant cannot transfer his privileges of targeting enemy combatants to a robot. Therefore, the human duty-holder who deploys AWS breaches the martial contract between human combatants and disrespects the targeted combatants. We consider whether this novel deontological objection to AWS forms the foundation of several other popular yet imperfect deontological objections to AWS.
DA  - 2020/09//
PY  - 2020
DO  - 10.1007/s10676-020-09528-0
VL  - 22
IS  - 3
SP  - 197
EP  - 209
SN  - 1388-1957
UR  - https://doi.org/10.1007/s10676-020-09528-0
KW  - Just war theory
KW  - Lethal autonomous weapons
KW  - Military ethics
KW  - Moral equality of combatants
ER  - 

TY  - CONF
TI  - A concept-based validation approach to validate security systems for protection of interconnected critical infrastructures
AU  - Stelkens-Kobsch, Tim H.
AU  - Boumann, Hilke
AU  - Piekert, Florian
AU  - Schaper, Meilin
AU  - Carstengerdes, Nils
T3  - Ares '23
AB  - When it comes to securing critical infrastructures, it is evident to not only provide a toolbox which allows to detect when vulnerabilities are exploited but also to support the operations in performing mitigation procedures. This paper explains how a validation was conducted in the Horizon 2020 project PRAETORIAN to evaluate the operational feasibility of a system which observes and manages security within interconnected critical infrastructures. To this end, a concept-based approach involving presentation of scenarios with the help of narrations and visual elements, hands-on experience as well as discussions and questionnaires was used. Some results are discussed to demonstrate the applicability of this approach.
C1  - Benevento, Italy and New York, NY, USA
C3  - Proceedings of the 18th international conference on availability, reliability and security
DA  - 2023///
PY  - 2023
DO  - 10.1145/3600160.3605025
PB  - Association for Computing Machinery
SN  - 979-8-4007-0772-8
UR  - https://doi.org/10.1145/3600160.3605025
KW  - Critical infrastructure protection
KW  - cyber-physical security
KW  - cybersecurity
KW  - hybrid security
KW  - validation
ER  - 

TY  - CONF
TI  - The importance of theory for understanding smart cities: Making a case for ambient theory
AU  - McKenna, H. Patricia
AB  - This paper seeks to develop a theoretical foundation for ambient theory as a theory in support of advancing definitions and understandings of smart cities and regions. Through a review of the evolving research literature for the ambient and for smart cities, a conceptual framework is formulated consisting of components and characteristics constituting ambient theory for smart cities and regions. The framework is then operationalized for use in this paper, exploring the practical application of ambient theory in smart cities and regions. Using a case study approach together with an explanatory correlational design, elements such as technology-driven services, creative opportunities, and access to public data are explored. Drawing additionally on other works where this approach is employed, elements such as awareness, information and communication technologies (ICTs), interactivity, and sensing are provided as further examples showing the potential for promising relationships in support of ambient theory for smart cities. Ambient theory as advanced in this paper is discussed in terms of theory usefulness, parsimony, and type. Future directions are identified for explorations of ambient theory going forward for both research and practice in contributing to definitions and understandings of smart cities and regions.
C1  - Berlin, Heidelberg
C3  - Distributed, ambient and pervasive interactions: 9th international conference, DAPI 2021, held as part of the 23rd HCI international conference, HCII 2021, virtual event, july 24–29, 2021, proceedings
DA  - 2021///
PY  - 2021
DO  - 10.1007/978-3-030-77015-0_4
SP  - 41
EP  - 54
PB  - Springer-Verlag
SN  - 978-3-030-77014-3
UR  - https://doi.org/10.1007/978-3-030-77015-0_4
KW  - Awareness
KW  - Adaptability
KW  - Ambient human-computer interaction
KW  - Ambient theory
KW  - Correlation
KW  - Creativity
KW  - Information and Communication Technologies (ICTs)
KW  - Interactivities
KW  - Sensing
KW  - Smart cities
KW  - Smart environments
KW  - Theory building
ER  - 

TY  - CONF
TI  - Explaining call recommendations in nursing homes: a user-centered design approach for interacting with knowledge-based health decision support systems
AU  - Gutiérrez, Francisco
AU  - Htun, Nyi Nyi
AU  - Vanden Abeele, Vero
AU  - De Croon, Robin
AU  - Verbert, Katrien
T3  - Iui '22
AB  - Recommender systems are increasingly used in high-risk application domains, including healthcare. It has been shown that explanations are crucial in this context to support decision-making. This paper explores how to explain call recommendations to nursing home staff, providing insights into call priority, notifications, and resident information. We present the design and implementation of a recommender engine and a mobile application designed to support call recommendations and explain these recommendations that may contribute to residents’ safety and quality of care. More specifically, we report on the results of a user-centered design approach with residents (N=12) and healthcare professionals (N=4), and a final evaluation (N=12) after four months of deployment. The results show that our design approach provides a valuable tool for more accurate and efficient decision-making. The overall system encourages nursing home staff to provide feedback and annotate, resulting in more confidence in the system. We discuss usability issues, challenges, and reflections to be considered in future health recommender systems.
C1  - Helsinki, Finland and New York, NY, USA
C3  - Proceedings of the 27th international conference on intelligent user interfaces
DA  - 2022///
PY  - 2022
DO  - 10.1145/3490099.3511158
SP  - 162
EP  - 172
PB  - Association for Computing Machinery
SN  - 978-1-4503-9144-3
UR  - https://doi.org/10.1145/3490099.3511158
ER  - 

TY  - JOUR
TI  - Principles and guidelines for evaluating social robot navigation algorithms
AU  - Francis, Anthony
AU  - Pérez-D’Arpino, Claudia
AU  - Li, Chengshu
AU  - Xia, Fei
AU  - Alahi, Alexandre
AU  - Alami, Rachid
AU  - Bera, Aniket
AU  - Biswas, Abhijat
AU  - Biswas, Joydeep
AU  - Chandra, Rohan
AU  - Chiang, Hao-Tien Lewis
AU  - Everett, Michael
AU  - Ha, Sehoon
AU  - Hart, Justin
AU  - How, Jonathan P.
AU  - Karnan, Haresh
AU  - Lee, Tsang-Wei Edward
AU  - Manso, Luis J.
AU  - Mirsky, Reuth
AU  - Pirk, Sören
AU  - Singamaneni, Phani Teja
AU  - Stone, Peter
AU  - Taylor, Ada V.
AU  - Trautman, Peter
AU  - Tsoi, Nathan
AU  - Vázquez, Marynel
AU  - Xiao, Xuesu
AU  - Xu, Peng
AU  - Yokoyama, Naoki
AU  - Toshev, Alexander
AU  - Martı́n-Martı́n, Roberto
T2  - J. Hum.-Robot Interact.
AB  - A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this article, we pave the road toward common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributions include (a) a definition of a socially navigating robot as one that respects the principles of safety, comfort, legibility, politeness, social competency, agent understanding, proactivity, and responsiveness to context, (b) guidelines for the use of metrics, development of scenarios, benchmarks, datasets, and simulators to evaluate social navigation, and (c) a design of a social navigation metrics framework to make it easier to compare results from different simulators, robots, and datasets.
DA  - 2025/02//
PY  - 2025
DO  - 10.1145/3700599
VL  - 14
IS  - 2
UR  - https://doi.org/10.1145/3700599
KW  - social robotics
KW  - benchmarks
KW  - datasets
KW  - robot navigation
KW  - simulators
ER  - 

TY  - CONF
TI  - Knowledge graph embedding in e-commerce applications: Attentive reasoning, explanations, and transferable rules
AU  - Zhang, Wen
AU  - Deng, Shumin
AU  - Chen, Mingyang
AU  - Wang, Liang
AU  - Chen, Qiang
AU  - Xiong, Feiyu
AU  - Liu, Xiangwen
AU  - Chen, Huajun
T3  - Ijckg '21
AB  - Knowledge Graphs (KGs), representing facts as triples, have been widely adopted in many applications. Reasoning tasks such as link prediction and rule induction are important for the development of KGs. Knowledge Graph Embeddings (KGEs) embedding entities and relations of a KG into continuous vector spaces, have been proposed for these reasoning tasks and proven to be efficient and robust. But the plausibility and feasibility of applying and deploying KGEs in real-work applications has not been well-explored. In this paper, we discuss and report our experiences of deploying KGEs in a real domain application: e-commerce. We first identity three important desiderata for e-commerce KG systems: 1) attentive reasoning, reasoning over a few target relations of more concerns instead of all; 2) explanation, providing explanations for a prediction to help both users and business operators understand why the prediction is made; 3) transferable rules, generating reusable rules to accelerate the deployment of a KG to new systems. While non existing KGE could meet all these desiderata, we propose a novel one, an explainable knowledge graph attention network that make prediction through modeling correlations between triples rather than purely relying on its head entity, relation and tail entity embeddings. It could automatically selects attentive triples for prediction and records the contribution of them at the same time, from which explanations could be easily provided and transferable rules could be efficiently produced. We empirically show that our method is capable of meeting all three desiderata in our e-commerce application and outperform typical baselines on datasets from real domain applications.
C1  - Virtual Event, Thailand and New York, NY, USA
C3  - Proceedings of the 10th international joint conference on knowledge graphs
DA  - 2022///
PY  - 2022
DO  - 10.1145/3502223.3502232
SP  - 71
EP  - 79
PB  - Association for Computing Machinery
SN  - 978-1-4503-9565-6
UR  - https://doi.org/10.1145/3502223.3502232
KW  - Reasoning
KW  - Explainable AI
KW  - Knowledge Graphs
KW  - E-commerce
KW  - Representation Learning
KW  - Rules
ER  - 

TY  - JOUR
TI  - MUREQ: a multilayer framework for analyzing and operationalizing visualization requirements
AU  - Li, Tong
AU  - Wang, Yiting
AU  - Wei, Xiang
AU  - Zhang, Xueying
AU  - Liu, Yu
T2  - Software and Systems Modeling
AB  - Understanding and interpreting vast amounts of information is pivotal in the contemporary data-rich age. Data visualization has emerged as a significant measure of comprehending these data. Similarly, an appropriate visualization can also enhance software modeling by providing straightforward and interactive representations. However, current data visualization methods predominantly require users to have data visualization-related expertise, which is usually challenging to obtain in reality. It is essential to bridge the gap between visualization requirements and visualization solutions for non-expert users, assisting them in automatically operationalizing their visualization requirements. This paper proposes a MUltilayer framework for analyzing and operationalizing visualization REQuirements that automatically derives appropriate visualization solutions based on users’ requirements. Specifically, we systematically investigate the connections among visualization requirements, visual variable characteristics, visual variable attributes, and visualization solutions, based on which we establish a conceptual framework that characterizes the relationships among different layers. Our proposal contributes to not only automatically operationalizing visualization requirements but also providing meaningful explanations for the derived visualization solutions. To promote our proposal and pragmatically benefit real users, we have developed and deployed a prototype tool based on the proposed framework, which is publicly available at . To evaluate our proposed framework, we conducted an initial controlled experiment with 44 participants to test the performance of the evolved mappings within our framework. Based on the expert’s feedback, we refined the mappings and incorporated a ranking system for visualization solutions tailored to specific requirements. To assess the current method, a subsequent experiment with another group of 44 participants and a focused case study involving two new participants were carried out. The results demonstrate that users perceive that the current method accelerates task completion, especially for complex tasks, by efficiently narrowing down options and prioritizing them. This approach is particularly advantageous for users with limited data visualization experience. Besides, the multilayer framework can be used to inspire the visualization of models in the software modeling community.
DA  - 2024/09//
PY  - 2024
DO  - 10.1007/s10270-024-01204-x
VL  - 23
IS  - 5
SP  - 1123
EP  - 1155
J2  - Softw. Syst. Model.
SN  - 1619-1366
UR  - https://doi.org/10.1007/s10270-024-01204-x
KW  - Empirical evaluation
KW  - Multilayer analysis
KW  - Prototype tool
KW  - Visualization requirements operationalization
ER  - 

TY  - CONF
TI  - Charting the design and analytics agenda of learnersourcing systems
AU  - Khosravi, Hassan
AU  - Demartini, Gianluca
AU  - Sadiq, Shazia
AU  - Gasevic, Dragan
T3  - LAK21
AB  - Learnersourcing is emerging as a viable learner-centred and pedagogically justified approach for harnessing the creativity and evaluation power of learners as experts-in-training. Despite the increasing adoption of learnersourcing in higher education, understanding students’ behaviour while engaged in learnersourcing and best practices for the design and development of learnersourcing systems are still largely under-researched. This paper offers data-driven reflections and lessons learned from the development and deployment of a learnersourcing adaptive educational system called RiPPLE, which to date, has been used in more than 50-course offerings with over 12,000 students. Our reflections are categorised into examples and best practices on (1) assessing the quality of students’ contributions using accurate, explainable and fair approaches to data analysis, (2) incentivising students to develop high-quality contributions and (3) empowering instructors with actionable and explainable insights to guide student learning. We discuss the implications of these findings and how they may contribute to the growing literature on the development of effective learnersourcing systems and more broadly technological educational solutions that support learner-centred learning at scale.
C1  - Irvine, CA, USA and New York, NY, USA
C3  - LAK21: 11th international learning analytics and knowledge conference
DA  - 2021///
PY  - 2021
DO  - 10.1145/3448139.3448143
SP  - 32
EP  - 42
PB  - Association for Computing Machinery
SN  - 978-1-4503-8935-8
UR  - https://doi.org/10.1145/3448139.3448143
KW  - explainable AI
KW  - crowdsourcing in education
KW  - human-centred computing
KW  - Learnersourcing
ER  - 

TY  - CONF
TI  - A first approach towards integrating computational argumentation in cognitive cities
AU  - Chesñevar, Carlos Iván
AU  - González, Marı́a Paula
AU  - Maguitman, Ana
AU  - Estevez, Elsa
T3  - Icegov '20
AB  - In the last years, the concept of Cognitive Smart City (CSC) emerged from the convergence of the Internet of Things, big data, smart city technologies, and artificial intelligence techniques. At the same time, computational argumentation has consolidated itself as a vibrant area in Artificial Intelligence (AI) which has engineered different approaches to reflect aspects of how humans build, exchange and analyze arguments in their daily lives, mainly to deal with controversial or inconsistent information. Thus, computational argumentation provides a valuable metaphor for reasoning on top of available data in order to draw conclusions and offer explanations for them. This paper discusses a first approach towards integrating computational argumentation in a layered model for cognitive cities, where the bottom layers comprise raw data collected from sensor, actuators and other artifacts deployed in the context of a smart city, and argumentation provides high-level intelligence abilities. We show how our approach can be paired with a layered model for computational argumentation. To illustrate our proposal, we analyze a case study, based on the DECIDE 2.0 framework.
C1  - Athens, Greece and New York, NY, USA
C3  - Proceedings of the 13th international conference on theory and practice of electronic governance
DA  - 2020///
PY  - 2020
DO  - 10.1145/3428502.3428506
SP  - 25
EP  - 32
PB  - Association for Computing Machinery
SN  - 978-1-4503-7674-7
UR  - https://doi.org/10.1145/3428502.3428506
KW  - Explainable Artificial Intelligence
KW  - Cognitive Cities Models
KW  - Computational Argumentation
ER  - 

TY  - JOUR
TI  - Algorithmic interactions in open source work
AU  - Shaikh, Maha
AU  - Vaast, Emmanuelle
AB  - This study focuses on algorithmic interactions in open source work. Algorithms are essential in open source because they remedy concerns incompletely addressed by parallel development or modularity. Following algorithmic interactions in open source allows us to map the operational performance of algorithms to understand how algorithms work with multiple other algorithms to accomplish work. Studying algorithms working together shows us how residual interdependencies of modularity and problems not resolved by dependence on parallel development are worked around to perform open source work. We examine the Linux Kernel case that reveals how algorithmic interactions facilitate open source work through the three processes of managing, organizing, and supervising development work. Our qualitative study theorizes how algorithmic interactions intensify through these processes that work together to facilitate development. We make a theoretical contribution to open source scholarship by explaining how algorithmic interactions navigate across module rigidity and enhance parallel development. Our work also reveals how, in open source, developers work to automate most tasks and augmentation is a bidirectional relationship of algorithms augmenting the work of developers and of developers augmenting the work of algorithms.This study focuses on algorithmic interactions in open source work. Algorithms are essential in open source because they remedy concerns incompletely addressed by parallel development or modularity. Following algorithmic interactions in open source allows us to map the performance of algorithms to understand the nature of work conducted by multiple algorithms functioning together. We zoom to the level of algorithmic interactions to show how residual interdependencies of modularity are worked around by algorithms. Moreover, the dependence on parallel development does not suffice to resolve all concerns related to the distributed work of open source. We examine the Linux Kernel case that reveals how algorithmic interactions facilitate open source work through the three processes of managing, organizing, and supervising development work. Our qualitative study theorizes how algorithmic interactions intensify through these processes that work together to facilitate development. We make a theoretical contribution to open source scholarship by explaining how algorithmic interactions navigate across module rigidity and enhance parallel development. Our work also reveals how, in open source, developers work to automate most tasks and augmentation is a bidirectional relationship of algorithms augmenting the work of developers and of developers augmenting the work of algorithms.History: Hemant Jain, Balaji Padmanabhan, Paul Pavlou, and Raghu Santanam, Senior Editors; Likoebe Maruping, Associate Editor.Supplemental Material: The online appendix is available at .
DA  - 2023/06//
PY  - 2023
DO  - 10.1287/isre.2022.1153
VL  - 34
IS  - 2
SP  - 744
EP  - 765
J2  - Info. Sys. Research
SN  - 1526-5536
UR  - https://doi.org/10.1287/isre.2022.1153
KW  - algorithmic interactions
KW  - augmentation and automation
KW  - modularity
KW  - open source work
KW  - parallel development
KW  - qualitative study
ER  - 

TY  - CONF
TI  - Computer assisted military experimentations
AU  - Cayirci, Erdal
AU  - AlNaimi, Ramzan
AU  - AlNabet, Sara Salem
T3  - Wsc '22
AB  - Computer assisted military experimentation methodology and process are explained. The military processes that can benefit from computer assisted military experimentation are introduced and the best practices for each process are elaborated on. Finally, emerging new concepts and their potential impact on the military experimentation requirements are briefly discussed and the tutorial is concluded. During the tutorial, live demonstrations are made for geostrategic foresight development, defense planning, operational plan analysis, computer assisted military experimentation design and conducting a computer assisted military experiment.
C1  - Singapore, Singapore
C3  - Proceedings of the winter simulation conference
DA  - 2023///
PY  - 2023
SP  - 1311
EP  - 1324
PB  - IEEE Press
ER  - 

TY  - JOUR
TI  - Agent-based simulation of unmanned aerial vehicles in civilian applications: A systematic literature review and research directions
AU  - Mualla, Yazan
AU  - Najjar, Amro
AU  - Daoud, Alaa
AU  - Galland, Stéphane
AU  - Nicolle, Christophe
AU  - Yasar, Ansar-Ul-Haque
AU  - Shakshuki, Elhadi
T2  - Future Gener. Comput. Syst.
DA  - 2019/11//
PY  - 2019
DO  - 10.1016/j.future.2019.04.051
VL  - 100
IS  - C
SP  - 344
EP  - 364
SN  - 0167-739X
UR  - https://doi.org/10.1016/j.future.2019.04.051
KW  - Multi-agent systems
KW  - Agent-based simulation
KW  - Civilian applications
KW  - Systematic literature review
KW  - Unmanned aerial vehicle
ER  - 

TY  - JOUR
TI  - Blazing the trail: Considering browsing path dependence in online service response strategy
AU  - Zuo, Meihua
AU  - Angelopoulos, Spyros
AU  - Liang, Zhouyang
AU  - Ou, Carol X. J.
T2  - Information Systems Frontiers
AB  - Competition on e-commerce platforms is becoming increasingly fierce, due to the ease of online searching for comparing products and services. We examine how the sequential browsing behavior of consumers can enable targeted marketing strategies on e-commerce platforms, by using clickstream data from one of the largest e-commerce platforms in Asia. We deploy duration analysis to i) explore how path dependence can better explain consumers’ sequential browsing behavior in different product categories, and ii) characterize the sequential browsing behavior of heterogeneous consumer groups. The findings of our work showcase i) the high accuracy of using sequential browsing path dependence to explain consumer behavior, ii) the patterns of their behavioral intentions and iii) the spell of the behavior of heterogeneous consumer groups. Our findings provide nuanced implications for strategically managing branding, marketing, and customer relations on e-commerce platforms. We discuss the implications of our findings for both research and practice, and we delineate an agenda for future research on the topic.
DA  - 2022/07//
PY  - 2022
DO  - 10.1007/s10796-022-10311-3
VL  - 25
IS  - 4
SP  - 1605
EP  - 1619
SN  - 1387-3326
UR  - https://doi.org/10.1007/s10796-022-10311-3
KW  - E-commerce
KW  - Business Analytics
KW  - Consumer Behavior
KW  - Duration Analysis
KW  - Path Dependence
ER  - 

TY  - CONF
TI  - Resignifying compliance between ontologies and epistemologies of law (invited paper)
AU  - Buffa, Matteo
AB  - This contribution is devoted to the topic of compliance in the legal-regulatory context, focusing on the need for new epistemologies of compliance that look at different, and interoperable, contexts. It is an attempt to understand and explain human and non-human cognitive processes at the basis of evaluations of a knowledge (and knowing) compliant capacity with the tension to reconstruct the different phases of the intelligibility of dispositions (and target documents that derive from them). The data analyzed so far allow us to imagine new (and plural) epistemic foundations capable of understanding and explaining this complexity. In particular, it seems to be possible to support a transition (linguistic, modeling, but also operational) from ontologies to epistemologies, all the more so when we are called upon to evaluate the issue of compliance.
C1  - Pittsburg, PA, USA and Berlin, Heidelberg
C3  - Advances in conceptual modeling: ER 2024 workshops, AISA, CMLS, EmpER, QUAMES, JUSMOD, llm4modeling, pittsburgh, PA, USA, october 28–31, 2024, proceedings
DA  - 2024///
PY  - 2024
DO  - 10.1007/978-3-031-75599-6_19
SP  - 253
EP  - 266
PB  - Springer-Verlag
SN  - 978-3-031-75598-9
UR  - https://doi.org/10.1007/978-3-031-75599-6_19
KW  - GDPR
KW  - Compliance
KW  - Epistemologies
KW  - Ontologies
KW  - Philosophy of Law
ER  - 

TY  - JOUR
TI  - When terminology hinders research: the colloquialisms of transitions of control in automated driving
AU  - Maggi, Davide
AU  - Romano, Richard
AU  - Carsten, Oliver
AU  - Winter, Joost C. F. De
AB  - During the last 20 years, technological advancement and economic interests have motivated research on automated driving and its impact on drivers’ behaviour, especially after transitions of control. Indeed, once the Automated Driving System (ADS) reaches its operational limits, it is forced to request human intervention. However, the fast accumulation and massive quantity of produced studies and the gaps left behind by standards have led to an imprecise and colloquial use of terms which, as technology and research interest evolve, creates confusion. The goal of this survey is to compare how different taxonomies describe transitions of control, address the current use of widely adopted terms in the field of transitions of control and explain how their use should be standardized to enhance future research. The first outcome of this analysis is a schematic representation of the correspondence among the elements of the reviewed taxonomies. Then, the definitions of “takeover” and “handover” are clarified as two parallel processes occurring in every transition of control. A second set of qualifiers, which are necessary to unequivocally define a transition of control and identify the agent requesting the transition and the agent receiving the request (ADS or the driver), is provided. The “initiator” is defined as the agent requesting the transition to take place, and the “receiver” is defined as the agent receiving that request.
DA  - 2022/08//
PY  - 2022
DO  - 10.1007/s10111-022-00705-3
VL  - 24
IS  - 3
SP  - 509
EP  - 520
J2  - Cogn. Technol. Work
SN  - 1435-5558
UR  - https://doi.org/10.1007/s10111-022-00705-3
KW  - Automation
KW  - Automated vehicles
KW  - Human factors
KW  - Taxonomies
KW  - Transitions of control
ER  - 

TY  - JOUR
TI  - Fog computing for big data analytics in IoT aided smart grid networks
AU  - Hussain, Md. Muzakkir
AU  - Beg, M. M. Sufyan
AU  - Alam, Mohammad Saad
AB  - The recent integration of Internet of Things and Cloud Computing (CC) technologies into a Smart Grid (SG) revolutionizes its operation. The scalable and unlimited Store Compute and Networking (SCN) resources offered by CC enables efficient Big Data Analytics of SG data. However, due to remote location of Cloud Data Centers and congested network traffic, the cloud often gives poor performance for latency and energy critical SG applications. Fog Computing (FC) is thus proposed as a model that distributes the SCN resources at the intermediary devices, termed as Fog Computing Nodes (FCN), viz. network gateways, battery powered servers, access points, etc. By executing application specific logic at those nodes, the FC astonishingly reduces the response time as well as energy consumption of network elements. In this paper, we propose a mathematical framework that explains the Planning and Placement of Fog computing in smart Grid (PPFG). Basically, the PPFG model is formulated as an Integer Linear Programming problem that determines the optimal location, the capacity and the number of FCNs, towards minimizing the average response delay and energy consumption of network elements. Since this optimization problem is trivially NP-Hard, we solve it using an evolutionary Non-dominated Sorting Genetic Algorithm. By running the model on an exemplary SG network, we demonstrate the operation of proposed PPFG model. In fact, we perform a complete analysis of the obtained Pareto Fronts (PF), in order to better understand the working of design constraints in the PPFG model. The PFs will enable the SG utilities and architectural designers to evaluate the pros and cons of each of the trade-off solutions, leading to intelligent planning, designing and deployment of FC based SG applications.
DA  - 2020/10//
PY  - 2020
DO  - 10.1007/s11277-020-07538-1
VL  - 114
IS  - 4
SP  - 3395
EP  - 3418
J2  - Wirel. Pers. Commun.
SN  - 0929-6212
UR  - https://doi.org/10.1007/s11277-020-07538-1
KW  - Big data analytics
KW  - Cloud computing
KW  - Fog computing
KW  - Smart grid
ER  - 

TY  - CONF
TI  - An konwledge-based semi-supervised active learning method for&nbsp;precision pest disease diagnostic
AU  - Zhu, Yong
AU  - Xiao, Shuai
AU  - Zhang, Zhuo
AU  - Wen, Jiabao
AU  - Xi, Meng
AU  - Yang, Jiachen
AB  - Over the past decade, deep learning (DL) has seen remarkable progress, and this advancement extends to the recognition of agricultural pests and diseases. However, the high expense of labeling agricultural images has posed a significant challenge for DL-based agricultural image analysis, thereby complicating the deployment of Internet of Things devices in agriculture. This paper introduces a knowledge-based, semi-supervised, and explanation-friendly active learning framework designed specifically for the recognition of agricultural pests and diseases. The proposed framework features an innovative active learning algorithm that efficiently selects both in-class and borderline samples. Additionally, it incorporates a semi-supervised strategy to harness the predictive capabilities of DL models, significantly reducing labeling costs. To integrate the scoring of individual samples by both active learning and semi-supervised methods, enhancing the diversity of sample selection, the framework includes a novel fusion strategy. The effectiveness of this approach is validated through experiments on a dataset of agricultural pest and disease images, with each module delivering promising results. This work offers a practical solution to reduce the costs associated with labeling agricultural data and to enhance the efficiency of model learning in the context of agricultural pest and disease recognition.
C1  - Birmingham, United Kingdom and Berlin, Heidelberg
C3  - Knowledge science, engineering and management: 17th international conference, KSEM 2024, birmingham, UK, august 16–18, 2024, proceedings, part I
DA  - 2024///
PY  - 2024
DO  - 10.1007/978-981-97-5492-2_11
SP  - 136
EP  - 147
PB  - Springer-Verlag
SN  - 978-981-97-5491-5
UR  - https://doi.org/10.1007/978-981-97-5492-2_11
KW  - Deep Learning
KW  - Active Learning
KW  - Pest Diagnostic
KW  - Semi-Supervised Learning
ER  - 

TY  - JOUR
TI  - Security and privacy oriented information security culture (ISC): Explaining unauthorized access to healthcare data by nursing employees
AU  - Mikuletič, Samanta
AU  - Vrhovec, Simon
AU  - Skela-Savič, Brigita
AU  - Žvanut, Boštjan
T2  - Computers & Security
DA  - 2024/01//
PY  - 2024
DO  - 10.1016/j.cose.2023.103489
VL  - 136
IS  - C
J2  - Comput. Secur.
SN  - 0167-4048
UR  - https://doi.org/10.1016/j.cose.2023.103489
KW  - Information security
KW  - Data breach
KW  - EHR
KW  - Electronic health records
KW  - Healthcare data
KW  - Information security culture
KW  - Nursing
ER  - 

TY  - CONF
TI  - Self-determination through explanation: an ethical perspective on the implementation of the transparency requirements for recommender systems set by the Digital Services Act of the European Union
AU  - Fabbri, Matteo
T3  - Aies '23
AB  - In the contemporary information age, recommender systems (RSs) play a critical role in influencing online behaviour: from social media to e-commerce, from music streaming to news aggregators, individuals are constantly targeted by personalized recommendations suggesting contents that may interest them. Despite such diffusion, the extent to which recommendations influence users’ decisions is still underexplored, given that independent audits on the structure and functioning of RSs deployed on online platforms are usually prevented by proprietary constraints. The nudging potential of RSs can represent a risk for vulnerable people: indeed, judicial cases involving platforms’ responsibility for displaying recommendations that may lead to political radicalization or endangerment of minors have recently caught public attention. The Digital Services Act of the European Union (DSA) is the first supranational regulation that sets specific transparency and auditing requirements for RSs implemented by online platforms with the aim of enhancing users’ self-determination: in particular, it allows users to modify the parameters on which recommendations rely so to let them choose autonomously which kind of content they want to see. This research focuses on whether and how the enforcement of this regulation can mitigate the unfair consequences of the power imbalance between online platforms and users. To this aim, I discuss the harms arising from digital nudging based on RSs and propose explanations as a tool that can reduce the impact of those harms by increasing users’ awareness. Through a comparative analysis of relevant articles of the DSA, the General Data Protection Regulation (GDPR) and the AI Act, I outline how the provisions of the DSA fill some of the gaps left by other relevant European regulations, while leaving the so-called right to explanation substantially unaddressed. As a result of this analysis, I argue that, in order for the implementation of the DSA provisions on recommender systems to be effective, policy-makers should: 1) enhance users’ awareness through clear and easily accessible explanations on how the recommendation process works and how they can be influenced by it; 2) grant users the possibility of intervening directly on the strategies through which RSs target them on the platform’s interface.
C1  - Montréal, QC, Canada and New York, NY, USA
C3  - Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society
DA  - 2023///
PY  - 2023
DO  - 10.1145/3600211.3604717
SP  - 653
EP  - 661
PB  - Association for Computing Machinery
SN  - 979-8-4007-0231-0
UR  - https://doi.org/10.1145/3600211.3604717
KW  - Transparency
KW  - Digital Nudging
KW  - Digital Services Act
KW  - Recommender Systems
KW  - Regulation of AI
ER  - 

TY  - CONF
TI  - Contagion in decentralized lending protocols: a case study of compound
AU  - Tovanich, Natkamon
AU  - Kassoul, Myriam
AU  - Weidenholzer, Simon
AU  - Prat, Julien
T3  - DeFi '23
AB  - We study financial contagion in Compound V2, a decentralized lending protocol deployed on the Ethereum blockchain. We explain how to construct the balance sheets of Compound's liquidity pools and use our methodology to characterize the financial network. Our analysis reveals that most users either borrow stablecoins or engage in liquidity mining. We then study the robustness of Compound through a series of stress tests, identifying the pools that are most likely to set off a cascade of defaults.
C1  - Copenhagen, Denmark and New York, NY, USA
C3  - Proceedings of the 2023 workshop on decentralized finance and security
DA  - 2023///
PY  - 2023
DO  - 10.1145/3605768.3623544
SP  - 55
EP  - 63
PB  - Association for Computing Machinery
SN  - 979-8-4007-0261-7
UR  - https://doi.org/10.1145/3605768.3623544
KW  - decentralized finance
KW  - financial contagion
KW  - financial network
KW  - stress test
KW  - systemic risk
ER  - 

TY  - CONF
TI  - Outlining traceability: a principle for operationalizing accountability in computing systems
AU  - Kroll, Joshua A.
T3  - FAccT '21
AB  - Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.
C1  - Virtual Event, Canada and New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445937
SP  - 758
EP  - 771
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445937
KW  - AI ethics
KW  - accountability
KW  - AI principles
KW  - traceability
KW  - transparency
ER  - 

TY  - CONF
TI  - Business process adaptation using discrete event controller synthesis
AU  - Uchitel, Sebastian
T3  - ForMABS 2016
AB  - Discrete Event Controller Synthesis is a fully automated procedure for producing operational reactive strategies for achieving declarative goals. In this talk I will discuss how synthesis at runtime can play a key role in achieving self-adaptation. I will discuss a reference architecture for self-adaptive systems and focus on the problem of dynamic controller update. I will also explain why discrete event controller synthesis is particularly well suited for runtime adaptation of business processes, in particular for dynamic update of workflows.
C1  - Singapore, Singapore and New York, NY, USA
C3  - Proceedings of the international workshop on formal methods for analysis of business systems
DA  - 2016///
PY  - 2016
DO  - 10.1145/2975941.2990289
SP  - 3
PB  - Association for Computing Machinery
SN  - 978-1-4503-4214-8
UR  - https://doi.org/10.1145/2975941.2990289
KW  - Controller Synthesis
KW  - Discrete Event Controller Systems
KW  - Supervisory Control
KW  - Workflow Systems
ER  - 

TY  - JOUR
TI  - Affordance-based altruistic robotic architecture for human–robot collaboration
AU  - Imre, Mert
AU  - Oztop, Erhan
AU  - Nagai, Yukie
AU  - Ugur, Emre
T2  - Adaptive Behavior
AB  - This article proposes a computational model for altruistic behavior, shows its implementation on a physical robot, and presents the results of human–robot interaction experiments conducted with the implemented system. Inspired from the sensorimotor mechanisms of the primate brain, object affordances are utilized for both intention estimation and action execution, in particular, to generate altruistic behavior. At the core of the model is the notion that sensorimotor systems developed for movement generation can be used to process the visual stimuli generated by actions of the others, infer the goals behind, and take the necessary actions to help achieving these goals, potentially leading to the emergence of altruistic behavior. Therefore, we argue that altruistic behavior is not necessarily a consequence of deliberate cognitive processing but may emerge through basic sensorimotor processes such as error minimization, that is, minimizing the difference between the observed and expected outcomes. In the model, affordances also play a key role by constraining the possible set of actions that an observed actor might be engaged in, enabling a fast and accurate intention inference. The model components are implemented on an upper-body humanoid robot. A set of experiments are conducted validating the workings of the components of the model, such as affordance extraction and task execution. Significantly, to assess how human partners interact with our altruistic model deployed robot, extensive experiments with naı̈ve subjects are conducted. Our results indicate that the proposed computational model can explain emergent altruistic behavior in reference to its biological counterpart and moreover engage human partners to exploit this behavior when implemented on an anthropomorphic robot.
DA  - 2019/08//
PY  - 2019
DO  - 10.1177/1059712318824697
VL  - 27
IS  - 4
SP  - 223
EP  - 241
J2  - Adapt Behav
SN  - 1059-7123
UR  - https://doi.org/10.1177/1059712318824697
KW  - affordances
KW  - Altruistic behavior
KW  - brain-inspired robotics
KW  - computational modeling
KW  - goal inference
KW  - human–robot interaction
ER  - 

TY  - JOUR
TI  - ReflAct: Formative assessment for teacher reflection in video-coaching settings
AU  - van der Linden, Sara
AU  - Papadopoulos, Pantelis M.
AU  - Nieveen, Nienke
AU  - McKenney, Susan
T2  - Computers & Education
DA  - 2023/10//
PY  - 2023
DO  - 10.1016/j.compedu.2023.104843
VL  - 203
IS  - C
J2  - Comput. Educ.
SN  - 0360-1315
UR  - https://doi.org/10.1016/j.compedu.2023.104843
KW  - Formative assessment
KW  - In-service teacher development
KW  - Pre-service teacher education
KW  - Professional development
KW  - ReflAct
KW  - Teacher coaching
KW  - Teacher reflection
KW  - Video coaching
ER  - 

TY  - JOUR
TI  - A cyber-physical robotic mobile fulfillment system in smart manufacturing: The simulation aspect
AU  - KEUNG, K.L.
AU  - LEE, C.K.M.
AU  - XIA, Liqiao
AU  - LIU, Chao
AU  - LIU, Bufan
AU  - JI, P.
T2  - Robot. Comput.-Integr. Manuf.
DA  - 2023/10//
PY  - 2023
DO  - 10.1016/j.rcim.2023.102578
VL  - 83
IS  - C
SN  - 0736-5845
UR  - https://doi.org/10.1016/j.rcim.2023.102578
KW  - Cyber-physical production system
KW  - Graph neural networks
KW  - Robotic mobile fulfillment system
KW  - Smart manufacturing
ER  - 

TY  - CONF
TI  - Hardware acceleration of explainable machine learning
AU  - Pan, Zhixin
AU  - Mishra, Prabhat
T3  - Date '22
AB  - Machine learning (ML) is successful in achieving human-level performance in various fields. However, it lacks the ability to explain an outcome due to its black-box nature. While recent efforts on explainable ML has received significant attention, the existing solutions are not applicable in real-time systems since they map interpretability as an optimization problem, which leads to numerous iterations of time-consuming complex computations. To make matters worse, existing implementations are not amenable for hardware-based acceleration. In this paper, we propose an efficient framework to enable acceleration of explainable ML procedure with hardware accelerators. We explore the effectiveness of both Tensor Processing Unit (TPU) and Graphics Processing Unit (GPU) based architectures in accelerating explainable ML. Specifically, this paper makes three important contributions. (1) To the best of our knowledge, our proposed work is the first attempt in enabling hardware acceleration of explainable ML. (2) Our proposed solution exploits the synergy between matrix convolution and Fourier transform, and therefore, it takes full advantage of TPU's inherent ability in accelerating matrix computations. (3) Our proposed approach can lead to real-time outcome interpretation. Extensive experimental evaluation demonstrates that proposed approach deployed on TPU can provide drastic improvement in interpretation time (39x on average) as well as energy efficiency (69x on average) compared to existing acceleration techniques.
C1  - Antwerp, Belgium and Leuven, BEL
C3  - Proceedings of the 2022 conference &amp; exhibition on design, automation &amp; test in europe
DA  - 2022///
PY  - 2022
SP  - 1127
EP  - 1130
PB  - European Design and Automation Association
SN  - 978-3-9819263-6-1
ER  - 

TY  - JOUR
TI  - One-shot pruning of gated recurrent unit neural network by sensitivity for time-series prediction
AU  - Tang, Hong
AU  - Ling, Xiangzheng
AU  - Li, Liangzhi
AU  - Xiong, Liyan
AU  - Yao, Yu
AU  - Huang, Xiaohui
T2  - Neurocomput.
DA  - 2022/11//
PY  - 2022
DO  - 10.1016/j.neucom.2022.09.026
VL  - 512
IS  - C
SP  - 15
EP  - 24
SN  - 0925-2312
UR  - https://doi.org/10.1016/j.neucom.2022.09.026
KW  - Deep learning
KW  - Gated recurrent units (GRU)
KW  - Pruning
KW  - Time-series
ER  - 

TY  - JOUR
TI  - A case-based reasoning system for aiding detection and classification of nosocomial infections
AU  - Gómez-Vallejo, H.J.
AU  - Uriel-Latorre, B.
AU  - Sande-Meijide, M.
AU  - Villamarı́n-Bello, B.
AU  - Pavón, R.
AU  - Fdez-Riverola, F.
AU  - Glez-Peña, D.
T2  - Decision Support Systems
AB  - Nowadays, it is recognized worldwide that healthcare-associated infections are responsible for an increase in patient morbidity, mortality, and higher costs related to prolonged hospital stays. As electronic health data are increasingly available today, there is a unique opportunity to implement real-time decision support systems for automating the surveillance of healthcare-associated infections. As a consequence, different electronic surveillance systems have been implemented to date with varying degrees of success. However, there have been few instances in which clinical data and physician narratives with the potential to significantly improve electronic surveillance alternatives have been adopted. In this context, the present work introduces a case-based reasoning system for the automatic surveillance and diagnosis of healthcare-associated infections. The developed system makes use of different machine learning techniques in order to (i) automatically extract evidence from different types of data including clinical unstructured documents, (ii) incorporate static a priori knowledge handled by infection preventionists, and (iii) dynamically generate new knowledge as well as understandable explanations about the system's decisions. Results obtained from a real deployment in a public hospital belonging to the Spanish National Health System trained with 2569 samples belonging to 1800 patients during more than 10 consecutive months recognize the usefulness of the system. Display Omitted Automatic surveillance of healthcare-associated infections.Diagnostic decision support system aiding monitoring and control.Case-based reasoning system for classifying nosocomial infections.Static rule-based knowledge representation and dynamic induction process.Natural language processing for physician narratives and nurses' comments.
DA  - 2016/04//
PY  - 2016
DO  - 10.1016/j.dss.2016.02.005
VL  - 84
IS  - C
SP  - 104
EP  - 116
J2  - Decis. Support Syst.
SN  - 0167-9236
UR  - https://doi.org/10.1016/j.dss.2016.02.005
KW  - Automatic surveillance
KW  - Case-based reasoning
KW  - Clinical decision support system
KW  - Healthcare-associated infections
KW  - Infection detection and classification
ER  - 

TY  - CONF
TI  - EMVE-DeCK: a theory-based framework for designing and tailoring persuasive technology
AU  - Oyibo, Kiemute
T3  - Umap '21
AB  - Although the importance of tailoring persuasive technologies (PTs) has been discussed extensively in the literature, there is insufficient theory-driven guidance on how to employ social psychology theories in the design of PT interventions. In this paper, we provide an overview of the key frameworks in the extant literature for designing information systems, in general, and persuasive systems, in particular. Specifically, we identify their limitations, and propose a new framework called ”EMVE-DeCK Framework” based on the synthesis of the strengths of the existing frameworks. The EMVE-DeCK Framework, which is grounded in Bandura’s Triad of Reciprocal Determinism, comprises seven steps, which include: (1) Explain: Employ “Theory” to explain the target “Behavior” by uncovering the relationship between the “Behavioral Determinants” and the target “Behavior”; (2) Map: Map the significant “Behavioral Determinants” in the “Theory” domain to “Persuasive Strategies” in the “Technology” domain; (3) Validate: Validate the target users’ receptiveness to the “Persuasive Strategies” in the “Technology” domain; (4) Explicate: Employ “Theory” to explicate (explain) the adoption of the proposed persuasive “Technology” by uncovering the relationship between the user experience (UX) “Design Attributes” and the persuasive “Technology Adoption”; (5) Design: Design and implement theory-driven, tailored persuasive “Technology”; (6) Change: Deploy the persuasive “Technology” to change “Behavior” in the field; and (7) Knowledge: Contribute “Findings” to Knowledge. We discuss the framework in the context of PT interventions.
C1  - Utrecht, Netherlands and New York, NY, USA
C3  - Adjunct proceedings of the 29th ACM conference on user modeling, adaptation and personalization
DA  - 2021///
PY  - 2021
DO  - 10.1145/3450614.3464617
SP  - 257
EP  - 267
PB  - Association for Computing Machinery
SN  - 978-1-4503-8367-7
UR  - https://doi.org/10.1145/3450614.3464617
KW  - framework
KW  - personalization
KW  - persuasive technology
KW  - tailoring
ER  - 

TY  - CONF
TI  - Application areas of ephemeral computing: a survey
AU  - Cotta, Carlos
AU  - Fernández-Leiva, Antonio J.
AU  - Vega, Francisco Fernández
AU  - Chávez, Francisco
AU  - Merelo, Juan J.
AU  - Castillo, Pedro A.
AU  - Camacho, David
AU  - R-Moreno, Marı́a D.
AB  - It is increasingly common that computational devices with significant computing power are underexploited. Some of the reasons for that are due to frequent idle-time or to the low computational demand of the tasks they perform, either sporadically or in their regular duty. The exploitation of this otherwise-wasted computational power is a cost-effective solution for solving complex computational tasks. Individually device-wise, this computational power can sometimes comprise a stable, long-lasting availability window but it will more frequently take the form of brief, ephemeral bursts. Then, in this context a highly dynamic and volatile computational landscape emerges from the collective contribution of such numerous devices. Algorithms consciously running on this kind of environment require specific properties in terms of flexibility, plasticity and robustness. Bioinspired algorithms are particularly well suited to this endeavor, thanks to some of the features they inherit from their biological sources of inspiration, namely decentralized functioning, intrinsic parallelism, resilience, and adaptiveness. Deploying bioinspired techniques on this scenario, and conducting analysis and modelling of the underlying Ephemeral Computing environment will also pave the way for the application of other non-bioinspired techniques on this computational domain. Computational creativity and content generation in video games are applications areas of the foremost economical interest and are well suited to Ephemeral Computing due to their intrinsic ephemeral nature and the widespread abundance of gaming applications in all kinds of devices. In this paper, we will explain why and how they can be adapted to this new environment.
C1  - Berlin, Heidelberg
C3  - Transactions on computational collective intelligence XXIV - volume 9770
DA  - 2016///
PY  - 2016
DO  - 10.1007/978-3-662-53525-7_9
SP  - 153
EP  - 167
PB  - Springer-Verlag
SN  - 978-3-662-53524-0
UR  - https://doi.org/10.1007/978-3-662-53525-7_9
KW  - Autonomic computing
KW  - Bioinspired optimization
KW  - Complex systems
KW  - Distributed computing
KW  - Ephemeral computing
KW  - Evolutionary computation
ER  - 

TY  - CONF
TI  - ODRL policy modelling and compliance checking
AU  - De Vos, Marina
AU  - Kirrane, Sabrina
AU  - Padget, Julian
AU  - Satoh, Ken
AB  - This paper addresses the problem of constructing a policy pipeline that enables compliance checking of business processes against regulatory obligations. Towards this end, we propose an Open Digital Rights Language (ODRL) profile that can be used to capture the semantics of both business policies in the form of sets of required permissions and regulatory requirements in the form of deontic concepts, and present their translation into Answer Set Programming (via the Institutional Action Language (InstAL)) for compliance checking purposes. The result of the compliance checking is either a positive compliance result or an explanation pertaining to the aspects of the policy that are causing the non-compliance. The pipeline is illustrated using two (key) fragments of the General Data Protect Regulation, namely Articles 6 (Lawfulness of processing) and Articles 46 (Transfers subject to appropriate safeguards) and industrially-relevant use cases that involve the specification of sets of permissions that are needed to execute business processes. The core contributions of this paper are the ODRL profile, which is capable of modelling regulatory obligations and business policies, the exercise of modelling elements of GDPR in this semantic formalism, and the operationalisation of the model to demonstrate its capability to support personal data processing compliance checking, and a basis for explaining why the request is deemed compliant or not.
C1  - Bolzano, Italy and Berlin, Heidelberg
C3  - Rules and reasoning: Third international joint conference, RuleML+RR 2019, bolzano, italy, september 16–19, 2019, proceedings
DA  - 2019///
PY  - 2019
DO  - 10.1007/978-3-030-31095-0_3
SP  - 36
EP  - 51
PB  - Springer-Verlag
SN  - 978-3-030-31094-3
UR  - https://doi.org/10.1007/978-3-030-31095-0_3
ER  - 

TY  - JOUR
TI  - Synthesizing robot programs with interactive tutor mode
AU  - Li, Hao
AU  - Wang, Yu-Ping
AU  - Mu, Tai-Jiang
T2  - International Journal of Automation and Computing
AB  - With the rapid development of the robotic industry, domestic robots have become increasingly popular. As domestic robots are expected to be personal assistants, it is important to develop a natural language-based human-robot interactive system for end-users who do not necessarily have much programming knowledge. To build such a system, we developed an interactive tutoring framework, named “Holert”, which can translate task descriptions in natural language to machine-interpretable logical forms automatically. Compared to previous works, Holert allows users to teach the robot by further explaining their intentions in an interactive tutor mode. Furthermore, Holert introduces a semantic dependency model to enable the robot to “understand” similar task descriptions. We have deployed Holert on an open-source robot platform, Turtlebot 2. Experimental results show that the system accuracy could be significantly improved by 163.9
DA  - 2019/08//
PY  - 2019
DO  - 10.1007/s11633-018-1154-7
VL  - 16
IS  - 4
SP  - 462
EP  - 474
J2  - Int. J. Autom. Comput.
SN  - 1476-8186
UR  - https://doi.org/10.1007/s11633-018-1154-7
KW  - Human-robot interaction
KW  - intelligent robotic systems
KW  - natural language understanding
KW  - program synthesis
KW  - semantic parsing
ER  - 

TY  - CONF
TI  - Update delivery mechanisms for prospective information needs: An analysis of attention in mobile users
AU  - Lin, Jimmy
AU  - Mohammed, Salman
AU  - Sequiera, Royal
AU  - Tan, Luchen
T3  - Sigir '18
AB  - Real-time summarization systems that monitor document streams to identify relevant content have a few options for delivering system updates to users. In a mobile context, systems could send push notifications to users' mobile devices, hoping to grab their attention immediately. Alternatively, systems could silently deposit updates into "inboxes" that users can access at their leisure. We refer to these mechanisms as push-based vs. pull-based, and present a two-year contrastive study that attempts to understand the effects of the delivery mechanism on mobile user behavior, in the context of the TREC Real-Time Summarization Tracks. Through a cluster analysis, we are able to identify three distinct and coherent patterns of behavior. As expected, we find that users are likely to ignore push notifications, but for those updates that users do pay attention to, content is consumed within a short amount of time. Interestingly, users bombarded with push notifications are less likely to consume updates on their own initiative and less likely to engage in long reading sessions—which is a common pattern for users who pull content from their inboxes. We characterize users as exhibiting "eager" or "apathetic" information consumption behavior as an explanation of these observations, and attempt to operationalize our findings into design recommendations.
C1  - Ann Arbor, MI, USA and New York, NY, USA
C3  - The 41st international ACM SIGIR conference on research &amp; development in information retrieval
DA  - 2018///
PY  - 2018
DO  - 10.1145/3209978.3210018
SP  - 785
EP  - 794
PB  - Association for Computing Machinery
SN  - 978-1-4503-5657-2
UR  - https://doi.org/10.1145/3209978.3210018
KW  - social media
KW  - mobile users
KW  - push notifications
KW  - real-time summarization
ER  - 

TY  - JOUR
TI  - Flexible inverse adaptive fuzzy inference model to identify the evolution of operational value at risk for improving operational risk management
AU  - Peña, Alejandro
AU  - Bonet, Isis
AU  - Lochmuller, Christian
AU  - Chiclana, Francisco
AU  - Góngora, Mario
T2  - Applied Soft Computing
DA  - 2018/04//
PY  - 2018
DO  - 10.1016/j.asoc.2018.01.024
VL  - 65
IS  - C
SP  - 614
EP  - 631
J2  - Appl. Soft Comput.
SN  - 1568-4946
UR  - https://doi.org/10.1016/j.asoc.2018.01.024
KW  - Adaptive fuzzy inference model
KW  - Basel Committee on Banking Supervision
KW  - Basel II
KW  - Loss distribution approach
KW  - Montecarlo sampling
KW  - Operational risk
KW  - Operational value at risk
KW  - Risk management matrix
ER  - 

TY  - CONF
TI  - Building python-based topologies for massive processing of social media data in real time
AU  - Martı́nez-Castaño, Rodrigo
AU  - Pichel, Juan C.
AU  - Losada, David E.
T3  - Ceri '18
AB  - In this paper we propose a streaming approach for real-time processing of huge amounts of data. CATENAE is a library for easy building and execution of Python topologies (e.g., web crawler, classifier). Topologies are designed for their deployment inside Docker containers and, thus, horizontal scaling, granular resource assignment and isolation can be achieved easily. Furthermore, micromodules can have its own dependencies (including the Python version), allowing the user to limit resources such as CPU or memory by instance. We describe an implementation of a use case composed of two topologies: (1) a crawler for tracking users in social media and (2) an early risk detector of depression. We also explain how CATENAE topologies can be connected to non-Python systems.
C1  - Zaragoza, Spain and New York, NY, USA
C3  - Proceedings of the 5th spanish conference on information retrieval
DA  - 2018///
PY  - 2018
DO  - 10.1145/3230599.3230618
PB  - Association for Computing Machinery
SN  - 978-1-4503-6543-7
UR  - https://doi.org/10.1145/3230599.3230618
KW  - Depression
KW  - Social Media
KW  - Docker
KW  - Python
KW  - Real-Time Processing
KW  - Stream Processing
KW  - Text Mining
ER  - 

TY  - THES
TI  - Earth observation
AU  - Singadi, Srinivas Kenchappa
AB  - This is a system project report explaining Earth observation project undertaken during my internship with XSealence S.A. The project was proposed by the company but since it was a large project it was divided into small several units and assigned to various groups. The report has been divided into various chapters which discuss the software development life cycle of Earth observation system. the project used agile development model which allowed the project to be divided into various phases. The first phase is discussed in the first chapter which helped state the motivations behind the project. Buoyed by rising demand for marine weather forecast XSealence S.A company developed a marine weather monitoring web application to monitor and predict weather data based on data downloaded from Copernicus marine monitoring service. So as to have an installable system that can be installed on sea vessels XSealence S.A developed a desktop version of the same. This report discusses the development of earth observation system which is the part of the main desktop application.The aim of developing earth observation was to help predict the marine weather. The project initialization involved setting project aims and goals which were to guide through the development process. The main project goal was to deliver a desktop application that will help forecast marine weather. The application was linked to Copernicus marine monitoring service where it downloaded data in a .nc format and converted to .txt format.The system requirement phase involved the collection of functional requirements of the system. With the help of my supervisor, it was established my part of the main system only needed to predict marine weather including water and ice speed, water velocity, sea surface height, temperature among other marine weather options. These capabilities of the system helped in system design and development to ensure the final product meets the set targets. UML diagrams including use case, data flow diagrams helped illustrate the flow of information in the system. These designs together with class diagrams were converted into a working solution during system development phase as planned in the project planning phase of the agile methodology.The system was tested for the achievement of system goals and objectives as stated in system functional requirements. Functional requirements achievement was tested using test use case. The test case tested every functionality of the system to establish the non-achieved goals and objectives. These defects found were communicated to the developer for correction in the subsequent releases. The system has been handed to the XSealence S.A for integration with another main desktop application. The entire system maintenance and deployment will be done by the company. To close the project, documentation was done to cover all activities of the project.
DA  - 2017///
PY  - 2017
M3  - phd
PB  - Instituto Politecnico de Leiria (Portugal)
N1  - AAI29137969
ER  - 

TY  - CONF
TI  - Improving the transparency of the sharing economy
AU  - Lecuyer, Mathias
AU  - Tucker, Max
AU  - Chaintreau, Augustin
T3  - WWW '17 companion
AB  - The idealistic beginnings of the sharing economy made ways to an entrenched battle to win over the public opinion and for law makers to appreciate its benefits and its risks. The stakes are high as the success of services like Airbnb reveals that under-utilized assets (e.g. spare rooms or apartments left vacant) can be efficiently matched to individual demands to generate a significant surplus to their owners. Rules and regulation, which are increasingly felt as necessary by many communities, also create friction over the best way to leverage these opportunities for growth. To make things worse, the sharing economy is complex and poorly documented: Three recent reports from public institutions and lobbying groups arrived at opposite conclusions with seemingly contradictory facts about the occupancy distribution.In this paper, we show how to overcome this opacity by offering the first large-scale, reproducible study of Airbnb's supply and transactions. We devised and deployed frequently repeated crawls using no proprietary data. We show that these can be used to accurately estimate not only the supply of available rooms, but the effective transactions, occupancy, and revenue of hosts. Our results provide the first complete view of the occupancy and the distribution of revenue, revealing important trends that generalize previous observations. In particular we found that previous observations that seemed at odds are all explained by a variant of the "inspection paradox". We also found from our detailed data that enforcing a maximum occupancy of 90 nights a year would greatly reduce most concerns raised by various advocacy groups, while affecting only marginally the justifying claims that Airbnb quotes to argue for its beneficial impact.
C1  - Perth, Australia and Republic and Canton of Geneva, CHE
C3  - Proceedings of the 26th international conference on world wide web companion
DA  - 2017///
PY  - 2017
DO  - 10.1145/3041021.3055136
SP  - 1043
EP  - 1051
PB  - International World Wide Web Conferences Steering Committee
SN  - 978-1-4503-4914-7
UR  - https://doi.org/10.1145/3041021.3055136
KW  - measurement
KW  - transparency
KW  - airbnb
KW  - sharing economy
ER  - 

