TY  - JOUR
AU  - Korteling, J E Hans
AU  - van de Boer-Visschedijk, G C
AU  - Blankendaal, R A M
AU  - Boonekamp, R C
AU  - Eikelboom, A R
TI  - Human- versus Artificial Intelligence.
T2  - Frontiers in artificial intelligence
M3  - Journal Article
M3  - Review
AB  - AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions and, for instance, the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. In order to provide more agreement and to substantiate possible future research objectives, this paper presents three notions on the similarities and differences between human- and artificial intelligence: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple (integrated) forms of narrow-hybrid AI applications. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and "collaborate" with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgment required? How can we capitalize on the specific strengths of human- and artificial intelligence? How to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition (and vice versa)? Should we pursue the development of AI "partners" with human (-level) intelligence or should we focus more at supplementing human limitations? In order to answer these questions, humans working with AI systems in the workplace or in policy making have to develop an adequate mental model of the underlying 'psychological' mechanisms of AI. So, in order to obtain well-functioning human-AI systems, Intelligence Awareness in humans should be addressed more vigorously. For this purpose a first framework for educational content is proposed.
DA  - 2021
PY  - 2021
VL  - 4
SP  - 622364
EP  - 622364
DO  - 10.3389/frai.2021.622364
AN  - MEDLINE:33981990
ER  -

TY  - JOUR
AU  - Blanco-Gonzalez, Alexandre
AU  - Cabezon, Alfonso
AU  - Seco-Gonzalez, Alejandro
AU  - Conde-Torres, Daniel
AU  - Antelo-Riveiro, Paula
AU  - Pineiro, Angel
AU  - Garcia-Fandino, Rebeca
TI  - The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies.
T2  - Pharmaceuticals (Basel, Switzerland)
M3  - Journal Article
M3  - Review
AB  - Artificial intelligence (AI) has the potential to revolutionize the drug discovery process, offering improved efficiency, accuracy, and speed. However, the successful application of AI is dependent on the availability of high-quality data, the addressing of ethical concerns, and the recognition of the limitations of AI-based approaches. In this article, the benefits, challenges, and drawbacks of AI in this field are reviewed, and possible strategies and approaches for overcoming the present obstacles are proposed. The use of data augmentation, explainable AI, and the integration of AI with traditional experimental methods, as well as the potential advantages of AI in pharmaceutical research, are also discussed. Overall, this review highlights the potential of AI in drug discovery and provides insights into the challenges and opportunities for realizing its potential in this field. Note from the human authors: This article was created to test the ability of ChatGPT, a chatbot based on the GPT-3.5 language model, in terms of assisting human authors in writing review articles. The text generated by the AI following our instructions (see Supporting Information) was used as a starting point, and its ability to automatically generate content was evaluated. After conducting a thorough review, the human authors practically rewrote the manuscript, striving to maintain a balance between the original proposal and the scientific criteria. The advantages and limitations of using AI for this purpose are discussed in the last section.
DA  - 2023 Jun 18
PY  - 2023
VL  - 16
IS  - 6
DO  - 10.3390/ph16060891
AN  - MEDLINE:37375838
ER  -

TY  - JOUR
AU  - Neves, Ines
AU  - Folgado, Duarte
AU  - Santos, Sara
AU  - Barandas, Marilia
AU  - Campagner, Andrea
AU  - Ronzio, Luca
AU  - Cabitza, Federico
AU  - Gamboa, Hugo
TI  - Interpretable heartbeat classification using local model-agnostic explanations on ECGs.
T2  - Computers in biology and medicine
M3  - Journal Article
AB  - Treatment and prevention of cardiovascular diseases often rely on Electrocardiogram (ECG) interpretation. Dependent on the physician's variability, ECG interpretation is subjective and prone to errors. Machine learning models are often developed and used to support doctors; however, their lack of interpretability stands as one of the main drawbacks of their widespread operation. This paper focuses on an Explainable Artificial Intelligence (XAI) solution to make heartbeat classification more explainable using several state-of-the-art model-agnostic methods. We introduce a high-level conceptual framework for explainable time series and propose an original method that adds temporal dependency between time samples using the time series' derivative. The results were validated in the MIT-BIH arrhythmia dataset: we performed a performance's analysis to evaluate whether the explanations fit the model's behaviour; and employed the 1-D Jaccard's index to compare the subsequences extracted from an interpretable model and the XAI methods used. Our results show that the use of the raw signal and its derivative includes temporal dependency between samples to promote classification explanation. A small but informative user study concludes this study to evaluate the potential of the visual explanations produced by our original method for being adopted in real-world clinical settings, either as diagnostic aids or training resource.
DA  - 2021 06 (Epub 2021 Apr 16)
PY  - 2021
VL  - 133
SP  - 104393
EP  - 104393
DO  - 10.1016/j.compbiomed.2021.104393
AN  - MEDLINE:33915362
ER  -

TY  - JOUR
AU  - Bruckert, Sebastian
AU  - Finzel, Bettina
AU  - Schmid, Ute
TI  - The Next Generation of Medical Decision Support: A Roadmap Toward Transparent Expert Companions.
T2  - Frontiers in artificial intelligence
M3  - Journal Article
M3  - Review
AB  - Increasing quality and performance of artificial intelligence (AI) in general and machine learning (ML) in particular is followed by a wider use of these approaches in everyday life. As part of this development, ML classifiers have also gained more importance for diagnosing diseases within biomedical engineering and medical sciences. However, many of those ubiquitous high-performing ML algorithms reveal a black-box-nature, leading to opaque and incomprehensible systems that complicate human interpretations of single predictions or the whole prediction process. This puts up a serious challenge on human decision makers to develop trust, which is much needed in life-changing decision tasks. This paper is designed to answer the question how expert companion systems for decision support can be designed to be interpretable and therefore transparent and comprehensible for humans. On the other hand, an approach for interactive ML as well as human-in-the-loop-learning is demonstrated in order to integrate human expert knowledge into ML models so that humans and machines act as companions within a critical decision task. We especially address the problem of Semantic Alignment between ML classifiers and its human users as a prerequisite for semantically relevant and useful explanations as well as interactions. Our roadmap paper presents and discusses an interdisciplinary yet integrated Comprehensible Artificial Intelligence (cAI)-transition-framework with regard to the task of medical diagnosis. We explain and integrate relevant concepts and research areas to provide the reader with a hands-on-cookbook for achieving the transition from opaque black-box models to interactive, transparent, comprehensible and trustworthy systems. To make our approach tangible, we present suitable state of the art methods with regard to the medical domain and include a realization concept of our framework. The emphasis is on the concept of Mutual Explanations (ME) that we introduce as a dialog-based, incremental process in order to provide human ML users with trust, but also with stronger participation within the learning process.
DA  - 2020
PY  - 2020
VL  - 3
SP  - 507973
EP  - 507973
DO  - 10.3389/frai.2020.507973
AN  - MEDLINE:33733193
ER  -

TY  - JOUR
AU  - Yuan, Luyao
AU  - Gao, Xiaofeng
AU  - Zheng, Zilong
AU  - Edmonds, Mark
AU  - Wu, Ying Nian
AU  - Rossano, Federico
AU  - Lu, Hongjing
AU  - Zhu, Yixin
AU  - Zhu, Song-Chun
TI  - In situ bidirectional human-robot value alignment.
T2  - Science robotics
M3  - Journal Article
M3  - Research Support, U.S. Gov't, Non-P.H.S.
AB  - A prerequisite for social coordination is bidirectional communication between teammates, each playing two roles simultaneously: as receptive listeners and expressive speakers. For robots working with humans in complex situations with multiple goals that differ in importance, failure to fulfill the expectation of either role could undermine group performance due to misalignment of values between humans and robots. Specifically, a robot needs to serve as an effective listener to infer human users' intents from instructions and feedback and as an expressive speaker to explain its decision processes to users. Here, we investigate how to foster effective bidirectional human-robot communications in the context of value alignment-collaborative robots and users form an aligned understanding of the importance of possible task goals. We propose an explainable artificial intelligence (XAI) system in which a group of robots predicts users' values by taking in situ feedback into consideration while communicating their decision processes to users through explanations. To learn from human feedback, our XAI system integrates a cooperative communication model for inferring human values associated with multiple desirable goals. To be interpretable to humans, the system simulates human mental dynamics and predicts optimal explanations using graphical models. We conducted psychological experiments to examine the core components of the proposed computational framework. Our results show that real-time human-robot mutual understanding in complex cooperative tasks is achievable with a learning model based on bidirectional communication. We believe that this interaction framework can shed light on bidirectional value alignment in communicative XAI systems and, more broadly, in future human-machine teaming systems.
DA  - 2022 07 13 (Epub 2022 Jul 13)
PY  - 2022
VL  - 7
IS  - 68
SP  - eabm4183
EP  - eabm4183
DO  - 10.1126/scirobotics.abm4183
AN  - MEDLINE:35857532
ER  -

TY  - JOUR
AU  - Esmaeili, Morteza
AU  - Vettukattil, Riyas
AU  - Banitalebi, Hasan
AU  - Krogh, Nina R
AU  - Geitung, Jonn Terje
TI  - Explainable Artificial Intelligence for Human-Machine Interaction in Brain Tumor Localization.
T2  - Journal of personalized medicine
M3  - Journal Article
AB  - Primary malignancies in adult brains are globally fatal. Computer vision, especially recent developments in artificial intelligence (AI), have created opportunities to automatically characterize and diagnose tumor lesions in the brain. AI approaches have provided scores of unprecedented accuracy in different image analysis tasks, including differentiating tumor-containing brains from healthy brains. AI models, however, perform as a black box, concealing the rational interpretations that are an essential step towards translating AI imaging tools into clinical routine. An explainable AI approach aims to visualize the high-level features of trained models or integrate into the training process. This study aims to evaluate the performance of selected deep-learning algorithms on localizing tumor lesions and distinguishing the lesion from healthy regions in magnetic resonance imaging contrasts. Despite a significant correlation between classification and lesion localization accuracy (R = 0.46, p = 0.005), the known AI algorithms, examined in this study, classify some tumor brains based on other non-relevant features. The results suggest that explainable AI approaches can develop an intuition for model interpretability and may play an important role in the performance evaluation of deep learning models. Developing explainable AI approaches will be an essential tool to improve human-machine interactions and assist in the selection of optimal training methods.
DA  - 2021 Nov 16
PY  - 2021
VL  - 11
IS  - 11
DO  - 10.3390/jpm11111213
AN  - MEDLINE:34834566
ER  -

TY  - JOUR
AU  - Gee, Alan H
AU  - Garcia-Olano, Diego
AU  - Ghosh, Joydeep
AU  - Paydarfar, David
TI  - Explaining Deep Classification of Time-Series Data with Learned Prototypes.
T2  - CEUR workshop proceedings
M3  - Journal Article
AB  - The emergence of deep learning networks raises a need for explainable AI so that users and domain experts can be confident applying them to high-risk decisions. In this paper, we leverage data from the latent space induced by deep learning models to learn stereotypical representations or "prototypes" during training to elucidate the algorithmic decision-making process. We study how leveraging prototypes effect classification decisions of two dimensional time-series data in a few different settings: (1) electrocardiogram (ECG) waveforms to detect clinical bradycardia, a slowing of heart rate, in preterm infants, (2) respiration waveforms to detect apnea of prematurity, and (3) audio waveforms to classify spoken digits. We improve upon existing models by optimizing for increased prototype diversity and robustness, visualize how these prototypes in the latent space are used by the model to distinguish classes, and show that prototypes are capable of learning features on two dimensional time-series data to produce explainable insights during classification tasks. We show that the prototypes are capable of learning real-world features - bradycardia in ECG, apnea in respiration, and articulation in speech - as well as features within sub-classes. Our novel work leverages learned prototypical framework on two dimensional time-series data to produce explainable insights during classification tasks.
DA  - 2019 Aug
PY  - 2019
VL  - 2429
SP  - 15
EP  - 22
AN  - MEDLINE:33867901
ER  -

TY  - JOUR
AU  - Terranova, Nadia
AU  - Renard, Didier
AU  - Shahin, Mohamed H
AU  - Menon, Sujatha
AU  - Cao, Youfang
AU  - Hop, Cornelis E C A
AU  - Hayes, Sean
AU  - Madrasi, Kumpal
AU  - Stodtmann, Sven
AU  - Tensfeldt, Thomas
AU  - Vaddady, Pavan
AU  - Ellinwood, Nicholas
AU  - Lu, James
TI  - Artificial Intelligence for Quantitative Modeling in Drug Discovery and Development: An Innovation and Quality Consortium Perspective on Use Cases and Best Practices.
T2  - Clinical pharmacology and therapeutics
M3  - Journal Article
M3  - Review
AB  - Recent breakthroughs in artificial intelligence (AI) and machine learning (ML) have ushered in a new era of possibilities across various scientific domains. One area where these advancements hold significant promise is model-informed drug discovery and development (MID3). To foster a wider adoption and acceptance of these advanced algorithms, the Innovation and Quality (IQ) Consortium initiated the AI/ML working group in 2021 with the aim of promoting their acceptance among the broader scientific community as well as by regulatory agencies. By drawing insights from workshops organized by the working group and attended by key stakeholders across the biopharma industry, academia, and regulatory agencies, this white paper provides a perspective from the IQ Consortium. The range of applications covered in this white paper encompass the following thematic topics: (i) AI/ML-enabled Analytics for Pharmacometrics and Quantitative Systems Pharmacology (QSP) Workflows; (ii) Explainable Artificial Intelligence and its Applications in Disease Progression Modeling; (iii) Natural Language Processing (NLP) in Quantitative Pharmacology Modeling; and (iv) AI/ML Utilization in Drug Discovery. Additionally, the paper offers a set of best practices to ensure an effective and responsible use of AI, including considering the context of use, explainability and generalizability of models, and having human-in-the-loop. We believe that embracing the transformative power of AI in quantitative modeling while adopting a set of good practices can unlock new opportunities for innovation, increase efficiency, and ultimately bring benefits to patients.
DA  - 2024 04 (Epub 2023 Oct 06)
PY  - 2024
VL  - 115
IS  - 4
SP  - 658
EP  - 672
DO  - 10.1002/cpt.3053
AN  - MEDLINE:37716910
ER  -

TY  - JOUR
AU  - Uegami, Wataru
AU  - Bychkov, Andrey
AU  - Ozasa, Mutsumi
AU  - Uehara, Kazuki
AU  - Kataoka, Kensuke
AU  - Johkoh, Takeshi
AU  - Kondoh, Yasuhiro
AU  - Sakanashi, Hidenori
AU  - Fukuoka, Junya
TI  - MIXTURE of human expertise and deep learning-developing an explainable model for predicting pathological diagnosis and survival in patients with interstitial lung disease.
T2  - Modern pathology : an official journal of the United States and Canadian Academy of Pathology, Inc
M3  - Journal Article
M3  - Research Support, Non-U.S. Gov't
AB  - Interstitial pneumonia is a heterogeneous disease with a progressive course and poor prognosis, at times even worse than those in the main cancer types. Histopathological examination is crucial for its diagnosis and estimation of prognosis. However, the evaluation strongly depends on the experience of pathologists, and the reproducibility of diagnosis is low. Herein, we propose MIXTURE (huMan-In-the-loop eXplainable artificial intelligence Through the Use of REcurrent training), an original method to develop deep learning models for extracting pathologically significant findings based on an expert pathologist's perspective with a small annotation effort. The procedure of MIXTURE consists of three steps as follows. First, we created feature extractors for tiles from whole slide images using self-supervised learning. The similar looking tiles were clustered based on the output features and then pathologists integrated the pathologically synonymous clusters. Using the integrated clusters as labeled data, deep learning models to classify the tiles into pathological findings were created by transfer-learning the feature extractors. We developed three models for different magnifications. Using these extracted findings, our model was able to predict the diagnosis of usual interstitial pneumonia, a finding suggestive of progressive disease, with high accuracy (AUC 0.90 in validation set and AUC 0.86 in test set). This high accuracy could not be achieved without the integration of findings by pathologists. The patients predicted as UIP had poorer prognosis (5-year overall survival [OS]: 55.4%) than those predicted as non-UIP (OS: 95.2%). The Cox proportional hazards model for each microscopic finding and prognosis pointed out dense fibrosis, fibroblastic foci, elastosis, and lymphocyte aggregation as independent risk factors. We suggest that MIXTURE may serve as a model approach to different diseases evaluated by medical imaging, including pathology and radiology, and be the prototype for explainable artificial intelligence that can collaborate with humans.
DA  - 2022 08 (Epub 2022 Feb 23)
PY  - 2022
VL  - 35
IS  - 8
SP  - 1083
EP  - 1091
DO  - 10.1038/s41379-022-01025-7
AN  - MEDLINE:35197560
ER  -

TY  - JOUR
AU  - Berretta, Sophie
AU  - Tausch, Alina
AU  - Ontrup, Greta
AU  - Gilles, Bjorn
AU  - Peifer, Corinna
AU  - Kluge, Annette
TI  - Defining human-AI teaming the human-centered way: a scoping review and network analysis.
T2  - Frontiers in artificial intelligence
M3  - Journal Article
M3  - Scoping Review
AB  - Introduction: With the advancement of technology and the increasing utilization of AI, the nature of human work is evolving, requiring individuals to collaborate not only with other humans but also with AI technologies to accomplish complex goals. This requires a shift in perspective from technology-driven questions to a human-centered research and design agenda putting people and evolving teams in the center of attention. A socio-technical approach is needed to view AI as more than just a technological tool, but as a team member, leading to the emergence of human-AI teaming (HAIT). In this new form of work, humans and AI synergistically combine their respective capabilities to accomplish shared goals.Methods: The aim of our work is to uncover current research streams on HAIT and derive a unified understanding of the construct through a bibliometric network analysis, a scoping review and synthetization of a definition from a socio-technical point of view. In addition, antecedents and outcomes examined in the literature are extracted to guide future research in this field.Results: Through network analysis, five clusters with different research focuses on HAIT were identified. These clusters revolve around (1) human and (2) task-dependent variables, (3) AI explainability, (4) AI-driven robotic systems, and (5) the effects of AI performance on human perception. Despite these diverse research focuses, the current body of literature is predominantly driven by a technology-centric and engineering perspective, with no consistent definition or terminology of HAIT emerging to date.Discussion: We propose a unifying definition combining a human-centered and team-oriented perspective as well as summarize what is still needed in future research regarding HAIT. Thus, this work contributes to support the idea of the Frontiers Research Topic of a theoretical and conceptual basis for human work with AI systems.
DA  - 2023
PY  - 2023
VL  - 6
SP  - 1250725
EP  - 1250725
DO  - 10.3389/frai.2023.1250725
AN  - MEDLINE:37841234
ER  -

TY  - JOUR
AU  - Naiseh, Mohammad
AU  - Al-Thani, Dena
AU  - Jiang, Nan
AU  - Ali, Raian
TI  - Explainable recommendation: when design meets trust calibration.
T2  - World wide web
M3  - Journal Article
AB  - Human-AI collaborative decision-making tools are being increasingly applied in critical domains such as healthcare. However, these tools are often seen as closed and intransparent for human decision-makers. An essential requirement for their success is the ability to provide explanations about themselves that are understandable and meaningful to the users. While explanations generally have positive connotations, studies showed that the assumption behind users interacting and engaging with these explanations could introduce trust calibration errors such as facilitating irrational or less thoughtful agreement or disagreement with the AI recommendation. In this paper, we explore how to help trust calibration through explanation interaction design. Our research method included two main phases. We first conducted a think-aloud study with 16 participants aiming to reveal main trust calibration errors concerning explainability in AI-Human collaborative decision-making tools. Then, we conducted two co-design sessions with eight participants to identify design principles and techniques for explanations that help trust calibration. As a conclusion of our research, we provide five design principles: Design for engagement, challenging habitual actions, attention guidance, friction and support training and learning. Our findings are meant to pave the way towards a more integrated framework for designing explanations with trust calibration as a primary goal.
DA  - 2021 (Epub 2021 Aug 02)
PY  - 2021
VL  - 24
IS  - 5
SP  - 1857
EP  - 1884
DO  - 10.1007/s11280-021-00916-0
AN  - MEDLINE:34366701
ER  -

TY  - JOUR
AU  - Semerci, Zeliha Merve
AU  - Yardimci, Selmi
TI  - Empowering Modern Dentistry: The Impact of Artificial Intelligence on Patient Care and Clinical Decision Making.
T2  - Diagnostics (Basel, Switzerland)
M3  - Journal Article
M3  - Review
AB  - Advancements in artificial intelligence (AI) are poised to catalyze a transformative shift across diverse dental disciplines including endodontics, oral radiology, orthodontics, pediatric dentistry, periodontology, prosthodontics, and restorative dentistry. This narrative review delineates the burgeoning role of AI in enhancing diagnostic precision, streamlining treatment planning, and potentially unveiling innovative therapeutic modalities, thereby elevating patient care standards. Recent analyses corroborate the superiority of AI-assisted methodologies over conventional techniques, affirming their capacity for personalization, accuracy, and efficiency in dental care. Central to these AI applications are convolutional neural networks and deep learning models, which have demonstrated efficacy in diagnosis, prognosis, and therapeutic decision making, in some instances surpassing traditional methods in complex cases. Despite these advancements, the integration of AI into clinical practice is accompanied by challenges, such as data security concerns, the demand for transparency in AI-generated outcomes, and the imperative for ongoing validation to establish the reliability and applicability of AI tools. This review underscores the prospective benefits of AI in dental practice, envisioning AI not as a replacement for dental professionals but as an adjunctive tool that fortifies the dental profession. While AI heralds improvements in diagnostics, treatment planning, and personalized care, ethical and practical considerations must be meticulously navigated to ensure responsible development of AI in dentistry.
DA  - 2024 Jun 14
PY  - 2024
VL  - 14
IS  - 12
DO  - 10.3390/diagnostics14121260
AN  - MEDLINE:38928675
ER  -

TY  - JOUR
AU  - van der Waa, Jasper
AU  - Verdult, Sabine
AU  - van den Bosch, Karel
AU  - van Diggelen, Jurriaan
AU  - Haije, Tjalling
AU  - van der Stigchel, Birgit
AU  - Cocu, Ioana
TI  - Moral Decision Making in Human-Agent Teams: Human Control and the Role of Explanations.
T2  - Frontiers in robotics and AI
M3  - Journal Article
AB  - With the progress of Artificial Intelligence, intelligent agents are increasingly being deployed in tasks for which ethical guidelines and moral values apply. As artificial agents do not have a legal position, humans should be held accountable if actions do not comply, implying humans need to exercise control. This is often labeled as Meaningful Human Control (MHC). In this paper, achieving MHC is addressed as a design problem, defining the collaboration between humans and agents. We propose three possible team designs (Team Design Patterns), varying in the level of autonomy on the agent's part. The team designs include explanations given by the agent to clarify its reasoning and decision-making. The designs were implemented in a simulation of a medical triage task, to be executed by a domain expert and an artificial agent. The triage task simulates making decisions under time pressure, with too few resources available to comply with all medical guidelines all the time, hence involving moral choices. Domain experts (i.e., health care professionals) participated in the present study. One goal was to assess the ecological relevance of the simulation. Secondly, to explore the control that the human has over the agent to warrant moral compliant behavior in each proposed team design. Thirdly, to evaluate the role of agent explanations on the human's understanding in the agent's reasoning. Results showed that the experts overall found the task a believable simulation of what might occur in reality. Domain experts experienced control over the team's moral compliance when consequences were quickly noticeable. When instead the consequences emerged much later, the experts experienced less control and felt less responsible. Possibly due to the experienced time pressure implemented in the task or over trust in the agent, the experts did not use explanations much during the task; when asked afterwards they however considered these to be useful. It is concluded that a team design should emphasize and support the human to develop a sense of responsibility for the agent's behavior and for the team's decisions. The design should include explanations that fit with the assigned team roles as well as the human cognitive state.
DA  - 2021
PY  - 2021
VL  - 8
SP  - 640647
EP  - 640647
DO  - 10.3389/frobt.2021.640647
AN  - MEDLINE:34124173
ER  -

TY  - JOUR
AU  - Verhagen, Ruben S
AU  - Neerincx, Mark A
AU  - Tielman, Myrthe L
TI  - The influence of interdependence and a transparent or explainable communication style on human-robot teamwork.
T2  - Frontiers in robotics and AI
M3  - Journal Article
AB  - Humans and robots are increasingly working together in human-robot teams. Teamwork requires communication, especially when interdependence between team members is high. In previous work, we identified a conceptual difference between sharing what you are doing (i.e., being transparent) and why you are doing it (i.e., being explainable). Although the second might sound better, it is important to avoid information overload. Therefore, an online experiment (n = 72) was conducted to study the effect of communication style of a robot (silent, transparent, explainable, or adaptive based on time pressure and relevancy) on human-robot teamwork. We examined the effects of these communication styles on trust in the robot, workload during the task, situation awareness, reliance on the robot, human contribution during the task, human communication frequency, and team performance. Moreover, we included two levels of interdependence between human and robot (high vs. low), since mutual dependency might influence which communication style is best. Participants collaborated with a virtual robot during two simulated search and rescue tasks varying in their level of interdependence. Results confirm that in general robot communication results in more trust in and understanding of the robot, while showing no evidence of a higher workload when the robot communicates or adds explanations to being transparent. Providing explanations, however, did result in more reliance on RescueBot. Furthermore, compared to being silent, only being explainable results a higher situation awareness when interdependence is high. Results further show that being highly interdependent decreases trust, reliance, and team performance while increasing workload and situation awareness. High interdependence also increases human communication if the robot is not silent, human rescue contribution if the robot does not provide explanations, and the strength of the positive association between situation awareness and team performance. From these results, we can conclude that robot communication is crucial for human-robot teamwork, and that important differences exist between being transparent, explainable, or adaptive. Our findings also highlight the fundamental importance of interdependence in studies on explainability in robots.
DA  - 2022
PY  - 2022
VL  - 9
SP  - 993997
EP  - 993997
DO  - 10.3389/frobt.2022.993997
AN  - MEDLINE:36158603
ER  -

TY  - JOUR
AU  - Holzinger, Andreas
AU  - Zatloukal, Kurt
AU  - Muller, Heimo
TI  - Is human oversight to AI systems still possible?
T2  - New biotechnology
M3  - Editorial
AB  - The rapid proliferation of artificial intelligence (AI) systems across diverse domains raises critical questions about the feasibility of meaningful human oversight, particularly in high-stakes domains such as new biotechnology. As AI systems grow increasingly complex, opaque, and autonomous, ensuring responsible use becomes a formidable challenge. During our editorial work for the special issue "Artificial Intelligence for Life Sciences", we placed increasing emphasis on the topic of "human oversight". Consequently, in this editorial we briefly discuss the evolving role of human oversight in AI governance, focusing on the practical, technical, and ethical dimensions of maintaining control. It examines how the complexity of contemporary AI architectures, such as large-scale neural networks and generative AI applications, undermine human understanding and decision-making capabilities. Furthermore, it evaluates emerging approaches-such as explainable AI (XAI), human-in-the-loop systems, and regulatory frameworks-that aim to enable oversight while acknowledging their limitations. Through a comprehensive analysis, the picture emerged while complete oversight may no longer be viable in certain contexts, strategic interventions leveraging human-AI collaboration and trustworthy AI design principles can preserve accountability and safety. The discussion highlights the urgent need for interdisciplinary efforts to rethink oversight mechanisms in an era where AI may outpace human comprehension.
DA  - 2025 Mar 25 (Epub 2024 Dec 13)
PY  - 2025
VL  - 85
SP  - 59
EP  - 62
DO  - 10.1016/j.nbt.2024.12.003
AN  - MEDLINE:39675423
ER  -

TY  - JOUR
AU  - Rau, Stephan
AU  - Rau, Alexander
AU  - Nattenmuller, Johanna
AU  - Fink, Anna
AU  - Bamberg, Fabian
AU  - Reisert, Marco
AU  - Russe, Maximilian F
TI  - A retrieval-augmented chatbot based on GPT-4 provides appropriate differential diagnosis in gastrointestinal radiology: a proof of concept study.
T2  - European radiology experimental
M3  - Journal Article
AB  - BACKGROUND: We investigated the potential of an imaging-aware GPT-4-based chatbot in providing diagnoses based on imaging descriptions of abdominal pathologies.METHODS: Utilizing zero-shot learning via the LlamaIndex framework, GPT-4 was enhanced using the 96 documents from the Radiographics Top 10 Reading List on gastrointestinal imaging, creating a gastrointestinal imaging-aware chatbot (GIA-CB). To assess its diagnostic capability, 50 cases on a variety of abdominal pathologies were created, comprising radiological findings in fluoroscopy, MRI, and CT. We compared the GIA-CB to the generic GPT-4 chatbot (g-CB) in providing the primary and 2 additional differential diagnoses, using interpretations from senior-level radiologists as ground truth. The trustworthiness of the GIA-CB was evaluated by investigating the source documents as provided by the knowledge-retrieval mechanism. Mann-Whitney U test was employed.RESULTS: The GIA-CB demonstrated a high capability to identify the most appropriate differential diagnosis in 39/50 cases (78%), significantly surpassing the g-CB in 27/50 cases (54%) (p=0.006). Notably, the GIA-CB offered the primary differential in the top 3 differential diagnoses in 45/50 cases (90%) versus g-CB with 37/50 cases (74%) (p=0.022) and always with appropriate explanations. The median response time was 29.8s for GIA-CB and 15.7s for g-CB, and the mean cost per case was $0.15 and $0.02, respectively.CONCLUSIONS: The GIA-CB not only provided an accurate diagnosis for gastrointestinal pathologies, but also direct access to source documents, providing insight into the decision-making process, a step towards trustworthy and explainable AI. Integrating context-specific data into AI models can support evidence-based clinical decision-making.RELEVANCE STATEMENT: A context-aware GPT-4 chatbot demonstrates high accuracy in providing differential diagnoses based on imaging descriptions, surpassing the generic GPT-4. It provided formulated rationale and source excerpts supporting the diagnoses, thus enhancing trustworthy decision-support.KEY POINTS:  Knowledge retrieval enhances differential diagnoses in a gastrointestinal imaging-aware chatbot (GIA-CB).  GIA-CB outperformed the generic counterpart, providing formulated rationale and source excerpts.  GIA-CB has the potential to pave the way for AI-assisted decision support systems.
DA  - 2024 May 17
PY  - 2024
VL  - 8
IS  - 1
SP  - 60
EP  - 60
DO  - 10.1186/s41747-024-00457-x
AN  - MEDLINE:38755410
ER  -

TY  - JOUR
AU  - Wahlstrom, Mikael
AU  - Tammentie, Bastian
AU  - Salonen, Tuisku-Tuuli
AU  - Karvonen, Antero
TI  - AI and the transformation of industrial work: Hybrid intelligence vs double-black box effect.
T2  - Applied ergonomics
M3  - Journal Article
AB  - It is uncertain how the application of artificial intelligence (AI) technology transforms industrial work. We address this question from the perspective of cognitive systems, which, in this case, includes considerations of AI and process transparency, resilience, division of labor, and worker skills. We draw from a case study on glass tempering that includes a machine-vision-based quality control system and an advanced automation process control system. Based on task analysis and background literature, we develop the concept of hybrid intelligence that implies balanced AI transparency that supports upskilling and resilience. So-called fragmented intelligence, in turn, may result from the combination of the complexity of advanced automation along with the complexity of the process physics that places critical emphasis on expert knowledge. This combination can result in the so-called "double black box effect", given that designing for understandability for the line workers might not be feasible: expert networks are needed for resilience.
DA  - 2024 Jul (Epub 2024 Apr 04)
PY  - 2024
VL  - 118
SP  - 104271
EP  - 104271
DO  - 10.1016/j.apergo.2024.104271
AN  - MEDLINE:38579495
ER  -

TY  - JOUR
AU  - Luo, Yi
AU  - Cuneo, Kyle C
AU  - Lawrence, Theodore S
AU  - Matuszak, Martha M
AU  - Dawson, Laura A
AU  - Niraula, Dipesh
AU  - Ten Haken, Randall K
AU  - El Naqa, Issam
TI  - A human-in-the-loop based Bayesian network approach to improve imbalanced radiation outcomes prediction for hepatocellular cancer patients with stereotactic body radiotherapy.
T2  - Frontiers in oncology
M3  - Journal Article
AB  - Background: Imbalanced outcome is one of common characteristics of oncology datasets. Current machine learning approaches have limitation in learning from such datasets. Here, we propose to resolve this problem by utilizing a human-in-the-loop (HITL) approach, which we hypothesize will also lead to more accurate and explainable outcome prediction models.Methods: A total of 119 HCC patients with 163 tumors were used in the study. 81 patients with 104 tumors from the University of Michigan Hospital treated with SBRT were considered as a discovery dataset for radiation outcomes model building. The external testing dataset included 59 tumors from 38 patients with SBRT from Princess Margaret Hospital. In the discovery dataset, 100 tumors from 77 patients had local control (LC) (96% of 104 tumors) and 23 patients had at least one grade increment of ALBI (I-ALBI) during six-month follow up (28% of 81 patients). Each patient had a total of 110 features, where 15 or 20 features were identified by physicians as expert knowledge features (EKFs) for LC or I-ALBI prediction. We proposed a HITL based Bayesian network (HITL-BN) approach to enhance the capability of selecting important features from imbalanced data in terms of accuracy and explainability through humans' participation by integrating feature importance ranking and Markov blanket algorithms. A pure data-driven Bayesian network (PD-BN) method was applied to the same discovery dataset of HCC patients as a benchmark.Results: In the training and testing phases, the areas under receiver operating characteristic curves of the HITL-BN models for LC or I-ALBI prediction during SBRT are 0.85 (95% confidence interval: 0.75-0.95) or 0.89 (0.81-0.95) and 0.77 or 0.78, respectively. They significantly outperformed the during-treatment PD-BN model in predicting LC or I-ALBI based on the discovery cross-validation and testing datasets from the Delong tests.Conclusion: By allowing the human expert to be part of the model building process, the HITL-BN approach yielded significantly improved accuracy as well as better explainability when dealing with imbalanced outcomes in the prediction of post-SBRT treatment response of HCC patients when compared to the PD-BN method.
DA  - 2022
PY  - 2022
VL  - 12
SP  - 1061024
EP  - 1061024
DO  - 10.3389/fonc.2022.1061024
AN  - MEDLINE:36568208
ER  -

TY  - JOUR
AU  - Jarrahi, Mohammad Hossein
AU  - Davoudi, Vahid
AU  - Haeri, Mohammad
TI  - The key to an effective AI-powered digital pathology: Establishing a symbiotic workflow between pathologists and machine.
T2  - Journal of pathology informatics
M3  - Journal Article
AB  - Pathology is a fundamental element of modern medicine that determines the final diagnosis of medical conditions, leads medical decisions, and portrays the prognosis. Due to continuous improvements in AI capabilities (e.g., object recognition and image processing), intelligent systems are bound to play a key role in augmenting pathology research and clinical practices. Despite the pervasive deployment of computational approaches in similar fields such as radiology, there has been less success in integrating AI in clinical practices and histopathological diagnosis. This is partly due to the opacity of end-to-end AI systems, which raises issues of interoperability and accountability of medical practices. In this article, we draw on interactive machine learning to take advantage of AI in digital pathology to open the black box of AI and generate a more effective partnership between pathologists and AI systems based on the metaphors of parameterization and implicitization.
DA  - 2022
PY  - 2022
VL  - 13
SP  - 100156
EP  - 100156
DO  - 10.1016/j.jpi.2022.100156
AN  - MEDLINE:36605113
ER  -

TY  - JOUR
AU  - Rosenbacke, Rikard
AU  - Melhus, Asa
AU  - McKee, Martin
AU  - Stuckler, David
TI  - AI and XAI second opinion: the danger of false confirmation in human-AI collaboration.
T2  - Journal of medical ethics
M3  - Journal Article
AB  - Can AI substitute a human physician's second opinion? Recently the Journal of Medical Ethics published two contrasting views: Kempt and Nagel advocate for using artificial intelligence (AI) for a second opinion except when its conclusions significantly diverge from the initial physician's while Jongsma and Sand argue for a second human opinion irrespective of AI's concurrence or dissent. The crux of this debate hinges on the prevalence and impact of 'false confirmation'-a scenario where AI erroneously validates an incorrect human decision. These errors seem exceedingly difficult to detect, reminiscent of heuristics akin to confirmation bias. However, this debate has yet to engage with the emergence of explainable AI (XAI), which elaborates on why the AI tool reaches its diagnosis. To progress this debate, we outline a framework for conceptualising decision-making errors in physician-AI collaborations. We then review emerging evidence on the magnitude of false confirmation errors. Our simulations show that they are likely to be pervasive in clinical practice, decreasing diagnostic accuracy to between 5% and 30%. We conclude with a pragmatic approach to employing AI as a second opinion, emphasising the need for physicians to make clinical decisions before consulting AI; employing nudges to increase awareness of false confirmations and critically engaging with XAI explanations. This approach underscores the necessity for a cautious, evidence-based methodology when integrating AI into clinical decision-making.
DA  - 2025 May 21
PY  - 2025
VL  - 51
IS  - 6
SP  - 396
EP  - 399
DO  - 10.1136/jme-2024-110074
AN  - MEDLINE:39074956
ER  -

TY  - JOUR
AU  - Finkelstein, Joseph
AU  - Gabriel, Aileen
AU  - Schmer, Susanna
AU  - Truong, Tuyet-Trinh
AU  - Dunn, Andrew
TI  - Identifying Facilitators and Barriers to Implementation of AI-Assisted Clinical Decision Support in an Electronic Health Record System.
T2  - Journal of medical systems
M3  - Journal Article
AB  - Recent advancements in computing have led to the development of artificial intelligence (AI) enabled healthcare technologies. AI-assisted clinical decision support (CDS) integrated into electronic health records (EHR) was demonstrated to have a significant potential to improve clinical care. With the rapid proliferation of AI-assisted CDS, came the realization that a lack of careful consideration of socio-technical issues surrounding the implementation and maintenance of these tools can result in unanticipated consequences, missed opportunities, and suboptimal uptake of these potentially useful technologies. The 48-h Discharge Prediction Tool (48DPT) is a new AI-assisted EHR CDS to facilitate discharge planning. This study aimed to methodologically assess the implementation of 48DPT and identify the barriers and facilitators of adoption and maintenance using the validated implementation science frameworks. The major dimensions of RE-AIM (Reach, Effectiveness, Adoption, Implementation, Maintenance) and the constructs of the Consolidated Framework for Implementation Research (CFIR) frameworks have been used to analyze interviews of 24 key stakeholders using 48DPT. The systematic assessment of the 48DPT implementation allowed us to describe facilitators and barriers to implementation such as lack of awareness, lack of accuracy and trust, limited accessibility, and transparency. Based on our evaluation, the factors that are crucial for the successful implementation of AI-assisted EHR CDS were identified. Future implementation efforts of AI-assisted EHR CDS should engage the key clinical stakeholders in the AI tool development from the very inception of the project, support transparency and explainability of the AI models, provide ongoing education and onboarding of the clinical users, and obtain continuous input from clinical staff on the CDS performance.
DA  - 2024 Sep 18
PY  - 2024
VL  - 48
IS  - 1
SP  - 89
EP  - 89
DO  - 10.1007/s10916-024-02104-9
AN  - MEDLINE:39292314
ER  -

TY  - JOUR
AU  - Di Stefano, Valeria
AU  - D'Angelo, Martina
AU  - Monaco, Francesco
AU  - Vignapiano, Annarita
AU  - Martiadis, Vassilis
AU  - Barone, Eugenia
AU  - Fornaro, Michele
AU  - Steardo, Luca
AU  - Solmi, Marco
AU  - Manchia, Mirko
AU  - Steardo, Luca Jr
TI  - Decoding Schizophrenia: How AI-Enhanced fMRI Unlocks New Pathways for Precision Psychiatry.
T2  - Brain sciences
M3  - Journal Article
AB  - Schizophrenia, a highly complex psychiatric disorder, presents significant challenges in diagnosis and treatment due to its multifaceted neurobiological underpinnings. Recent advancements in functional magnetic resonance imaging (fMRI) and artificial intelligence (AI) have revolutionized the understanding and management of this condition. This manuscript explores how the integration of these technologies has unveiled key insights into schizophrenia's structural and functional neural anomalies. fMRI research highlights disruptions in crucial brain regions like the prefrontal cortex and hippocampus, alongside impaired connectivity within networks such as the default mode network (DMN). These alterations correlate with the cognitive deficits and emotional dysregulation characteristic of schizophrenia. AI techniques, including machine learning (ML) and deep learning (DL), have enhanced the detection and analysis of these complex patterns, surpassing traditional methods in precision. Algorithms such as support vector machines (SVMs) and Vision Transformers (ViTs) have proven particularly effective in identifying biomarkers and aiding early diagnosis. Despite these advancements, challenges such as variability in methodologies and the disorder's heterogeneity persist, necessitating large-scale, collaborative studies for clinical translation. Moreover, ethical considerations surrounding data integrity, algorithmic transparency, and patient individuality must guide AI's integration into psychiatry. Looking ahead, AI-augmented fMRI holds promise for tailoring personalized interventions, addressing unique neural dysfunctions, and improving therapeutic outcomes for individuals with schizophrenia. This convergence of neuroimaging and computational innovation heralds a transformative era in precision psychiatry.
DA  - 2024 Nov 27
PY  - 2024
VL  - 14
IS  - 12
DO  - 10.3390/brainsci14121196
AN  - MEDLINE:39766395
ER  -

TY  - JOUR
AU  - Hashemian, Hesam
AU  - Peto, Tunde
AU  - Ambrosio, Renato Jr
AU  - Lengyel, Imre
AU  - Kafieh, Rahele
AU  - Muhammed Noori, Ahmed
AU  - Khorrami-Nejad, Masoud
TI  - Application of Artificial Intelligence in Ophthalmology: An Updated Comprehensive Review.
T2  - Journal of ophthalmic & vision research
M3  - Journal Article
M3  - Review
AB  - Artificial intelligence (AI) holds immense promise for transforming ophthalmic care through automated screening, precision diagnostics, and optimized treatment planning. This paper reviews recent advances and challenges in applying AI techniques such as machine learning and deep learning to major eye diseases. In diabetic retinopathy, AI algorithms analyze retinal images to accurately identify lesions, which helps clinicians in ophthalmology practice. Systems like IDx-DR (IDx Technologies Inc, USA) are FDA-approved for autonomous detection of referable diabetic retinopathy. For glaucoma, deep learning models assess optic nerve head morphology in fundus photographs to detect damage. In age-related macular degeneration, AI can quantify drusen and diagnose disease severity from both color fundus and optical coherence tomography images. AI has also been used in screening for retinopathy of prematurity, keratoconus, and dry eye disease. Beyond screening, AI can aid treatment decisions by forecasting disease progression and anti-VEGF response. However, potential limitations such as the quality and diversity of training data, lack of rigorous clinical validation, and challenges in regulatory approval and clinician trust must be addressed for the widespread adoption of AI. Two other significant hurdles include the integration of AI into existing clinical workflows and ensuring transparency in AI decision-making processes. With continued research to address these limitations, AI promises to enable earlier diagnosis, optimized resource allocation, personalized treatment, and improved patient outcomes. Besides, synergistic human-AI systems could set a new standard for evidence-based, precise ophthalmic care.
DA  - 2024 Jul-Sep
PY  - 2024
VL  - 19
IS  - 3
SP  - 354
EP  - 367
DO  - 10.18502/jovr.v19i3.15893
AN  - MEDLINE:39359529
ER  -

TY  - JOUR
AU  - Hsiao, Janet Hui-Wen
TI  - Understanding Human Cognition Through Computational Modeling.
T2  - Topics in cognitive science
M3  - Journal Article
AB  - One important goal of cognitive science is to understand the mind in terms of its representational and computational capacities, where computational modeling plays an essential role in providing theoretical explanations and predictions of human behavior and mental phenomena. In my research, I have been using computational modeling, together with behavioral experiments and cognitive neuroscience methods, to investigate the information processing mechanisms underlying learning and visual cognition in terms of perceptual representation and attention strategy. In perceptual representation, I have used neural network models to understand how the split architecture in the human visual system influences visual cognition, and to examine perceptual representation development as the results of expertise. In attention strategy, I have developed the Eye Movement analysis with Hidden Markov Models method for quantifying eye movement pattern and consistency using both spatial and temporal information, which has led to novel findings across disciplines not discoverable using traditional methods. By integrating it with deep neural networks (DNN), I have developed DNN+HMM to account for eye movement strategy learning in human visual cognition. The understanding of the human mind through computational modeling also facilitates research on artificial intelligence's (AI) comparability with human cognition, which can in turn help explainable AI systems infer humans' belief on AI's operations and provide human-centered explanations to enhance human-AI interaction and mutual understanding. Together, these demonstrate the essential role of computational modeling methods in providing theoretical accounts of the human mind as well as its interaction with its environment and AI systems.
DA  - 2024 Jul (Epub 2024 May 23)
PY  - 2024
VL  - 16
IS  - 3
SP  - 349
EP  - 376
DO  - 10.1111/tops.12737
AN  - MEDLINE:38781432
ER  -

TY  - JOUR
AU  - Zerbe, Norman
AU  - Schwen, Lars Ole
AU  - GeiSSler, Christian
AU  - Wiesemann, Katja
AU  - Bisson, Tom
AU  - Boor, Peter
AU  - Carvalho, Rita
AU  - Franz, Michael
AU  - Jansen, Christoph
AU  - Kiehl, Tim-Rasmus
AU  - Lindequist, Bjorn
AU  - Pohlan, Nora Charlotte
AU  - Schmell, Sarah
AU  - Strohmenger, Klaus
AU  - Zakrzewski, Falk
AU  - Plass, Markus
AU  - Takla, Michael
AU  - Kuster, Tobias
AU  - Homeyer, Andre
AU  - Hufnagl, Peter
TI  - Joining forces for pathology diagnostics with AI assistance: The EMPAIA initiative.
T2  - Journal of pathology informatics
M3  - Journal Article
M3  - Review
AB  - Over the past decade, artificial intelligence (AI) methods in pathology have advanced substantially. However, integration into routine clinical practice has been slow due to numerous challenges, including technical and regulatory hurdles in translating research results into clinical diagnostic products and the lack of standardized interfaces. The open and vendor-neutral EMPAIA initiative addresses these challenges. Here, we provide an overview of EMPAIA's achievements and lessons learned. EMPAIA integrates various stakeholders of the pathology AI ecosystem, i.e., pathologists, computer scientists, and industry. In close collaboration, we developed technical interoperability standards, recommendations for AI testing and product development, and explainability methods. We implemented the modular and open-source EMPAIA Platform and successfully integrated 14 AI-based image analysis apps from eight different vendors, demonstrating how different apps can use a single standardized interface. We prioritized requirements and evaluated the use of AI in real clinical settings with 14 different pathology laboratories in Europe and Asia. In addition to technical developments, we created a forum for all stakeholders to share information and experiences on digital pathology and AI. Commercial, clinical, and academic stakeholders can now adopt EMPAIA's common open-source interfaces, providing a unique opportunity for large-scale standardization and streamlining of processes. Further efforts are needed to effectively and broadly establish AI assistance in routine laboratory use. To this end, a sustainable infrastructure, the non-profit association EMPAIA International, has been established to continue standardization and support broad implementation and advocacy for an AI-assisted digital pathology future.
DA  - 2024 Dec
PY  - 2024
VL  - 15
SP  - 100387
EP  - 100387
DO  - 10.1016/j.jpi.2024.100387
AN  - MEDLINE:38984198
ER  -

TY  - JOUR
AU  - Xu, Haoli
AU  - Yang, Xing
AU  - Hu, Yihua
AU  - Wang, Daqing
AU  - Liang, Zhenyu
AU  - Mu, Hua
AU  - Wang, Yangyang
AU  - Shi, Liang
AU  - Gao, Haoqi
AU  - Song, Daoqing
AU  - Cheng, Zijian
AU  - Lu, Zhao
AU  - Zhao, Xiaoning
AU  - Lu, Jun
AU  - Wang, Bingwen
AU  - Hu, Zhiyang
TI  - Trusted artificial intelligence for environmental assessments: An explainable high-precision model with multi-source big data.
T2  - Environmental science and ecotechnology
M3  - Journal Article
AB  - Environmental assessments are critical for ensuring the sustainable development of human civilization. The integration of artificial intelligence (AI) in these assessments has shown great promise, yet the "black box" nature of AI models often undermines trust due to the lack of transparency in their decision-making processes, even when these models demonstrate high accuracy. To address this challenge, we evaluated the performance of a transformer model against other AI approaches, utilizing extensive multivariate and spatiotemporal environmental datasets encompassing both natural and anthropogenic indicators. We further explored the application of saliency maps as a novel explainability tool in multi-source AI-driven environmental assessments, enabling the identification of individual indicators' contributions to the model's predictions. We find that the transformer model outperforms others, achieving an accuracy of about 98% and an area under the receiver operating characteristic curve (AUC) of 0.891. Regionally, the environmental assessment values are predominantly classified as level II or III in the central and southwestern study areas, level IV in the northern region, and level V in the western region. Through explainability analysis, we identify that water hardness, total dissolved solids, and arsenic concentrations are the most influential indicators in the model. Our AI-driven environmental assessment model is accurate and explainable, offering actionable insights for targeted environmental management. Furthermore, this study advances the application of AI in environmental science by presenting a robust, explainable model that bridges the gap between machine learning and environmental governance, enhancing both understanding and trust in AI-assisted environmental assessments.
DA  - 2024 Nov
PY  - 2024
VL  - 22
SP  - 100479
EP  - 100479
DO  - 10.1016/j.ese.2024.100479
AN  - MEDLINE:39286480
ER  -

TY  - JOUR
AU  - Kayadibi, Ismail
AU  - Kose, Utku
AU  - Guraksin, Gur Emre
AU  - Cetin, Bilgun
TI  - An AI-assisted explainable mTMCNN architecture for detection of mandibular third molar presence from panoramic radiography.
T2  - International journal of medical informatics
M3  - Journal Article
AB  - OBJECTIVE: This study aimed to design and systematically evaluate an architecture, proposed as the Explainable Mandibular Third Molar Convolutional Neural Network (E-mTMCNN), for detecting the presence of mandibular third molars (m-M3) in panoramic radiography (PR). The proposed architecture seeks to enhance the accuracy of early detection and improve clinical decision-making and treatment planning in dentistry.METHODS: A new dataset, named the Mandibular Third Molar (m-TM) dataset, was developed through expert labeling of raw PR images from the UESB dataset. This dataset was subsequently made publicly accessible to support further research. Several advanced image preprocessing techniques, including Gaussian filtering, gamma correction, and data augmentation, were applied to improve image quality. Various Deep learning (DL) based Convolutional Neural Network (CNN) architectures were trained and validated using Transfer Learning (TL) methodologies. Among these, the E-mTMCNN, leveraging the GoogLeNet architecture, achieved the highest performance metrics. To ensure transparency in the model's decision-making process, Local Interpretable Model-Agnostic Explanations (LIME) were integrated as an eXplainable Artificial Intelligence (XAI) approach. Clinical reliability and applicability were assessed through an expert survey conducted among specialized dentists using a decision support system based on the E-mTMCNN.RESULTS: The E-mTMCNN architecture demonstrated a classification accuracy of 87.02%, with a sensitivity of 75%, specificity of 94.73%, precision of 77.68%, an F1 score of 75.51%, and an area under the curve (AUC) of 87.01%. The integration of LIME provided visual explanations of the model's decision-making rationale, reinforcing the robustness of the proposed architecture. Results from the expert survey indicated high clinical acceptance and confidence in the reliability of the system.CONCLUSION: The findings demonstrate that the E-mTMCNN architecture effectively detects the presence of m-M3 in PRs, outperforming current state-of-the-art methodologies. The proposed architecture shows considerable potential for integration into computer-aided diagnostic systems, advancing early detection capabilities and enhancing the precision of treatment planning in dental practice.
DA  - 2025 Mar (Epub 2024 Nov 23)
PY  - 2025
VL  - 195
SP  - 105724
EP  - 105724
DO  - 10.1016/j.ijmedinf.2024.105724
AN  - MEDLINE:39626596
ER  -

TY  - JOUR
AU  - Gomez, Catalina
AU  - Smith, Brittany-Lee
AU  - Zayas, Alisa
AU  - Unberath, Mathias
AU  - Canares, Therese
TI  - Explainable AI decision support improves accuracy during telehealth strep throat screening.
T2  - Communications medicine
M3  - Journal Article
AB  - BACKGROUND: Artificial intelligence-based (AI) clinical decision support systems (CDSS) using unconventional data, like smartphone-acquired images, promise transformational opportunities for telehealth; including remote diagnosis. Although such solutions' potential remains largely untapped, providers' trust and understanding are vital for effective adoption. This study examines how different human-AI interaction paradigms affect clinicians' responses to an emerging AI CDSS for streptococcal pharyngitis (strep throat) detection from smartphone throat images.METHODS: In a randomized experiment, we tested explainable AI strategies using three AI-based CDSS prototypes for strep throat prediction. Participants received clinical vignettes via an online survey to predict the disease state and offer clinical recommendations. The first set included a validated CDSS prediction (Modified Centor Score) and the second introduced an explainable AI prototype randomly. We used linear models to assess explainable AI's effect on clinicians' accuracy, confirmatory testing rates, and perceived trust and understanding of the CDSS.RESULTS: The study, involving 121 telehealth providers, shows that compared to using the Centor Score, AI-based CDSS can improve clinicians' predictions. Despite higher agreement with AI, participants report lower trust in its advice than in the Centor Score, leading to more requests for in-person confirmatory testing.CONCLUSIONS: Effectively integrating AI is crucial in the telehealth-based diagnosis of infectious diseases, given the implications of antibiotic over-prescriptions. We demonstrate that AI-based CDSS can improve the accuracy of remote strep throat screening yet underscores the necessity to enhance human-machine collaboration, particularly in trust and intelligibility. This ensures providers and patients can capitalize on AI interventions and smartphones for virtual healthcare.
AB  - Strep pharyngitis, or strep throat, is a bacterial infection that can cause a sore throat. Artificial intelligence (AI) can use photos taken on a persons phone to help diagnose strep throat, offering an additional way for doctors to screen patients during virtual appointments. However, it is currently unclear whether doctors will trust AI recommendations or how they might use them in decision-making. We surveyed clinicians about their use of an AI system for strep throat screening with smartphone images. We compared different ways of providing AI recommendations to standard medical guidelines. We found that all testedAImethods helped clinicians to identify strep throat cases. However, clinicians trusted AI less than their usual clinical guidelines, leading to more requests for follow-up in-person testing. Our results show how AI may improve the accuracy of pharyngitis assessment. Still, further research is needed to ensure doctors trust and collaborate with AI to improve remote healthcare.
DA  - 2024 Jul 24
PY  - 2024
VL  - 4
IS  - 1
SP  - 149
EP  - 149
DO  - 10.1038/s43856-024-00568-x
AN  - MEDLINE:39048726
ER  -

TY  - JOUR
AU  - Schraagen, Jan Maarten
TI  - Responsible use of AI in military systems: prospects and challenges.
T2  - Ergonomics
M3  - Journal Article
AB  - Artificial Intelligence (AI) holds great potential for the military domain but is also seen as prone to data bias and lacking transparency and explainability. In order to advance the trustworthiness of AI-enabled systems, a dynamic approach to the development, deployment and use of AI systems is required. This approach, when incorporating ethical principles such as lawfulness, traceability, reliability and bias mitigation, is called 'Responsible AI'. This article describes the challenges of using AI responsibly in the military domain from a human factors and ergonomics perspective. Many of the ironies of automation originally described by Bainbridge still apply in the field of AI, but there are also some unique challenges and requirements that need to be considered, such as a larger emphasis on ethical risk analyses and validation and verification up-front, as well as moral situation awareness during deployment and use of AI in military systems.
AB  - Responsible AI is a relatively novel transdisciplinary field incorporating ethical principles in the development and use of AI in military systems. I describe the prospects and challenges with Responsible AI from a human factors and ergonomics perspective. There is in particular a need for new methods for testing and evaluation, validation and verification, explainability and transparency of AI, as well as for new ways of Human-AI Teaming.
DA  - 2023 Nov (Epub 2024 Jan 02)
PY  - 2023
VL  - 66
IS  - 11
SP  - 1719
EP  - 1729
DO  - 10.1080/00140139.2023.2278394
AN  - MEDLINE:37905780
ER  -

TY  - JOUR
AU  - Rudrauf, David
AU  - Sergeant-Perthuis, Gregoire
AU  - Tisserand, Yvain
AU  - Poloudenny, Germain
AU  - Williford, Kenneth
AU  - Amorim, Michel-Ange
TI  - The Projective Consciousness Model: Projective Geometry at the Core of Consciousness and the Integration of Perception, Imagination, Motivation, Emotion, Social Cognition and Action.
T2  - Brain sciences
M3  - Journal Article
AB  - Consciousness has been described as acting as a global workspace that integrates perception, imagination, emotion and action programming for adaptive decision making. The mechanisms of this workspace and their relationships to the phenomenology of consciousness need to be further specified. Much research in this area has focused on the neural correlates of consciousness, but, arguably, computational modeling can better be used toward this aim. According to the Projective Consciousness Model (PCM), consciousness is structured as a viewpoint-organized, internal space, relying on 3D projective geometry and governed by the action of the Projective Group as part of a process of active inference. The geometry induces a group-structured subjective perspective on an encoded world model, enabling adaptive perspective taking in agents. Here, we review and discuss the PCM. We emphasize the role of projective mechanisms in perception and the appraisal of affective and epistemic values as tied to the motivation of action, under an optimization process of Free Energy minimization, or more generally stochastic optimal control. We discuss how these mechanisms enable us to model and simulate group-structured drives in the context of social cognition and to understand the mechanisms underpinning empathy, emotion expression and regulation, and approach-avoidance behaviors. We review previous results, drawing on applications in robotics and virtual humans. We briefly discuss future axes of research relating to applications of the model to simulation- and model-based behavioral science, geometrically structured artificial neural networks, the relevance of the approach for explainable AI and human-machine interactions, and the study of the neural correlates of consciousness.
DA  - 2023 Oct 09
PY  - 2023
VL  - 13
IS  - 10
DO  - 10.3390/brainsci13101435
AN  - MEDLINE:37891803
ER  -

TY  - JOUR
AU  - Din, Munaib
AU  - Daga, Karan
AU  - Saoud, Jihad
AU  - Wood, David
AU  - Kierkegaard, Patrick
AU  - Brex, Peter
AU  - Booth, Thomas C
TI  - Clinicians' perspectives on the use of artificial intelligence to triage MRI brain scans.
T2  - European journal of radiology
M3  - Journal Article
AB  - Artificial intelligence (AI) tools can triage radiology scans to streamline the patient pathway and also relieve clinician workload. Validated AI tools can mitigate the delays in reporting scans by flagging time-sensitive and actionable findings. In this study, we aim to investigate current stakeholder perspectives and identify obstacles to integrating AI in clinical pathways. We created a survey to ascertain the perspectives of 133 clinicians across the United Kingdom regarding the acceptability of an AI tool that triages MRI brain scans into 'normal' and 'abnormal'. As part of this survey, we supplied clinicians with information on training and validation case numbers, model performance, validation using unseen data, and explainability saliency maps. With regards to the specific use case of AI in MRI brain scans, 71% of respondents preferred the use of an AI-assisted triage compared to the current system without triage, typically chronologically. Notably, information that explained and helped visualise the AI model's decision making was found to improve clinician confidence. When shown a heatmap, 60% of participants felt more confident in the AI's decision. The results of this short communication demonstrate a positive support for the implementation of AI-assistive tools in triage.
DA  - 2025 Feb (Epub 2025 Jan 06)
PY  - 2025
VL  - 183
SP  - 111921
EP  - 111921
DO  - 10.1016/j.ejrad.2025.111921
AN  - MEDLINE:39805194
ER  -

TY  - JOUR
AU  - Bae, Sang Won
AU  - Chung, Tammy
AU  - Zhang, Tongze
AU  - Dey, Anind K
AU  - Islam, Rahul
TI  - Enhancing Interpretable, Transparent, and Unobtrusive Detection of Acute Marijuana Intoxication in Natural Environments: Harnessing Smart Devices and Explainable AI to Empower Just-In-Time Adaptive Interventions: Longitudinal Observational Study.
T2  - JMIR AI
M3  - Journal Article
AB  - BACKGROUND: Acute marijuana intoxication can impair motor skills and cognitive functions such as attention and information processing. However, traditional tests, like blood, urine, and saliva, fail to accurately detect acute marijuana intoxication in real time.OBJECTIVE: This study aims to explore whether integrating smartphone-based sensors with readily accessible wearable activity trackers, like Fitbit, can enhance the detection of acute marijuana intoxication in naturalistic settings. No previous research has investigated the effectiveness of passive sensing technologies for enhancing algorithm accuracy or enhancing the interpretability of digital phenotyping through explainable artificial intelligence in real-life scenarios. This approach aims to provide insights into how individuals interact with digital devices during algorithmic decision-making, particularly for detecting moderate to intensive marijuana intoxication in real-world contexts.METHODS: Sensor data from smartphones and Fitbits, along with self-reported marijuana use, were collected from 33 young adults over a 30-day period using the experience sampling method. Participants rated their level of intoxication on a scale from 1 to 10 within 15 minutes of consuming marijuana and during 3 daily semirandom prompts. The ratings were categorized as not intoxicated (0), low (1-3), and moderate to intense intoxication (4-10). The study analyzed the performance of models using mobile phone data only, Fitbit data only, and a combination of both (MobiFit) in detecting acute marijuana intoxication.RESULTS: The eXtreme Gradient Boosting Machine classifier showed that the MobiFit model, which combines mobile phone and wearable device data, achieved 99% accuracy (area under the curve=0.99; F1-score=0.85) in detecting acute marijuana intoxication in natural environments. The F1-score indicated significant improvements in sensitivity and specificity for the combined MobiFit model compared to using mobile or Fitbit data alone. Explainable artificial intelligence revealed that moderate to intense self-reported marijuana intoxication was associated with specific smartphone and Fitbit metrics, including elevated minimum heart rate, reduced macromovement, and increased noise energy around participants.CONCLUSIONS: This study demonstrates the potential of using smartphone sensors and wearable devices for interpretable, transparent, and unobtrusive monitoring of acute marijuana intoxication in daily life. Advanced algorithmic decision-making provides valuable insight into behavioral, physiological, and environmental factors that could support timely interventions to reduce marijuana-related harm. Future real-world applications of these algorithms should be evaluated in collaboration with clinical experts to enhance their practicality and effectiveness.
DA  - 2025 Jan 02
PY  - 2025
VL  - 4
SP  - e52270
EP  - e52270
DO  - 10.2196/52270
AN  - MEDLINE:39746202
ER  -

TY  - JOUR
AU  - Senoner, Julian
AU  - Schallmoser, Simon
AU  - Kratzwald, Bernhard
AU  - Feuerriegel, Stefan
AU  - Netland, Torbjorn
TI  - Explainable AI improves task performance in human-AI collaboration.
T2  - Scientific reports
M3  - Journal Article
AB  - Artificial intelligence (AI) provides considerable opportunities to assist human work. However, one crucial challenge of human-AI collaboration is that many AI algorithms operate in a black-box manner where the way how the AI makes predictions remains opaque. This makes it difficult for humans to validate a prediction made by AI against their own domain knowledge. For this reason, we hypothesize that augmenting humans with explainable AI improves task performance in human-AI collaboration. To test this hypothesis, we implement explainable AI in the form of visual heatmaps in inspection tasks conducted by domain experts. Visual heatmaps have the advantage that they are easy to understand and help to localize relevant parts of an image. We then compare participants that were either supported by (a)black-box AI or (b)explainable AI, where the latter supports them to follow AI predictions when the AI is accurate or overrule the AI when the AI predictions are wrong. We conducted two preregistered experiments with representative, real-world visual inspection tasks from manufacturing and medicine. The first experiment was conducted with factory workers from an electronics factory, who performed [Formula: see text] assessments of whether electronic products have defects. The second experiment was conducted with radiologists, who performed [Formula: see text] assessments of chest X-ray images to identify lung lesions. The results of our experiments with domain experts performing real-world tasks show that task performance improves when participants are supported by explainable AI with heatmaps instead of black-box AI. We find that explainable AI as a decision aid improved the task performance by 7.7 percentage points (95% confidence interval [CI]: 3.3% to 12.0%, [Formula: see text]) in the manufacturing experiment and by 4.7 percentage points (95% CI: 1.1% to 8.3%, [Formula: see text]) in the medical experiment compared to black-box AI. These gains represent a significant improvement in task performance.
DA  - 2024 12 28
PY  - 2024
VL  - 14
IS  - 1
SP  - 31150
EP  - 31150
DO  - 10.1038/s41598-024-82501-9
AN  - MEDLINE:39730794
ER  -

TY  - JOUR
AU  - Ling, Shihong
AU  - Zhang, Yutong
AU  - Du, Na
TI  - More Is Not Always Better: Impacts of AI-Generated Confidence and Explanations in Human-Automation Interaction.
T2  - Human factors
M3  - Journal Article
AB  - OBJECTIVE: The study aimed to enhance transparency in autonomous systems by automatically generating and visualizing confidence and explanations and assessing their impacts on performance, trust, preference, and eye-tracking behaviors in human-automation interaction.BACKGROUND: System transparency is vital to maintaining appropriate levels of trust and mission success. Previous studies presented mixed results regarding the impact of displaying likelihood information and explanations, and often relied on hand-created information, limiting scalability and failing to address real-world dynamics.METHOD: We conducted a dual-task experiment involving 42 university students who operated a simulated surveillance testbed with assistance from intelligent detectors. The study used a 2 (confidence visualization: yes vs. no) * 3 (visual explanations: none, bounding boxes, bounding boxes and keypoints) mixed design. Task performance, human trust, preference for intelligent detectors, and eye-tracking behaviors were evaluated.RESULTS: Visual explanations using bounding boxes and keypoints improved detection task performance when confidence was not displayed. Meanwhile, visual explanations enhanced trust and preference for the intelligent detector, regardless of the explanation type. Confidence visualization did not influence human trust in and preference for the intelligent detector. Moreover, both visual information slowed saccade velocities.CONCLUSION: The study demonstrated that visual explanations could improve performance, trust, and preference in human-automation interaction without confidence visualization partially by changing the search strategies. However, excessive information might cause adverse effects.APPLICATION: These findings provide guidance for the design of transparent automation, emphasizing the importance of context-appropriate and user-centered explanations to foster effective human-machine collaboration.
DA  - 2024 12 (Epub 2024 Mar 04)
PY  - 2024
VL  - 66
IS  - 12
SP  - 2606
EP  - 2620
DO  - 10.1177/00187208241234810
AN  - MEDLINE:38437598
ER  -

TY  - JOUR
AU  - Marmolejo-Ramos, Fernando
AU  - Marrone, Rebecca
AU  - Korolkiewicz, Malgorzata
AU  - Gabriel, Florence
AU  - Siemens, George
AU  - Joksimovic, Srecko
AU  - Yamada, Yuki
AU  - Mori, Yuki
AU  - Rahwan, Talal
AU  - Sahakyan, Maria
AU  - Sonna, Belona
AU  - Meirmanov, Assylbek
AU  - Bolatov, Aidos
AU  - Som, Bidisha
AU  - Ndukaihe, Izuchukwu
AU  - Arinze, Nwadiogo C
AU  - Kundrat, Josef
AU  - Skanderova, Lenka
AU  - Ngo, Van-Giang
AU  - Nguyen, Giang
AU  - Lacia, Michelle
AU  - Kung, Chun-Chia
AU  - Irmayanti, Meiselina
AU  - Muktadir, Abdul
AU  - Samosir, Fransiska Timoria
AU  - Liuzza, Marco Tullio
AU  - Giorgini, Roberto
AU  - Khatin-Zadeh, Omid
AU  - Banaruee, Hassan
AU  - Ozdogru, Asil Ali
AU  - Ariyabuddhiphongs, Kris
AU  - Rakchai, Wachirawit
AU  - Trujillo, Natalia
AU  - Valencia, Stella Maris
AU  - Janyan, Armina
AU  - Kostov, Kiril
AU  - Montoro, Pedro R
AU  - Hinojosa, Jose
AU  - Medeiros, Kelsey
AU  - Hunt, Thomas E
AU  - Posada, Julian
AU  - Freitag, Raquel Meister Ko
AU  - Tejada, Julian
TI  - Factors influencing trust in algorithmic decision-making: an indirect scenario-based experiment.
T2  - Frontiers in artificial intelligence
M3  - Journal Article
AB  - Algorithms are involved in decisions ranging from trivial to significant, but people often express distrust toward them. Research suggests that educational efforts to explain how algorithms work may help mitigate this distrust. In a study of 1,921 participants from 20 countries, we examined differences in algorithmic trust for low-stakes and high-stakes decisions. Our results suggest that statistical literacy is negatively associated with trust in algorithms for high-stakes situations, while it is positively associated with trust in low-stakes scenarios with high algorithm familiarity. However, explainability did not appear to influence trust in algorithms. We conclude that having statistical literacy enables individuals to critically evaluate the decisions made by algorithms, data and AI, and consider them alongside other factors before making significant life decisions. This ensures that individuals are not solely relying on algorithms that may not fully capture the complexity and nuances of human behavior and decision-making. Therefore, policymakers should consider promoting statistical/AI literacy to address some of the complexities associated with trust in algorithms. This work paves the way for further research, including the triangulation of data with direct observations of user interactions with algorithms or physiological measures to assess trust more accurately.
DA  - 2024
PY  - 2024
VL  - 7
SP  - 1465605
EP  - 1465605
DO  - 10.3389/frai.2024.1465605
AN  - MEDLINE:39968162
ER  -

TY  - JOUR
AU  - Yuan, Liuhong
AU  - Zhou, Henghua
AU  - Xiao, Xiao
AU  - Zhang, Xiuqin
AU  - Chen, Feier
AU  - Liu, Lin
AU  - Liu, Jingjia
AU  - Bao, Shisan
AU  - Tao, Kun
TI  - Development and external validation of a transfer learning-based system for the pathological diagnosis of colorectal cancer: a large emulated prospective study.
T2  - Frontiers in oncology
M3  - Journal Article
AB  - Background: The progress in Colorectal cancer (CRC) screening and management has resulted in an unprecedented caseload for histopathological diagnosis. While artificial intelligence (AI) presents a potential solution, the predominant emphasis on slide-level aggregation performance without thorough verification of cancer in each location, impedes both explainability and transparency. Effectively addressing these challenges is crucial to ensuring the reliability and efficacy of AI in histology applications.Method: In this study, we created an innovative AI algorithm using transfer learning from a polyp segmentation model in endoscopy. The algorithm precisely localized CRC targets within 0.25 mm grids from whole slide imaging (WSI). We assessed the CRC detection capabilities at this fine granularity and examined the influence of AI on the diagnostic behavior of pathologists. The evaluation utilized an extensive dataset comprising 858 consecutive patient cases with 1418 WSIs obtained from an external center.Results: Our results underscore a notable sensitivity of 90.25% and specificity of 96.60% at the grid level, accompanied by a commendable area under the curve (AUC) of 0.962. This translates to an impressive 99.39% sensitivity at the slide level, coupled with a negative likelihood ratio of <0.01, signifying the dependability of the AI system to preclude diagnostic considerations. The positive likelihood ratio of 26.54, surpassing 10 at the grid level, underscores the imperative for meticulous scrutiny of any AI-generated highlights. Consequently, all four participating pathologists demonstrated statistically significant diagnostic improvements with AI assistance.Conclusion: Our transfer learning approach has successfully yielded an algorithm that can be validated for CRC histological localizations in whole slide imaging. The outcome advocates for the integration of the AI system into histopathological diagnosis, serving either as a diagnostic exclusion application or a computer-aided detection (CADe) tool. This integration has the potential to alleviate the workload of pathologists and ultimately benefit patients.
DA  - 2024
PY  - 2024
VL  - 14
SP  - 1365364
EP  - 1365364
DO  - 10.3389/fonc.2024.1365364
AN  - MEDLINE:38725622
ER  -

TY  - JOUR
AU  - Galitsky, Boris
AU  - Ilvovsky, Dmitry
AU  - Goldberg, Saveli
TI  - Shaped-Charge Learning Architecture for the Human-Machine Teams.
T2  - Entropy (Basel, Switzerland)
M3  - Journal Article
AB  - In spite of great progress in recent years, deep learning (DNN) and transformers have strong limitations for supporting human-machine teams due to a lack of explainability, information on what exactly was generalized, and machinery to be integrated with various reasoning techniques, and weak defense against possible adversarial attacks of opponent team members. Due to these shortcomings, stand-alone DNNs have limited support for human-machine teams. We propose a Meta-learning/DNN  kNN architecture that overcomes these limitations by integrating deep learning with explainable nearest neighbor learning (kNN) to form the object level, having a deductive reasoning-based meta-level control learning process, and performing validation and correction of predictions in a way that is more interpretable by peer team members. We address our proposal from structural and maximum entropy production perspectives.
DA  - 2023 Jun 12
PY  - 2023
VL  - 25
IS  - 6
DO  - 10.3390/e25060924
AN  - MEDLINE:37372268
ER  -

TY  - JOUR
AU  - Ott, Tabea
AU  - Dabrock, Peter
TI  - Transparent human - (non-) transparent technology? The Janus-faced call for transparency in AI-based health care technologies.
T2  - Frontiers in genetics
M3  - Journal Article
AB  - The use of Artificial Intelligence and Big Data in health care opens up new opportunities for the measurement of the human. Their application aims not only at gathering more and better data points but also at doing it less invasive. With this change in health care towards its extension to almost all areas of life and its increasing invisibility and opacity, new questions of transparency arise. While the complex human-machine interactions involved in deploying and using AI tend to become non-transparent, the use of these technologies makes the patient seemingly transparent. Papers on the ethical implementation of AI plead for transparency but neglect the factor of the "transparent patient" as intertwined with AI. Transparency in this regard appears to be Janus-faced: The precondition for receiving help - e.g., treatment advice regarding the own health - is to become transparent for the digitized health care system. That is, for instance, to donate data and become visible to the AI and its operators. The paper reflects on this entanglement of transparent patients and (non-) transparent technology. It argues that transparency regarding both AI and humans is not an ethical principle per se but an infraethical concept. Further, it is no sufficient basis for avoiding harm and human dignity violations. Rather, transparency must be enriched by intelligibility following Judith Butler's use of the term. Intelligibility is understood as an epistemological presupposition for recognition and the ensuing humane treatment. Finally, the paper highlights ways to testify intelligibility in dealing with AI in health care ex ante, ex post, and continuously.
DA  - 2022
PY  - 2022
VL  - 13
SP  - 902960
EP  - 902960
DO  - 10.3389/fgene.2022.902960
AN  - MEDLINE:36072654
ER  -

TY  - JOUR
AU  - Sobhi, Navid
AU  - Sadeghi-Bazargani, Yasin
AU  - Mirzaei, Majid
AU  - Abdollahi, Mirsaeed
AU  - Jafarizadeh, Ali
AU  - Pedrammehr, Siamak
AU  - Alizadehsani, Roohallah
AU  - Tan, Ru-San
AU  - Islam, Sheikh Mohammed Shariful
AU  - Acharya, U Rajendra
TI  - Artificial intelligence for early detection of diabetes mellitus complications via retinal imaging.
T2  - Journal of diabetes and metabolic disorders
M3  - Journal Article
M3  - Review
AB  - Background: Diabetes mellitus (DM) increases the risk of vascular complications, and retinal vasculature imaging serves as a valuable indicator of both microvascular and macrovascular health. Moreover, artificial intelligence (AI)-enabled systems developed for high-throughput detection of diabetic retinopathy (DR) using digitized retinal images have become clinically adopted. This study reviews AI applications using retinal images for DM-related complications, highlighting advancements beyond DR screening, diagnosis, and prognosis, and addresses implementation challenges, such as ethics, data privacy, equitable access, and explainability.Methods: We conducted a thorough literature search across several databases, including PubMed, Scopus, and Web of Science, focusing on studies involving diabetes, the retina, and artificial intelligence. We reviewed the original research based on their methodology, AI algorithms, data processing techniques, and validation procedures to ensure a detailed analysis of AI applications in diabetic retinal imaging.Results: Retinal images can be used to diagnose DM complications including DR, neuropathy, nephropathy, and atherosclerotic cardiovascular disease, as well as to predict the risk of cardiovascular events. Beyond DR screening, AI integration also offers significant potential to address the challenges in the comprehensive care of patients with DM.Conclusion: With the ability to evaluate the patient's health status in relation to DM complications as well as risk prognostication of future cardiovascular complications, AI-assisted retinal image analysis has the potential to become a central tool for modern personalized medicine in patients with DM.
DA  - 2025 Jun
PY  - 2025
VL  - 24
IS  - 1
SP  - 104
EP  - 104
DO  - 10.1007/s40200-025-01596-7
AN  - MEDLINE:40224528
ER  -

TY  - JOUR
AU  - Akhtar, Masheera
AU  - Nehal, Nida
AU  - Gull, Azka
AU  - Parveen, Rabea
AU  - Khan, Sana
AU  - Khan, Saba
AU  - Ali, Javed
TI  - Explicating the transformative role of artificial intelligence in designing targeted nanomedicine.
T2  - Expert opinion on drug delivery
M3  - Journal Article
M3  - Review
AB  - INTRODUCTION: Artificial intelligence (AI) has emerged as a transformative force in nanomedicine, revolutionizing drug delivery, diagnostics, and personalized treatment. While nanomedicine offers precise targeted drug delivery and reduced toxic effects, its clinical translation is hindered by biological complexity, unpredictable in vivo behavior, and inefficient trial-and-error approaches.AREAS COVERED: This review covers the application of AI and Machine Learning (ML) across the nanomedicine development pipeline, starting from drug and target identification to nanoparticle design, toxicity prediction, and personalized dosing. Different AI/ML models like QSAR, MTK-QSBER, and Alchemite, along with data sources and high-throughput screening methods, have been explored. Real-world applications are critically discussed, including AI-assisted drug repurposing, controlled-release formulations, and cancer-specific delivery systems.EXPERT OPINION: AI has emerged as an essential component in designing next-generation nanomedicine. Efficiently handling multidimensional datasets, optimizing formulations, and personalizing treatment regimens, it has sped up the innovation process. However, challenges like data heterogeneity, model transparency, and regulatory gaps remain. Addressing these hurdles through interdisciplinary efforts and emerging innovations like explainable AI and federated learning will pave the way for the clinical translation of AI-driven nanomedicine.
DA  - 2025 May 21 (Epub 2025 May 21)
PY  - 2025
SP  - 1
EP  - 21
DO  - 10.1080/17425247.2025.2502022
AN  - MEDLINE:40321117
ER  -

TY  - JOUR
AU  - Cho, Sung-Hyun
AU  - Kim, Yang-Soo
TI  - An overview of artificial intelligence and machine learning in shoulder surgery.
T2  - Clinics in shoulder and elbow
M3  - Journal Article
AB  - Machine learning (ML), a subset of artificial intelligence (AI), utilizes advanced algorithms to learn patterns from data, enabling accurate predictions and decision-making without explicit programming. In orthopedic surgery, ML is transforming clinical practice, particularly in shoulder arthroplasty and rotator cuff tears (RCTs) management. This review explores the fundamental paradigms of ML, including supervised, unsupervised, and reinforcement learning, alongside key algorithms such as XGBoost, neural networks, and generative adversarial networks. In shoulder arthroplasty, ML accurately predicts postoperative outcomes, complications, and implant selection, facilitating personalized surgical planning and cost optimization. Predictive models, including ensemble learning methods, achieve over 90% accuracy in forecasting complications, while neural networks enhance surgical precision through AI-assisted navigation. In RCTs treatment, ML enhances diagnostic accuracy using deep learning models on magnetic resonance imaging and ultrasound, achieving area under the curve values exceeding 0.90. ML models also predict tear reparability with 85% accuracy and postoperative functional outcomes, including range of motion and patient-reported outcomes. Despite remarkable advancements, challenges such as data variability, model interpretability, and integration into clinical workflows persist. Future directions involve federated learning for robust model generalization and explainable AI to enhance transparency. ML continues to revolutionize orthopedic care by providing data-driven, personalized treatment strategies and optimizing surgical outcomes.
DA  - 2025 May 19 (Epub 2025 May 19)
PY  - 2025
DO  - 10.5397/cise.2025.00185
AN  - MEDLINE:40405638
ER  -

TY  - JOUR
AU  - Han, Jihyung
AU  - Ko, Daekyun
TI  - Consumer Autonomy in Generative AI Services: The Role of Task Difficulty and AI Design Elements in Enhancing Trust, Satisfaction, and Usage Intention.
T2  - Behavioral sciences (Basel, Switzerland)
M3  - Journal Article
AB  - As generative AI services become increasingly integrated into consumer decision making, concerns have grown regarding their influence on consumer autonomy-the extent to which individuals retain independent control over AI-assisted decisions. Although these services offer efficiency and convenience, they can simultaneously constrain consumer decision making, potentially impacting trust, satisfaction, and usage intention. This study investigates the role of perceived consumer autonomy in shaping consumer responses, specifically examining how task difficulty (Study 1) and AI service design elements-explainability, feedback, and shared responsibility (Study 2)-influence autonomy perceptions and subsequent consumer outcomes. Using two scenario-based experiments involving a total of 708 participants, the results reveal that perceived autonomy significantly enhances consumer trust, particularly in contexts involving high task difficulty. Among the tested AI design interventions, shared responsibility emerged as most effective in enhancing perceived autonomy, trust, satisfaction, and long-term engagement, whereas explainability and feedback alone showed limited impact. These findings underscore the importance of designing AI services that actively support consumer agency through user-involved decision-making frameworks rather than relying solely on passive informational transparency. Theoretical implications for consumer autonomy in AI interactions are discussed, along with practical recommendations for designing consumer-centered AI services.
DA  - 2025 Apr 15
PY  - 2025
VL  - 15
IS  - 4
DO  - 10.3390/bs15040534
AN  - MEDLINE:40282155
ER  -

TY  - JOUR
AU  - Chevalier, Olivia
AU  - Dubey, Gerard
AU  - Benkabbou, Amine
AU  - Majbar, Mohammed Anass
AU  - Souadka, Amine
TI  - Comprehensive overview of artificial intelligence in surgery: a systematic review and perspectives.
T2  - Pflugers Archiv : European journal of physiology
M3  - Journal Article
M3  - Systematic Review
M3  - Review
AB  - The rapid integration of artificial intelligence (AI) into surgical practice necessitates a comprehensive evaluation of its applications, challenges, and physiological impact. This systematic review synthesizes current AI applications in surgery, with a particular focus on machine learning (ML) and its role in optimizing preoperative planning, intraoperative decision-making, and postoperative patient management. Using PRISMA guidelines and PICO criteria, we analyzed key studies addressing AI's contributions to surgical precision, outcome prediction, and real-time physiological monitoring. While AI has demonstrated significant promise-from enhancing diagnostics to improving intraoperative safety-many surgeons remain skeptical due to concerns over algorithmic unpredictability, surgeon autonomy, and ethical transparency. This review explores AI's physiological integration into surgery, discussing its role in real-time hemodynamic assessments, AI-guided tissue characterization, and intraoperative physiological modeling. Ethical concerns, including algorithmic opacity and liability in high-stakes scenarios, are critically examined alongside AI's potential to augment surgical expertise. We conclude that longitudinal validation, improved AI explainability, and adaptive regulatory frameworks are essential to ensure safe, effective, and ethically sound integration of AI into surgical decision-making. Future research should focus on bridging AI-driven analytics with real-time physiological feedback to refine precision surgery and patient safety strategies.
DA  - 2025 Apr (Epub 2025 Mar 15)
PY  - 2025
VL  - 477
IS  - 4
SP  - 617
EP  - 626
DO  - 10.1007/s00424-025-03076-6
AN  - MEDLINE:40087157
ER  -

TY  - JOUR
AU  - Adako, Oyeyemi Patricia
AU  - Adeusi, Oluwafemi Clement
AU  - Alaba, Peter Adeniyi
TI  - Enhancing education for children with ASD: a review of evaluation and measurement in AI tool implementation.
T2  - Disability and rehabilitation. Assistive technology
M3  - Journal Article
M3  - Review
AB  - This article addresses the gaps in current research regarding the use of artificial intelligence (AI) tools to educate children with autism spectrum disorder (ASD). The proposed metrics are specifically designed to evaluate the progress of learning in AI-assisted education, considering the unique needs of this demographic. The review highlights the potential of long-term impact studies to determine the lasting effects of AI on social skills, emotional development, and overall academic achievement. The ethical considerations surrounding AI intervention in autistic education are thoroughly examined. By combining diverse methodologies utilized in existing studies, a comprehensive analysis of the challenges is presented, along with interdisciplinary approaches for improvement that can serve as a roadmap for future research. The manuscript provides innovative perspectives, bridges existing gaps, and advocates for the ethical and effective integration of AI tools in educating children with ASD.
AB  - Metrics: In the context of AI-assisted education, metrics refer to the quantitative and qualitative measures used to evaluate the effectiveness of AI tools in supporting learning. These may include student engagement levels, progress in skill development, accuracy in task completion, and behavioral improvements.Traditional Assessment Methods: Traditional assessment methods refer to conventional ways of evaluating learning outcomes, such as standardized tests, teacher observations, and structured evaluations. These methods typically focus on academic performance but may not always capture the unique learning patterns of children with autism spectrum disorder (ASD).AI Interventions: AI interventions in autism education involve the application of artificial intelligence technologiessuch as machine learning algorithms, speech recognition, and adaptive learning platformsto personalize learning experiences, provide real-time feedback, and enhance the educational process for children with ASD.AI-Supported Progress Assessment: AI-supported progress assessment refers to the use of AI-driven analytics to track and measure learning advancements over time. These assessments often include dynamic data analysis, pattern recognition, and adaptive modifications to teaching strategies based on individual learning needs.Standardized Assessment Frameworks: Standardized assessment frameworks are structured models used to evaluate educational progress consistently across different learners. In AI-assisted autism education, these frameworks help ensure that AI tools align with established educational benchmarks while allowing for personalized learning adaptations.Explainability and Accountability in AI: Explainability refers to the ability of an AI system to provide clear, understandable reasons for its decisions or recommendations. Accountability ensures that AI developers, educators, and stakeholders take responsibility for the outcomes of AI-driven educational interventions, ensuring fairness, reliability, and ethical use.Privacy and Data Security: Privacy and data security involve the ethical handling, storage, and protection of sensitive personal and behavioral data collected by AI systems. Given the vulnerable nature of children with ASD, AI interventions must comply with strict data protection regulations to safeguard student information.Ethical AI Implementation: Ethical AI implementation refers to the responsible design, deployment, and use of AI technologies in autism education. This includes considerations such as bias prevention, equitable access, human oversight, and ensuring that AI complements rather than replaces human educators.Interdisciplinary Collaboration: Interdisciplinary collaboration in AI-assisted education involves experts from various fieldssuch as education, psychology, computer science, and behavioral therapyworking together to develop, assess, and refine AI tools that best support the needs of children with ASD.Inclusive Research Practices: Inclusive research practices ensure that AI-driven educational tools and methodologies account for the diverse needs of children with autism, incorporating perspectives from students, educators, parents, and autism advocates to minimize bias and promote equitable learning opportunities.
DA  - 2025 Mar 13 (Epub 2025 Mar 13)
PY  - 2025
SP  - 1
EP  - 18
DO  - 10.1080/17483107.2025.2477678
AN  - MEDLINE:40079459
ER  -

TY  - JOUR
AU  - Alami, Jawad
AU  - El Iskandarani, Mohamad
AU  - Riggs, Sara Lu
TI  - The Effect of Workload and Task Priority on Multitasking Performance and Reliance on Level 1 Explainable AI (XAI) Use.
T2  - Human factors
M3  - Journal Article
AB  - ObjectiveThis study investigates the effects of workload and task priority on multitasking performance and reliance on Level 1 Explainable Artificial Intelligence (XAI) systems in high-stakes decision environments.BackgroundOperators in critical settings manage multiple tasks under varying levels of workload and priority, potentially leading to performance degradation. XAI offers opportunities to support decision making by providing insights into AI's reasoning, yet its adoption and effectiveness in multitasking scenarios remain underexplored.MethodThirty participants engaged in a simulated multitasking environment, involving UAV command and control tasks, with the assistance of a Level 1 (i.e., basic perceptual information) XAI system on one of the tasks. The study utilized a within-subjects experimental design, manipulating workload (low, medium, and high) and AI-supported-task priority (low and high) across six conditions. Participants' accuracy, use of automatic rerouting, AI miss detection, false alert identification, and use of AI explanations were measured and analyzed across the different experimental conditions.ResultsWorkload significantly hindered performance on the AI-assisted task and increased reliance on the AI system especially when the AI-assisted task was given low priority. The use of AI explanations was significantly affected by task priority only.ConclusionAn increase in workload led to proper offloading by relying on the AI's alerts, but it also led to a lower rate of alert verification despite the alert feature's high false alert rate.ApplicationThe findings from the present work help inform AI system designers on how to design their systems for high-stakes environments such that reliance on AI is properly calibrated.
DA  - 2025 Mar 12 (Epub 2025 Mar 12)
PY  - 2025
SP  - 187208251323478
EP  - 187208251323478
DO  - 10.1177/00187208251323478
AN  - MEDLINE:40071729
ER  -

TY  - JOUR
AU  - Giavina Bianchi, Mara
AU  - D'adario, Andrew
AU  - Giavina Bianchi, Pedro
AU  - Machado, Birajara Soares
A1  - IChat Group
TI  - Three versions of an atopic dermatitis case report written by humans, artificial intelligence, or both: Identification of authorship and preferences.
T2  - The journal of allergy and clinical immunology. Global
M3  - Journal Article
AB  - Background: The use of artificial intelligence (AI) in scientific writing is rapidly increasing, raising concerns about authorship identification, content quality, and writing efficiency.Objectives: This study investigates the real-world impact of ChatGPT, a large language model, on those aspects in a simulated publication scenario.Methods: Forty-eight individuals representing 3 medical expertise levels (medical students, residents, and experts in allergy or dermatology) evaluated 3 blinded versions of an atopic dermatitis case report: one each human written (HUM), AI generated (AI), and combined written (COM). The survey assessed authorship, ranked their preference, and graded 13 quality criteria for each text. Time taken to generate each manuscript was also recorded.Results: Authorship identification accuracy mirrored the odds at 33%. Expert participants (50.9%) demonstrated significantly higher accuracy compared to residents (27.7%) and students (19.6%, P< .001). Participants favored AI-assisted versions (AI and COM) over HUM (P< .001), with COM receiving the highest quality scores. COM and AI achieved 83.8% and 84.3% reduction in writing time, respectively, compared to HUM, while showing 13.9% (P< .001) and 11.1% improvement in quality (P< .001), respectively. However, experts assigned the lowest score for the references of the AI manuscript, potentially hindering its publication.Conclusion: AI can deceptively mimic human writing, particularly for less experienced readers. Although AI-assisted writing is appealing and offers significant time savings, human oversight remains crucial to ensure accuracy, ethical considerations, and optimal quality. These findings underscore the need for transparency in AI use and highlight the potential of human-AI collaboration in the future of scientific writing.
DA  - 2025 Feb
PY  - 2025
VL  - 4
IS  - 1
SP  - 100373
EP  - 100373
DO  - 10.1016/j.jacig.2024.100373
AN  - MEDLINE:39759954
ER  -

TY  - JOUR
AU  - Hua, David
AU  - Petrina, Neysa
AU  - Sacks, Alan J
AU  - Young, Noel
AU  - Cho, Jin-Gun
AU  - Smith, Ross
AU  - Poon, Simon K
TI  - Towards human-AI collaboration in radiology: a multidimensional evaluation of the acceptability of AI for chest radiograph analysis in supporting pulmonary tuberculosis diagnosis.
T2  - JAMIA open
M3  - Journal Article
AB  - Objective: Artificial intelligence (AI) technology promises to be a powerful tool in addressing the global health challenges posed by tuberculosis (TB). However, evidence for its real-world impact is lacking, which may hinder safe, responsible adoption. This case study addresses this gap by assessing the technical performance, usability and workflow aspects, and health impact of implementing a commercial AI system (qXR by Qure.ai) to support Australian radiologists in diagnosing pulmonary TB.Materials and Methods: A retrospective diagnostic accuracy evaluation was conducted to establish the technical performance of qXR in detecting TB compared to a human radiologist and microbiological reference standard. A qualitative human factors assessment was performed to investigate the user experience and clinical decision-making process of radiologists using qXR. A task productivity analysis was completed to quantify how the radiological screening turnaround time is impacted.Results: qXR displays near-human performance satisfying the World Health Organization's suggested accuracy profile. Radiologists reported high satisfaction with using qXR based on minimal workflow disruptions, respect for their professional autonomy, and limited increases in workload burden despite poor algorithm explainability. qXR delivers considerable productivity gains for normal cases and optimizes resource allocation through redistributing time from normal to abnormal cases.Discussion and Conclusion: This study provides preliminary evidence of how an AI system with reasonable diagnostic accuracy and a human-centered user experience can meaningfully augment the TB diagnostic workflow. Future research needs to investigate the impact of AI on clinician accuracy, its relationship with efficiency, and best practices for optimizing the impact of clinician-AI collaboration.
DA  - 2025 Feb
PY  - 2025
VL  - 8
IS  - 1
SP  - ooae151
EP  - ooae151
DO  - 10.1093/jamiaopen/ooae151
AN  - MEDLINE:39911582
ER  -

TY  - JOUR
AU  - A Hasib, Uddin
AU  - Md Abu, Raihan
AU  - Yang, Jing
AU  - Bhatti, Uzair Aslam
AU  - Ku, Chin Soon
AU  - Por, Lip Yee
TI  - YOLOv8 framework for COVID-19 and pneumonia detection using synthetic image augmentation.
T2  - Digital health
M3  - Journal Article
AB  - Objective: Early and accurate detection of COVID-19 and pneumonia through medical imaging is critical for effective patient management. This study aims to develop a robust framework that integrates synthetic image augmentation with advanced deep learning (DL) models to address dataset imbalance, improve diagnostic accuracy, and enhance trust in artificial intelligence (AI)-driven diagnoses through Explainable AI (XAI) techniques.Methods: The proposed framework benchmarks state-of-the-art models (InceptionV3, DenseNet, ResNet) for initial performance evaluation. Synthetic images are generated using Feature Interpolation through Linear Mapping and principal component analysis to enrich dataset diversity and balance class distribution. YOLOv8 and InceptionV3 models, fine-tuned via transfer learning, are trained on the augmented dataset. Grad-CAM is used for model explainability, while large language models (LLMs) support visualization analysis to enhance interpretability.Results: YOLOv8 achieved superior performance with 97% accuracy, precision, recall, and F1-score, outperforming benchmark models. Synthetic data generation effectively reduced class imbalance and improved recall for underrepresented classes. Comparative analysis demonstrated significant advancements over existing methodologies. XAI visualizations (Grad-CAM heatmaps) highlighted anatomically plausible focus areas aligned with clinical markers of COVID-19 and pneumonia, thereby validating the model's decision-making process.Conclusion: The integration of synthetic data generation, advanced DL, and XAI significantly enhances the detection of COVID-19 and pneumonia while fostering trust in AI systems. YOLOv8's high accuracy, coupled with interpretable Grad-CAM visualizations and LLM-driven analysis, promotes transparency crucial for clinical adoption. Future research will focus on developing a clinically viable, human-in-the-loop diagnostic workflow, further optimizing performance through the integration of transformer-based language models to improve interpretability and decision-making.
DA  - 2025 Jan-Dec
PY  - 2025
VL  - 11
SP  - 20552076251341092
EP  - 20552076251341092
DO  - 10.1177/20552076251341092
AN  - MEDLINE:40376574
ER  -

TY  - JOUR
AU  - El Arab, Rabie Adel
AU  - Almoosa, Zainab
AU  - Alkhunaizi, May
AU  - Abuadas, Fuad H
AU  - Somerville, Joel
TI  - Artificial intelligence in hospital infection prevention: an integrative review.
T2  - Frontiers in public health
M3  - Journal Article
M3  - Review
AB  - Background: Hospital-acquired infections (HAIs) represent a persistent challenge in healthcare, contributing to substantial morbidity, mortality, and economic burden. Artificial intelligence (AI) offers promising potential for improving HAIs prevention through advanced predictive capabilities.Objective: To evaluate the effectiveness, usability, and challenges of AI models in preventing, detecting, and managing HAIs.Methods: This integrative review synthesized findings from 42 studies, guided by the SPIDER framework for inclusion criteria. We assessed the quality of included studies by applying the TRIPOD checklist to individual predictive studies and the AMSTAR 2 tool for reviews.Results: AI models demonstrated high predictive accuracy for the detection, surveillance, and prevention of multiple HAIs, with models for surgical site infections and urinary tract infections frequently achieving area-under-the-curve (AUC) scores exceeding 0.80, indicating strong reliability. Comparative data suggest that while both machine learning and deep learning approaches perform well, some deep learning models may offer slight advantages in complex data environments. Advanced algorithms, including neural networks, decision trees, and random forests, significantly improved detection rates when integrated with EHRs, enabling real-time surveillance and timely interventions. In resource-constrained settings, non-real-time AI models utilizing historical EHR data showed considerable scalability, facilitating broader implementation in infection surveillance and control. AI-supported surveillance systems outperformed traditional methods in accurately identifying infection rates and enhancing compliance with hand hygiene protocols. Furthermore, Explainable AI (XAI) frameworks and interpretability tools such as Shapley additive explanations (SHAP) values increased clinician trust and facilitated actionable insights. AI also played a pivotal role in antimicrobial stewardship by predicting the emergence of multidrug-resistant organisms and guiding optimal antibiotic usage, thereby reducing reliance on second-line treatments. However, challenges including the need for comprehensive clinician training, high integration costs, and ensuring compatibility with existing workflows were identified as barriers to widespread adoption.Discussion: The integration of AI in HAI prevention and management represents a potentially transformative shift in enhancing predictive capabilities and supporting effective infection control measures. Successful implementation necessitates standardized validation protocols, transparent data reporting, and the development of user-friendly interfaces to ensure seamless adoption by healthcare professionals. Variability in data sources and model validations across studies underscores the necessity for multicenter collaborations and external validations to ensure consistent performance across diverse healthcare environments. Innovations in non-real-time AI frameworks offer viable solutions for scaling AI applications in low- and middle-income countries (LMICs), addressing the higher prevalence of HAIs in these regions.Conclusions: Artificial Intelligence stands as a transformative tool in the fight against hospital-acquired infections, offering advanced solutions for prevention, surveillance, and management. To fully realize its potential, the healthcare sector must prioritize rigorous validation standards, comprehensive data quality reporting, and the incorporation of interpretability tools to build clinician confidence. By adopting scalable AI models and fostering interdisciplinary collaborations, healthcare systems can overcome existing barriers, integrating AI seamlessly into infection control policies and ultimately enhancing patient safety and care quality. Further research is needed to evaluate cost-effectiveness, real-world applications, and strategies (e.g., clinician training and the integration of explainable AI) to improve trust and broaden clinical adoption.
DA  - 2025
PY  - 2025
VL  - 13
SP  - 1547450
EP  - 1547450
DO  - 10.3389/fpubh.2025.1547450
AN  - MEDLINE:40241963
ER  -

TY  - JOUR
AU  - Gaffney, Harry
AU  - Mirza, Kamran M
TI  - Pathology in the artificial intelligenceera: Guiding innovation and implementation to preserve human insight.
T2  - Academic pathology
M3  - Journal Article
AB  - The integration of artificial intelligence in pathology has ignited discussions about the role of technology in diagnostics-whether artificial intelligence serves as a tool for augmentation or risks replacing human expertise. This manuscript explores artificial intelligence's evolving contributions to pathology, emphasizing its potential capacity to enhance, rather than eclipse, the pathologist's role. Through historical comparisons, such as the transition from analog to digital in radiology, this paper highlights how technological advancements have historically expanded professional capabilities without diminishing the essential human element. Current applications of artificial intelligence in pathology-from diagnostic standardization to workflow efficiency-demonstrate its potential to augment diagnostic accuracy, expedite processes, and improve consistency across institutions. However, challenges remain in algorithmic bias, regulatory oversight, and maintaining interpretive skills among pathologists. The discussion underscores the importance of comprehensive governance frameworks, evolving educational curricula, and public engagement initiatives to ensure artificial intelligence in pathology remains a collaborative endeavor that empowers professionals, upholds ethical standards, and enhances patient outcomes. This manuscript ultimately advocates for a balanced approach where artificial intelligence and human expertise work in concert to advance the future of diagnostic medicine.
DA  - 2025 Jan-Mar
PY  - 2025
VL  - 12
IS  - 1
SP  - 100166
EP  - 100166
DO  - 10.1016/j.acpath.2025.100166
AN  - MEDLINE:40104157
ER  -

TY  - JOUR
AU  - Han Wang, Mini
AU  - Cui, Jiazheng
AU  - Lee, Simon Ming-Yuen
AU  - Lin, Zhiyuan
AU  - Zeng, Peijin
AU  - Li, Xinyue
AU  - Liu, Haoyang
AU  - Liu, Yunxiao
AU  - Xu, Yang
AU  - Wang, Yapeng
AU  - Alves, Jose Lopes Camilo Da Costa
AU  - Hou, Guanghui
AU  - Fang, Junbin
AU  - Yu, Xiangrong
AU  - Chong, Kelvin Kam-Lung
AU  - Pan, Yi
TI  - Applied machine learning in intelligent systems: knowledge graph-enhanced ophthalmic contrastive learning with "clinical profile" prompts.
T2  - Frontiers in artificial intelligence
M3  - Journal Article
AB  - Introduction: The integration of artificial intelligence (AI) into ophthalmic diagnostics has the potential to significantly enhance diagnostic accuracy and interpretability, thereby supporting clinical decision-making. However, a major challenge in AI-driven medical applications is the lack of transparency, which limits clinicians' trust in automated recommendations. This study investigates the application of machine learning techniques by integrating knowledge graphs with contrastive learning and utilizing "clinical profile" prompts to refine the performance of the ophthalmology-specific large language model, MeEYE, which is built on the CHATGLM3-6B architecture. This approach aims to improve the model's ability to capture clinically relevant features while enhancing both the accuracy and explainability of diagnostic predictions.Methods: This study employs a novel methodological framework that incorporates domain-specific knowledge through knowledge graphs and enhances feature representation using contrastive learning. The MeEYE model is fine-tuned with structured clinical knowledge, enabling it to better distinguish subtle yet significant ophthalmic features. Additionally, "clinical profile" prompts are incorporated to further improve contextual understanding and diagnostic precision. The proposed method is evaluated through comprehensive performance benchmarking, including quantitative assessments and clinical case studies, to ensure its efficacy in real-world ophthalmic diagnosis.Results: The experimental findings demonstrate that integrating knowledge graphs and contrastive learning into the MeEYE model significantly improves both diagnostic accuracy and model interpretability. Comparative analyses against baseline models reveal that the proposed approach enhances the identification of ophthalmic conditions with higher precision and clarity. Furthermore, the model's ability to generate transparent and clinically relevant AI recommendations is substantiated through rigorous evaluation, highlighting its potential for real-world clinical implementation.Discussion: The results underscore the importance of explainable AI in medical diagnostics, particularly in ophthalmology, where model transparency is critical for clinical acceptance and utility. By incorporating domain-specific knowledge with advanced machine learning techniques, the proposed approach not only enhances model performance but also ensures that AI-generated insights are interpretable and reliable for clinical decision-making. These findings suggest that integrating structured medical knowledge with machine learning frameworks can address key challenges in AI-driven diagnostics, ultimately contributing to improved patient outcomes. Future research should explore the adaptability of this approach across various medical domains to further advance AI-assisted diagnostic systems.
DA  - 2025
PY  - 2025
VL  - 8
SP  - 1527010
EP  - 1527010
DO  - 10.3389/frai.2025.1527010
AN  - MEDLINE:40166361
ER  -

TY  - JOUR
AU  - Stodt, Jan
AU  - Reich, Christoph
AU  - Knahl, Martin
TI  - Demystifying XAI: Requirements for Understandable XAI Explanations.
T2  - Studies in health technology and informatics
M3  - Journal Article
AB  - This paper establishes requirements for assessing the usability of Explainable Artificial Intelligence (XAI) methods, focusing on non-AI experts like healthcare professionals. Through a synthesis of literature and empirical findings, it emphasizes achieving optimal cognitive load, task performance, and task time in XAI explanations. Key components include tailoring explanations to user expertise, integrating domain knowledge, and using non-propositional representations for comprehension. The paper highlights the critical role of relevance, accuracy, and truthfulness in fostering user trust. Practical guidelines are provided for designing transparent and user-friendly XAI explanations, especially in high-stakes contexts like healthcare. Overall, the paper's primary contribution lies in delineating clear requirements for effective XAI explanations, facilitating human-AI collaboration across diverse domains.
DA  - 2024 Aug 22
PY  - 2024
VL  - 316
SP  - 565
EP  - 569
DO  - 10.3233/SHTI240477
AN  - MEDLINE:39176805
ER  -

TY  - JOUR
AU  - Holm, Benedikt
AU  - Jouan, Gabriel
AU  - Hardarson, Emil
AU  - Sigurdardottir, Sigridur
AU  - Hoelke, Kenan
AU  - Murphy, Conor
AU  - Arnardottir, Erna Sif
AU  - Oskarsdottir, Maria
AU  - Islind, Anna Sigridur
TI  - An optimized framework for processing multicentric polysomnographic data incorporating expert human oversight.
T2  - Frontiers in neuroinformatics
M3  - Journal Article
AB  - Introduction: Polysomnographic recordings are essential for diagnosing many sleep disorders, yet their detailed analysis presents considerable challenges. With the rise of machine learning methodologies, researchers have created various algorithms to automatically score and extract clinically relevant features from polysomnography, but less research has been devoted to how exactly the algorithms should be incorporated into the workflow of sleep technologists. This paper presents a sophisticated data collection platform developed under the Sleep Revolution project, to harness polysomnographic data from multiple European centers.Methods: A tripartite platform is presented: a user-friendly web platform for uploading three-night polysomnographic recordings, a dedicated splitter that segments these into individual one-night recordings, and an advanced processor that enhances the one-night polysomnography with contemporary automatic scoring algorithms. The platform is evaluated using real-life data and human scorers, whereby scoring time, accuracy, and trust are quantified. Additionally, the scorers were interviewed about their trust in the platform, along with the impact of its integration into their workflow.Results: We found that incorporating AI into the workflow of sleep technologists both decreased the time to score by up to 65 min and increased the agreement between technologists by as much as 0.17 kappa.Discussion: We conclude that while the inclusion of AI into the workflow of sleep technologists can have a positive impact in terms of speed and agreement, there is a need for trust in the algorithms.
DA  - 2024
PY  - 2024
VL  - 18
SP  - 1379932
EP  - 1379932
DO  - 10.3389/fninf.2024.1379932
AN  - MEDLINE:38803523
ER  -

TY  - JOUR
AU  - Galitsky, Boris
TI  - Applications of Shaped-Charge Learning.
T2  - Entropy (Basel, Switzerland)
M3  - Journal Article
AB  - It is well known that deep learning (DNN) has strong limitations due to a lack of explainability and weak defense against possible adversarial attacks. These attacks would be a concern for autonomous teams producing a state of high entropy for the team's structure. In our first article for this Special Issue, we propose a meta-learning/DNN  kNN architecture that overcomes these limitations by integrating deep learning with explainable nearest neighbor learning (kNN). This architecture is named "shaped charge". The focus of the current article is the empirical validation of "shaped charge". We evaluate the proposed architecture for summarization, question answering, and content creation tasks and observe a significant improvement in performance along with enhanced usability by team members. We observe a substantial improvement in question answering accuracy and also the truthfulness of the generated content due to the application of the shaped-charge learning approach.
DA  - 2023 Oct 30
PY  - 2023
VL  - 25
IS  - 11
DO  - 10.3390/e25111496
AN  - MEDLINE:37998188
ER  -

TY  - JOUR
AU  - Lage, Isaac
AU  - Lifschitz, Daphna
AU  - Doshi-Velez, Finale
AU  - Amir, Ofra
TI  - Toward Robust policy Summarization: Extended Abstract.
T2  - Autonomous agents and multi-agent systems
M3  - Journal Article
AB  - AI agents are being developed to help people with high stakes decision-making processes from driving cars to prescribing drugs. It is therefore becoming increasingly important to develop "explainable AI" methods that help people understand the behavior of such agents. Summaries of agent policies can help human users anticipate agent behavior and facilitate more effective collaboration. Prior work has framed agent summarization as a machine teaching problem where examples of agent behavior are chosen to maximize reconstruction quality under the assumption that people do inverse reinforcement learning to infer an agent's policy from demonstrations. We compare summaries generated under this assumption to summaries generated under the assumption that people use imitation learning. We show through simulations that in some domains, there exist summaries that produce high-quality reconstructions under different models, but in other domains, only matching the summary extraction model to the reconstruction model produces high-quality reconstructions. These results highlight the importance of assuming correct computational models for how humans extrapolate from a summary, suggesting human-in-the-loop approaches to summary extraction.
DA  - 2019 May
PY  - 2019
VL  - 2019
SP  - 2081
EP  - 2083
AN  - MEDLINE:33628085
ER  -

