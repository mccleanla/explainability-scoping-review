==I'm interested in determining what we understand about closing the gap between what is being explained to users and what they understand==. 
	- In other words, ==minimizing miscommunication and misinterpretation.== 

to do this, I'm interested in the 'lifecycle"  of explanation design decisions in the wild - from conceptualisation of needs, to design trade-offs, to evaluation of outcomes.
- [[Operational Explainability Definition]] - *seed papers in the literature here that distinguishes operational needs from technical explainability, and a systematic review that define explainability/interpretability using safety-critical literature.*

**Scoping Review Question**
- **‚ÄúIn what ways are explanations conceptualised, designed, and evaluated to support users in operational human-AI systems across domains?‚Äù**

## 1. üéØ Target Databases

| **Academic Databases**     | **Justification**                                             |
| -------------------------- | ------------------------------------------------------------- |
| **Scopus**                 | Broad, good for interdisciplinary searches                    |
| **Web of Science**         | Useful for cross-domain coverage                              |
| **IEEE Xplore**            | Technical systems, HCI, robotics, and real-world applications |
| **ACM Digital Library**    | Human-AI interaction, HCI, CSCW, usability                    |
| **PubMed**                 | Healthcare-related human-AI systems and explainability        |
| **ScienceDirect**          | Engineering and system design contexts                        |
| **Medline**                | medical use cases (expect small)                              |
| **ProQuest Dissertations** | Good for grey literature & PhD theses                         |

## 2. Eligibility criteria:

Specify the characteristics of the sources of evidence that will be included (e.g., years considered, language, publication status).

|                      | Inclusion Criteria                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Exclusion Criteria                                                                                                                                                                                                                                                                                                   |
| -------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Population           | People trained to do an activity but without specialist AI knowledge, needing to act or make decisions (collaborating with the AI system).  <br><br>May be beyond safety-critical industry studies if studies show the role of explanation in high stakes or fast-paced decision making.<br>                                                                                                                                                                                                       | Purely algorithmic XAI papers (no human interaction or interface)<br><br>Studies focusing exclusively on developers, data scientists, or model builders (AI experts).<br><br>                                                                                                                                        |
| Concept              | Operational explainability: Conceptualisation, Design & Evaluation                                                                                                                                                                                                                                                                                                                                                                                                                                 | Technical explainability conceptualisation, design and evaluation                                                                                                                                                                                                                                                    |
| Context              | Must involve users making decisions in an operational environment, in a scenario where humans & ai interact/collaborate towards an outcome.<br><br>Studies involving systems where explanations may emerge from interactions among multiple agents, not solely from a single AI component are of interest.<br><br>Include diverse research methodologies (e.g., user studies, simulations, observational studies, etc.), as long as the explanation‚Äôs role in an operational context is addressed. | Studies that evaluate explanation quality without user interaction or action/decision-making consequently.<br><br>Studies involving only data annotation tasks, ai acting without human collaboration/interaction, or study of perception of explanations in isolation of decision making in an operational context. |
| Outcomes of Interest | Research that discusses the conceptualisation, design, or evaluation of explanations within human-AI collaborative action scenarios.<br><br>Papers addressing the timing of explanations and how they are distributed across agents or system components.                                                                                                                                                                                                                                          | Papers that do not evaluate or theorise the role of outcomes from human-ai interaction /communication and/or the purpose/design/evaluation of explanation in that context.                                                                                                                                           |
| Study Types          | Peer-reviewed journal articles, conference papers, technical reports, empirical case studies, relevant reviews.                                                                                                                                                                                                                                                                                                                                                                                    | Opinion pieces or discussion articles, commentaries, letters to the editor, with no evidence or operational context.                                                                                                                                                                                                 |
| Timeframe            | 2016 onwards                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                      |
| Language             | English.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                      |

### üîç Screening Questions

1. **Does the study involve operational human-AI systems?**
2. **Is explanation (needs, types, triggers and mechanisms) a central focus of the research?**
3. **Are operational needs from explanation discussed?**
4. **Does the study consider explanations at a systems level (potentially containing multiple AI components and multiple people interacting)?**


## 2. üîó Search 

The search will find papers discussing how explanations are designed to support human action within human-AI operational contexts. 
[Search Keywords](Search%20Keywords.md)

